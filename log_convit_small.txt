Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Model vit_small_patch16_224 created, param count:22264910
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 310
Train: 0 [   0/1251 (  0%)]  Loss:  7.003093 (7.0031)  Time: 7.891s,  129.77/s  (7.891s,  129.77/s)  LR: 1.000e-06  Data: 4.482 (4.482)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1251 (  4%)]  Loss:  6.976212 (6.9897)  Time: 0.590s, 1734.56/s  (0.751s, 1364.11/s)  LR: 1.000e-06  Data: 0.018 (0.101)
Train: 0 [ 100/1251 (  8%)]  Loss:  6.978266 (6.9859)  Time: 0.590s, 1734.43/s  (0.671s, 1526.47/s)  LR: 1.000e-06  Data: 0.012 (0.058)
Train: 0 [ 150/1251 ( 12%)]  Loss:  6.964083 (6.9804)  Time: 0.585s, 1750.11/s  (0.643s, 1593.01/s)  LR: 1.000e-06  Data: 0.011 (0.044)
Train: 0 [ 200/1251 ( 16%)]  Loss:  6.965479 (6.9774)  Time: 0.585s, 1750.55/s  (0.629s, 1627.60/s)  LR: 1.000e-06  Data: 0.012 (0.036)
Train: 0 [ 250/1251 ( 20%)]  Loss:  6.951593 (6.9731)  Time: 0.588s, 1741.33/s  (0.621s, 1649.41/s)  LR: 1.000e-06  Data: 0.014 (0.032)
Train: 0 [ 300/1251 ( 24%)]  Loss:  6.931793 (6.9672)  Time: 0.602s, 1700.58/s  (0.615s, 1663.85/s)  LR: 1.000e-06  Data: 0.013 (0.029)
Train: 0 [ 350/1251 ( 28%)]  Loss:  6.941665 (6.9640)  Time: 0.586s, 1748.11/s  (0.612s, 1673.95/s)  LR: 1.000e-06  Data: 0.016 (0.027)
Train: 0 [ 400/1251 ( 32%)]  Loss:  6.943627 (6.9618)  Time: 0.593s, 1726.33/s  (0.609s, 1682.23/s)  LR: 1.000e-06  Data: 0.014 (0.026)
Train: 0 [ 450/1251 ( 36%)]  Loss:  6.937901 (6.9594)  Time: 0.585s, 1750.54/s  (0.606s, 1689.32/s)  LR: 1.000e-06  Data: 0.015 (0.024)
Train: 0 [ 500/1251 ( 40%)]  Loss:  6.928392 (6.9566)  Time: 0.584s, 1754.40/s  (0.604s, 1695.08/s)  LR: 1.000e-06  Data: 0.017 (0.023)
Train: 0 [ 550/1251 ( 44%)]  Loss:  6.925658 (6.9540)  Time: 0.585s, 1749.14/s  (0.602s, 1700.02/s)  LR: 1.000e-06  Data: 0.017 (0.022)
Train: 0 [ 600/1251 ( 48%)]  Loss:  6.938227 (6.9528)  Time: 0.578s, 1771.05/s  (0.601s, 1703.80/s)  LR: 1.000e-06  Data: 0.012 (0.022)
Train: 0 [ 650/1251 ( 52%)]  Loss:  6.923599 (6.9507)  Time: 0.588s, 1742.08/s  (0.600s, 1707.33/s)  LR: 1.000e-06  Data: 0.015 (0.021)
Train: 0 [ 700/1251 ( 56%)]  Loss:  6.931998 (6.9494)  Time: 0.577s, 1774.24/s  (0.599s, 1710.19/s)  LR: 1.000e-06  Data: 0.009 (0.021)
Train: 0 [ 750/1251 ( 60%)]  Loss:  6.923567 (6.9478)  Time: 0.580s, 1766.05/s  (0.598s, 1712.88/s)  LR: 1.000e-06  Data: 0.013 (0.020)
Train: 0 [ 800/1251 ( 64%)]  Loss:  6.926289 (6.9466)  Time: 0.583s, 1755.08/s  (0.597s, 1715.18/s)  LR: 1.000e-06  Data: 0.014 (0.020)
Train: 0 [ 850/1251 ( 68%)]  Loss:  6.941751 (6.9463)  Time: 0.578s, 1771.55/s  (0.596s, 1717.06/s)  LR: 1.000e-06  Data: 0.009 (0.020)
Train: 0 [ 900/1251 ( 72%)]  Loss:  6.933489 (6.9456)  Time: 0.580s, 1766.33/s  (0.596s, 1718.78/s)  LR: 1.000e-06  Data: 0.014 (0.020)
Train: 0 [ 950/1251 ( 76%)]  Loss:  6.931657 (6.9449)  Time: 0.604s, 1694.78/s  (0.595s, 1720.44/s)  LR: 1.000e-06  Data: 0.015 (0.019)
Train: 0 [1000/1251 ( 80%)]  Loss:  6.919325 (6.9437)  Time: 0.621s, 1648.60/s  (0.595s, 1721.74/s)  LR: 1.000e-06  Data: 0.021 (0.019)
Train: 0 [1050/1251 ( 84%)]  Loss:  6.924144 (6.9428)  Time: 0.587s, 1745.83/s  (0.594s, 1722.95/s)  LR: 1.000e-06  Data: 0.017 (0.019)
Train: 0 [1100/1251 ( 88%)]  Loss:  6.906716 (6.9412)  Time: 0.581s, 1762.11/s  (0.594s, 1723.98/s)  LR: 1.000e-06  Data: 0.014 (0.019)
Train: 0 [1150/1251 ( 92%)]  Loss:  6.930099 (6.9408)  Time: 0.582s, 1758.86/s  (0.594s, 1725.04/s)  LR: 1.000e-06  Data: 0.009 (0.018)
Train: 0 [1200/1251 ( 96%)]  Loss:  6.911685 (6.9396)  Time: 0.582s, 1759.52/s  (0.593s, 1726.20/s)  LR: 1.000e-06  Data: 0.016 (0.018)
Train: 0 [1250/1251 (100%)]  Loss:  6.909529 (6.9385)  Time: 0.568s, 1802.83/s  (0.593s, 1726.84/s)  LR: 1.000e-06  Data: 0.000 (0.018)
Test: [   0/48]  Time: 6.537 (6.537)  Loss:  6.9297 (6.9297)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.4883 ( 0.4883)
Test: [  48/48]  Time: 0.820 (0.328)  Loss:  6.8359 (6.8981)  Acc@1:  0.0000 ( 0.2200)  Acc@5:  0.0000 ( 0.9000)
Test (EMA): [   0/48]  Time: 5.496 (5.496)  Loss:  6.9922 (6.9922)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.3906 ( 0.3906)
Test (EMA): [  48/48]  Time: 0.125 (0.303)  Loss:  7.0000 (6.9720)  Acc@1:  0.3538 ( 0.1160)  Acc@5:  1.0613 ( 0.4980)
Current checkpoints:
 ('./output/train/20211106-125804-vit_small_patch16_224-224/checkpoint-0.pth.tar', 0.11600000015258789)

Train: 1 [   0/1251 (  0%)]  Loss:  6.924469 (6.9245)  Time: 1.499s,  682.93/s  (1.499s,  682.93/s)  LR: 9.462e-05  Data: 0.017 (0.017)
Train: 1 [  50/1251 (  4%)]  Loss:  6.889723 (6.9071)  Time: 0.587s, 1745.68/s  (0.615s, 1664.52/s)  LR: 9.462e-05  Data: 0.016 (0.015)
Train: 1 [ 100/1251 (  8%)]  Loss:  6.870161 (6.8948)  Time: 0.594s, 1723.17/s  (0.600s, 1705.35/s)  LR: 9.462e-05  Data: 0.016 (0.015)
Train: 1 [ 150/1251 ( 12%)]  Loss:  6.841422 (6.8814)  Time: 0.582s, 1759.03/s  (0.595s, 1720.14/s)  LR: 9.462e-05  Data: 0.014 (0.015)
Train: 1 [ 200/1251 ( 16%)]  Loss:  6.816749 (6.8685)  Time: 0.582s, 1760.08/s  (0.592s, 1728.67/s)  LR: 9.462e-05  Data: 0.017 (0.015)
Train: 1 [ 250/1251 ( 20%)]  Loss:  6.778368 (6.8535)  Time: 0.585s, 1750.36/s  (0.591s, 1731.71/s)  LR: 9.462e-05  Data: 0.016 (0.015)
Train: 1 [ 300/1251 ( 24%)]  Loss:  6.773386 (6.8420)  Time: 0.586s, 1747.97/s  (0.591s, 1733.81/s)  LR: 9.462e-05  Data: 0.019 (0.015)
Train: 1 [ 350/1251 ( 28%)]  Loss:  6.757511 (6.8315)  Time: 0.588s, 1741.83/s  (0.590s, 1735.12/s)  LR: 9.462e-05  Data: 0.018 (0.015)
Train: 1 [ 400/1251 ( 32%)]  Loss:  6.689816 (6.8157)  Time: 0.589s, 1737.57/s  (0.590s, 1735.81/s)  LR: 9.462e-05  Data: 0.010 (0.015)
Train: 1 [ 450/1251 ( 36%)]  Loss:  6.694315 (6.8036)  Time: 0.584s, 1753.16/s  (0.590s, 1735.88/s)  LR: 9.462e-05  Data: 0.013 (0.014)
Train: 1 [ 500/1251 ( 40%)]  Loss:  6.712235 (6.7953)  Time: 0.586s, 1746.14/s  (0.590s, 1735.93/s)  LR: 9.462e-05  Data: 0.011 (0.014)
Train: 1 [ 550/1251 ( 44%)]  Loss:  6.686930 (6.7863)  Time: 0.587s, 1744.73/s  (0.590s, 1736.04/s)  LR: 9.462e-05  Data: 0.016 (0.014)
Train: 1 [ 600/1251 ( 48%)]  Loss:  6.638680 (6.7749)  Time: 0.589s, 1739.61/s  (0.590s, 1736.60/s)  LR: 9.462e-05  Data: 0.016 (0.015)
Train: 1 [ 650/1251 ( 52%)]  Loss:  6.643048 (6.7655)  Time: 0.591s, 1731.45/s  (0.590s, 1736.53/s)  LR: 9.462e-05  Data: 0.019 (0.015)
Train: 1 [ 700/1251 ( 56%)]  Loss:  6.600907 (6.7545)  Time: 0.605s, 1692.52/s  (0.590s, 1736.47/s)  LR: 9.462e-05  Data: 0.013 (0.015)
Train: 1 [ 750/1251 ( 60%)]  Loss:  6.525646 (6.7402)  Time: 0.585s, 1748.97/s  (0.590s, 1736.42/s)  LR: 9.462e-05  Data: 0.009 (0.015)
Train: 1 [ 800/1251 ( 64%)]  Loss:  6.628143 (6.7336)  Time: 0.587s, 1745.92/s  (0.590s, 1736.77/s)  LR: 9.462e-05  Data: 0.017 (0.015)
Train: 1 [ 850/1251 ( 68%)]  Loss:  6.602108 (6.7263)  Time: 0.582s, 1758.66/s  (0.590s, 1737.02/s)  LR: 9.462e-05  Data: 0.011 (0.015)
Train: 1 [ 900/1251 ( 72%)]  Loss:  6.549282 (6.7170)  Time: 0.585s, 1751.83/s  (0.590s, 1736.97/s)  LR: 9.462e-05  Data: 0.011 (0.015)
Train: 1 [ 950/1251 ( 76%)]  Loss:  6.572568 (6.7098)  Time: 0.588s, 1740.94/s  (0.589s, 1737.16/s)  LR: 9.462e-05  Data: 0.017 (0.015)
Train: 1 [1000/1251 ( 80%)]  Loss:  6.583090 (6.7037)  Time: 0.587s, 1744.86/s  (0.589s, 1737.33/s)  LR: 9.462e-05  Data: 0.016 (0.015)
Train: 1 [1050/1251 ( 84%)]  Loss:  6.510415 (6.6950)  Time: 0.586s, 1747.93/s  (0.589s, 1737.60/s)  LR: 9.462e-05  Data: 0.013 (0.015)
Train: 1 [1100/1251 ( 88%)]  Loss:  6.588306 (6.6903)  Time: 0.585s, 1750.81/s  (0.589s, 1737.59/s)  LR: 9.462e-05  Data: 0.011 (0.015)
Train: 1 [1150/1251 ( 92%)]  Loss:  6.495323 (6.6822)  Time: 0.583s, 1755.85/s  (0.589s, 1737.84/s)  LR: 9.462e-05  Data: 0.013 (0.015)
Train: 1 [1200/1251 ( 96%)]  Loss:  6.437420 (6.6724)  Time: 0.587s, 1745.43/s  (0.589s, 1738.09/s)  LR: 9.462e-05  Data: 0.018 (0.015)
Train: 1 [1250/1251 (100%)]  Loss:  6.492930 (6.6655)  Time: 0.569s, 1799.63/s  (0.589s, 1738.05/s)  LR: 9.462e-05  Data: 0.000 (0.015)
Test: [   0/48]  Time: 5.726 (5.726)  Loss:  5.6367 (5.6367)  Acc@1:  5.7617 ( 5.7617)  Acc@5: 20.8984 (20.8984)
Test: [  48/48]  Time: 0.125 (0.317)  Loss:  5.1758 (5.9383)  Acc@1: 22.4057 ( 4.3520)  Acc@5: 33.7264 (12.8180)
Test (EMA): [   0/48]  Time: 5.841 (5.841)  Loss:  6.9688 (6.9688)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0977 ( 0.0977)
Test (EMA): [  48/48]  Time: 0.125 (0.305)  Loss:  6.9922 (6.9612)  Acc@1:  0.3538 ( 0.1000)  Acc@5:  0.9434 ( 0.4920)
Current checkpoints:
 ('./output/train/20211106-125804-vit_small_patch16_224-224/checkpoint-0.pth.tar', 0.11600000015258789)
 ('./output/train/20211106-125804-vit_small_patch16_224-224/checkpoint-1.pth.tar', 0.10000000015258789)

Train: 2 [   0/1251 (  0%)]  Loss:  6.470922 (6.4709)  Time: 1.003s, 1021.26/s  (1.003s, 1021.26/s)  LR: 1.882e-04  Data: 0.018 (0.018)
Train: 2 [  50/1251 (  4%)]  Loss:  6.467084 (6.4690)  Time: 0.583s, 1755.92/s  (0.601s, 1704.89/s)  LR: 1.882e-04  Data: 0.011 (0.015)
Train: 2 [ 100/1251 (  8%)]  Loss:  6.500903 (6.4796)  Time: 0.588s, 1741.85/s  (0.595s, 1722.22/s)  LR: 1.882e-04  Data: 0.016 (0.015)
Train: 2 [ 150/1251 ( 12%)]  Loss:  6.471080 (6.4775)  Time: 0.605s, 1692.06/s  (0.592s, 1728.37/s)  LR: 1.882e-04  Data: 0.015 (0.015)
Train: 2 [ 200/1251 ( 16%)]  Loss:  6.325860 (6.4472)  Time: 0.587s, 1745.07/s  (0.591s, 1732.01/s)  LR: 1.882e-04  Data: 0.015 (0.015)
Train: 2 [ 250/1251 ( 20%)]  Loss:  6.558256 (6.4657)  Time: 0.594s, 1722.80/s  (0.590s, 1734.27/s)  LR: 1.882e-04  Data: 0.013 (0.015)
Train: 2 [ 300/1251 ( 24%)]  Loss:  6.407632 (6.4574)  Time: 0.586s, 1748.67/s  (0.590s, 1735.98/s)  LR: 1.882e-04  Data: 0.016 (0.015)
Train: 2 [ 350/1251 ( 28%)]  Loss:  6.405303 (6.4509)  Time: 0.602s, 1699.95/s  (0.590s, 1736.67/s)  LR: 1.882e-04  Data: 0.015 (0.015)
Train: 2 [ 400/1251 ( 32%)]  Loss:  6.353960 (6.4401)  Time: 0.586s, 1748.85/s  (0.589s, 1737.37/s)  LR: 1.882e-04  Data: 0.012 (0.015)
Train: 2 [ 450/1251 ( 36%)]  Loss:  6.300582 (6.4262)  Time: 0.585s, 1749.92/s  (0.589s, 1738.21/s)  LR: 1.882e-04  Data: 0.014 (0.015)
Train: 2 [ 500/1251 ( 40%)]  Loss:  6.247784 (6.4099)  Time: 0.588s, 1742.00/s  (0.589s, 1738.09/s)  LR: 1.882e-04  Data: 0.017 (0.015)
Train: 2 [ 550/1251 ( 44%)]  Loss:  6.275353 (6.3987)  Time: 0.585s, 1750.58/s  (0.589s, 1738.50/s)  LR: 1.882e-04  Data: 0.014 (0.015)
Train: 2 [ 600/1251 ( 48%)]  Loss:  6.264108 (6.3884)  Time: 0.585s, 1748.98/s  (0.589s, 1738.83/s)  LR: 1.882e-04  Data: 0.015 (0.015)
Train: 2 [ 650/1251 ( 52%)]  Loss:  6.202685 (6.3751)  Time: 0.583s, 1757.28/s  (0.589s, 1738.85/s)  LR: 1.882e-04  Data: 0.014 (0.015)
Train: 2 [ 700/1251 ( 56%)]  Loss:  6.344073 (6.3730)  Time: 0.598s, 1713.01/s  (0.589s, 1739.01/s)  LR: 1.882e-04  Data: 0.011 (0.015)
Train: 2 [ 750/1251 ( 60%)]  Loss:  6.343233 (6.3712)  Time: 0.579s, 1768.24/s  (0.589s, 1739.13/s)  LR: 1.882e-04  Data: 0.010 (0.014)
Train: 2 [ 800/1251 ( 64%)]  Loss:  6.305240 (6.3673)  Time: 0.603s, 1697.35/s  (0.589s, 1739.29/s)  LR: 1.882e-04  Data: 0.008 (0.014)
Train: 2 [ 850/1251 ( 68%)]  Loss:  6.252297 (6.3609)  Time: 0.590s, 1736.36/s  (0.589s, 1739.19/s)  LR: 1.882e-04  Data: 0.008 (0.014)
Train: 2 [ 900/1251 ( 72%)]  Loss:  6.389637 (6.3624)  Time: 0.588s, 1742.74/s  (0.589s, 1739.44/s)  LR: 1.882e-04  Data: 0.016 (0.014)
Train: 2 [ 950/1251 ( 76%)]  Loss:  6.374660 (6.3630)  Time: 0.586s, 1747.61/s  (0.589s, 1739.56/s)  LR: 1.882e-04  Data: 0.016 (0.014)
Train: 2 [1000/1251 ( 80%)]  Loss:  6.168140 (6.3538)  Time: 0.596s, 1717.50/s  (0.589s, 1739.37/s)  LR: 1.882e-04  Data: 0.008 (0.014)
Train: 2 [1050/1251 ( 84%)]  Loss:  6.263479 (6.3496)  Time: 0.589s, 1738.71/s  (0.589s, 1739.58/s)  LR: 1.882e-04  Data: 0.018 (0.014)
Train: 2 [1100/1251 ( 88%)]  Loss:  6.281297 (6.3467)  Time: 0.587s, 1744.04/s  (0.589s, 1739.40/s)  LR: 1.882e-04  Data: 0.011 (0.014)
Train: 2 [1150/1251 ( 92%)]  Loss:  6.223825 (6.3416)  Time: 0.589s, 1738.91/s  (0.589s, 1739.13/s)  LR: 1.882e-04  Data: 0.012 (0.014)
Train: 2 [1200/1251 ( 96%)]  Loss:  6.191917 (6.3356)  Time: 0.596s, 1717.84/s  (0.589s, 1738.90/s)  LR: 1.882e-04  Data: 0.013 (0.014)
Train: 2 [1250/1251 (100%)]  Loss:  6.247507 (6.3322)  Time: 0.568s, 1802.47/s  (0.589s, 1738.16/s)  LR: 1.882e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 5.560 (5.560)  Loss:  4.9180 (4.9180)  Acc@1: 12.3047 (12.3047)  Acc@5: 36.0352 (36.0352)
Test: [  48/48]  Time: 0.125 (0.325)  Loss:  4.6133 (5.3580)  Acc@1: 25.3538 ( 8.1120)  Acc@5: 38.2075 (21.1360)
Test (EMA): [   0/48]  Time: 6.195 (6.195)  Loss:  6.9492 (6.9492)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0977 ( 0.0977)
Test (EMA): [  48/48]  Time: 0.125 (0.302)  Loss:  6.9727 (6.9482)  Acc@1:  0.0000 ( 0.1040)  Acc@5:  0.2358 ( 0.5080)
Current checkpoints:
 ('./output/train/20211106-125804-vit_small_patch16_224-224/checkpoint-0.pth.tar', 0.11600000015258789)
 ('./output/train/20211106-125804-vit_small_patch16_224-224/checkpoint-2.pth.tar', 0.104)
 ('./output/train/20211106-125804-vit_small_patch16_224-224/checkpoint-1.pth.tar', 0.10000000015258789)

Train: 3 [   0/1251 (  0%)]  Loss:  6.232138 (6.2321)  Time: 1.408s,  727.10/s  (1.408s,  727.10/s)  LR: 2.819e-04  Data: 0.018 (0.018)
Train: 3 [  50/1251 (  4%)]  Loss:  6.355999 (6.2941)  Time: 0.586s, 1747.74/s  (0.620s, 1652.61/s)  LR: 2.819e-04  Data: 0.012 (0.012)
Train: 3 [ 100/1251 (  8%)]  Loss:  6.269306 (6.2858)  Time: 0.594s, 1722.88/s  (0.604s, 1694.84/s)  LR: 2.819e-04  Data: 0.024 (0.012)
Train: 3 [ 150/1251 ( 12%)]  Loss:  6.428391 (6.3215)  Time: 0.588s, 1741.39/s  (0.599s, 1709.22/s)  LR: 2.819e-04  Data: 0.007 (0.013)
Train: 3 [ 200/1251 ( 16%)]  Loss:  6.089539 (6.2751)  Time: 0.589s, 1737.12/s  (0.597s, 1715.88/s)  LR: 2.819e-04  Data: 0.016 (0.013)
Train: 3 [ 250/1251 ( 20%)]  Loss:  6.064589 (6.2400)  Time: 0.588s, 1741.35/s  (0.595s, 1719.57/s)  LR: 2.819e-04  Data: 0.019 (0.013)
Train: 3 [ 300/1251 ( 24%)]  Loss:  6.078800 (6.2170)  Time: 0.603s, 1697.46/s  (0.595s, 1722.11/s)  LR: 2.819e-04  Data: 0.013 (0.013)
Train: 3 [ 350/1251 ( 28%)]  Loss:  6.097785 (6.2021)  Time: 0.589s, 1737.94/s  (0.594s, 1724.00/s)  LR: 2.819e-04  Data: 0.010 (0.013)
Train: 3 [ 400/1251 ( 32%)]  Loss:  6.228287 (6.2050)  Time: 0.581s, 1762.54/s  (0.593s, 1725.92/s)  LR: 2.819e-04  Data: 0.013 (0.013)
Train: 3 [ 450/1251 ( 36%)]  Loss:  6.209197 (6.2054)  Time: 0.586s, 1748.17/s  (0.593s, 1727.02/s)  LR: 2.819e-04  Data: 0.012 (0.013)
Train: 3 [ 500/1251 ( 40%)]  Loss:  6.221479 (6.2069)  Time: 0.599s, 1708.46/s  (0.593s, 1727.63/s)  LR: 2.819e-04  Data: 0.015 (0.013)
Train: 3 [ 550/1251 ( 44%)]  Loss:  6.092540 (6.1973)  Time: 0.590s, 1735.23/s  (0.593s, 1728.01/s)  LR: 2.819e-04  Data: 0.011 (0.013)
