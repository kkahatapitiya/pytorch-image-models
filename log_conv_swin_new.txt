/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 6.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 6 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 6.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 6.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 6.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 6.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 6.
Model swin_tiny_patch4_window7_224 created, param count:27110704
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Using NVIDIA APEX AMP. Training in mixed precision.
Using NVIDIA APEX DistributedDataParallel.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Scheduled epochs: 310
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Train: 0 [   0/1256 (  0%)]  Loss:  7.005838 (7.0058)  Time: 15.111s,   67.50/s  (15.111s,   67.50/s)  LR: 5.000e-07  Data: 2.980 (2.980)
Train: 0 [  50/1256 (  4%)]  Loss:  6.977806 (6.9918)  Time: 1.448s,  704.47/s  (1.712s,  595.78/s)  LR: 5.000e-07  Data: 0.014 (0.072)
Train: 0 [ 100/1256 (  8%)]  Loss:  6.968403 (6.9840)  Time: 1.429s,  713.91/s  (1.577s,  646.88/s)  LR: 5.000e-07  Data: 0.013 (0.043)
Train: 0 [ 150/1256 ( 12%)]  Loss:  6.963695 (6.9789)  Time: 1.443s,  706.66/s  (1.531s,  666.21/s)  LR: 5.000e-07  Data: 0.013 (0.033)
Train: 0 [ 200/1256 ( 16%)]  Loss:  6.970397 (6.9772)  Time: 1.427s,  715.03/s  (1.507s,  676.80/s)  LR: 5.000e-07  Data: 0.014 (0.028)
Train: 0 [ 250/1256 ( 20%)]  Loss:  6.957799 (6.9740)  Time: 1.427s,  714.54/s  (1.493s,  683.12/s)  LR: 5.000e-07  Data: 0.017 (0.025)
Train: 0 [ 300/1256 ( 24%)]  Loss:  6.954904 (6.9713)  Time: 1.425s,  715.61/s  (1.484s,  687.44/s)  LR: 5.000e-07  Data: 0.013 (0.023)
Train: 0 [ 350/1256 ( 28%)]  Loss:  6.935510 (6.9668)  Time: 1.427s,  715.04/s  (1.477s,  690.61/s)  LR: 5.000e-07  Data: 0.014 (0.022)
Train: 0 [ 400/1256 ( 32%)]  Loss:  6.951660 (6.9651)  Time: 1.428s,  714.24/s  (1.472s,  693.03/s)  LR: 5.000e-07  Data: 0.013 (0.021)
Train: 0 [ 450/1256 ( 36%)]  Loss:  6.940971 (6.9627)  Time: 1.434s,  711.44/s  (1.467s,  695.23/s)  LR: 5.000e-07  Data: 0.016 (0.020)
Train: 0 [ 500/1256 ( 40%)]  Loss:  6.952939 (6.9618)  Time: 1.463s,  697.23/s  (1.464s,  696.76/s)  LR: 5.000e-07  Data: 0.012 (0.020)
Train: 0 [ 550/1256 ( 44%)]  Loss:  6.934988 (6.9596)  Time: 1.461s,  698.19/s  (1.461s,  698.09/s)  LR: 5.000e-07  Data: 0.016 (0.019)
Train: 0 [ 600/1256 ( 48%)]  Loss:  6.950258 (6.9589)  Time: 1.448s,  704.61/s  (1.459s,  698.89/s)  LR: 5.000e-07  Data: 0.013 (0.019)
Train: 0 [ 650/1256 ( 52%)]  Loss:  6.952786 (6.9584)  Time: 1.435s,  710.98/s  (1.458s,  699.64/s)  LR: 5.000e-07  Data: 0.012 (0.018)
Train: 0 [ 700/1256 ( 56%)]  Loss:  6.932941 (6.9567)  Time: 1.424s,  716.47/s  (1.457s,  700.25/s)  LR: 5.000e-07  Data: 0.012 (0.018)
Train: 0 [ 750/1256 ( 60%)]  Loss:  6.940443 (6.9557)  Time: 1.424s,  716.27/s  (1.456s,  700.77/s)  LR: 5.000e-07  Data: 0.015 (0.018)
Train: 0 [ 800/1256 ( 64%)]  Loss:  6.925081 (6.9539)  Time: 1.428s,  714.22/s  (1.454s,  701.36/s)  LR: 5.000e-07  Data: 0.013 (0.017)
Train: 0 [ 850/1256 ( 68%)]  Loss:  6.933004 (6.9527)  Time: 1.448s,  704.18/s  (1.453s,  701.99/s)  LR: 5.000e-07  Data: 0.013 (0.017)
Train: 0 [ 900/1256 ( 72%)]  Loss:  6.940301 (6.9521)  Time: 1.427s,  714.54/s  (1.452s,  702.41/s)  LR: 5.000e-07  Data: 0.013 (0.017)
Train: 0 [ 950/1256 ( 76%)]  Loss:  6.941106 (6.9515)  Time: 1.425s,  715.92/s  (1.451s,  702.83/s)  LR: 5.000e-07  Data: 0.016 (0.017)
Train: 0 [1000/1256 ( 80%)]  Loss:  6.928512 (6.9504)  Time: 1.454s,  701.30/s  (1.450s,  703.33/s)  LR: 5.000e-07  Data: 0.016 (0.017)
Train: 0 [1050/1256 ( 84%)]  Loss:  6.931146 (6.9496)  Time: 1.427s,  714.76/s  (1.450s,  703.63/s)  LR: 5.000e-07  Data: 0.016 (0.017)
Train: 0 [1100/1256 ( 88%)]  Loss:  6.936746 (6.9490)  Time: 1.430s,  713.47/s  (1.449s,  704.03/s)  LR: 5.000e-07  Data: 0.012 (0.017)
Train: 0 [1150/1256 ( 92%)]  Loss:  6.924869 (6.9480)  Time: 1.452s,  702.69/s  (1.448s,  704.18/s)  LR: 5.000e-07  Data: 0.013 (0.016)
Train: 0 [1200/1256 ( 96%)]  Loss:  6.930120 (6.9473)  Time: 1.424s,  716.10/s  (1.448s,  704.49/s)  LR: 5.000e-07  Data: 0.016 (0.016)
Train: 0 [1250/1256 (100%)]  Loss:  6.927011 (6.9465)  Time: 1.462s,  697.74/s  (1.448s,  704.57/s)  LR: 5.000e-07  Data: 0.014 (0.016)
Train: 0 [1255/1256 (100%)]  Loss:  6.923225 (6.9456)  Time: 1.427s,  714.58/s  (1.448s,  704.58/s)  LR: 5.000e-07  Data: 0.000 (0.016)
Test: [   0/49]  Time: 5.643 (5.643)  Loss:  6.8782 (6.8782)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
Test: [  49/49]  Time: 1.206 (0.579)  Loss:  6.9211 (6.9066)  Acc@1:  0.0000 ( 0.1520)  Acc@5:  0.0000 ( 0.6879)
Test (EMA): [   0/49]  Time: 4.132 (4.132)  Loss:  6.9557 (6.9557)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.2941 ( 0.2941)
Test (EMA): [  49/49]  Time: 0.035 (0.502)  Loss:  6.7612 (6.9668)  Acc@1:  0.0000 ( 0.1100)  Acc@5:  0.0000 ( 0.5160)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 1 [   0/1256 (  0%)]  Loss:  6.918691 (6.9187)  Time: 1.506s,  677.38/s  (1.506s,  677.38/s)  LR: 2.548e-05  Data: 0.096 (0.096)
Train: 1 [  50/1256 (  4%)]  Loss:  6.913145 (6.9159)  Time: 1.450s,  703.32/s  (1.436s,  710.10/s)  LR: 2.548e-05  Data: 0.014 (0.015)
Train: 1 [ 100/1256 (  8%)]  Loss:  6.895100 (6.9090)  Time: 1.463s,  697.42/s  (1.443s,  706.67/s)  LR: 2.548e-05  Data: 0.013 (0.014)
Train: 1 [ 150/1256 ( 12%)]  Loss:  6.883909 (6.9027)  Time: 1.424s,  716.36/s  (1.440s,  708.39/s)  LR: 2.548e-05  Data: 0.012 (0.014)
Train: 1 [ 200/1256 ( 16%)]  Loss:  6.858974 (6.8940)  Time: 1.441s,  707.98/s  (1.438s,  709.25/s)  LR: 2.548e-05  Data: 0.013 (0.014)
Train: 1 [ 250/1256 ( 20%)]  Loss:  6.868651 (6.8897)  Time: 1.427s,  714.96/s  (1.437s,  709.94/s)  LR: 2.548e-05  Data: 0.012 (0.013)
Train: 1 [ 300/1256 ( 24%)]  Loss:  6.843223 (6.8831)  Time: 1.431s,  713.02/s  (1.436s,  710.38/s)  LR: 2.548e-05  Data: 0.012 (0.013)
Train: 1 [ 350/1256 ( 28%)]  Loss:  6.802530 (6.8730)  Time: 1.435s,  710.65/s  (1.435s,  710.69/s)  LR: 2.548e-05  Data: 0.015 (0.013)
Train: 1 [ 400/1256 ( 32%)]  Loss:  6.837318 (6.8691)  Time: 1.430s,  713.11/s  (1.435s,  711.01/s)  LR: 2.548e-05  Data: 0.012 (0.013)
Train: 1 [ 450/1256 ( 36%)]  Loss:  6.794658 (6.8616)  Time: 1.426s,  715.07/s  (1.435s,  710.99/s)  LR: 2.548e-05  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 1 [ 500/1256 ( 40%)]  Loss:  6.796505 (6.8557)  Time: 1.435s,  710.85/s  (1.434s,  711.30/s)  LR: 2.548e-05  Data: 0.018 (0.013)
Train: 1 [ 550/1256 ( 44%)]  Loss:  6.823176 (6.8530)  Time: 1.424s,  716.08/s  (1.434s,  711.42/s)  LR: 2.548e-05  Data: 0.015 (0.013)
Train: 1 [ 600/1256 ( 48%)]  Loss:  6.772784 (6.8468)  Time: 1.430s,  713.13/s  (1.433s,  711.62/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [ 650/1256 ( 52%)]  Loss:  6.828238 (6.8455)  Time: 1.429s,  713.85/s  (1.434s,  711.49/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [ 700/1256 ( 56%)]  Loss:  6.812916 (6.8433)  Time: 1.421s,  717.60/s  (1.433s,  711.64/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [ 750/1256 ( 60%)]  Loss:  6.769787 (6.8387)  Time: 1.424s,  716.48/s  (1.434s,  711.48/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [ 800/1256 ( 64%)]  Loss:  6.736978 (6.8327)  Time: 1.423s,  716.98/s  (1.433s,  711.68/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [ 850/1256 ( 68%)]  Loss:  6.772938 (6.8294)  Time: 1.428s,  714.40/s  (1.433s,  711.83/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [ 900/1256 ( 72%)]  Loss:  6.738871 (6.8247)  Time: 1.427s,  714.78/s  (1.433s,  711.87/s)  LR: 2.548e-05  Data: 0.019 (0.013)
Train: 1 [ 950/1256 ( 76%)]  Loss:  6.746341 (6.8207)  Time: 1.427s,  714.98/s  (1.433s,  711.94/s)  LR: 2.548e-05  Data: 0.012 (0.013)
Train: 1 [1000/1256 ( 80%)]  Loss:  6.697359 (6.8149)  Time: 1.429s,  713.88/s  (1.433s,  711.92/s)  LR: 2.548e-05  Data: 0.012 (0.013)
Train: 1 [1050/1256 ( 84%)]  Loss:  6.744949 (6.8117)  Time: 1.429s,  713.86/s  (1.433s,  712.04/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [1100/1256 ( 88%)]  Loss:  6.753326 (6.8091)  Time: 1.424s,  716.42/s  (1.433s,  711.94/s)  LR: 2.548e-05  Data: 0.012 (0.013)
Train: 1 [1150/1256 ( 92%)]  Loss:  6.708986 (6.8050)  Time: 1.425s,  715.95/s  (1.432s,  712.07/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [1200/1256 ( 96%)]  Loss:  6.667599 (6.7995)  Time: 1.425s,  715.97/s  (1.432s,  712.07/s)  LR: 2.548e-05  Data: 0.013 (0.013)
Train: 1 [1250/1256 (100%)]  Loss:  6.660964 (6.7942)  Time: 1.448s,  704.59/s  (1.432s,  712.08/s)  LR: 2.548e-05  Data: 0.014 (0.013)
Train: 1 [1255/1256 (100%)]  Loss:  6.711905 (6.7911)  Time: 1.409s,  723.68/s  (1.432s,  712.11/s)  LR: 2.548e-05  Data: 0.000 (0.013)
Test: [   0/49]  Time: 4.660 (4.660)  Loss:  6.0405 (6.0405)  Acc@1:  1.1765 ( 1.1765)  Acc@5: 11.9608 (11.9608)
Test: [  49/49]  Time: 0.032 (0.516)  Loss:  6.5236 (6.3358)  Acc@1:  4.1667 ( 1.8659)  Acc@5:  8.3333 ( 6.4355)
Test (EMA): [   0/49]  Time: 4.151 (4.151)  Loss:  6.9403 (6.9403)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.1961 ( 0.1961)
Test (EMA): [  49/49]  Time: 0.044 (0.503)  Loss:  6.7773 (6.9567)  Acc@1:  0.0000 ( 0.1180)  Acc@5:  0.0000 ( 0.5320)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 2 [   0/1256 (  0%)]  Loss:  6.626086 (6.6261)  Time: 1.481s,  688.78/s  (1.481s,  688.78/s)  LR: 5.045e-05  Data: 0.027 (0.027)
Train: 2 [  50/1256 (  4%)]  Loss:  6.696408 (6.6612)  Time: 1.434s,  711.06/s  (1.439s,  708.97/s)  LR: 5.045e-05  Data: 0.013 (0.015)
Train: 2 [ 100/1256 (  8%)]  Loss:  6.731522 (6.6847)  Time: 1.428s,  714.14/s  (1.434s,  711.32/s)  LR: 5.045e-05  Data: 0.016 (0.014)
Train: 2 [ 150/1256 ( 12%)]  Loss:  6.680758 (6.6837)  Time: 1.428s,  714.35/s  (1.435s,  710.89/s)  LR: 5.045e-05  Data: 0.017 (0.014)
Train: 2 [ 200/1256 ( 16%)]  Loss:  6.716461 (6.6902)  Time: 1.422s,  717.36/s  (1.433s,  711.73/s)  LR: 5.045e-05  Data: 0.014 (0.014)
Train: 2 [ 250/1256 ( 20%)]  Loss:  6.629127 (6.6801)  Time: 1.427s,  714.85/s  (1.433s,  712.02/s)  LR: 5.045e-05  Data: 0.013 (0.014)
Train: 2 [ 300/1256 ( 24%)]  Loss:  6.673028 (6.6791)  Time: 1.427s,  714.92/s  (1.433s,  711.96/s)  LR: 5.045e-05  Data: 0.012 (0.014)
Train: 2 [ 350/1256 ( 28%)]  Loss:  6.640150 (6.6742)  Time: 1.426s,  715.13/s  (1.432s,  712.32/s)  LR: 5.045e-05  Data: 0.012 (0.014)
Train: 2 [ 400/1256 ( 32%)]  Loss:  6.682137 (6.6751)  Time: 1.427s,  714.55/s  (1.432s,  712.26/s)  LR: 5.045e-05  Data: 0.012 (0.014)
Train: 2 [ 450/1256 ( 36%)]  Loss:  6.685214 (6.6761)  Time: 1.431s,  713.03/s  (1.432s,  712.47/s)  LR: 5.045e-05  Data: 0.013 (0.014)
Train: 2 [ 500/1256 ( 40%)]  Loss:  6.632479 (6.6721)  Time: 1.424s,  716.38/s  (1.432s,  712.19/s)  LR: 5.045e-05  Data: 0.015 (0.014)
Train: 2 [ 550/1256 ( 44%)]  Loss:  6.660797 (6.6712)  Time: 1.427s,  714.68/s  (1.432s,  712.32/s)  LR: 5.045e-05  Data: 0.018 (0.014)
Train: 2 [ 600/1256 ( 48%)]  Loss:  6.581470 (6.6643)  Time: 1.425s,  715.97/s  (1.432s,  712.12/s)  LR: 5.045e-05  Data: 0.018 (0.013)
Train: 2 [ 650/1256 ( 52%)]  Loss:  6.547139 (6.6559)  Time: 1.436s,  710.40/s  (1.432s,  712.28/s)  LR: 5.045e-05  Data: 0.013 (0.013)
Train: 2 [ 700/1256 ( 56%)]  Loss:  6.569304 (6.6501)  Time: 1.431s,  712.89/s  (1.432s,  712.17/s)  LR: 5.045e-05  Data: 0.012 (0.013)
Train: 2 [ 750/1256 ( 60%)]  Loss:  6.585279 (6.6461)  Time: 1.430s,  713.27/s  (1.432s,  712.31/s)  LR: 5.045e-05  Data: 0.013 (0.013)
Train: 2 [ 800/1256 ( 64%)]  Loss:  6.601007 (6.6434)  Time: 1.424s,  716.10/s  (1.432s,  712.28/s)  LR: 5.045e-05  Data: 0.012 (0.013)
Train: 2 [ 850/1256 ( 68%)]  Loss:  6.557586 (6.6387)  Time: 1.432s,  712.22/s  (1.432s,  712.30/s)  LR: 5.045e-05  Data: 0.017 (0.013)
Train: 2 [ 900/1256 ( 72%)]  Loss:  6.476337 (6.6301)  Time: 1.427s,  714.96/s  (1.432s,  712.34/s)  LR: 5.045e-05  Data: 0.013 (0.013)
Train: 2 [ 950/1256 ( 76%)]  Loss:  6.574626 (6.6273)  Time: 1.432s,  712.25/s  (1.432s,  712.35/s)  LR: 5.045e-05  Data: 0.013 (0.013)
Train: 2 [1000/1256 ( 80%)]  Loss:  6.495375 (6.6211)  Time: 1.425s,  716.01/s  (1.432s,  712.40/s)  LR: 5.045e-05  Data: 0.012 (0.013)
Train: 2 [1050/1256 ( 84%)]  Loss:  6.456454 (6.6136)  Time: 1.427s,  714.71/s  (1.432s,  712.38/s)  LR: 5.045e-05  Data: 0.013 (0.013)
Train: 2 [1100/1256 ( 88%)]  Loss:  6.533055 (6.6101)  Time: 1.437s,  709.94/s  (1.432s,  712.45/s)  LR: 5.045e-05  Data: 0.015 (0.013)
Train: 2 [1150/1256 ( 92%)]  Loss:  6.591514 (6.6093)  Time: 1.428s,  714.32/s  (1.432s,  712.33/s)  LR: 5.045e-05  Data: 0.012 (0.013)
Train: 2 [1200/1256 ( 96%)]  Loss:  6.616740 (6.6096)  Time: 1.427s,  714.75/s  (1.432s,  712.41/s)  LR: 5.045e-05  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 2 [1250/1256 (100%)]  Loss:  6.507145 (6.6057)  Time: 1.425s,  715.94/s  (1.432s,  712.26/s)  LR: 5.045e-05  Data: 0.012 (0.013)
Train: 2 [1255/1256 (100%)]  Loss:  6.497718 (6.6017)  Time: 1.408s,  724.20/s  (1.432s,  712.28/s)  LR: 5.045e-05  Data: 0.000 (0.013)
Test: [   0/49]  Time: 4.251 (4.251)  Loss:  5.6193 (5.6193)  Acc@1:  3.7255 ( 3.7255)  Acc@5: 18.7255 (18.7255)
Test: [  49/49]  Time: 0.032 (0.499)  Loss:  6.5055 (5.7589)  Acc@1:  4.1667 ( 5.0896)  Acc@5:  4.1667 (14.7888)
Test (EMA): [   0/49]  Time: 4.435 (4.435)  Loss:  6.9285 (6.9285)  Acc@1:  0.0980 ( 0.0980)  Acc@5:  0.5882 ( 0.5882)
Test (EMA): [  49/49]  Time: 0.045 (0.504)  Loss:  6.8083 (6.9471)  Acc@1:  0.0000 ( 0.1160)  Acc@5:  0.0000 ( 0.5700)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 3 [   0/1256 (  0%)]  Loss:  6.527446 (6.5274)  Time: 1.437s,  709.57/s  (1.437s,  709.57/s)  LR: 7.543e-05  Data: 0.028 (0.028)
Train: 3 [  50/1256 (  4%)]  Loss:  6.383684 (6.4556)  Time: 1.419s,  718.74/s  (1.430s,  713.36/s)  LR: 7.543e-05  Data: 0.012 (0.013)
Train: 3 [ 100/1256 (  8%)]  Loss:  6.476333 (6.4625)  Time: 1.425s,  715.87/s  (1.433s,  711.73/s)  LR: 7.543e-05  Data: 0.014 (0.013)
Train: 3 [ 150/1256 ( 12%)]  Loss:  6.517467 (6.4762)  Time: 1.437s,  709.63/s  (1.432s,  712.50/s)  LR: 7.543e-05  Data: 0.012 (0.013)
Train: 3 [ 200/1256 ( 16%)]  Loss:  6.525187 (6.4860)  Time: 1.537s,  663.54/s  (1.433s,  711.72/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [ 250/1256 ( 20%)]  Loss:  6.337762 (6.4613)  Time: 1.425s,  715.97/s  (1.432s,  712.23/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [ 300/1256 ( 24%)]  Loss:  6.403625 (6.4531)  Time: 1.427s,  714.87/s  (1.432s,  712.30/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [ 350/1256 ( 28%)]  Loss:  6.472366 (6.4555)  Time: 1.440s,  708.50/s  (1.432s,  712.11/s)  LR: 7.543e-05  Data: 0.012 (0.013)
Train: 3 [ 400/1256 ( 32%)]  Loss:  6.478470 (6.4580)  Time: 1.421s,  717.81/s  (1.432s,  712.29/s)  LR: 7.543e-05  Data: 0.011 (0.013)
Train: 3 [ 450/1256 ( 36%)]  Loss:  6.362430 (6.4485)  Time: 1.422s,  717.06/s  (1.432s,  712.40/s)  LR: 7.543e-05  Data: 0.014 (0.013)
Train: 3 [ 500/1256 ( 40%)]  Loss:  6.464725 (6.4500)  Time: 1.427s,  714.95/s  (1.432s,  712.49/s)  LR: 7.543e-05  Data: 0.012 (0.013)
Train: 3 [ 550/1256 ( 44%)]  Loss:  6.544872 (6.4579)  Time: 1.428s,  714.08/s  (1.432s,  712.36/s)  LR: 7.543e-05  Data: 0.017 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 3 [ 600/1256 ( 48%)]  Loss:  6.441544 (6.4566)  Time: 1.426s,  715.21/s  (1.432s,  712.53/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [ 650/1256 ( 52%)]  Loss:  6.391065 (6.4519)  Time: 1.439s,  708.77/s  (1.432s,  712.39/s)  LR: 7.543e-05  Data: 0.012 (0.013)
Train: 3 [ 700/1256 ( 56%)]  Loss:  6.455314 (6.4522)  Time: 1.425s,  715.80/s  (1.432s,  712.53/s)  LR: 7.543e-05  Data: 0.015 (0.013)
Train: 3 [ 750/1256 ( 60%)]  Loss:  6.322573 (6.4441)  Time: 1.425s,  715.70/s  (1.432s,  712.50/s)  LR: 7.543e-05  Data: 0.016 (0.013)
Train: 3 [ 800/1256 ( 64%)]  Loss:  6.451499 (6.4445)  Time: 1.425s,  715.62/s  (1.431s,  712.60/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [ 850/1256 ( 68%)]  Loss:  6.285645 (6.4357)  Time: 1.451s,  702.91/s  (1.431s,  712.68/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [ 900/1256 ( 72%)]  Loss:  6.436293 (6.4357)  Time: 1.428s,  714.27/s  (1.431s,  712.58/s)  LR: 7.543e-05  Data: 0.011 (0.013)
Train: 3 [ 950/1256 ( 76%)]  Loss:  6.235815 (6.4257)  Time: 1.427s,  714.66/s  (1.431s,  712.57/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [1000/1256 ( 80%)]  Loss:  6.438875 (6.4263)  Time: 1.427s,  714.65/s  (1.432s,  712.53/s)  LR: 7.543e-05  Data: 0.015 (0.013)
Train: 3 [1050/1256 ( 84%)]  Loss:  6.444309 (6.4271)  Time: 1.426s,  715.50/s  (1.431s,  712.58/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [1100/1256 ( 88%)]  Loss:  6.264613 (6.4201)  Time: 1.428s,  714.04/s  (1.432s,  712.49/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [1150/1256 ( 92%)]  Loss:  6.137460 (6.4083)  Time: 1.429s,  714.02/s  (1.431s,  712.55/s)  LR: 7.543e-05  Data: 0.018 (0.013)
Train: 3 [1200/1256 ( 96%)]  Loss:  6.253427 (6.4021)  Time: 1.426s,  715.24/s  (1.432s,  712.49/s)  LR: 7.543e-05  Data: 0.013 (0.013)
Train: 3 [1250/1256 (100%)]  Loss:  6.346409 (6.4000)  Time: 1.425s,  715.59/s  (1.432s,  712.48/s)  LR: 7.543e-05  Data: 0.015 (0.013)
Train: 3 [1255/1256 (100%)]  Loss:  6.229127 (6.3936)  Time: 1.415s,  720.76/s  (1.432s,  712.49/s)  LR: 7.543e-05  Data: 0.000 (0.013)
Test: [   0/49]  Time: 4.067 (4.067)  Loss:  4.9057 (4.9057)  Acc@1:  9.6078 ( 9.6078)  Acc@5: 31.4706 (31.4706)
Test: [  49/49]  Time: 0.032 (0.490)  Loss:  6.4161 (5.2505)  Acc@1:  0.0000 ( 9.2953)  Acc@5:  0.0000 (23.5801)
Test (EMA): [   0/49]  Time: 4.354 (4.354)  Loss:  6.9144 (6.9144)  Acc@1:  0.3922 ( 0.3922)  Acc@5:  0.6863 ( 0.6863)
Test (EMA): [  49/49]  Time: 0.061 (0.506)  Loss:  6.8357 (6.9360)  Acc@1:  0.0000 ( 0.1340)  Acc@5:  0.0000 ( 0.6080)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 4 [   0/1256 (  0%)]  Loss:  6.195669 (6.1957)  Time: 1.574s,  648.22/s  (1.574s,  648.22/s)  LR: 1.004e-04  Data: 0.034 (0.034)
Train: 4 [  50/1256 (  4%)]  Loss:  6.248844 (6.2223)  Time: 1.461s,  698.33/s  (1.438s,  709.17/s)  LR: 1.004e-04  Data: 0.012 (0.015)
Train: 4 [ 100/1256 (  8%)]  Loss:  6.147743 (6.1974)  Time: 1.427s,  714.83/s  (1.435s,  711.01/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 150/1256 ( 12%)]  Loss:  6.406976 (6.2498)  Time: 1.439s,  708.90/s  (1.436s,  710.24/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 200/1256 ( 16%)]  Loss:  6.348293 (6.2695)  Time: 1.426s,  715.44/s  (1.435s,  711.03/s)  LR: 1.004e-04  Data: 0.014 (0.014)
Train: 4 [ 250/1256 ( 20%)]  Loss:  6.221322 (6.2615)  Time: 1.425s,  715.59/s  (1.434s,  711.33/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 300/1256 ( 24%)]  Loss:  6.094827 (6.2377)  Time: 1.431s,  712.88/s  (1.434s,  711.07/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 350/1256 ( 28%)]  Loss:  6.266671 (6.2413)  Time: 1.431s,  712.73/s  (1.434s,  711.23/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 400/1256 ( 32%)]  Loss:  6.288877 (6.2466)  Time: 1.425s,  715.64/s  (1.434s,  711.37/s)  LR: 1.004e-04  Data: 0.014 (0.014)
Train: 4 [ 450/1256 ( 36%)]  Loss:  5.998239 (6.2217)  Time: 1.431s,  712.96/s  (1.433s,  711.59/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 500/1256 ( 40%)]  Loss:  6.121819 (6.2127)  Time: 1.425s,  715.94/s  (1.434s,  711.51/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 550/1256 ( 44%)]  Loss:  6.176359 (6.2096)  Time: 1.430s,  713.31/s  (1.433s,  711.73/s)  LR: 1.004e-04  Data: 0.018 (0.014)
Train: 4 [ 600/1256 ( 48%)]  Loss:  6.265996 (6.2140)  Time: 1.429s,  713.61/s  (1.434s,  711.42/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 650/1256 ( 52%)]  Loss:  6.237749 (6.2157)  Time: 1.426s,  715.22/s  (1.433s,  711.62/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 700/1256 ( 56%)]  Loss:  6.269516 (6.2193)  Time: 1.436s,  710.12/s  (1.434s,  711.30/s)  LR: 1.004e-04  Data: 0.011 (0.014)
Train: 4 [ 750/1256 ( 60%)]  Loss:  6.280000 (6.2231)  Time: 1.434s,  711.27/s  (1.434s,  711.45/s)  LR: 1.004e-04  Data: 0.012 (0.014)
Train: 4 [ 800/1256 ( 64%)]  Loss:  6.076040 (6.2144)  Time: 1.426s,  715.46/s  (1.434s,  711.32/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 850/1256 ( 68%)]  Loss:  6.270090 (6.2175)  Time: 1.425s,  715.57/s  (1.434s,  711.48/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 900/1256 ( 72%)]  Loss:  6.291149 (6.2214)  Time: 1.427s,  714.74/s  (1.433s,  711.55/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [ 950/1256 ( 76%)]  Loss:  5.752514 (6.1979)  Time: 1.430s,  713.49/s  (1.433s,  711.55/s)  LR: 1.004e-04  Data: 0.012 (0.014)
Train: 4 [1000/1256 ( 80%)]  Loss:  6.146720 (6.1955)  Time: 1.432s,  712.24/s  (1.433s,  711.64/s)  LR: 1.004e-04  Data: 0.018 (0.014)
Train: 4 [1050/1256 ( 84%)]  Loss:  6.010395 (6.1871)  Time: 1.436s,  710.47/s  (1.434s,  711.51/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [1100/1256 ( 88%)]  Loss:  6.204327 (6.1878)  Time: 1.523s,  669.84/s  (1.433s,  711.58/s)  LR: 1.004e-04  Data: 0.013 (0.014)
Train: 4 [1150/1256 ( 92%)]  Loss:  6.183990 (6.1877)  Time: 1.429s,  713.61/s  (1.434s,  711.46/s)  LR: 1.004e-04  Data: 0.015 (0.014)
Train: 4 [1200/1256 ( 96%)]  Loss:  5.938114 (6.1777)  Time: 1.432s,  712.38/s  (1.434s,  711.53/s)  LR: 1.004e-04  Data: 0.013 (0.013)
Train: 4 [1250/1256 (100%)]  Loss:  5.948476 (6.1689)  Time: 1.428s,  714.16/s  (1.434s,  711.48/s)  LR: 1.004e-04  Data: 0.012 (0.014)
Train: 4 [1255/1256 (100%)]  Loss:  6.155782 (6.1684)  Time: 1.410s,  723.54/s  (1.434s,  711.50/s)  LR: 1.004e-04  Data: 0.000 (0.014)
Test: [   0/49]  Time: 4.171 (4.171)  Loss:  4.0468 (4.0468)  Acc@1: 22.3529 (22.3529)  Acc@5: 51.0784 (51.0784)
Test: [  49/49]  Time: 0.032 (0.494)  Loss:  6.1967 (4.6801)  Acc@1:  4.1667 (15.0508)  Acc@5:  4.1667 (34.1333)
Test (EMA): [   0/49]  Time: 4.133 (4.133)  Loss:  6.8915 (6.8915)  Acc@1:  0.3922 ( 0.3922)  Acc@5:  1.5686 ( 1.5686)
Test (EMA): [  49/49]  Time: 0.034 (0.503)  Loss:  6.8675 (6.9217)  Acc@1:  0.0000 ( 0.1540)  Acc@5:  0.0000 ( 0.7319)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 0.15398768882886493)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 5 [   0/1256 (  0%)]  Loss:  6.149775 (6.1498)  Time: 1.560s,  653.98/s  (1.560s,  653.98/s)  LR: 1.254e-04  Data: 0.027 (0.027)
Train: 5 [  50/1256 (  4%)]  Loss:  6.164434 (6.1571)  Time: 1.418s,  719.56/s  (1.436s,  710.13/s)  LR: 1.254e-04  Data: 0.012 (0.014)
Train: 5 [ 100/1256 (  8%)]  Loss:  6.050097 (6.1214)  Time: 1.449s,  703.74/s  (1.436s,  710.20/s)  LR: 1.254e-04  Data: 0.012 (0.014)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 5 [ 150/1256 ( 12%)]  Loss:  5.932175 (6.0741)  Time: 1.426s,  715.25/s  (1.433s,  711.66/s)  LR: 1.254e-04  Data: 0.015 (0.014)
Train: 5 [ 200/1256 ( 16%)]  Loss:  6.090014 (6.0773)  Time: 1.521s,  670.42/s  (1.434s,  711.08/s)  LR: 1.254e-04  Data: 0.013 (0.014)
Train: 5 [ 250/1256 ( 20%)]  Loss:  6.020948 (6.0679)  Time: 1.425s,  715.79/s  (1.433s,  711.74/s)  LR: 1.254e-04  Data: 0.012 (0.014)
Train: 5 [ 300/1256 ( 24%)]  Loss:  5.981337 (6.0555)  Time: 1.535s,  664.66/s  (1.433s,  711.74/s)  LR: 1.254e-04  Data: 0.016 (0.013)
Train: 5 [ 350/1256 ( 28%)]  Loss:  6.072930 (6.0577)  Time: 1.427s,  714.69/s  (1.433s,  711.80/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 400/1256 ( 32%)]  Loss:  6.156101 (6.0686)  Time: 1.427s,  714.65/s  (1.433s,  712.01/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 450/1256 ( 36%)]  Loss:  6.036649 (6.0654)  Time: 1.427s,  714.91/s  (1.433s,  711.75/s)  LR: 1.254e-04  Data: 0.012 (0.013)
Train: 5 [ 500/1256 ( 40%)]  Loss:  6.085310 (6.0673)  Time: 1.432s,  712.07/s  (1.433s,  711.95/s)  LR: 1.254e-04  Data: 0.015 (0.013)
Train: 5 [ 550/1256 ( 44%)]  Loss:  5.819271 (6.0466)  Time: 1.427s,  715.02/s  (1.433s,  711.71/s)  LR: 1.254e-04  Data: 0.015 (0.013)
Train: 5 [ 600/1256 ( 48%)]  Loss:  5.857807 (6.0321)  Time: 1.425s,  716.03/s  (1.433s,  711.81/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 650/1256 ( 52%)]  Loss:  5.854812 (6.0194)  Time: 1.427s,  714.60/s  (1.433s,  711.76/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 700/1256 ( 56%)]  Loss:  5.853770 (6.0084)  Time: 1.424s,  716.53/s  (1.433s,  711.91/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 750/1256 ( 60%)]  Loss:  5.701352 (5.9892)  Time: 1.427s,  714.69/s  (1.433s,  711.71/s)  LR: 1.254e-04  Data: 0.016 (0.013)
Train: 5 [ 800/1256 ( 64%)]  Loss:  6.017849 (5.9909)  Time: 1.426s,  715.43/s  (1.433s,  711.85/s)  LR: 1.254e-04  Data: 0.012 (0.013)
Train: 5 [ 850/1256 ( 68%)]  Loss:  5.750165 (5.9775)  Time: 1.433s,  711.84/s  (1.433s,  711.92/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 900/1256 ( 72%)]  Loss:  5.632647 (5.9593)  Time: 1.425s,  716.00/s  (1.433s,  711.89/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [ 950/1256 ( 76%)]  Loss:  5.828105 (5.9528)  Time: 1.429s,  713.75/s  (1.433s,  711.93/s)  LR: 1.254e-04  Data: 0.013 (0.013)
Train: 5 [1000/1256 ( 80%)]  Loss:  5.900319 (5.9503)  Time: 1.428s,  714.22/s  (1.433s,  711.80/s)  LR: 1.254e-04  Data: 0.014 (0.013)
Train: 5 [1050/1256 ( 84%)]  Loss:  5.773494 (5.9422)  Time: 1.455s,  700.98/s  (1.433s,  711.82/s)  LR: 1.254e-04  Data: 0.012 (0.013)
Train: 5 [1100/1256 ( 88%)]  Loss:  5.894748 (5.9402)  Time: 1.429s,  713.89/s  (1.433s,  711.81/s)  LR: 1.254e-04  Data: 0.012 (0.013)
Train: 5 [1150/1256 ( 92%)]  Loss:  5.987781 (5.9422)  Time: 1.426s,  715.06/s  (1.433s,  711.87/s)  LR: 1.254e-04  Data: 0.012 (0.013)
Train: 5 [1200/1256 ( 96%)]  Loss:  5.644400 (5.9303)  Time: 1.429s,  713.92/s  (1.433s,  711.82/s)  LR: 1.254e-04  Data: 0.016 (0.013)
Train: 5 [1250/1256 (100%)]  Loss:  5.678605 (5.9206)  Time: 1.428s,  714.08/s  (1.433s,  711.82/s)  LR: 1.254e-04  Data: 0.013 (0.014)
Train: 5 [1255/1256 (100%)]  Loss:  6.069662 (5.9261)  Time: 1.412s,  722.14/s  (1.433s,  711.80/s)  LR: 1.254e-04  Data: 0.000 (0.014)
Test: [   0/49]  Time: 4.194 (4.194)  Loss:  3.4402 (3.4402)  Acc@1: 30.4902 (30.4902)  Acc@5: 63.2353 (63.2353)
Test: [  49/49]  Time: 0.032 (0.499)  Loss:  5.4664 (4.1388)  Acc@1:  4.1667 (21.8103)  Acc@5: 16.6667 (44.1505)
Test (EMA): [   0/49]  Time: 4.594 (4.594)  Loss:  6.8599 (6.8599)  Acc@1:  0.3922 ( 0.3922)  Acc@5:  1.8627 ( 1.8627)
Test (EMA): [  49/49]  Time: 0.032 (0.514)  Loss:  6.9010 (6.9041)  Acc@1:  0.0000 ( 0.2180)  Acc@5:  0.0000 ( 0.8779)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 0.21798257910221178)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 0.15398768882886493)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 6 [   0/1256 (  0%)]  Loss:  5.861345 (5.8613)  Time: 1.442s,  707.31/s  (1.442s,  707.31/s)  LR: 1.504e-04  Data: 0.033 (0.033)
Train: 6 [  50/1256 (  4%)]  Loss:  5.856215 (5.8588)  Time: 1.429s,  713.74/s  (1.435s,  710.79/s)  LR: 1.504e-04  Data: 0.013 (0.015)
Train: 6 [ 100/1256 (  8%)]  Loss:  5.774624 (5.8307)  Time: 1.432s,  712.21/s  (1.432s,  712.36/s)  LR: 1.504e-04  Data: 0.013 (0.014)
Train: 6 [ 150/1256 ( 12%)]  Loss:  5.753080 (5.8113)  Time: 1.432s,  712.26/s  (1.435s,  710.97/s)  LR: 1.504e-04  Data: 0.013 (0.014)
Train: 6 [ 200/1256 ( 16%)]  Loss:  5.906599 (5.8304)  Time: 1.522s,  670.02/s  (1.434s,  711.28/s)  LR: 1.504e-04  Data: 0.015 (0.014)
Train: 6 [ 250/1256 ( 20%)]  Loss:  5.788529 (5.8234)  Time: 1.428s,  714.45/s  (1.434s,  711.46/s)  LR: 1.504e-04  Data: 0.013 (0.014)
Train: 6 [ 300/1256 ( 24%)]  Loss:  5.905704 (5.8352)  Time: 1.431s,  713.02/s  (1.433s,  711.56/s)  LR: 1.504e-04  Data: 0.014 (0.014)
Train: 6 [ 350/1256 ( 28%)]  Loss:  6.039079 (5.8606)  Time: 1.444s,  706.44/s  (1.433s,  711.80/s)  LR: 1.504e-04  Data: 0.012 (0.013)
Train: 6 [ 400/1256 ( 32%)]  Loss:  6.050745 (5.8818)  Time: 1.427s,  714.87/s  (1.433s,  711.72/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [ 450/1256 ( 36%)]  Loss:  5.784959 (5.8721)  Time: 1.426s,  715.24/s  (1.433s,  711.97/s)  LR: 1.504e-04  Data: 0.012 (0.013)
Train: 6 [ 500/1256 ( 40%)]  Loss:  5.734667 (5.8596)  Time: 1.427s,  714.97/s  (1.433s,  711.69/s)  LR: 1.504e-04  Data: 0.017 (0.013)
Train: 6 [ 550/1256 ( 44%)]  Loss:  5.702737 (5.8465)  Time: 1.428s,  714.22/s  (1.433s,  711.79/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [ 600/1256 ( 48%)]  Loss:  5.696835 (5.8350)  Time: 1.428s,  714.37/s  (1.433s,  711.59/s)  LR: 1.504e-04  Data: 0.016 (0.013)
Train: 6 [ 650/1256 ( 52%)]  Loss:  5.859115 (5.8367)  Time: 1.434s,  711.07/s  (1.433s,  711.67/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [ 700/1256 ( 56%)]  Loss:  5.599269 (5.8209)  Time: 1.424s,  716.49/s  (1.433s,  711.59/s)  LR: 1.504e-04  Data: 0.014 (0.013)
Train: 6 [ 750/1256 ( 60%)]  Loss:  5.813281 (5.8204)  Time: 1.439s,  708.75/s  (1.433s,  711.70/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [ 800/1256 ( 64%)]  Loss:  5.761152 (5.8169)  Time: 1.433s,  711.73/s  (1.433s,  711.77/s)  LR: 1.504e-04  Data: 0.016 (0.013)
Train: 6 [ 850/1256 ( 68%)]  Loss:  5.651383 (5.8077)  Time: 1.426s,  715.35/s  (1.433s,  711.75/s)  LR: 1.504e-04  Data: 0.014 (0.013)
Train: 6 [ 900/1256 ( 72%)]  Loss:  5.631954 (5.7985)  Time: 1.429s,  713.94/s  (1.433s,  711.84/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [ 950/1256 ( 76%)]  Loss:  5.531042 (5.7851)  Time: 1.427s,  714.73/s  (1.433s,  711.80/s)  LR: 1.504e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 6 [1000/1256 ( 80%)]  Loss:  5.802788 (5.7860)  Time: 1.428s,  714.44/s  (1.433s,  711.84/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [1050/1256 ( 84%)]  Loss:  5.494107 (5.7727)  Time: 1.427s,  714.75/s  (1.433s,  711.86/s)  LR: 1.504e-04  Data: 0.012 (0.013)
Train: 6 [1100/1256 ( 88%)]  Loss:  5.502464 (5.7609)  Time: 1.425s,  715.62/s  (1.433s,  711.95/s)  LR: 1.504e-04  Data: 0.012 (0.013)
Train: 6 [1150/1256 ( 92%)]  Loss:  5.507270 (5.7504)  Time: 1.426s,  715.27/s  (1.433s,  711.87/s)  LR: 1.504e-04  Data: 0.017 (0.013)
Train: 6 [1200/1256 ( 96%)]  Loss:  5.642820 (5.7461)  Time: 1.424s,  716.06/s  (1.433s,  711.91/s)  LR: 1.504e-04  Data: 0.013 (0.013)
Train: 6 [1250/1256 (100%)]  Loss:  5.552537 (5.7386)  Time: 1.426s,  715.50/s  (1.433s,  711.82/s)  LR: 1.504e-04  Data: 0.012 (0.014)
Train: 6 [1255/1256 (100%)]  Loss:  5.866773 (5.7434)  Time: 1.409s,  723.77/s  (1.433s,  711.83/s)  LR: 1.504e-04  Data: 0.000 (0.014)
Test: [   0/49]  Time: 4.173 (4.173)  Loss:  2.6928 (2.6928)  Acc@1: 42.0588 (42.0588)  Acc@5: 73.9216 (73.9216)
Test: [  49/49]  Time: 0.033 (0.499)  Loss:  5.8846 (3.7056)  Acc@1:  4.1667 (27.4998)  Acc@5: 12.5000 (51.8319)
Test (EMA): [   0/49]  Time: 4.162 (4.162)  Loss:  6.8227 (6.8227)  Acc@1:  0.3922 ( 0.3922)  Acc@5:  2.1569 ( 2.1569)
Test (EMA): [  49/49]  Time: 0.032 (0.501)  Loss:  6.9130 (6.8843)  Acc@1:  0.0000 ( 0.2900)  Acc@5:  0.0000 ( 1.1339)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 0.2899768155757432)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 0.21798257910221178)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 0.15398768882886493)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 7 [   0/1256 (  0%)]  Loss:  5.728621 (5.7286)  Time: 1.440s,  708.46/s  (1.440s,  708.46/s)  LR: 1.753e-04  Data: 0.032 (0.032)
Train: 7 [  50/1256 (  4%)]  Loss:  5.461524 (5.5951)  Time: 1.426s,  715.43/s  (1.433s,  711.92/s)  LR: 1.753e-04  Data: 0.013 (0.014)
Train: 7 [ 100/1256 (  8%)]  Loss:  5.760892 (5.6503)  Time: 1.428s,  714.32/s  (1.435s,  710.94/s)  LR: 1.753e-04  Data: 0.019 (0.014)
Train: 7 [ 150/1256 ( 12%)]  Loss:  5.646117 (5.6493)  Time: 1.428s,  714.19/s  (1.433s,  711.97/s)  LR: 1.753e-04  Data: 0.015 (0.014)
Train: 7 [ 200/1256 ( 16%)]  Loss:  5.676487 (5.6547)  Time: 1.425s,  715.66/s  (1.433s,  711.67/s)  LR: 1.753e-04  Data: 0.015 (0.014)
Train: 7 [ 250/1256 ( 20%)]  Loss:  5.794399 (5.6780)  Time: 1.427s,  714.76/s  (1.432s,  712.12/s)  LR: 1.753e-04  Data: 0.012 (0.014)
Train: 7 [ 300/1256 ( 24%)]  Loss:  5.312988 (5.6259)  Time: 1.527s,  667.84/s  (1.433s,  711.88/s)  LR: 1.753e-04  Data: 0.013 (0.014)
Train: 7 [ 350/1256 ( 28%)]  Loss:  5.445972 (5.6034)  Time: 1.435s,  710.95/s  (1.433s,  711.85/s)  LR: 1.753e-04  Data: 0.013 (0.014)
Train: 7 [ 400/1256 ( 32%)]  Loss:  5.751937 (5.6199)  Time: 1.426s,  715.31/s  (1.433s,  711.86/s)  LR: 1.753e-04  Data: 0.012 (0.013)
Train: 7 [ 450/1256 ( 36%)]  Loss:  5.688001 (5.6267)  Time: 1.427s,  715.00/s  (1.433s,  711.73/s)  LR: 1.753e-04  Data: 0.018 (0.013)
Train: 7 [ 500/1256 ( 40%)]  Loss:  5.531581 (5.6180)  Time: 1.433s,  711.90/s  (1.433s,  711.90/s)  LR: 1.753e-04  Data: 0.012 (0.013)
Train: 7 [ 550/1256 ( 44%)]  Loss:  5.376871 (5.5979)  Time: 1.450s,  703.23/s  (1.433s,  711.66/s)  LR: 1.753e-04  Data: 0.015 (0.013)
Train: 7 [ 600/1256 ( 48%)]  Loss:  5.233987 (5.5700)  Time: 1.430s,  713.40/s  (1.433s,  711.77/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [ 650/1256 ( 52%)]  Loss:  5.741545 (5.5822)  Time: 1.442s,  707.48/s  (1.433s,  711.62/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [ 700/1256 ( 56%)]  Loss:  5.358912 (5.5673)  Time: 1.425s,  715.79/s  (1.433s,  711.75/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [ 750/1256 ( 60%)]  Loss:  5.430957 (5.5588)  Time: 1.432s,  712.35/s  (1.433s,  711.63/s)  LR: 1.753e-04  Data: 0.012 (0.013)
Train: 7 [ 800/1256 ( 64%)]  Loss:  5.346094 (5.5463)  Time: 1.419s,  718.90/s  (1.433s,  711.77/s)  LR: 1.753e-04  Data: 0.012 (0.013)
Train: 7 [ 850/1256 ( 68%)]  Loss:  5.407993 (5.5386)  Time: 1.426s,  715.39/s  (1.433s,  711.80/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [ 900/1256 ( 72%)]  Loss:  5.596258 (5.5416)  Time: 1.426s,  715.15/s  (1.433s,  711.76/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [ 950/1256 ( 76%)]  Loss:  5.566932 (5.5429)  Time: 1.431s,  712.84/s  (1.433s,  711.81/s)  LR: 1.753e-04  Data: 0.012 (0.013)
Train: 7 [1000/1256 ( 80%)]  Loss:  5.405096 (5.5363)  Time: 1.430s,  713.29/s  (1.433s,  711.67/s)  LR: 1.753e-04  Data: 0.012 (0.013)
Train: 7 [1050/1256 ( 84%)]  Loss:  5.636930 (5.5409)  Time: 1.424s,  716.20/s  (1.433s,  711.73/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [1100/1256 ( 88%)]  Loss:  5.505077 (5.5394)  Time: 1.425s,  715.73/s  (1.433s,  711.68/s)  LR: 1.753e-04  Data: 0.018 (0.013)
Train: 7 [1150/1256 ( 92%)]  Loss:  5.679628 (5.5452)  Time: 1.429s,  713.56/s  (1.433s,  711.76/s)  LR: 1.753e-04  Data: 0.016 (0.013)
Train: 7 [1200/1256 ( 96%)]  Loss:  5.510737 (5.5438)  Time: 1.428s,  714.25/s  (1.433s,  711.62/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [1250/1256 (100%)]  Loss:  5.321642 (5.5353)  Time: 1.418s,  719.35/s  (1.433s,  711.56/s)  LR: 1.753e-04  Data: 0.013 (0.013)
Train: 7 [1255/1256 (100%)]  Loss:  5.423481 (5.5311)  Time: 1.406s,  725.26/s  (1.433s,  711.56/s)  LR: 1.753e-04  Data: 0.000 (0.013)
Test: [   0/49]  Time: 3.957 (3.957)  Loss:  2.2923 (2.2923)  Acc@1: 53.8235 (53.8235)  Acc@5: 79.3137 (79.3137)
Test: [  49/49]  Time: 0.033 (0.493)  Loss:  5.4587 (3.2921)  Acc@1:  4.1667 (33.5573)  Acc@5: 16.6667 (59.1913)
Test (EMA): [   0/49]  Time: 4.301 (4.301)  Loss:  6.7834 (6.7834)  Acc@1:  0.3922 ( 0.3922)  Acc@5:  2.4510 ( 2.4510)
Test (EMA): [  49/49]  Time: 0.057 (0.506)  Loss:  6.9057 (6.8641)  Acc@1:  0.0000 ( 0.3480)  Acc@5:  0.0000 ( 1.4359)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 0.3479721813049424)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 0.2899768155757432)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 0.21798257910221178)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 0.15398768882886493)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 8 [   0/1256 (  0%)]  Loss:  5.400892 (5.4009)  Time: 1.555s,  656.02/s  (1.555s,  656.02/s)  LR: 2.003e-04  Data: 0.037 (0.037)
Train: 8 [  50/1256 (  4%)]  Loss:  5.391294 (5.3961)  Time: 1.424s,  716.10/s  (1.435s,  710.72/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [ 100/1256 (  8%)]  Loss:  5.360202 (5.3841)  Time: 1.425s,  716.02/s  (1.433s,  711.83/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [ 150/1256 ( 12%)]  Loss:  5.576168 (5.4321)  Time: 1.426s,  715.44/s  (1.434s,  711.38/s)  LR: 2.003e-04  Data: 0.012 (0.013)
Train: 8 [ 200/1256 ( 16%)]  Loss:  5.413593 (5.4284)  Time: 1.424s,  716.43/s  (1.433s,  712.04/s)  LR: 2.003e-04  Data: 0.013 (0.013)
Train: 8 [ 250/1256 ( 20%)]  Loss:  5.657641 (5.4666)  Time: 1.536s,  663.99/s  (1.433s,  711.67/s)  LR: 2.003e-04  Data: 0.013 (0.013)
Train: 8 [ 300/1256 ( 24%)]  Loss:  5.732405 (5.5046)  Time: 1.426s,  715.07/s  (1.433s,  711.91/s)  LR: 2.003e-04  Data: 0.014 (0.013)
Train: 8 [ 350/1256 ( 28%)]  Loss:  5.184546 (5.4646)  Time: 1.428s,  714.14/s  (1.432s,  712.07/s)  LR: 2.003e-04  Data: 0.016 (0.014)
Train: 8 [ 400/1256 ( 32%)]  Loss:  5.282363 (5.4443)  Time: 1.455s,  701.22/s  (1.432s,  712.04/s)  LR: 2.003e-04  Data: 0.013 (0.013)
Train: 8 [ 450/1256 ( 36%)]  Loss:  5.654113 (5.4653)  Time: 1.428s,  714.09/s  (1.432s,  712.11/s)  LR: 2.003e-04  Data: 0.013 (0.013)
Train: 8 [ 500/1256 ( 40%)]  Loss:  5.196715 (5.4409)  Time: 1.427s,  714.69/s  (1.433s,  711.95/s)  LR: 2.003e-04  Data: 0.013 (0.013)
Train: 8 [ 550/1256 ( 44%)]  Loss:  5.544059 (5.4495)  Time: 1.431s,  712.85/s  (1.432s,  712.14/s)  LR: 2.003e-04  Data: 0.015 (0.014)
Train: 8 [ 600/1256 ( 48%)]  Loss:  5.450155 (5.4495)  Time: 1.429s,  713.81/s  (1.433s,  711.95/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [ 650/1256 ( 52%)]  Loss:  5.471321 (5.4511)  Time: 1.424s,  716.45/s  (1.432s,  712.05/s)  LR: 2.003e-04  Data: 0.016 (0.014)
Train: 8 [ 700/1256 ( 56%)]  Loss:  5.582518 (5.4599)  Time: 1.425s,  715.70/s  (1.433s,  712.04/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [ 750/1256 ( 60%)]  Loss:  5.565286 (5.4665)  Time: 1.427s,  714.95/s  (1.432s,  712.09/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [ 800/1256 ( 64%)]  Loss:  5.545263 (5.4711)  Time: 1.426s,  715.51/s  (1.433s,  712.02/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [ 850/1256 ( 68%)]  Loss:  5.284136 (5.4607)  Time: 1.427s,  715.01/s  (1.432s,  712.07/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 8 [ 900/1256 ( 72%)]  Loss:  5.427030 (5.4589)  Time: 1.430s,  713.25/s  (1.432s,  712.07/s)  LR: 2.003e-04  Data: 0.012 (0.014)
Train: 8 [ 950/1256 ( 76%)]  Loss:  4.885495 (5.4303)  Time: 1.431s,  712.95/s  (1.433s,  712.04/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [1000/1256 ( 80%)]  Loss:  5.545213 (5.4357)  Time: 1.429s,  714.02/s  (1.432s,  712.08/s)  LR: 2.003e-04  Data: 0.014 (0.013)
Train: 8 [1050/1256 ( 84%)]  Loss:  5.241095 (5.4269)  Time: 1.428s,  714.27/s  (1.433s,  712.02/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [1100/1256 ( 88%)]  Loss:  5.561861 (5.4328)  Time: 1.423s,  716.60/s  (1.432s,  712.12/s)  LR: 2.003e-04  Data: 0.012 (0.014)
Train: 8 [1150/1256 ( 92%)]  Loss:  5.215911 (5.4237)  Time: 1.440s,  708.28/s  (1.433s,  712.02/s)  LR: 2.003e-04  Data: 0.015 (0.013)
Train: 8 [1200/1256 ( 96%)]  Loss:  4.782880 (5.3981)  Time: 1.428s,  714.40/s  (1.432s,  712.08/s)  LR: 2.003e-04  Data: 0.013 (0.013)
Train: 8 [1250/1256 (100%)]  Loss:  5.753475 (5.4118)  Time: 1.427s,  714.73/s  (1.433s,  712.00/s)  LR: 2.003e-04  Data: 0.013 (0.014)
Train: 8 [1255/1256 (100%)]  Loss:  5.565257 (5.4174)  Time: 1.415s,  720.81/s  (1.433s,  712.02/s)  LR: 2.003e-04  Data: 0.000 (0.014)
Test: [   0/49]  Time: 4.246 (4.246)  Loss:  2.1716 (2.1716)  Acc@1: 55.0980 (55.0980)  Acc@5: 80.5882 (80.5882)
Test: [  49/49]  Time: 0.032 (0.495)  Loss:  5.1458 (3.0050)  Acc@1:  8.3333 (38.2409)  Acc@5: 16.6667 (63.6609)
Test (EMA): [   0/49]  Time: 4.161 (4.161)  Loss:  6.7295 (6.7295)  Acc@1:  0.3922 ( 0.3922)  Acc@5:  3.1373 ( 3.1373)
Test (EMA): [  49/49]  Time: 0.048 (0.497)  Loss:  6.8767 (6.8412)  Acc@1:  0.0000 ( 0.4600)  Acc@5:  0.0000 ( 1.7999)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 0.45996322750487373)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 0.3479721813049424)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 0.2899768155757432)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 0.21798257910221178)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 0.15398768882886493)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 9 [   0/1256 (  0%)]  Loss:  5.669160 (5.6692)  Time: 1.608s,  634.36/s  (1.608s,  634.36/s)  LR: 2.253e-04  Data: 0.028 (0.028)
Train: 9 [  50/1256 (  4%)]  Loss:  5.390180 (5.5297)  Time: 1.430s,  713.07/s  (1.433s,  711.62/s)  LR: 2.253e-04  Data: 0.016 (0.014)
Train: 9 [ 100/1256 (  8%)]  Loss:  5.529502 (5.5296)  Time: 1.425s,  715.73/s  (1.436s,  710.11/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 150/1256 ( 12%)]  Loss:  5.195457 (5.4461)  Time: 1.430s,  713.41/s  (1.435s,  710.77/s)  LR: 2.253e-04  Data: 0.012 (0.014)
Train: 9 [ 200/1256 ( 16%)]  Loss:  5.275407 (5.4119)  Time: 1.427s,  714.59/s  (1.435s,  710.78/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 250/1256 ( 20%)]  Loss:  5.212801 (5.3788)  Time: 1.432s,  712.34/s  (1.434s,  711.09/s)  LR: 2.253e-04  Data: 0.021 (0.014)
Train: 9 [ 300/1256 ( 24%)]  Loss:  5.604970 (5.4111)  Time: 1.586s,  643.00/s  (1.435s,  711.02/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 350/1256 ( 28%)]  Loss:  5.321782 (5.3999)  Time: 1.429s,  713.70/s  (1.434s,  711.29/s)  LR: 2.253e-04  Data: 0.012 (0.014)
Train: 9 [ 400/1256 ( 32%)]  Loss:  5.341384 (5.3934)  Time: 1.535s,  664.52/s  (1.434s,  711.35/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 450/1256 ( 36%)]  Loss:  5.682356 (5.4223)  Time: 1.426s,  715.20/s  (1.434s,  711.23/s)  LR: 2.253e-04  Data: 0.012 (0.014)
Train: 9 [ 500/1256 ( 40%)]  Loss:  5.399339 (5.4202)  Time: 1.428s,  714.17/s  (1.434s,  711.34/s)  LR: 2.253e-04  Data: 0.016 (0.014)
Train: 9 [ 550/1256 ( 44%)]  Loss:  4.976686 (5.3833)  Time: 1.430s,  713.16/s  (1.434s,  711.27/s)  LR: 2.253e-04  Data: 0.016 (0.014)
Train: 9 [ 600/1256 ( 48%)]  Loss:  5.243082 (5.3725)  Time: 1.425s,  715.68/s  (1.434s,  711.44/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 650/1256 ( 52%)]  Loss:  5.525504 (5.3834)  Time: 1.437s,  709.71/s  (1.434s,  711.35/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 700/1256 ( 56%)]  Loss:  4.987133 (5.3570)  Time: 1.428s,  714.47/s  (1.433s,  711.56/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 750/1256 ( 60%)]  Loss:  5.135707 (5.3432)  Time: 1.430s,  713.53/s  (1.434s,  711.44/s)  LR: 2.253e-04  Data: 0.012 (0.014)
Train: 9 [ 800/1256 ( 64%)]  Loss:  5.002046 (5.3231)  Time: 1.442s,  707.37/s  (1.434s,  711.44/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [ 850/1256 ( 68%)]  Loss:  5.262434 (5.3197)  Time: 1.427s,  715.00/s  (1.434s,  711.43/s)  LR: 2.253e-04  Data: 0.014 (0.014)
Train: 9 [ 900/1256 ( 72%)]  Loss:  5.221576 (5.3146)  Time: 1.427s,  714.88/s  (1.434s,  711.38/s)  LR: 2.253e-04  Data: 0.017 (0.014)
Train: 9 [ 950/1256 ( 76%)]  Loss:  4.794752 (5.2886)  Time: 1.430s,  713.28/s  (1.434s,  711.41/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [1000/1256 ( 80%)]  Loss:  5.303646 (5.2893)  Time: 1.428s,  714.22/s  (1.434s,  711.43/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [1050/1256 ( 84%)]  Loss:  5.107320 (5.2810)  Time: 1.429s,  713.98/s  (1.434s,  711.54/s)  LR: 2.253e-04  Data: 0.012 (0.014)
Train: 9 [1100/1256 ( 88%)]  Loss:  5.436162 (5.2878)  Time: 1.430s,  713.18/s  (1.434s,  711.48/s)  LR: 2.253e-04  Data: 0.014 (0.014)
Train: 9 [1150/1256 ( 92%)]  Loss:  4.975407 (5.2747)  Time: 1.424s,  716.05/s  (1.434s,  711.51/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [1200/1256 ( 96%)]  Loss:  4.730353 (5.2530)  Time: 1.443s,  706.89/s  (1.434s,  711.43/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [1250/1256 (100%)]  Loss:  4.939126 (5.2409)  Time: 1.442s,  707.26/s  (1.434s,  711.40/s)  LR: 2.253e-04  Data: 0.013 (0.014)
Train: 9 [1255/1256 (100%)]  Loss:  5.051936 (5.2339)  Time: 1.409s,  724.06/s  (1.434s,  711.41/s)  LR: 2.253e-04  Data: 0.000 (0.014)
Test: [   0/49]  Time: 3.945 (3.945)  Loss:  1.8317 (1.8317)  Acc@1: 61.4706 (61.4706)  Acc@5: 83.9216 (83.9216)
Test: [  49/49]  Time: 0.040 (0.494)  Loss:  4.7403 (2.7488)  Acc@1: 12.5000 (42.6306)  Acc@5: 29.1667 (68.3845)
Test (EMA): [   0/49]  Time: 4.416 (4.416)  Loss:  6.6328 (6.6328)  Acc@1:  0.6863 ( 0.6863)  Acc@5:  5.0980 ( 5.0980)
Test (EMA): [  49/49]  Time: 0.046 (0.501)  Loss:  6.8152 (6.8090)  Acc@1:  0.0000 ( 0.5520)  Acc@5:  0.0000 ( 2.2318)
Current checkpoints:
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 0.5519558772004878)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 0.45996322750487373)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 0.3479721813049424)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 0.2899768155757432)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 0.21798257910221178)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 0.15398768882886493)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 0.13398929110871022)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 0.11799056709656648)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 0.11599072720296726)
 ('./output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.10999120691425085)

Train: 10 [   0/1256 (  0%)]  Loss:  5.169493 (5.1695)  Time: 1.650s,  618.20/s  (1.650s,  618.20/s)  LR: 2.503e-04  Data: 0.030 (0.030)
Train: 10 [  50/1256 (  4%)]  Loss:  5.253652 (5.2116)  Time: 1.424s,  716.27/s  (1.439s,  708.77/s)  LR: 2.503e-04  Data: 0.013 (0.014)
Train: 10 [ 100/1256 (  8%)]  Loss:  5.261569 (5.2282)  Time: 1.426s,  715.12/s  (1.437s,  709.64/s)  LR: 2.503e-04  Data: 0.012 (0.014)
Train: 10 [ 150/1256 ( 12%)]  Loss:  5.316674 (5.2503)  Time: 1.427s,  714.78/s  (1.439s,  709.05/s)  LR: 2.503e-04  Data: 0.013 (0.014)
Train: 10 [ 200/1256 ( 16%)]  Loss:  5.659289 (5.3321)  Time: 1.446s,  705.39/s  (1.437s,  709.92/s)  LR: 2.503e-04  Data: 0.015 (0.014)
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 6
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Model swin_tiny_patch4_window7_224 created, param count:27110704
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Using NVIDIA APEX AMP. Training in mixed precision.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Restoring AMP loss scaler state from checkpoint...
Loaded checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar' (epoch 9)
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Using NVIDIA APEX DistributedDataParallel.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Loaded state_dict_ema from checkpoint 'output/train/20220303-133959-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar'
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Scheduled epochs: 310
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Train: 10 [   0/1251 (  0%)]  Loss:  5.218002 (5.2180)  Time: 11.868s,   86.28/s  (11.868s,   86.28/s)  LR: 2.503e-04  Data: 2.711 (2.711)
Train: 10 [  50/1251 (  4%)]  Loss:  5.351936 (5.2850)  Time: 1.099s,  932.02/s  (1.332s,  768.84/s)  LR: 2.503e-04  Data: 0.012 (0.066)
Train: 10 [ 100/1251 (  8%)]  Loss:  5.146826 (5.2389)  Time: 1.120s,  914.50/s  (1.225s,  835.92/s)  LR: 2.503e-04  Data: 0.010 (0.039)
Train: 10 [ 150/1251 ( 12%)]  Loss:  4.780845 (5.1244)  Time: 1.122s,  912.75/s  (1.189s,  861.48/s)  LR: 2.503e-04  Data: 0.013 (0.030)
Train: 10 [ 200/1251 ( 16%)]  Loss:  5.507493 (5.2010)  Time: 1.118s,  915.62/s  (1.171s,  874.38/s)  LR: 2.503e-04  Data: 0.015 (0.026)
Train: 10 [ 250/1251 ( 20%)]  Loss:  5.400526 (5.2343)  Time: 1.115s,  918.59/s  (1.159s,  883.59/s)  LR: 2.503e-04  Data: 0.011 (0.023)
Train: 10 [ 300/1251 ( 24%)]  Loss:  5.300117 (5.2437)  Time: 1.092s,  937.74/s  (1.151s,  889.97/s)  LR: 2.503e-04  Data: 0.009 (0.021)
Train: 10 [ 350/1251 ( 28%)]  Loss:  4.789017 (5.1868)  Time: 1.122s,  912.48/s  (1.145s,  894.19/s)  LR: 2.503e-04  Data: 0.012 (0.020)
Train: 10 [ 400/1251 ( 32%)]  Loss:  5.197515 (5.1880)  Time: 1.098s,  932.48/s  (1.141s,  897.46/s)  LR: 2.503e-04  Data: 0.011 (0.019)
Train: 10 [ 450/1251 ( 36%)]  Loss:  5.208580 (5.1901)  Time: 1.099s,  931.77/s  (1.138s,  899.69/s)  LR: 2.503e-04  Data: 0.013 (0.018)
Train: 10 [ 500/1251 ( 40%)]  Loss:  5.018563 (5.1745)  Time: 1.099s,  931.94/s  (1.135s,  902.12/s)  LR: 2.503e-04  Data: 0.016 (0.017)
Train: 10 [ 550/1251 ( 44%)]  Loss:  5.150879 (5.1725)  Time: 1.121s,  913.74/s  (1.133s,  903.95/s)  LR: 2.503e-04  Data: 0.010 (0.017)
Train: 10 [ 600/1251 ( 48%)]  Loss:  4.796270 (5.1436)  Time: 1.096s,  934.10/s  (1.131s,  905.53/s)  LR: 2.503e-04  Data: 0.012 (0.016)
Train: 10 [ 650/1251 ( 52%)]  Loss:  5.218146 (5.1489)  Time: 1.096s,  934.33/s  (1.129s,  907.10/s)  LR: 2.503e-04  Data: 0.011 (0.016)
Train: 10 [ 700/1251 ( 56%)]  Loss:  5.321565 (5.1604)  Time: 1.099s,  932.03/s  (1.127s,  908.69/s)  LR: 2.503e-04  Data: 0.011 (0.016)
Train: 10 [ 750/1251 ( 60%)]  Loss:  4.988910 (5.1497)  Time: 1.102s,  928.85/s  (1.125s,  909.90/s)  LR: 2.503e-04  Data: 0.013 (0.016)
Train: 10 [ 800/1251 ( 64%)]  Loss:  4.746300 (5.1260)  Time: 1.121s,  913.17/s  (1.124s,  910.89/s)  LR: 2.503e-04  Data: 0.011 (0.015)
Train: 10 [ 850/1251 ( 68%)]  Loss:  5.069251 (5.1228)  Time: 1.098s,  932.33/s  (1.123s,  911.68/s)  LR: 2.503e-04  Data: 0.011 (0.015)
Train: 10 [ 900/1251 ( 72%)]  Loss:  5.311387 (5.1327)  Time: 1.103s,  928.67/s  (1.123s,  912.24/s)  LR: 2.503e-04  Data: 0.010 (0.015)
Train: 10 [ 950/1251 ( 76%)]  Loss:  4.844192 (5.1183)  Time: 1.103s,  928.09/s  (1.122s,  912.70/s)  LR: 2.503e-04  Data: 0.011 (0.015)
Train: 10 [1000/1251 ( 80%)]  Loss:  4.944832 (5.1101)  Time: 1.125s,  910.61/s  (1.122s,  912.88/s)  LR: 2.503e-04  Data: 0.014 (0.014)
Train: 10 [1050/1251 ( 84%)]  Loss:  4.977633 (5.1040)  Time: 1.096s,  934.20/s  (1.121s,  913.11/s)  LR: 2.503e-04  Data: 0.011 (0.014)
Train: 10 [1100/1251 ( 88%)]  Loss:  4.792562 (5.0905)  Time: 1.096s,  933.96/s  (1.121s,  913.32/s)  LR: 2.503e-04  Data: 0.010 (0.014)
Train: 10 [1150/1251 ( 92%)]  Loss:  5.253674 (5.0973)  Time: 1.099s,  931.96/s  (1.120s,  914.04/s)  LR: 2.503e-04  Data: 0.013 (0.014)
Train: 10 [1200/1251 ( 96%)]  Loss:  5.178942 (5.1006)  Time: 1.099s,  931.55/s  (1.120s,  914.56/s)  LR: 2.503e-04  Data: 0.012 (0.014)
Train: 10 [1250/1251 (100%)]  Loss:  5.145200 (5.1023)  Time: 1.104s,  927.32/s  (1.119s,  914.92/s)  LR: 2.503e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 4.903 (4.903)  Loss:  1.6913 (1.6913)  Acc@1: 65.1367 (65.1367)  Acc@5: 86.1328 (86.1328)
Test: [  48/48]  Time: 2.545 (0.495)  Loss:  1.6089 (2.5683)  Acc@1: 68.6321 (45.9300)  Acc@5: 82.9009 (71.3320)
Test (EMA): [   0/48]  Time: 3.218 (3.218)  Loss:  5.8732 (5.8732)  Acc@1:  4.1992 ( 4.1992)  Acc@5: 17.8711 (17.8711)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  5.3018 (6.1212)  Acc@1: 11.5566 ( 2.6220)  Acc@5: 31.1321 ( 9.7940)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 11 [   0/1251 (  0%)]  Loss:  4.952101 (4.9521)  Time: 1.121s,  913.32/s  (1.121s,  913.32/s)  LR: 2.752e-04  Data: 0.031 (0.031)
Train: 11 [  50/1251 (  4%)]  Loss:  4.856498 (4.9043)  Time: 1.121s,  913.21/s  (1.115s,  918.77/s)  LR: 2.752e-04  Data: 0.010 (0.012)
Train: 11 [ 100/1251 (  8%)]  Loss:  5.362723 (5.0571)  Time: 1.095s,  935.09/s  (1.109s,  923.42/s)  LR: 2.752e-04  Data: 0.011 (0.012)
Train: 11 [ 150/1251 ( 12%)]  Loss:  5.008529 (5.0450)  Time: 1.100s,  930.56/s  (1.107s,  925.26/s)  LR: 2.752e-04  Data: 0.011 (0.011)
Train: 11 [ 200/1251 ( 16%)]  Loss:  5.358832 (5.1077)  Time: 1.097s,  933.24/s  (1.108s,  924.02/s)  LR: 2.752e-04  Data: 0.011 (0.011)
Train: 11 [ 250/1251 ( 20%)]  Loss:  4.920660 (5.0766)  Time: 1.097s,  933.35/s  (1.107s,  924.65/s)  LR: 2.752e-04  Data: 0.011 (0.011)
Train: 11 [ 300/1251 ( 24%)]  Loss:  4.940193 (5.0571)  Time: 1.098s,  932.46/s  (1.108s,  923.91/s)  LR: 2.752e-04  Data: 0.013 (0.011)
Train: 11 [ 350/1251 ( 28%)]  Loss:  5.088918 (5.0611)  Time: 1.097s,  933.66/s  (1.107s,  924.71/s)  LR: 2.752e-04  Data: 0.013 (0.011)
Train: 11 [ 400/1251 ( 32%)]  Loss:  5.327087 (5.0906)  Time: 1.103s,  928.53/s  (1.108s,  924.16/s)  LR: 2.752e-04  Data: 0.012 (0.011)
Train: 11 [ 450/1251 ( 36%)]  Loss:  5.312559 (5.1128)  Time: 1.098s,  932.99/s  (1.107s,  924.88/s)  LR: 2.752e-04  Data: 0.011 (0.012)
Train: 11 [ 500/1251 ( 40%)]  Loss:  4.755405 (5.0803)  Time: 1.101s,  929.94/s  (1.108s,  924.52/s)  LR: 2.752e-04  Data: 0.012 (0.012)
Train: 11 [ 550/1251 ( 44%)]  Loss:  5.173426 (5.0881)  Time: 1.096s,  934.55/s  (1.107s,  925.06/s)  LR: 2.752e-04  Data: 0.011 (0.012)
Train: 11 [ 600/1251 ( 48%)]  Loss:  4.872132 (5.0715)  Time: 1.118s,  916.07/s  (1.108s,  924.36/s)  LR: 2.752e-04  Data: 0.009 (0.011)
Train: 11 [ 650/1251 ( 52%)]  Loss:  4.900634 (5.0593)  Time: 1.102s,  928.98/s  (1.108s,  924.50/s)  LR: 2.752e-04  Data: 0.010 (0.011)
Train: 11 [ 700/1251 ( 56%)]  Loss:  5.363518 (5.0795)  Time: 1.101s,  930.04/s  (1.108s,  924.60/s)  LR: 2.752e-04  Data: 0.011 (0.012)
Train: 11 [ 750/1251 ( 60%)]  Loss:  4.681301 (5.0547)  Time: 1.099s,  931.89/s  (1.108s,  923.78/s)  LR: 2.752e-04  Data: 0.011 (0.011)
Train: 11 [ 800/1251 ( 64%)]  Loss:  4.743895 (5.0364)  Time: 1.098s,  932.61/s  (1.109s,  923.61/s)  LR: 2.752e-04  Data: 0.012 (0.011)
Train: 11 [ 850/1251 ( 68%)]  Loss:  4.557571 (5.0098)  Time: 1.101s,  930.42/s  (1.109s,  923.56/s)  LR: 2.752e-04  Data: 0.013 (0.011)
Train: 11 [ 900/1251 ( 72%)]  Loss:  5.359394 (5.0282)  Time: 1.117s,  917.08/s  (1.108s,  923.89/s)  LR: 2.752e-04  Data: 0.011 (0.011)
Train: 11 [ 950/1251 ( 76%)]  Loss:  5.062547 (5.0299)  Time: 1.104s,  927.51/s  (1.108s,  923.99/s)  LR: 2.752e-04  Data: 0.011 (0.012)
Train: 11 [1000/1251 ( 80%)]  Loss:  4.759414 (5.0170)  Time: 1.103s,  928.32/s  (1.108s,  924.26/s)  LR: 2.752e-04  Data: 0.014 (0.012)
Train: 11 [1050/1251 ( 84%)]  Loss:  4.948925 (5.0139)  Time: 1.099s,  931.48/s  (1.108s,  924.23/s)  LR: 2.752e-04  Data: 0.010 (0.012)
Train: 11 [1100/1251 ( 88%)]  Loss:  4.913055 (5.0095)  Time: 1.149s,  891.55/s  (1.108s,  924.29/s)  LR: 2.752e-04  Data: 0.012 (0.012)
Train: 11 [1150/1251 ( 92%)]  Loss:  4.915973 (5.0056)  Time: 1.099s,  932.15/s  (1.108s,  924.22/s)  LR: 2.752e-04  Data: 0.012 (0.012)
Train: 11 [1200/1251 ( 96%)]  Loss:  5.085998 (5.0089)  Time: 1.101s,  930.42/s  (1.108s,  924.47/s)  LR: 2.752e-04  Data: 0.011 (0.012)
Train: 11 [1250/1251 (100%)]  Loss:  5.211983 (5.0167)  Time: 1.091s,  938.65/s  (1.108s,  924.53/s)  LR: 2.752e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.213 (3.213)  Loss:  1.5436 (1.5436)  Acc@1: 68.0664 (68.0664)  Acc@5: 87.9883 (87.9883)
Test: [  48/48]  Time: 0.229 (0.413)  Loss:  1.5013 (2.4209)  Acc@1: 70.1651 (48.8240)  Acc@5: 86.3208 (74.0280)
Test (EMA): [   0/48]  Time: 3.163 (3.163)  Loss:  5.6754 (5.6754)  Acc@1:  4.6875 ( 4.6875)  Acc@5: 20.3125 (20.3125)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  5.0217 (5.9575)  Acc@1: 14.9764 ( 3.4560)  Acc@5: 36.2028 (12.1360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 12 [   0/1251 (  0%)]  Loss:  5.009710 (5.0097)  Time: 1.116s,  917.88/s  (1.116s,  917.88/s)  LR: 3.002e-04  Data: 0.027 (0.027)
Train: 12 [  50/1251 (  4%)]  Loss:  4.644617 (4.8272)  Time: 1.097s,  933.39/s  (1.102s,  928.93/s)  LR: 3.002e-04  Data: 0.012 (0.012)
Train: 12 [ 100/1251 (  8%)]  Loss:  4.731829 (4.7954)  Time: 1.100s,  930.77/s  (1.111s,  921.62/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 150/1251 ( 12%)]  Loss:  4.949634 (4.8339)  Time: 1.096s,  934.18/s  (1.111s,  922.08/s)  LR: 3.002e-04  Data: 0.010 (0.011)
Train: 12 [ 200/1251 ( 16%)]  Loss:  4.790104 (4.8252)  Time: 1.104s,  927.26/s  (1.109s,  923.11/s)  LR: 3.002e-04  Data: 0.013 (0.011)
Train: 12 [ 250/1251 ( 20%)]  Loss:  4.758956 (4.8141)  Time: 1.104s,  927.69/s  (1.110s,  922.80/s)  LR: 3.002e-04  Data: 0.011 (0.011)
Train: 12 [ 300/1251 ( 24%)]  Loss:  5.356327 (4.8916)  Time: 1.103s,  928.72/s  (1.108s,  924.11/s)  LR: 3.002e-04  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 12 [ 350/1251 ( 28%)]  Loss:  5.032217 (4.9092)  Time: 1.096s,  934.47/s  (1.107s,  924.66/s)  LR: 3.002e-04  Data: 0.010 (0.011)
Train: 12 [ 400/1251 ( 32%)]  Loss:  4.688066 (4.8846)  Time: 1.123s,  911.95/s  (1.107s,  924.99/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 450/1251 ( 36%)]  Loss:  4.819521 (4.8781)  Time: 1.099s,  931.89/s  (1.108s,  924.38/s)  LR: 3.002e-04  Data: 0.011 (0.011)
Train: 12 [ 500/1251 ( 40%)]  Loss:  4.748091 (4.8663)  Time: 1.097s,  933.20/s  (1.107s,  924.97/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 550/1251 ( 44%)]  Loss:  4.877445 (4.8672)  Time: 1.119s,  915.26/s  (1.107s,  924.94/s)  LR: 3.002e-04  Data: 0.009 (0.011)
Train: 12 [ 600/1251 ( 48%)]  Loss:  4.970325 (4.8751)  Time: 1.100s,  931.29/s  (1.107s,  925.12/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 650/1251 ( 52%)]  Loss:  4.898442 (4.8768)  Time: 1.103s,  928.39/s  (1.106s,  925.62/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 700/1251 ( 56%)]  Loss:  5.121587 (4.8931)  Time: 1.121s,  913.13/s  (1.107s,  925.43/s)  LR: 3.002e-04  Data: 0.010 (0.011)
Train: 12 [ 750/1251 ( 60%)]  Loss:  4.671571 (4.8793)  Time: 1.121s,  913.15/s  (1.107s,  924.89/s)  LR: 3.002e-04  Data: 0.010 (0.011)
Train: 12 [ 800/1251 ( 64%)]  Loss:  4.670369 (4.8670)  Time: 1.119s,  915.18/s  (1.107s,  924.77/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 850/1251 ( 68%)]  Loss:  5.192366 (4.8851)  Time: 1.098s,  932.55/s  (1.107s,  925.02/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [ 900/1251 ( 72%)]  Loss:  4.604301 (4.8703)  Time: 1.109s,  923.19/s  (1.107s,  925.16/s)  LR: 3.002e-04  Data: 0.011 (0.011)
Train: 12 [ 950/1251 ( 76%)]  Loss:  4.679520 (4.8607)  Time: 1.095s,  935.04/s  (1.107s,  925.23/s)  LR: 3.002e-04  Data: 0.011 (0.011)
Train: 12 [1000/1251 ( 80%)]  Loss:  4.998520 (4.8673)  Time: 1.101s,  929.83/s  (1.107s,  925.40/s)  LR: 3.002e-04  Data: 0.010 (0.011)
Train: 12 [1050/1251 ( 84%)]  Loss:  4.935657 (4.8704)  Time: 1.101s,  930.45/s  (1.107s,  925.13/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [1100/1251 ( 88%)]  Loss:  4.860252 (4.8700)  Time: 1.098s,  932.76/s  (1.107s,  924.68/s)  LR: 3.002e-04  Data: 0.012 (0.011)
Train: 12 [1150/1251 ( 92%)]  Loss:  4.631113 (4.8600)  Time: 1.096s,  933.96/s  (1.107s,  925.00/s)  LR: 3.002e-04  Data: 0.011 (0.011)
Train: 12 [1200/1251 ( 96%)]  Loss:  4.797719 (4.8575)  Time: 1.099s,  931.45/s  (1.107s,  925.27/s)  LR: 3.002e-04  Data: 0.010 (0.011)
Train: 12 [1250/1251 (100%)]  Loss:  4.543901 (4.8455)  Time: 1.079s,  948.76/s  (1.107s,  925.29/s)  LR: 3.002e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.333 (3.333)  Loss:  1.4594 (1.4594)  Acc@1: 71.8750 (71.8750)  Acc@5: 88.9648 (88.9648)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  1.4192 (2.2676)  Acc@1: 71.1085 (51.4960)  Acc@5: 86.5566 (76.4860)
Test (EMA): [   0/48]  Time: 3.080 (3.080)  Loss:  5.4349 (5.4349)  Acc@1:  6.6406 ( 6.6406)  Acc@5: 22.3633 (22.3633)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  4.7238 (5.7628)  Acc@1: 18.3962 ( 4.7820)  Acc@5: 42.2170 (15.4140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 13 [   0/1251 (  0%)]  Loss:  4.885221 (4.8852)  Time: 1.133s,  903.43/s  (1.133s,  903.43/s)  LR: 3.252e-04  Data: 0.026 (0.026)
Train: 13 [  50/1251 (  4%)]  Loss:  4.628797 (4.7570)  Time: 1.096s,  934.53/s  (1.107s,  924.83/s)  LR: 3.252e-04  Data: 0.013 (0.012)
Train: 13 [ 100/1251 (  8%)]  Loss:  5.062023 (4.8587)  Time: 1.118s,  915.63/s  (1.108s,  923.97/s)  LR: 3.252e-04  Data: 0.010 (0.011)
Train: 13 [ 150/1251 ( 12%)]  Loss:  4.838964 (4.8538)  Time: 1.094s,  935.81/s  (1.110s,  922.57/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 200/1251 ( 16%)]  Loss:  5.144462 (4.9119)  Time: 1.097s,  933.48/s  (1.109s,  923.44/s)  LR: 3.252e-04  Data: 0.015 (0.011)
Train: 13 [ 250/1251 ( 20%)]  Loss:  4.907869 (4.9112)  Time: 1.096s,  934.07/s  (1.110s,  922.73/s)  LR: 3.252e-04  Data: 0.012 (0.011)
Train: 13 [ 300/1251 ( 24%)]  Loss:  4.742607 (4.8871)  Time: 1.099s,  931.79/s  (1.109s,  923.18/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 350/1251 ( 28%)]  Loss:  4.892180 (4.8878)  Time: 1.092s,  937.84/s  (1.108s,  924.23/s)  LR: 3.252e-04  Data: 0.009 (0.011)
Train: 13 [ 400/1251 ( 32%)]  Loss:  4.511947 (4.8460)  Time: 1.097s,  933.75/s  (1.108s,  924.58/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 450/1251 ( 36%)]  Loss:  4.676597 (4.8291)  Time: 1.097s,  933.46/s  (1.108s,  924.46/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 500/1251 ( 40%)]  Loss:  4.804211 (4.8268)  Time: 1.096s,  934.35/s  (1.108s,  924.59/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 550/1251 ( 44%)]  Loss:  5.063269 (4.8465)  Time: 1.096s,  934.29/s  (1.107s,  924.98/s)  LR: 3.252e-04  Data: 0.012 (0.011)
Train: 13 [ 600/1251 ( 48%)]  Loss:  4.569279 (4.8252)  Time: 1.101s,  930.30/s  (1.107s,  925.43/s)  LR: 3.252e-04  Data: 0.010 (0.011)
Train: 13 [ 650/1251 ( 52%)]  Loss:  4.798313 (4.8233)  Time: 1.102s,  929.26/s  (1.107s,  924.95/s)  LR: 3.252e-04  Data: 0.012 (0.011)
Train: 13 [ 700/1251 ( 56%)]  Loss:  4.938704 (4.8310)  Time: 1.096s,  934.57/s  (1.107s,  924.80/s)  LR: 3.252e-04  Data: 0.010 (0.011)
Train: 13 [ 750/1251 ( 60%)]  Loss:  4.776845 (4.8276)  Time: 1.099s,  931.71/s  (1.108s,  923.85/s)  LR: 3.252e-04  Data: 0.013 (0.011)
Train: 13 [ 800/1251 ( 64%)]  Loss:  5.241265 (4.8519)  Time: 1.108s,  924.06/s  (1.109s,  923.61/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 850/1251 ( 68%)]  Loss:  4.502186 (4.8325)  Time: 1.122s,  912.28/s  (1.109s,  923.54/s)  LR: 3.252e-04  Data: 0.012 (0.011)
Train: 13 [ 900/1251 ( 72%)]  Loss:  4.688122 (4.8249)  Time: 1.103s,  928.41/s  (1.109s,  923.69/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [ 950/1251 ( 76%)]  Loss:  5.151374 (4.8412)  Time: 1.096s,  934.10/s  (1.108s,  923.80/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [1000/1251 ( 80%)]  Loss:  4.615478 (4.8305)  Time: 1.097s,  933.48/s  (1.108s,  924.18/s)  LR: 3.252e-04  Data: 0.011 (0.011)
Train: 13 [1050/1251 ( 84%)]  Loss:  5.032495 (4.8396)  Time: 1.096s,  934.16/s  (1.109s,  923.67/s)  LR: 3.252e-04  Data: 0.012 (0.011)
Train: 13 [1100/1251 ( 88%)]  Loss:  4.342802 (4.8180)  Time: 1.096s,  933.91/s  (1.109s,  923.72/s)  LR: 3.252e-04  Data: 0.013 (0.011)
Train: 13 [1150/1251 ( 92%)]  Loss:  4.670114 (4.8119)  Time: 1.093s,  936.73/s  (1.109s,  923.65/s)  LR: 3.252e-04  Data: 0.010 (0.011)
Train: 13 [1200/1251 ( 96%)]  Loss:  4.631966 (4.8047)  Time: 1.095s,  935.30/s  (1.109s,  923.70/s)  LR: 3.252e-04  Data: 0.012 (0.011)
Train: 13 [1250/1251 (100%)]  Loss:  5.062892 (4.8146)  Time: 1.094s,  935.65/s  (1.108s,  923.84/s)  LR: 3.252e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.298 (3.298)  Loss:  1.2656 (1.2656)  Acc@1: 73.4375 (73.4375)  Acc@5: 91.5039 (91.5039)
Test: [  48/48]  Time: 0.230 (0.401)  Loss:  1.3033 (2.1765)  Acc@1: 73.4670 (53.2040)  Acc@5: 89.2689 (77.9300)
Test (EMA): [   0/48]  Time: 3.167 (3.167)  Loss:  5.1508 (5.1508)  Acc@1:  9.7656 ( 9.7656)  Acc@5: 27.3438 (27.3438)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  4.4150 (5.5356)  Acc@1: 22.8774 ( 6.7040)  Acc@5: 47.7594 (19.6520)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 14 [   0/1251 (  0%)]  Loss:  4.762897 (4.7629)  Time: 1.106s,  925.82/s  (1.106s,  925.82/s)  LR: 3.502e-04  Data: 0.024 (0.024)
Train: 14 [  50/1251 (  4%)]  Loss:  4.559227 (4.6611)  Time: 1.096s,  934.55/s  (1.116s,  917.93/s)  LR: 3.502e-04  Data: 0.013 (0.012)
Train: 14 [ 100/1251 (  8%)]  Loss:  4.466328 (4.5962)  Time: 1.097s,  933.13/s  (1.111s,  921.39/s)  LR: 3.502e-04  Data: 0.012 (0.012)
Train: 14 [ 150/1251 ( 12%)]  Loss:  4.427662 (4.5540)  Time: 1.097s,  933.47/s  (1.110s,  922.52/s)  LR: 3.502e-04  Data: 0.013 (0.012)
Train: 14 [ 200/1251 ( 16%)]  Loss:  4.768619 (4.5969)  Time: 1.120s,  914.37/s  (1.110s,  922.47/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 250/1251 ( 20%)]  Loss:  5.104727 (4.6816)  Time: 1.099s,  931.86/s  (1.111s,  921.54/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 300/1251 ( 24%)]  Loss:  4.766996 (4.6938)  Time: 1.097s,  933.52/s  (1.111s,  921.97/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 350/1251 ( 28%)]  Loss:  4.803589 (4.7075)  Time: 1.096s,  934.30/s  (1.110s,  922.62/s)  LR: 3.502e-04  Data: 0.013 (0.011)
Train: 14 [ 400/1251 ( 32%)]  Loss:  4.704217 (4.7071)  Time: 1.215s,  843.07/s  (1.110s,  922.27/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 450/1251 ( 36%)]  Loss:  5.049616 (4.7414)  Time: 1.101s,  930.12/s  (1.111s,  922.09/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 500/1251 ( 40%)]  Loss:  4.933054 (4.7588)  Time: 1.092s,  937.58/s  (1.110s,  922.90/s)  LR: 3.502e-04  Data: 0.010 (0.011)
Train: 14 [ 550/1251 ( 44%)]  Loss:  4.949516 (4.7747)  Time: 1.098s,  932.50/s  (1.110s,  922.92/s)  LR: 3.502e-04  Data: 0.013 (0.011)
Train: 14 [ 600/1251 ( 48%)]  Loss:  4.566330 (4.7587)  Time: 1.091s,  938.39/s  (1.110s,  922.20/s)  LR: 3.502e-04  Data: 0.009 (0.011)
Train: 14 [ 650/1251 ( 52%)]  Loss:  4.166671 (4.7164)  Time: 1.098s,  932.75/s  (1.110s,  922.46/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 700/1251 ( 56%)]  Loss:  4.469467 (4.6999)  Time: 1.100s,  931.27/s  (1.110s,  922.65/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 750/1251 ( 60%)]  Loss:  4.713202 (4.7008)  Time: 1.097s,  933.47/s  (1.109s,  923.00/s)  LR: 3.502e-04  Data: 0.013 (0.011)
Train: 14 [ 800/1251 ( 64%)]  Loss:  5.150892 (4.7272)  Time: 1.096s,  934.03/s  (1.109s,  923.03/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 850/1251 ( 68%)]  Loss:  4.343408 (4.7059)  Time: 1.102s,  928.87/s  (1.109s,  923.38/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 900/1251 ( 72%)]  Loss:  4.889487 (4.7156)  Time: 1.103s,  927.99/s  (1.109s,  923.43/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [ 950/1251 ( 76%)]  Loss:  4.469379 (4.7033)  Time: 1.098s,  932.54/s  (1.108s,  923.80/s)  LR: 3.502e-04  Data: 0.012 (0.011)
Train: 14 [1000/1251 ( 80%)]  Loss:  4.990561 (4.7169)  Time: 1.096s,  934.03/s  (1.108s,  923.77/s)  LR: 3.502e-04  Data: 0.012 (0.011)
Train: 14 [1050/1251 ( 84%)]  Loss:  4.897667 (4.7252)  Time: 1.098s,  932.80/s  (1.108s,  924.11/s)  LR: 3.502e-04  Data: 0.012 (0.011)
Train: 14 [1100/1251 ( 88%)]  Loss:  4.929293 (4.7340)  Time: 1.130s,  906.38/s  (1.108s,  924.33/s)  LR: 3.502e-04  Data: 0.010 (0.011)
Train: 14 [1150/1251 ( 92%)]  Loss:  4.829368 (4.7380)  Time: 1.106s,  926.17/s  (1.108s,  924.36/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [1200/1251 ( 96%)]  Loss:  5.030488 (4.7497)  Time: 1.094s,  936.25/s  (1.107s,  924.63/s)  LR: 3.502e-04  Data: 0.011 (0.011)
Train: 14 [1250/1251 (100%)]  Loss:  5.154802 (4.7653)  Time: 1.102s,  929.06/s  (1.108s,  924.43/s)  LR: 3.502e-04  Data: 0.000 (0.011)
Test: [   0/48]  Time: 3.298 (3.298)  Loss:  1.1813 (1.1813)  Acc@1: 76.0742 (76.0742)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.230 (0.405)  Loss:  1.3624 (2.0851)  Acc@1: 73.8208 (54.8580)  Acc@5: 87.6179 (79.2860)
Test (EMA): [   0/48]  Time: 3.126 (3.126)  Loss:  4.8150 (4.8150)  Acc@1: 13.5742 (13.5742)  Acc@5: 34.6680 (34.6680)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  4.0906 (5.2715)  Acc@1: 27.9481 ( 9.5180)  Acc@5: 54.5991 (24.9060)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 15 [   0/1251 (  0%)]  Loss:  4.672184 (4.6722)  Time: 1.110s,  922.46/s  (1.110s,  922.46/s)  LR: 3.751e-04  Data: 0.028 (0.028)
Train: 15 [  50/1251 (  4%)]  Loss:  4.634804 (4.6535)  Time: 1.098s,  932.82/s  (1.102s,  929.19/s)  LR: 3.751e-04  Data: 0.012 (0.012)
Train: 15 [ 100/1251 (  8%)]  Loss:  4.527666 (4.6116)  Time: 1.098s,  932.96/s  (1.109s,  923.42/s)  LR: 3.751e-04  Data: 0.012 (0.012)
Train: 15 [ 150/1251 ( 12%)]  Loss:  4.555257 (4.5975)  Time: 1.100s,  931.07/s  (1.107s,  925.42/s)  LR: 3.751e-04  Data: 0.011 (0.012)
Train: 15 [ 200/1251 ( 16%)]  Loss:  4.648118 (4.6076)  Time: 1.123s,  911.45/s  (1.107s,  924.73/s)  LR: 3.751e-04  Data: 0.011 (0.012)
Train: 15 [ 250/1251 ( 20%)]  Loss:  4.958745 (4.6661)  Time: 1.131s,  905.73/s  (1.109s,  922.95/s)  LR: 3.751e-04  Data: 0.011 (0.012)
Train: 15 [ 300/1251 ( 24%)]  Loss:  4.939749 (4.7052)  Time: 1.096s,  934.28/s  (1.111s,  921.42/s)  LR: 3.751e-04  Data: 0.012 (0.012)
Train: 15 [ 350/1251 ( 28%)]  Loss:  4.339734 (4.6595)  Time: 1.099s,  931.38/s  (1.110s,  922.79/s)  LR: 3.751e-04  Data: 0.010 (0.012)
Train: 15 [ 400/1251 ( 32%)]  Loss:  4.438958 (4.6350)  Time: 1.096s,  934.29/s  (1.109s,  923.18/s)  LR: 3.751e-04  Data: 0.010 (0.012)
Train: 15 [ 450/1251 ( 36%)]  Loss:  4.248841 (4.5964)  Time: 1.095s,  934.80/s  (1.108s,  923.78/s)  LR: 3.751e-04  Data: 0.011 (0.012)
Train: 15 [ 500/1251 ( 40%)]  Loss:  4.687335 (4.6047)  Time: 1.096s,  934.09/s  (1.108s,  924.47/s)  LR: 3.751e-04  Data: 0.012 (0.012)
Train: 15 [ 550/1251 ( 44%)]  Loss:  5.099401 (4.6459)  Time: 1.097s,  933.36/s  (1.107s,  924.94/s)  LR: 3.751e-04  Data: 0.011 (0.012)
Train: 15 [ 600/1251 ( 48%)]  Loss:  4.621400 (4.6440)  Time: 1.121s,  913.34/s  (1.107s,  924.71/s)  LR: 3.751e-04  Data: 0.010 (0.012)
Train: 15 [ 650/1251 ( 52%)]  Loss:  4.291741 (4.6189)  Time: 1.101s,  929.97/s  (1.108s,  924.09/s)  LR: 3.751e-04  Data: 0.012 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 15 [ 700/1251 ( 56%)]  Loss:  4.745620 (4.6273)  Time: 1.094s,  936.32/s  (1.108s,  924.33/s)  LR: 3.751e-04  Data: 0.009 (0.011)
Train: 15 [ 750/1251 ( 60%)]  Loss:  4.399613 (4.6131)  Time: 1.129s,  906.95/s  (1.108s,  924.24/s)  LR: 3.751e-04  Data: 0.010 (0.011)
Train: 15 [ 800/1251 ( 64%)]  Loss:  4.766577 (4.6221)  Time: 1.104s,  927.34/s  (1.108s,  924.41/s)  LR: 3.751e-04  Data: 0.010 (0.011)
Train: 15 [ 850/1251 ( 68%)]  Loss:  4.864791 (4.6356)  Time: 1.121s,  913.56/s  (1.108s,  924.54/s)  LR: 3.751e-04  Data: 0.011 (0.011)
Train: 15 [ 900/1251 ( 72%)]  Loss:  4.648100 (4.6362)  Time: 1.098s,  932.44/s  (1.107s,  924.62/s)  LR: 3.751e-04  Data: 0.013 (0.011)
Train: 15 [ 950/1251 ( 76%)]  Loss:  4.651967 (4.6370)  Time: 1.100s,  930.50/s  (1.108s,  924.59/s)  LR: 3.751e-04  Data: 0.011 (0.011)
Train: 15 [1000/1251 ( 80%)]  Loss:  4.305106 (4.6212)  Time: 1.102s,  928.98/s  (1.108s,  924.30/s)  LR: 3.751e-04  Data: 0.009 (0.011)
Train: 15 [1050/1251 ( 84%)]  Loss:  4.679482 (4.6239)  Time: 1.095s,  935.41/s  (1.108s,  924.16/s)  LR: 3.751e-04  Data: 0.011 (0.011)
Train: 15 [1100/1251 ( 88%)]  Loss:  4.603580 (4.6230)  Time: 1.096s,  934.27/s  (1.108s,  924.19/s)  LR: 3.751e-04  Data: 0.012 (0.011)
Train: 15 [1150/1251 ( 92%)]  Loss:  4.646887 (4.6240)  Time: 1.096s,  934.60/s  (1.108s,  924.36/s)  LR: 3.751e-04  Data: 0.011 (0.011)
Train: 15 [1200/1251 ( 96%)]  Loss:  4.773726 (4.6300)  Time: 1.097s,  933.56/s  (1.108s,  924.28/s)  LR: 3.751e-04  Data: 0.012 (0.011)
Train: 15 [1250/1251 (100%)]  Loss:  4.314432 (4.6178)  Time: 1.079s,  948.86/s  (1.108s,  924.25/s)  LR: 3.751e-04  Data: 0.000 (0.011)
Test: [   0/48]  Time: 3.348 (3.348)  Loss:  1.1414 (1.1414)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  1.3215 (1.9918)  Acc@1: 74.7642 (56.8280)  Acc@5: 88.0896 (80.5960)
Test (EMA): [   0/48]  Time: 3.077 (3.077)  Loss:  4.4262 (4.4262)  Acc@1: 20.4102 (20.4102)  Acc@5: 43.9453 (43.9453)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  3.7659 (4.9689)  Acc@1: 35.0236 (13.1700)  Acc@5: 59.9057 (31.1100)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 16 [   0/1251 (  0%)]  Loss:  4.673960 (4.6740)  Time: 1.110s,  922.25/s  (1.110s,  922.25/s)  LR: 4.001e-04  Data: 0.029 (0.029)
Train: 16 [  50/1251 (  4%)]  Loss:  4.409911 (4.5419)  Time: 1.100s,  930.98/s  (1.108s,  923.93/s)  LR: 4.001e-04  Data: 0.011 (0.012)
Train: 16 [ 100/1251 (  8%)]  Loss:  4.653884 (4.5793)  Time: 1.096s,  934.33/s  (1.109s,  923.57/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 16 [ 150/1251 ( 12%)]  Loss:  4.836616 (4.6436)  Time: 1.100s,  930.75/s  (1.109s,  923.39/s)  LR: 4.001e-04  Data: 0.011 (0.012)
Train: 16 [ 200/1251 ( 16%)]  Loss:  4.652932 (4.6455)  Time: 1.096s,  934.09/s  (1.107s,  925.02/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 16 [ 250/1251 ( 20%)]  Loss:  4.622135 (4.6416)  Time: 1.097s,  933.13/s  (1.108s,  923.90/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 300/1251 ( 24%)]  Loss:  4.334977 (4.5978)  Time: 1.143s,  895.75/s  (1.108s,  924.25/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 350/1251 ( 28%)]  Loss:  4.496190 (4.5851)  Time: 1.098s,  932.62/s  (1.108s,  924.52/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 400/1251 ( 32%)]  Loss:  4.380426 (4.5623)  Time: 1.097s,  933.86/s  (1.107s,  925.03/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [ 450/1251 ( 36%)]  Loss:  4.237045 (4.5298)  Time: 1.095s,  935.45/s  (1.106s,  925.60/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [ 500/1251 ( 40%)]  Loss:  4.480193 (4.5253)  Time: 1.121s,  913.30/s  (1.107s,  924.76/s)  LR: 4.001e-04  Data: 0.010 (0.011)
Train: 16 [ 550/1251 ( 44%)]  Loss:  4.519007 (4.5248)  Time: 1.098s,  932.67/s  (1.107s,  925.10/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [ 600/1251 ( 48%)]  Loss:  4.496536 (4.5226)  Time: 1.124s,  911.09/s  (1.107s,  924.83/s)  LR: 4.001e-04  Data: 0.015 (0.011)
Train: 16 [ 650/1251 ( 52%)]  Loss:  4.906742 (4.5500)  Time: 1.096s,  934.54/s  (1.107s,  925.15/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 700/1251 ( 56%)]  Loss:  4.579920 (4.5520)  Time: 1.103s,  928.30/s  (1.106s,  925.62/s)  LR: 4.001e-04  Data: 0.017 (0.011)
Train: 16 [ 750/1251 ( 60%)]  Loss:  4.587734 (4.5543)  Time: 1.099s,  931.43/s  (1.106s,  925.90/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 800/1251 ( 64%)]  Loss:  4.759059 (4.5663)  Time: 1.093s,  936.48/s  (1.106s,  925.45/s)  LR: 4.001e-04  Data: 0.010 (0.011)
Train: 16 [ 850/1251 ( 68%)]  Loss:  4.405086 (4.5574)  Time: 1.096s,  934.45/s  (1.107s,  925.09/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 900/1251 ( 72%)]  Loss:  4.352220 (4.5466)  Time: 1.197s,  855.75/s  (1.107s,  924.68/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [ 950/1251 ( 76%)]  Loss:  4.504494 (4.5445)  Time: 1.194s,  857.45/s  (1.107s,  924.84/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [1000/1251 ( 80%)]  Loss:  4.692056 (4.5515)  Time: 1.095s,  935.35/s  (1.107s,  925.23/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [1050/1251 ( 84%)]  Loss:  4.613236 (4.5543)  Time: 1.094s,  935.59/s  (1.107s,  925.37/s)  LR: 4.001e-04  Data: 0.011 (0.011)
Train: 16 [1100/1251 ( 88%)]  Loss:  4.963855 (4.5721)  Time: 1.096s,  933.92/s  (1.106s,  925.49/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [1150/1251 ( 92%)]  Loss:  4.186385 (4.5560)  Time: 1.103s,  928.02/s  (1.107s,  925.38/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [1200/1251 ( 96%)]  Loss:  4.502909 (4.5539)  Time: 1.127s,  908.81/s  (1.107s,  925.43/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 16 [1250/1251 (100%)]  Loss:  4.715933 (4.5601)  Time: 1.080s,  947.96/s  (1.107s,  925.31/s)  LR: 4.001e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.238 (3.238)  Loss:  1.1112 (1.1112)  Acc@1: 76.0742 (76.0742)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  1.1006 (1.9229)  Acc@1: 76.6509 (57.8820)  Acc@5: 91.3915 (81.4260)
Test (EMA): [   0/48]  Time: 3.075 (3.075)  Loss:  3.9794 (3.9794)  Acc@1: 27.0508 (27.0508)  Acc@5: 55.2734 (55.2734)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  3.4302 (4.6299)  Acc@1: 41.3915 (17.6400)  Acc@5: 65.6840 (38.1280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 17 [   0/1251 (  0%)]  Loss:  4.601943 (4.6019)  Time: 1.111s,  921.44/s  (1.111s,  921.44/s)  LR: 4.251e-04  Data: 0.029 (0.029)
Train: 17 [  50/1251 (  4%)]  Loss:  4.340024 (4.4710)  Time: 1.104s,  927.52/s  (1.103s,  928.32/s)  LR: 4.251e-04  Data: 0.010 (0.012)
Train: 17 [ 100/1251 (  8%)]  Loss:  4.501454 (4.4811)  Time: 1.097s,  933.16/s  (1.108s,  924.30/s)  LR: 4.251e-04  Data: 0.012 (0.012)
Train: 17 [ 150/1251 ( 12%)]  Loss:  4.341816 (4.4463)  Time: 1.097s,  933.68/s  (1.107s,  924.64/s)  LR: 4.251e-04  Data: 0.011 (0.012)
Train: 17 [ 200/1251 ( 16%)]  Loss:  4.366416 (4.4303)  Time: 1.097s,  933.29/s  (1.108s,  924.04/s)  LR: 4.251e-04  Data: 0.010 (0.011)
Train: 17 [ 250/1251 ( 20%)]  Loss:  4.640614 (4.4654)  Time: 1.097s,  933.16/s  (1.108s,  924.42/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [ 300/1251 ( 24%)]  Loss:  4.862254 (4.5221)  Time: 1.097s,  933.64/s  (1.107s,  925.44/s)  LR: 4.251e-04  Data: 0.014 (0.011)
Train: 17 [ 350/1251 ( 28%)]  Loss:  4.406490 (4.5076)  Time: 1.097s,  933.82/s  (1.107s,  925.13/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [ 400/1251 ( 32%)]  Loss:  4.399759 (4.4956)  Time: 1.096s,  934.24/s  (1.106s,  925.88/s)  LR: 4.251e-04  Data: 0.010 (0.011)
Train: 17 [ 450/1251 ( 36%)]  Loss:  4.420234 (4.4881)  Time: 1.220s,  839.07/s  (1.107s,  925.17/s)  LR: 4.251e-04  Data: 0.013 (0.011)
Train: 17 [ 500/1251 ( 40%)]  Loss:  4.715070 (4.5087)  Time: 1.105s,  926.83/s  (1.106s,  925.58/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [ 550/1251 ( 44%)]  Loss:  4.528526 (4.5104)  Time: 1.095s,  935.11/s  (1.107s,  925.31/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [ 600/1251 ( 48%)]  Loss:  4.687803 (4.5240)  Time: 1.097s,  933.14/s  (1.106s,  925.47/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [ 650/1251 ( 52%)]  Loss:  4.235506 (4.5034)  Time: 1.122s,  912.61/s  (1.106s,  925.53/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [ 700/1251 ( 56%)]  Loss:  4.560513 (4.5072)  Time: 1.098s,  932.98/s  (1.106s,  925.56/s)  LR: 4.251e-04  Data: 0.014 (0.011)
Train: 17 [ 750/1251 ( 60%)]  Loss:  4.345312 (4.4971)  Time: 1.101s,  930.13/s  (1.107s,  925.31/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [ 800/1251 ( 64%)]  Loss:  4.350313 (4.4885)  Time: 1.097s,  933.27/s  (1.106s,  925.52/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [ 850/1251 ( 68%)]  Loss:  4.715787 (4.5011)  Time: 1.191s,  859.63/s  (1.107s,  925.41/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [ 900/1251 ( 72%)]  Loss:  4.469586 (4.4994)  Time: 1.097s,  933.84/s  (1.106s,  925.67/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [ 950/1251 ( 76%)]  Loss:  4.310116 (4.4900)  Time: 1.093s,  936.52/s  (1.106s,  925.68/s)  LR: 4.251e-04  Data: 0.010 (0.011)
Train: 17 [1000/1251 ( 80%)]  Loss:  4.591815 (4.4948)  Time: 1.095s,  934.76/s  (1.106s,  925.57/s)  LR: 4.251e-04  Data: 0.011 (0.011)
Train: 17 [1050/1251 ( 84%)]  Loss:  4.924063 (4.5143)  Time: 1.104s,  927.40/s  (1.106s,  925.54/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [1100/1251 ( 88%)]  Loss:  4.859605 (4.5293)  Time: 1.096s,  934.13/s  (1.107s,  925.43/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [1150/1251 ( 92%)]  Loss:  4.710879 (4.5369)  Time: 1.098s,  932.88/s  (1.106s,  925.63/s)  LR: 4.251e-04  Data: 0.012 (0.011)
Train: 17 [1200/1251 ( 96%)]  Loss:  4.450879 (4.5335)  Time: 1.118s,  915.77/s  (1.107s,  925.22/s)  LR: 4.251e-04  Data: 0.010 (0.011)
Train: 17 [1250/1251 (100%)]  Loss:  4.674996 (4.5389)  Time: 1.081s,  947.30/s  (1.107s,  925.32/s)  LR: 4.251e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.294 (3.294)  Loss:  1.0365 (1.0365)  Acc@1: 76.8555 (76.8555)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.229 (0.400)  Loss:  1.1326 (1.8410)  Acc@1: 75.0000 (59.1040)  Acc@5: 90.2123 (82.4560)
Test (EMA): [   0/48]  Time: 3.256 (3.256)  Loss:  3.5134 (3.5134)  Acc@1: 35.4492 (35.4492)  Acc@5: 64.1602 (64.1602)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  3.1219 (4.2847)  Acc@1: 47.6415 (22.3000)  Acc@5: 69.8113 (44.6860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 18 [   0/1251 (  0%)]  Loss:  4.433364 (4.4334)  Time: 1.137s,  900.41/s  (1.137s,  900.41/s)  LR: 4.501e-04  Data: 0.029 (0.029)
Train: 18 [  50/1251 (  4%)]  Loss:  4.374184 (4.4038)  Time: 1.120s,  914.36/s  (1.112s,  920.73/s)  LR: 4.501e-04  Data: 0.015 (0.012)
Train: 18 [ 100/1251 (  8%)]  Loss:  4.077778 (4.2951)  Time: 1.096s,  934.26/s  (1.108s,  923.88/s)  LR: 4.501e-04  Data: 0.010 (0.012)
Train: 18 [ 150/1251 ( 12%)]  Loss:  4.706413 (4.3979)  Time: 1.100s,  931.32/s  (1.107s,  924.83/s)  LR: 4.501e-04  Data: 0.011 (0.012)
Train: 18 [ 200/1251 ( 16%)]  Loss:  4.212905 (4.3609)  Time: 1.093s,  936.79/s  (1.106s,  926.13/s)  LR: 4.501e-04  Data: 0.010 (0.012)
Train: 18 [ 250/1251 ( 20%)]  Loss:  4.245849 (4.3417)  Time: 1.097s,  933.05/s  (1.106s,  925.87/s)  LR: 4.501e-04  Data: 0.012 (0.012)
Train: 18 [ 300/1251 ( 24%)]  Loss:  4.491179 (4.3631)  Time: 1.133s,  903.57/s  (1.107s,  925.10/s)  LR: 4.501e-04  Data: 0.011 (0.012)
Train: 18 [ 350/1251 ( 28%)]  Loss:  4.503200 (4.3806)  Time: 1.102s,  929.25/s  (1.106s,  925.45/s)  LR: 4.501e-04  Data: 0.011 (0.012)
Train: 18 [ 400/1251 ( 32%)]  Loss:  4.508178 (4.3948)  Time: 1.096s,  934.31/s  (1.107s,  925.33/s)  LR: 4.501e-04  Data: 0.011 (0.012)
Train: 18 [ 450/1251 ( 36%)]  Loss:  4.612671 (4.4166)  Time: 1.101s,  930.18/s  (1.107s,  924.84/s)  LR: 4.501e-04  Data: 0.011 (0.011)
Train: 18 [ 500/1251 ( 40%)]  Loss:  4.606827 (4.4339)  Time: 1.097s,  933.69/s  (1.107s,  924.73/s)  LR: 4.501e-04  Data: 0.013 (0.012)
Train: 18 [ 550/1251 ( 44%)]  Loss:  4.623620 (4.4497)  Time: 1.098s,  932.22/s  (1.107s,  925.28/s)  LR: 4.501e-04  Data: 0.011 (0.012)
Train: 18 [ 600/1251 ( 48%)]  Loss:  4.526157 (4.4556)  Time: 1.095s,  935.14/s  (1.108s,  924.46/s)  LR: 4.501e-04  Data: 0.012 (0.012)
Train: 18 [ 650/1251 ( 52%)]  Loss:  4.152244 (4.4339)  Time: 1.097s,  933.76/s  (1.107s,  924.84/s)  LR: 4.501e-04  Data: 0.012 (0.012)
Train: 18 [ 700/1251 ( 56%)]  Loss:  4.949483 (4.4683)  Time: 1.121s,  913.21/s  (1.108s,  923.96/s)  LR: 4.501e-04  Data: 0.012 (0.011)
Train: 18 [ 750/1251 ( 60%)]  Loss:  4.551225 (4.4735)  Time: 1.104s,  927.19/s  (1.108s,  924.34/s)  LR: 4.501e-04  Data: 0.011 (0.011)
Train: 18 [ 800/1251 ( 64%)]  Loss:  4.635344 (4.4830)  Time: 1.098s,  932.25/s  (1.108s,  924.55/s)  LR: 4.501e-04  Data: 0.011 (0.011)
Train: 18 [ 850/1251 ( 68%)]  Loss:  4.722184 (4.4963)  Time: 1.122s,  912.75/s  (1.108s,  924.44/s)  LR: 4.501e-04  Data: 0.010 (0.011)
Train: 18 [ 900/1251 ( 72%)]  Loss:  4.440609 (4.4933)  Time: 1.096s,  934.44/s  (1.107s,  924.81/s)  LR: 4.501e-04  Data: 0.012 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 18 [ 950/1251 ( 76%)]  Loss:  4.455303 (4.4914)  Time: 1.123s,  912.14/s  (1.108s,  924.35/s)  LR: 4.501e-04  Data: 0.012 (0.011)
Train: 18 [1000/1251 ( 80%)]  Loss:  4.180008 (4.4766)  Time: 1.103s,  928.62/s  (1.107s,  924.67/s)  LR: 4.501e-04  Data: 0.011 (0.011)
Train: 18 [1050/1251 ( 84%)]  Loss:  4.322133 (4.4696)  Time: 1.102s,  929.03/s  (1.108s,  924.25/s)  LR: 4.501e-04  Data: 0.011 (0.011)
Train: 18 [1100/1251 ( 88%)]  Loss:  4.280945 (4.4614)  Time: 1.096s,  934.46/s  (1.108s,  924.41/s)  LR: 4.501e-04  Data: 0.011 (0.011)
Train: 18 [1150/1251 ( 92%)]  Loss:  4.818843 (4.4763)  Time: 1.105s,  926.52/s  (1.108s,  924.18/s)  LR: 4.501e-04  Data: 0.010 (0.011)
Train: 18 [1200/1251 ( 96%)]  Loss:  4.621353 (4.4821)  Time: 1.099s,  932.07/s  (1.108s,  924.29/s)  LR: 4.501e-04  Data: 0.010 (0.011)
Train: 18 [1250/1251 (100%)]  Loss:  4.519805 (4.4835)  Time: 1.079s,  948.68/s  (1.108s,  923.93/s)  LR: 4.501e-04  Data: 0.000 (0.011)
Test: [   0/48]  Time: 3.294 (3.294)  Loss:  1.0478 (1.0478)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.230 (0.410)  Loss:  1.0408 (1.7677)  Acc@1: 78.5377 (60.3640)  Acc@5: 91.0377 (83.4900)
Test (EMA): [   0/48]  Time: 3.190 (3.190)  Loss:  3.0623 (3.0623)  Acc@1: 45.0195 (45.0195)  Acc@5: 71.6797 (71.6797)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  2.8196 (3.9485)  Acc@1: 53.0660 (27.1720)  Acc@5: 73.2311 (50.8560)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 19 [   0/1251 (  0%)]  Loss:  4.700410 (4.7004)  Time: 1.115s,  918.73/s  (1.115s,  918.73/s)  LR: 4.750e-04  Data: 0.031 (0.031)
Train: 19 [  50/1251 (  4%)]  Loss:  4.731225 (4.7158)  Time: 1.100s,  930.60/s  (1.105s,  926.66/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 100/1251 (  8%)]  Loss:  4.516031 (4.6492)  Time: 1.103s,  928.78/s  (1.108s,  924.60/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [ 150/1251 ( 12%)]  Loss:  4.593844 (4.6354)  Time: 1.101s,  929.66/s  (1.106s,  925.55/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 200/1251 ( 16%)]  Loss:  4.250979 (4.5585)  Time: 1.106s,  926.14/s  (1.107s,  924.70/s)  LR: 4.750e-04  Data: 0.012 (0.011)
Train: 19 [ 250/1251 ( 20%)]  Loss:  4.617466 (4.5683)  Time: 1.096s,  933.99/s  (1.107s,  925.01/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 300/1251 ( 24%)]  Loss:  4.333606 (4.5348)  Time: 1.095s,  935.34/s  (1.106s,  925.87/s)  LR: 4.750e-04  Data: 0.012 (0.011)
Train: 19 [ 350/1251 ( 28%)]  Loss:  4.498064 (4.5302)  Time: 1.093s,  936.54/s  (1.106s,  925.53/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [ 400/1251 ( 32%)]  Loss:  4.344017 (4.5095)  Time: 1.113s,  920.10/s  (1.106s,  925.82/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [ 450/1251 ( 36%)]  Loss:  4.679160 (4.5265)  Time: 1.096s,  933.91/s  (1.108s,  924.41/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 500/1251 ( 40%)]  Loss:  4.398142 (4.5148)  Time: 1.132s,  904.87/s  (1.108s,  924.23/s)  LR: 4.750e-04  Data: 0.013 (0.011)
Train: 19 [ 550/1251 ( 44%)]  Loss:  4.564736 (4.5190)  Time: 1.097s,  933.06/s  (1.109s,  923.58/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [ 600/1251 ( 48%)]  Loss:  4.074369 (4.4848)  Time: 1.101s,  930.37/s  (1.108s,  924.03/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 650/1251 ( 52%)]  Loss:  4.664002 (4.4976)  Time: 1.115s,  918.24/s  (1.109s,  923.41/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [ 700/1251 ( 56%)]  Loss:  4.402914 (4.4913)  Time: 1.099s,  932.02/s  (1.108s,  923.83/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 750/1251 ( 60%)]  Loss:  4.821939 (4.5119)  Time: 1.101s,  930.02/s  (1.110s,  922.90/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [ 800/1251 ( 64%)]  Loss:  4.101196 (4.4878)  Time: 1.098s,  932.82/s  (1.109s,  923.30/s)  LR: 4.750e-04  Data: 0.012 (0.011)
Train: 19 [ 850/1251 ( 68%)]  Loss:  4.370420 (4.4813)  Time: 1.099s,  931.70/s  (1.109s,  923.63/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [ 900/1251 ( 72%)]  Loss:  4.233581 (4.4682)  Time: 1.097s,  933.80/s  (1.109s,  923.50/s)  LR: 4.750e-04  Data: 0.012 (0.011)
Train: 19 [ 950/1251 ( 76%)]  Loss:  4.179881 (4.4538)  Time: 1.105s,  926.82/s  (1.108s,  923.90/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [1000/1251 ( 80%)]  Loss:  4.168474 (4.4402)  Time: 1.104s,  927.13/s  (1.109s,  923.73/s)  LR: 4.750e-04  Data: 0.012 (0.011)
Train: 19 [1050/1251 ( 84%)]  Loss:  4.506558 (4.4432)  Time: 1.101s,  930.42/s  (1.108s,  923.94/s)  LR: 4.750e-04  Data: 0.018 (0.011)
Train: 19 [1100/1251 ( 88%)]  Loss:  4.237051 (4.4343)  Time: 1.103s,  928.56/s  (1.109s,  923.72/s)  LR: 4.750e-04  Data: 0.010 (0.011)
Train: 19 [1150/1251 ( 92%)]  Loss:  4.365276 (4.4314)  Time: 1.096s,  934.12/s  (1.108s,  923.88/s)  LR: 4.750e-04  Data: 0.011 (0.011)
Train: 19 [1200/1251 ( 96%)]  Loss:  3.969350 (4.4129)  Time: 1.099s,  931.56/s  (1.108s,  923.83/s)  LR: 4.750e-04  Data: 0.017 (0.011)
Train: 19 [1250/1251 (100%)]  Loss:  5.018080 (4.4362)  Time: 1.080s,  948.11/s  (1.109s,  923.72/s)  LR: 4.750e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.284 (3.284)  Loss:  1.1031 (1.1031)  Acc@1: 78.7109 (78.7109)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  1.1249 (1.8293)  Acc@1: 76.7689 (60.5760)  Acc@5: 91.8632 (83.7660)
Test (EMA): [   0/48]  Time: 3.120 (3.120)  Loss:  2.6662 (2.6662)  Acc@1: 50.8789 (50.8789)  Acc@5: 75.9766 (75.9766)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  2.5552 (3.6433)  Acc@1: 56.1321 (31.5300)  Acc@5: 75.7076 (56.1420)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 2.6219999942016603)

Train: 20 [   0/1251 (  0%)]  Loss:  4.710016 (4.7100)  Time: 1.131s,  905.19/s  (1.131s,  905.19/s)  LR: 4.946e-04  Data: 0.037 (0.037)
Train: 20 [  50/1251 (  4%)]  Loss:  4.355256 (4.5326)  Time: 1.098s,  932.88/s  (1.117s,  916.97/s)  LR: 4.946e-04  Data: 0.012 (0.011)
Train: 20 [ 100/1251 (  8%)]  Loss:  4.492156 (4.5191)  Time: 1.110s,  922.59/s  (1.111s,  921.86/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 150/1251 ( 12%)]  Loss:  3.762986 (4.3301)  Time: 1.131s,  905.49/s  (1.112s,  921.25/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 200/1251 ( 16%)]  Loss:  4.503012 (4.3647)  Time: 1.124s,  910.99/s  (1.111s,  922.10/s)  LR: 4.946e-04  Data: 0.013 (0.011)
Train: 20 [ 250/1251 ( 20%)]  Loss:  4.302548 (4.3543)  Time: 1.131s,  905.08/s  (1.112s,  921.20/s)  LR: 4.946e-04  Data: 0.011 (0.011)
Train: 20 [ 300/1251 ( 24%)]  Loss:  4.448647 (4.3678)  Time: 1.097s,  933.60/s  (1.112s,  921.10/s)  LR: 4.946e-04  Data: 0.012 (0.011)
Train: 20 [ 350/1251 ( 28%)]  Loss:  4.508289 (4.3854)  Time: 1.097s,  933.55/s  (1.110s,  922.39/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 400/1251 ( 32%)]  Loss:  4.436024 (4.3910)  Time: 1.131s,  905.66/s  (1.111s,  921.98/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 450/1251 ( 36%)]  Loss:  4.125635 (4.3645)  Time: 1.105s,  926.44/s  (1.110s,  922.39/s)  LR: 4.946e-04  Data: 0.016 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 20 [ 500/1251 ( 40%)]  Loss:  4.481855 (4.3751)  Time: 1.106s,  926.15/s  (1.110s,  922.22/s)  LR: 4.946e-04  Data: 0.011 (0.011)
Train: 20 [ 550/1251 ( 44%)]  Loss:  4.366415 (4.3744)  Time: 1.103s,  928.63/s  (1.110s,  922.39/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 600/1251 ( 48%)]  Loss:  4.684472 (4.3983)  Time: 1.120s,  913.93/s  (1.111s,  922.10/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 650/1251 ( 52%)]  Loss:  4.641511 (4.4156)  Time: 1.101s,  930.04/s  (1.110s,  922.30/s)  LR: 4.946e-04  Data: 0.012 (0.011)
Train: 20 [ 700/1251 ( 56%)]  Loss:  4.176509 (4.3997)  Time: 1.220s,  839.55/s  (1.110s,  922.23/s)  LR: 4.946e-04  Data: 0.015 (0.011)
Train: 20 [ 750/1251 ( 60%)]  Loss:  4.209131 (4.3878)  Time: 1.120s,  914.31/s  (1.110s,  922.55/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [ 800/1251 ( 64%)]  Loss:  4.600615 (4.4003)  Time: 1.209s,  847.29/s  (1.110s,  922.47/s)  LR: 4.946e-04  Data: 0.011 (0.011)
Train: 20 [ 850/1251 ( 68%)]  Loss:  4.520462 (4.4070)  Time: 1.103s,  928.77/s  (1.110s,  922.59/s)  LR: 4.946e-04  Data: 0.011 (0.011)
Train: 20 [ 900/1251 ( 72%)]  Loss:  4.369705 (4.4050)  Time: 1.100s,  930.60/s  (1.109s,  923.10/s)  LR: 4.946e-04  Data: 0.013 (0.011)
Train: 20 [ 950/1251 ( 76%)]  Loss:  4.725414 (4.4210)  Time: 1.124s,  910.96/s  (1.110s,  922.63/s)  LR: 4.946e-04  Data: 0.011 (0.011)
Train: 20 [1000/1251 ( 80%)]  Loss:  4.620835 (4.4305)  Time: 1.104s,  927.29/s  (1.110s,  922.30/s)  LR: 4.946e-04  Data: 0.011 (0.011)
Train: 20 [1050/1251 ( 84%)]  Loss:  4.386795 (4.4286)  Time: 1.098s,  932.26/s  (1.110s,  922.21/s)  LR: 4.946e-04  Data: 0.012 (0.011)
Train: 20 [1100/1251 ( 88%)]  Loss:  4.047381 (4.4120)  Time: 1.100s,  930.56/s  (1.110s,  922.21/s)  LR: 4.946e-04  Data: 0.012 (0.011)
Train: 20 [1150/1251 ( 92%)]  Loss:  3.957622 (4.3931)  Time: 1.091s,  938.42/s  (1.111s,  922.10/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [1200/1251 ( 96%)]  Loss:  4.329928 (4.3905)  Time: 1.096s,  934.38/s  (1.110s,  922.21/s)  LR: 4.946e-04  Data: 0.010 (0.011)
Train: 20 [1250/1251 (100%)]  Loss:  4.323635 (4.3880)  Time: 1.078s,  950.31/s  (1.111s,  921.87/s)  LR: 4.946e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.202 (3.202)  Loss:  1.0961 (1.0961)  Acc@1: 77.4414 (77.4414)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  1.0506 (1.7471)  Acc@1: 77.9481 (61.7000)  Acc@5: 91.5094 (84.3480)
Test (EMA): [   0/48]  Time: 3.080 (3.080)  Loss:  2.3482 (2.3482)  Acc@1: 56.6406 (56.6406)  Acc@5: 79.5898 (79.5898)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  2.3347 (3.3708)  Acc@1: 59.9057 (35.7540)  Acc@5: 78.1840 (60.8260)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 3.4560000091552734)

Train: 21 [   0/1251 (  0%)]  Loss:  4.075140 (4.0751)  Time: 1.125s,  910.41/s  (1.125s,  910.41/s)  LR: 4.940e-04  Data: 0.030 (0.030)
Train: 21 [  50/1251 (  4%)]  Loss:  4.465028 (4.2701)  Time: 1.097s,  933.09/s  (1.108s,  924.20/s)  LR: 4.940e-04  Data: 0.011 (0.012)
Train: 21 [ 100/1251 (  8%)]  Loss:  4.477374 (4.3392)  Time: 1.122s,  912.32/s  (1.113s,  920.06/s)  LR: 4.940e-04  Data: 0.021 (0.012)
Train: 21 [ 150/1251 ( 12%)]  Loss:  4.496834 (4.3786)  Time: 1.098s,  932.36/s  (1.110s,  922.35/s)  LR: 4.940e-04  Data: 0.010 (0.012)
Train: 21 [ 200/1251 ( 16%)]  Loss:  4.237908 (4.3505)  Time: 1.098s,  932.35/s  (1.108s,  923.93/s)  LR: 4.940e-04  Data: 0.010 (0.012)
Train: 21 [ 250/1251 ( 20%)]  Loss:  4.532671 (4.3808)  Time: 1.108s,  923.98/s  (1.111s,  921.87/s)  LR: 4.940e-04  Data: 0.024 (0.012)
Train: 21 [ 300/1251 ( 24%)]  Loss:  4.603971 (4.4127)  Time: 1.108s,  924.16/s  (1.110s,  922.49/s)  LR: 4.940e-04  Data: 0.011 (0.012)
Train: 21 [ 350/1251 ( 28%)]  Loss:  4.544480 (4.4292)  Time: 1.096s,  934.33/s  (1.110s,  922.44/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [ 400/1251 ( 32%)]  Loss:  4.553654 (4.4430)  Time: 1.123s,  912.08/s  (1.111s,  922.06/s)  LR: 4.940e-04  Data: 0.012 (0.011)
Train: 21 [ 450/1251 ( 36%)]  Loss:  4.285435 (4.4272)  Time: 1.103s,  928.57/s  (1.111s,  922.08/s)  LR: 4.940e-04  Data: 0.012 (0.011)
Train: 21 [ 500/1251 ( 40%)]  Loss:  4.332369 (4.4186)  Time: 1.097s,  933.73/s  (1.109s,  922.98/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [ 550/1251 ( 44%)]  Loss:  4.043301 (4.3873)  Time: 1.093s,  936.94/s  (1.110s,  922.77/s)  LR: 4.940e-04  Data: 0.010 (0.011)
Train: 21 [ 600/1251 ( 48%)]  Loss:  3.894741 (4.3495)  Time: 1.095s,  934.97/s  (1.109s,  923.47/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [ 650/1251 ( 52%)]  Loss:  4.435072 (4.3556)  Time: 1.096s,  933.88/s  (1.109s,  923.48/s)  LR: 4.940e-04  Data: 0.012 (0.011)
Train: 21 [ 700/1251 ( 56%)]  Loss:  4.374508 (4.3568)  Time: 1.118s,  916.32/s  (1.108s,  923.89/s)  LR: 4.940e-04  Data: 0.009 (0.011)
Train: 21 [ 750/1251 ( 60%)]  Loss:  4.416170 (4.3605)  Time: 1.102s,  929.28/s  (1.108s,  924.41/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [ 800/1251 ( 64%)]  Loss:  4.685833 (4.3797)  Time: 1.096s,  934.29/s  (1.108s,  923.95/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [ 850/1251 ( 68%)]  Loss:  4.587963 (4.3912)  Time: 1.096s,  934.07/s  (1.108s,  923.95/s)  LR: 4.940e-04  Data: 0.010 (0.011)
Train: 21 [ 900/1251 ( 72%)]  Loss:  4.362116 (4.3897)  Time: 1.120s,  914.04/s  (1.109s,  923.49/s)  LR: 4.940e-04  Data: 0.010 (0.011)
Train: 21 [ 950/1251 ( 76%)]  Loss:  4.270545 (4.3838)  Time: 1.095s,  935.00/s  (1.109s,  923.66/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [1000/1251 ( 80%)]  Loss:  3.917754 (4.3616)  Time: 1.102s,  929.38/s  (1.109s,  923.51/s)  LR: 4.940e-04  Data: 0.011 (0.011)
Train: 21 [1050/1251 ( 84%)]  Loss:  4.396555 (4.3632)  Time: 1.094s,  935.74/s  (1.109s,  923.69/s)  LR: 4.940e-04  Data: 0.010 (0.011)
Train: 21 [1100/1251 ( 88%)]  Loss:  4.076836 (4.3507)  Time: 1.103s,  928.37/s  (1.109s,  923.66/s)  LR: 4.940e-04  Data: 0.013 (0.011)
Train: 21 [1150/1251 ( 92%)]  Loss:  4.106260 (4.3405)  Time: 1.120s,  914.04/s  (1.108s,  923.85/s)  LR: 4.940e-04  Data: 0.010 (0.011)
Train: 21 [1200/1251 ( 96%)]  Loss:  4.214718 (4.3355)  Time: 1.102s,  929.39/s  (1.109s,  923.62/s)  LR: 4.940e-04  Data: 0.012 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 21 [1250/1251 (100%)]  Loss:  4.061918 (4.3250)  Time: 1.085s,  943.58/s  (1.109s,  923.77/s)  LR: 4.940e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.288 (3.288)  Loss:  1.0062 (1.0062)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.9706 (1.6842)  Acc@1: 79.2453 (62.8300)  Acc@5: 93.6321 (85.4320)
Test (EMA): [   0/48]  Time: 3.179 (3.179)  Loss:  2.0841 (2.0841)  Acc@1: 60.9375 (60.9375)  Acc@5: 83.4961 (83.4961)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  2.1427 (3.1295)  Acc@1: 61.4387 (39.4860)  Acc@5: 79.9528 (64.7100)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 4.78200000793457)

Train: 22 [   0/1251 (  0%)]  Loss:  4.137334 (4.1373)  Time: 1.112s,  921.01/s  (1.112s,  921.01/s)  LR: 4.935e-04  Data: 0.026 (0.026)
Train: 22 [  50/1251 (  4%)]  Loss:  4.574475 (4.3559)  Time: 1.202s,  852.25/s  (1.115s,  918.34/s)  LR: 4.935e-04  Data: 0.010 (0.011)
Train: 22 [ 100/1251 (  8%)]  Loss:  3.936472 (4.2161)  Time: 1.133s,  903.99/s  (1.111s,  921.33/s)  LR: 4.935e-04  Data: 0.012 (0.011)
Train: 22 [ 150/1251 ( 12%)]  Loss:  4.061382 (4.1774)  Time: 1.098s,  932.36/s  (1.110s,  922.34/s)  LR: 4.935e-04  Data: 0.013 (0.011)
Train: 22 [ 200/1251 ( 16%)]  Loss:  4.355371 (4.2130)  Time: 1.120s,  914.60/s  (1.112s,  921.07/s)  LR: 4.935e-04  Data: 0.011 (0.011)
Train: 22 [ 250/1251 ( 20%)]  Loss:  4.671714 (4.2895)  Time: 1.095s,  934.84/s  (1.110s,  922.80/s)  LR: 4.935e-04  Data: 0.013 (0.011)
Train: 22 [ 300/1251 ( 24%)]  Loss:  4.589043 (4.3323)  Time: 1.101s,  929.89/s  (1.112s,  920.77/s)  LR: 4.935e-04  Data: 0.013 (0.011)
Train: 22 [ 350/1251 ( 28%)]  Loss:  4.469291 (4.3494)  Time: 1.098s,  932.45/s  (1.112s,  920.56/s)  LR: 4.935e-04  Data: 0.013 (0.011)
Train: 22 [ 400/1251 ( 32%)]  Loss:  4.074075 (4.3188)  Time: 1.096s,  934.18/s  (1.112s,  920.59/s)  LR: 4.935e-04  Data: 0.011 (0.011)
Train: 22 [ 450/1251 ( 36%)]  Loss:  4.262636 (4.3132)  Time: 1.104s,  927.14/s  (1.111s,  921.53/s)  LR: 4.935e-04  Data: 0.012 (0.011)
Train: 22 [ 500/1251 ( 40%)]  Loss:  4.409462 (4.3219)  Time: 1.098s,  932.54/s  (1.111s,  922.01/s)  LR: 4.935e-04  Data: 0.014 (0.012)
Train: 22 [ 550/1251 ( 44%)]  Loss:  4.257376 (4.3166)  Time: 1.095s,  935.31/s  (1.110s,  922.74/s)  LR: 4.935e-04  Data: 0.010 (0.012)
Train: 22 [ 600/1251 ( 48%)]  Loss:  4.546988 (4.3343)  Time: 1.096s,  934.61/s  (1.110s,  922.86/s)  LR: 4.935e-04  Data: 0.011 (0.012)
Train: 22 [ 650/1251 ( 52%)]  Loss:  4.307332 (4.3324)  Time: 1.100s,  930.91/s  (1.110s,  922.28/s)  LR: 4.935e-04  Data: 0.012 (0.012)
Train: 22 [ 700/1251 ( 56%)]  Loss:  4.306926 (4.3307)  Time: 1.098s,  932.94/s  (1.110s,  922.87/s)  LR: 4.935e-04  Data: 0.012 (0.012)
Train: 22 [ 750/1251 ( 60%)]  Loss:  4.174711 (4.3209)  Time: 1.096s,  934.17/s  (1.110s,  922.27/s)  LR: 4.935e-04  Data: 0.012 (0.012)
Train: 22 [ 800/1251 ( 64%)]  Loss:  4.119373 (4.3091)  Time: 1.101s,  930.41/s  (1.110s,  922.85/s)  LR: 4.935e-04  Data: 0.011 (0.012)
Train: 22 [ 850/1251 ( 68%)]  Loss:  4.221303 (4.3042)  Time: 1.121s,  913.29/s  (1.110s,  922.87/s)  LR: 4.935e-04  Data: 0.012 (0.011)
Train: 22 [ 900/1251 ( 72%)]  Loss:  4.293536 (4.3036)  Time: 1.097s,  933.81/s  (1.110s,  922.88/s)  LR: 4.935e-04  Data: 0.012 (0.011)
Train: 22 [ 950/1251 ( 76%)]  Loss:  4.444710 (4.3107)  Time: 1.098s,  932.92/s  (1.110s,  922.59/s)  LR: 4.935e-04  Data: 0.013 (0.011)
Train: 22 [1000/1251 ( 80%)]  Loss:  4.161107 (4.3036)  Time: 1.096s,  933.92/s  (1.109s,  923.01/s)  LR: 4.935e-04  Data: 0.012 (0.011)
Train: 22 [1050/1251 ( 84%)]  Loss:  4.405208 (4.3082)  Time: 1.099s,  931.36/s  (1.110s,  922.83/s)  LR: 4.935e-04  Data: 0.011 (0.011)
Train: 22 [1100/1251 ( 88%)]  Loss:  4.289551 (4.3074)  Time: 1.098s,  932.96/s  (1.109s,  923.17/s)  LR: 4.935e-04  Data: 0.012 (0.011)
Train: 22 [1150/1251 ( 92%)]  Loss:  4.141685 (4.3005)  Time: 1.100s,  930.85/s  (1.109s,  923.28/s)  LR: 4.935e-04  Data: 0.017 (0.011)
Train: 22 [1200/1251 ( 96%)]  Loss:  4.347085 (4.3023)  Time: 1.096s,  934.06/s  (1.109s,  923.52/s)  LR: 4.935e-04  Data: 0.011 (0.011)
Train: 22 [1250/1251 (100%)]  Loss:  4.271791 (4.3012)  Time: 1.080s,  948.06/s  (1.109s,  923.69/s)  LR: 4.935e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.345 (3.345)  Loss:  0.9456 (0.9456)  Acc@1: 78.9062 (78.9062)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.9828 (1.6338)  Acc@1: 77.0047 (64.0080)  Acc@5: 93.2783 (86.0760)
Test (EMA): [   0/48]  Time: 3.243 (3.243)  Loss:  1.8627 (1.8627)  Acc@1: 64.9414 (64.9414)  Acc@5: 85.9375 (85.9375)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  1.9751 (2.9158)  Acc@1: 63.6792 (42.8360)  Acc@5: 81.9576 (68.1140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 6.704000015258789)

Train: 23 [   0/1251 (  0%)]  Loss:  4.242015 (4.2420)  Time: 1.107s,  924.89/s  (1.107s,  924.89/s)  LR: 4.929e-04  Data: 0.023 (0.023)
Train: 23 [  50/1251 (  4%)]  Loss:  3.845745 (4.0439)  Time: 1.095s,  935.06/s  (1.102s,  929.46/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [ 100/1251 (  8%)]  Loss:  4.057612 (4.0485)  Time: 1.101s,  930.13/s  (1.107s,  925.37/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [ 150/1251 ( 12%)]  Loss:  4.540462 (4.1715)  Time: 1.099s,  931.77/s  (1.109s,  923.13/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [ 200/1251 ( 16%)]  Loss:  4.209873 (4.1791)  Time: 1.097s,  933.38/s  (1.107s,  925.07/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [ 250/1251 ( 20%)]  Loss:  4.639278 (4.2558)  Time: 1.096s,  934.40/s  (1.108s,  924.01/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [ 300/1251 ( 24%)]  Loss:  4.560736 (4.2994)  Time: 1.107s,  925.23/s  (1.108s,  924.58/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [ 350/1251 ( 28%)]  Loss:  4.208025 (4.2880)  Time: 1.096s,  933.92/s  (1.108s,  924.27/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [ 400/1251 ( 32%)]  Loss:  4.078493 (4.2647)  Time: 1.101s,  929.96/s  (1.107s,  925.10/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [ 450/1251 ( 36%)]  Loss:  4.063586 (4.2446)  Time: 1.124s,  911.33/s  (1.107s,  924.67/s)  LR: 4.929e-04  Data: 0.010 (0.012)
Train: 23 [ 500/1251 ( 40%)]  Loss:  4.404478 (4.2591)  Time: 1.097s,  933.17/s  (1.107s,  925.37/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [ 550/1251 ( 44%)]  Loss:  4.289904 (4.2617)  Time: 1.097s,  933.26/s  (1.107s,  924.75/s)  LR: 4.929e-04  Data: 0.010 (0.012)
Train: 23 [ 600/1251 ( 48%)]  Loss:  3.881727 (4.2325)  Time: 1.096s,  934.39/s  (1.107s,  924.69/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [ 650/1251 ( 52%)]  Loss:  3.940039 (4.2116)  Time: 1.098s,  932.96/s  (1.107s,  925.22/s)  LR: 4.929e-04  Data: 0.013 (0.012)
Train: 23 [ 700/1251 ( 56%)]  Loss:  4.339873 (4.2201)  Time: 1.123s,  911.75/s  (1.108s,  924.38/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 23 [ 750/1251 ( 60%)]  Loss:  4.394544 (4.2310)  Time: 1.097s,  933.86/s  (1.107s,  924.90/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [ 800/1251 ( 64%)]  Loss:  4.127978 (4.2250)  Time: 1.106s,  926.27/s  (1.107s,  924.91/s)  LR: 4.929e-04  Data: 0.010 (0.012)
Train: 23 [ 850/1251 ( 68%)]  Loss:  4.397691 (4.2346)  Time: 1.094s,  936.14/s  (1.107s,  925.23/s)  LR: 4.929e-04  Data: 0.010 (0.012)
Train: 23 [ 900/1251 ( 72%)]  Loss:  3.861311 (4.2149)  Time: 1.124s,  910.71/s  (1.108s,  924.42/s)  LR: 4.929e-04  Data: 0.013 (0.012)
Train: 23 [ 950/1251 ( 76%)]  Loss:  4.166395 (4.2125)  Time: 1.098s,  932.58/s  (1.107s,  924.71/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [1000/1251 ( 80%)]  Loss:  4.025968 (4.2036)  Time: 1.119s,  914.76/s  (1.108s,  924.36/s)  LR: 4.929e-04  Data: 0.011 (0.012)
Train: 23 [1050/1251 ( 84%)]  Loss:  4.076977 (4.1979)  Time: 1.096s,  934.32/s  (1.107s,  924.69/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [1100/1251 ( 88%)]  Loss:  4.077735 (4.1926)  Time: 1.100s,  931.09/s  (1.108s,  924.52/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [1150/1251 ( 92%)]  Loss:  4.292218 (4.1968)  Time: 1.095s,  935.09/s  (1.107s,  924.78/s)  LR: 4.929e-04  Data: 0.010 (0.012)
Train: 23 [1200/1251 ( 96%)]  Loss:  4.561414 (4.2114)  Time: 1.099s,  932.01/s  (1.107s,  924.96/s)  LR: 4.929e-04  Data: 0.012 (0.012)
Train: 23 [1250/1251 (100%)]  Loss:  3.856120 (4.1977)  Time: 1.104s,  927.95/s  (1.107s,  924.74/s)  LR: 4.929e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.136 (3.136)  Loss:  0.9594 (0.9594)  Acc@1: 79.9805 (79.9805)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.9947 (1.6202)  Acc@1: 78.7736 (65.0380)  Acc@5: 94.5755 (86.6260)
Test (EMA): [   0/48]  Time: 3.074 (3.074)  Loss:  1.6794 (1.6794)  Acc@1: 68.5547 (68.5547)  Acc@5: 88.0859 (88.0859)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  1.8286 (2.7243)  Acc@1: 66.2736 (46.0580)  Acc@5: 83.8443 (71.0240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 9.517999987792969)

Train: 24 [   0/1251 (  0%)]  Loss:  3.951553 (3.9516)  Time: 1.118s,  916.14/s  (1.118s,  916.14/s)  LR: 4.922e-04  Data: 0.029 (0.029)
Train: 24 [  50/1251 (  4%)]  Loss:  4.089438 (4.0205)  Time: 1.122s,  912.46/s  (1.104s,  927.82/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 100/1251 (  8%)]  Loss:  4.234674 (4.0919)  Time: 1.096s,  934.62/s  (1.107s,  925.21/s)  LR: 4.922e-04  Data: 0.012 (0.012)
Train: 24 [ 150/1251 ( 12%)]  Loss:  3.935286 (4.0527)  Time: 1.133s,  903.72/s  (1.112s,  921.21/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 200/1251 ( 16%)]  Loss:  4.367312 (4.1157)  Time: 1.123s,  911.90/s  (1.114s,  919.52/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 250/1251 ( 20%)]  Loss:  3.815860 (4.0657)  Time: 1.096s,  934.37/s  (1.113s,  920.05/s)  LR: 4.922e-04  Data: 0.012 (0.012)
Train: 24 [ 300/1251 ( 24%)]  Loss:  4.147940 (4.0774)  Time: 1.121s,  913.79/s  (1.114s,  918.98/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 350/1251 ( 28%)]  Loss:  4.178345 (4.0901)  Time: 1.099s,  931.34/s  (1.112s,  920.46/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 400/1251 ( 32%)]  Loss:  4.273156 (4.1104)  Time: 1.099s,  932.06/s  (1.112s,  920.59/s)  LR: 4.922e-04  Data: 0.015 (0.012)
Train: 24 [ 450/1251 ( 36%)]  Loss:  4.277442 (4.1271)  Time: 1.097s,  933.61/s  (1.111s,  921.45/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 500/1251 ( 40%)]  Loss:  4.263748 (4.1395)  Time: 1.094s,  936.13/s  (1.111s,  921.71/s)  LR: 4.922e-04  Data: 0.010 (0.012)
Train: 24 [ 550/1251 ( 44%)]  Loss:  4.283373 (4.1515)  Time: 1.098s,  932.62/s  (1.110s,  922.43/s)  LR: 4.922e-04  Data: 0.012 (0.012)
Train: 24 [ 600/1251 ( 48%)]  Loss:  4.332401 (4.1654)  Time: 1.096s,  934.28/s  (1.109s,  922.98/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 650/1251 ( 52%)]  Loss:  3.779092 (4.1378)  Time: 1.097s,  933.33/s  (1.110s,  922.77/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 700/1251 ( 56%)]  Loss:  4.067802 (4.1332)  Time: 1.092s,  937.71/s  (1.109s,  923.30/s)  LR: 4.922e-04  Data: 0.010 (0.012)
Train: 24 [ 750/1251 ( 60%)]  Loss:  4.008625 (4.1254)  Time: 1.099s,  931.52/s  (1.109s,  923.52/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 800/1251 ( 64%)]  Loss:  3.904492 (4.1124)  Time: 1.099s,  932.04/s  (1.109s,  923.30/s)  LR: 4.922e-04  Data: 0.012 (0.012)
Train: 24 [ 850/1251 ( 68%)]  Loss:  4.167707 (4.1155)  Time: 1.094s,  935.92/s  (1.109s,  923.21/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 900/1251 ( 72%)]  Loss:  4.225183 (4.1212)  Time: 1.121s,  913.80/s  (1.109s,  923.36/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [ 950/1251 ( 76%)]  Loss:  4.241265 (4.1272)  Time: 1.098s,  932.68/s  (1.109s,  922.99/s)  LR: 4.922e-04  Data: 0.013 (0.012)
Train: 24 [1000/1251 ( 80%)]  Loss:  4.297456 (4.1353)  Time: 1.095s,  934.86/s  (1.109s,  923.18/s)  LR: 4.922e-04  Data: 0.012 (0.012)
Train: 24 [1050/1251 ( 84%)]  Loss:  4.208802 (4.1387)  Time: 1.095s,  935.05/s  (1.109s,  923.32/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [1100/1251 ( 88%)]  Loss:  4.000873 (4.1327)  Time: 1.119s,  915.22/s  (1.109s,  923.55/s)  LR: 4.922e-04  Data: 0.011 (0.012)
Train: 24 [1150/1251 ( 92%)]  Loss:  4.156198 (4.1337)  Time: 1.119s,  914.86/s  (1.109s,  923.57/s)  LR: 4.922e-04  Data: 0.012 (0.012)
Train: 24 [1200/1251 ( 96%)]  Loss:  4.561607 (4.1508)  Time: 1.100s,  930.66/s  (1.109s,  923.61/s)  LR: 4.922e-04  Data: 0.014 (0.012)
Train: 24 [1250/1251 (100%)]  Loss:  4.217610 (4.1534)  Time: 1.104s,  927.94/s  (1.109s,  923.74/s)  LR: 4.922e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.231 (3.231)  Loss:  0.8767 (0.8767)  Acc@1: 81.0547 (81.0547)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.9609 (1.5589)  Acc@1: 79.2453 (65.2920)  Acc@5: 93.7500 (86.9640)
Test (EMA): [   0/48]  Time: 3.100 (3.100)  Loss:  1.5284 (1.5284)  Acc@1: 70.7031 (70.7031)  Acc@5: 89.4531 (89.4531)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  1.7071 (2.5555)  Acc@1: 68.5142 (48.7620)  Acc@5: 84.6698 (73.4820)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 13.17000002319336)

Train: 25 [   0/1251 (  0%)]  Loss:  4.216688 (4.2167)  Time: 1.121s,  913.82/s  (1.121s,  913.82/s)  LR: 4.916e-04  Data: 0.028 (0.028)
Train: 25 [  50/1251 (  4%)]  Loss:  3.982063 (4.0994)  Time: 1.099s,  931.43/s  (1.120s,  914.22/s)  LR: 4.916e-04  Data: 0.014 (0.012)
Train: 25 [ 100/1251 (  8%)]  Loss:  4.199725 (4.1328)  Time: 1.097s,  933.46/s  (1.111s,  921.82/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 150/1251 ( 12%)]  Loss:  4.142588 (4.1353)  Time: 1.097s,  933.15/s  (1.111s,  921.72/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [ 200/1251 ( 16%)]  Loss:  3.884157 (4.0850)  Time: 1.096s,  934.26/s  (1.109s,  923.24/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 250/1251 ( 20%)]  Loss:  4.495168 (4.1534)  Time: 1.096s,  934.07/s  (1.110s,  922.53/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 25 [ 300/1251 ( 24%)]  Loss:  3.963317 (4.1262)  Time: 1.096s,  934.43/s  (1.109s,  923.58/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 350/1251 ( 28%)]  Loss:  4.201920 (4.1357)  Time: 1.096s,  934.35/s  (1.108s,  923.88/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [ 400/1251 ( 32%)]  Loss:  4.234403 (4.1467)  Time: 1.095s,  935.44/s  (1.107s,  924.88/s)  LR: 4.916e-04  Data: 0.010 (0.012)
Train: 25 [ 450/1251 ( 36%)]  Loss:  4.371181 (4.1691)  Time: 1.195s,  856.95/s  (1.108s,  924.15/s)  LR: 4.916e-04  Data: 0.010 (0.012)
Train: 25 [ 500/1251 ( 40%)]  Loss:  3.779934 (4.1337)  Time: 1.099s,  931.69/s  (1.107s,  924.84/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 550/1251 ( 44%)]  Loss:  4.189847 (4.1384)  Time: 1.124s,  911.03/s  (1.107s,  925.07/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [ 600/1251 ( 48%)]  Loss:  3.853138 (4.1165)  Time: 1.093s,  937.16/s  (1.107s,  925.22/s)  LR: 4.916e-04  Data: 0.010 (0.012)
Train: 25 [ 650/1251 ( 52%)]  Loss:  4.433448 (4.1391)  Time: 1.121s,  913.54/s  (1.107s,  925.21/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 700/1251 ( 56%)]  Loss:  4.227402 (4.1450)  Time: 1.125s,  910.44/s  (1.107s,  924.88/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 750/1251 ( 60%)]  Loss:  3.976154 (4.1344)  Time: 1.102s,  928.92/s  (1.107s,  924.97/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [ 800/1251 ( 64%)]  Loss:  4.099446 (4.1324)  Time: 1.097s,  933.72/s  (1.107s,  924.70/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 850/1251 ( 68%)]  Loss:  4.375780 (4.1459)  Time: 1.094s,  935.99/s  (1.107s,  925.04/s)  LR: 4.916e-04  Data: 0.010 (0.012)
Train: 25 [ 900/1251 ( 72%)]  Loss:  4.182819 (4.1479)  Time: 1.125s,  910.20/s  (1.107s,  924.65/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [ 950/1251 ( 76%)]  Loss:  3.979831 (4.1395)  Time: 1.097s,  933.19/s  (1.107s,  924.75/s)  LR: 4.916e-04  Data: 0.014 (0.012)
Train: 25 [1000/1251 ( 80%)]  Loss:  4.324313 (4.1483)  Time: 1.133s,  903.62/s  (1.108s,  924.22/s)  LR: 4.916e-04  Data: 0.010 (0.012)
Train: 25 [1050/1251 ( 84%)]  Loss:  4.248500 (4.1528)  Time: 1.099s,  931.73/s  (1.108s,  924.29/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [1100/1251 ( 88%)]  Loss:  4.259598 (4.1575)  Time: 1.097s,  933.45/s  (1.108s,  924.47/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [1150/1251 ( 92%)]  Loss:  4.300933 (4.1634)  Time: 1.099s,  931.84/s  (1.108s,  924.41/s)  LR: 4.916e-04  Data: 0.012 (0.012)
Train: 25 [1200/1251 ( 96%)]  Loss:  4.170784 (4.1637)  Time: 1.129s,  906.78/s  (1.107s,  924.64/s)  LR: 4.916e-04  Data: 0.011 (0.012)
Train: 25 [1250/1251 (100%)]  Loss:  4.324787 (4.1699)  Time: 1.080s,  948.17/s  (1.108s,  924.48/s)  LR: 4.916e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.245 (3.245)  Loss:  0.8527 (0.8527)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.9490 (1.5362)  Acc@1: 79.7170 (65.9460)  Acc@5: 93.6321 (87.3860)
Test (EMA): [   0/48]  Time: 3.355 (3.355)  Loss:  1.4062 (1.4062)  Acc@1: 72.2656 (72.2656)  Acc@5: 90.8203 (90.8203)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  1.5951 (2.4075)  Acc@1: 71.2264 (51.2580)  Acc@5: 85.3774 (75.6940)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 17.640000009765625)

Train: 26 [   0/1251 (  0%)]  Loss:  3.920996 (3.9210)  Time: 1.112s,  921.13/s  (1.112s,  921.13/s)  LR: 4.909e-04  Data: 0.029 (0.029)
Train: 26 [  50/1251 (  4%)]  Loss:  4.251145 (4.0861)  Time: 1.106s,  925.68/s  (1.101s,  930.41/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 100/1251 (  8%)]  Loss:  4.074983 (4.0824)  Time: 1.122s,  912.88/s  (1.105s,  926.29/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 150/1251 ( 12%)]  Loss:  4.049092 (4.0741)  Time: 1.121s,  913.78/s  (1.108s,  924.40/s)  LR: 4.909e-04  Data: 0.010 (0.012)
Train: 26 [ 200/1251 ( 16%)]  Loss:  4.341124 (4.1275)  Time: 1.097s,  933.59/s  (1.108s,  923.87/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Train: 26 [ 250/1251 ( 20%)]  Loss:  3.995927 (4.1055)  Time: 1.096s,  933.89/s  (1.107s,  924.62/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Train: 26 [ 300/1251 ( 24%)]  Loss:  3.477511 (4.0158)  Time: 1.095s,  935.36/s  (1.108s,  924.07/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Train: 26 [ 350/1251 ( 28%)]  Loss:  3.914264 (4.0031)  Time: 1.123s,  911.77/s  (1.107s,  924.67/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 400/1251 ( 32%)]  Loss:  4.049659 (4.0083)  Time: 1.186s,  863.41/s  (1.108s,  924.16/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 450/1251 ( 36%)]  Loss:  3.747673 (3.9822)  Time: 1.162s,  881.37/s  (1.109s,  923.32/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 500/1251 ( 40%)]  Loss:  4.204962 (4.0025)  Time: 1.095s,  935.17/s  (1.109s,  923.46/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 550/1251 ( 44%)]  Loss:  4.408648 (4.0363)  Time: 1.099s,  931.68/s  (1.109s,  923.54/s)  LR: 4.909e-04  Data: 0.013 (0.012)
Train: 26 [ 600/1251 ( 48%)]  Loss:  4.197792 (4.0488)  Time: 1.095s,  935.42/s  (1.109s,  923.63/s)  LR: 4.909e-04  Data: 0.010 (0.012)
Train: 26 [ 650/1251 ( 52%)]  Loss:  4.574105 (4.0863)  Time: 1.097s,  933.61/s  (1.109s,  923.43/s)  LR: 4.909e-04  Data: 0.014 (0.012)
Train: 26 [ 700/1251 ( 56%)]  Loss:  3.929853 (4.0758)  Time: 1.094s,  936.11/s  (1.108s,  923.94/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [ 750/1251 ( 60%)]  Loss:  4.059945 (4.0749)  Time: 1.100s,  930.70/s  (1.109s,  923.45/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Train: 26 [ 800/1251 ( 64%)]  Loss:  4.044485 (4.0731)  Time: 1.094s,  936.30/s  (1.109s,  923.67/s)  LR: 4.909e-04  Data: 0.010 (0.012)
Train: 26 [ 850/1251 ( 68%)]  Loss:  3.963066 (4.0670)  Time: 1.099s,  932.17/s  (1.109s,  923.48/s)  LR: 4.909e-04  Data: 0.013 (0.012)
Train: 26 [ 900/1251 ( 72%)]  Loss:  4.041706 (4.0656)  Time: 1.095s,  935.01/s  (1.109s,  923.65/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Train: 26 [ 950/1251 ( 76%)]  Loss:  4.117800 (4.0682)  Time: 1.098s,  932.87/s  (1.109s,  923.64/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Train: 26 [1000/1251 ( 80%)]  Loss:  4.278037 (4.0782)  Time: 1.097s,  933.10/s  (1.108s,  924.03/s)  LR: 4.909e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 26 [1050/1251 ( 84%)]  Loss:  4.062665 (4.0775)  Time: 1.100s,  931.27/s  (1.108s,  924.11/s)  LR: 4.909e-04  Data: 0.013 (0.012)
Train: 26 [1100/1251 ( 88%)]  Loss:  3.847609 (4.0675)  Time: 1.095s,  934.85/s  (1.108s,  924.08/s)  LR: 4.909e-04  Data: 0.011 (0.012)
Train: 26 [1150/1251 ( 92%)]  Loss:  3.976015 (4.0637)  Time: 1.197s,  855.38/s  (1.108s,  924.00/s)  LR: 4.909e-04  Data: 0.013 (0.012)
Train: 26 [1200/1251 ( 96%)]  Loss:  3.918258 (4.0579)  Time: 1.120s,  914.33/s  (1.108s,  923.84/s)  LR: 4.909e-04  Data: 0.010 (0.012)
Train: 26 [1250/1251 (100%)]  Loss:  3.933676 (4.0531)  Time: 1.080s,  947.98/s  (1.108s,  924.03/s)  LR: 4.909e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.265 (3.265)  Loss:  0.7772 (0.7772)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.8918 (1.4755)  Acc@1: 81.0141 (66.8120)  Acc@5: 93.2783 (87.8760)
Test (EMA): [   0/48]  Time: 3.192 (3.192)  Loss:  1.3047 (1.3047)  Acc@1: 73.4375 (73.4375)  Acc@5: 91.7969 (91.7969)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  1.4949 (2.2753)  Acc@1: 72.7594 (53.4280)  Acc@5: 86.2028 (77.5280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 22.300000009765625)

Train: 27 [   0/1251 (  0%)]  Loss:  4.039983 (4.0400)  Time: 1.126s,  909.42/s  (1.126s,  909.42/s)  LR: 4.902e-04  Data: 0.027 (0.027)
Train: 27 [  50/1251 (  4%)]  Loss:  4.173840 (4.1069)  Time: 1.119s,  915.40/s  (1.121s,  913.45/s)  LR: 4.902e-04  Data: 0.009 (0.012)
Train: 27 [ 100/1251 (  8%)]  Loss:  3.873499 (4.0291)  Time: 1.102s,  928.97/s  (1.115s,  918.76/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 150/1251 ( 12%)]  Loss:  3.862370 (3.9874)  Time: 1.097s,  933.25/s  (1.113s,  919.85/s)  LR: 4.902e-04  Data: 0.013 (0.012)
Train: 27 [ 200/1251 ( 16%)]  Loss:  3.836334 (3.9572)  Time: 1.096s,  933.98/s  (1.111s,  921.94/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [ 250/1251 ( 20%)]  Loss:  3.878157 (3.9440)  Time: 1.121s,  913.48/s  (1.112s,  920.96/s)  LR: 4.902e-04  Data: 0.010 (0.012)
Train: 27 [ 300/1251 ( 24%)]  Loss:  4.152911 (3.9739)  Time: 1.095s,  934.99/s  (1.110s,  922.48/s)  LR: 4.902e-04  Data: 0.010 (0.012)
Train: 27 [ 350/1251 ( 28%)]  Loss:  4.185432 (4.0003)  Time: 1.097s,  933.37/s  (1.111s,  921.73/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 400/1251 ( 32%)]  Loss:  3.804805 (3.9786)  Time: 1.098s,  932.98/s  (1.110s,  922.89/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [ 450/1251 ( 36%)]  Loss:  4.436382 (4.0244)  Time: 1.100s,  930.50/s  (1.109s,  923.57/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 500/1251 ( 40%)]  Loss:  4.328887 (4.0521)  Time: 1.107s,  924.88/s  (1.109s,  923.66/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 550/1251 ( 44%)]  Loss:  4.194592 (4.0639)  Time: 1.135s,  902.35/s  (1.109s,  923.31/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 600/1251 ( 48%)]  Loss:  4.412442 (4.0907)  Time: 1.120s,  914.42/s  (1.109s,  923.33/s)  LR: 4.902e-04  Data: 0.013 (0.012)
Train: 27 [ 650/1251 ( 52%)]  Loss:  3.991432 (4.0836)  Time: 1.097s,  933.66/s  (1.108s,  923.94/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 700/1251 ( 56%)]  Loss:  4.319981 (4.0994)  Time: 1.097s,  933.86/s  (1.109s,  923.71/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 750/1251 ( 60%)]  Loss:  4.370294 (4.1163)  Time: 1.100s,  930.80/s  (1.108s,  924.17/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [ 800/1251 ( 64%)]  Loss:  3.520655 (4.0813)  Time: 1.100s,  930.70/s  (1.108s,  924.06/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [ 850/1251 ( 68%)]  Loss:  4.192375 (4.0875)  Time: 1.102s,  929.57/s  (1.108s,  924.52/s)  LR: 4.902e-04  Data: 0.016 (0.012)
Train: 27 [ 900/1251 ( 72%)]  Loss:  4.087658 (4.0875)  Time: 1.100s,  931.28/s  (1.108s,  924.58/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [ 950/1251 ( 76%)]  Loss:  4.227438 (4.0945)  Time: 1.097s,  933.47/s  (1.107s,  924.72/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [1000/1251 ( 80%)]  Loss:  4.008594 (4.0904)  Time: 1.121s,  913.61/s  (1.107s,  924.83/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [1050/1251 ( 84%)]  Loss:  3.568074 (4.0666)  Time: 1.101s,  930.16/s  (1.107s,  924.75/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [1100/1251 ( 88%)]  Loss:  4.292443 (4.0765)  Time: 1.133s,  904.18/s  (1.108s,  924.35/s)  LR: 4.902e-04  Data: 0.010 (0.012)
Train: 27 [1150/1251 ( 92%)]  Loss:  4.060781 (4.0758)  Time: 1.096s,  933.98/s  (1.108s,  924.00/s)  LR: 4.902e-04  Data: 0.011 (0.012)
Train: 27 [1200/1251 ( 96%)]  Loss:  4.345170 (4.0866)  Time: 1.098s,  932.75/s  (1.108s,  924.26/s)  LR: 4.902e-04  Data: 0.012 (0.012)
Train: 27 [1250/1251 (100%)]  Loss:  3.907723 (4.0797)  Time: 1.095s,  935.52/s  (1.108s,  923.95/s)  LR: 4.902e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.326 (3.326)  Loss:  0.8375 (0.8375)  Acc@1: 83.7891 (83.7891)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.8548 (1.4626)  Acc@1: 81.6038 (67.3660)  Acc@5: 94.3396 (88.1640)
Test (EMA): [   0/48]  Time: 3.098 (3.098)  Loss:  1.2198 (1.2198)  Acc@1: 74.5117 (74.5117)  Acc@5: 92.4805 (92.4805)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  1.4037 (2.1577)  Acc@1: 74.2925 (55.4380)  Acc@5: 87.3821 (79.2460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 27.1720000390625)

Train: 28 [   0/1251 (  0%)]  Loss:  3.758742 (3.7587)  Time: 1.109s,  923.71/s  (1.109s,  923.71/s)  LR: 4.894e-04  Data: 0.024 (0.024)
Train: 28 [  50/1251 (  4%)]  Loss:  4.027763 (3.8933)  Time: 1.108s,  924.42/s  (1.105s,  926.67/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [ 100/1251 (  8%)]  Loss:  4.316401 (4.0343)  Time: 1.124s,  910.93/s  (1.113s,  920.29/s)  LR: 4.894e-04  Data: 0.013 (0.012)
Train: 28 [ 150/1251 ( 12%)]  Loss:  3.756364 (3.9648)  Time: 1.120s,  914.04/s  (1.109s,  923.00/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [ 200/1251 ( 16%)]  Loss:  4.311932 (4.0342)  Time: 1.098s,  932.59/s  (1.109s,  923.42/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [ 250/1251 ( 20%)]  Loss:  4.520343 (4.1153)  Time: 1.099s,  931.82/s  (1.109s,  923.56/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [ 300/1251 ( 24%)]  Loss:  3.751164 (4.0632)  Time: 1.098s,  932.80/s  (1.109s,  923.63/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [ 350/1251 ( 28%)]  Loss:  3.973888 (4.0521)  Time: 1.099s,  931.85/s  (1.108s,  924.23/s)  LR: 4.894e-04  Data: 0.015 (0.012)
Train: 28 [ 400/1251 ( 32%)]  Loss:  4.398267 (4.0905)  Time: 1.133s,  903.56/s  (1.108s,  924.54/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [ 450/1251 ( 36%)]  Loss:  3.996233 (4.0811)  Time: 1.103s,  927.98/s  (1.108s,  924.52/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [ 500/1251 ( 40%)]  Loss:  3.955274 (4.0697)  Time: 1.122s,  912.52/s  (1.107s,  925.20/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 28 [ 550/1251 ( 44%)]  Loss:  4.020303 (4.0656)  Time: 1.097s,  933.19/s  (1.107s,  925.21/s)  LR: 4.894e-04  Data: 0.010 (0.012)
Train: 28 [ 600/1251 ( 48%)]  Loss:  3.902979 (4.0531)  Time: 1.106s,  926.03/s  (1.106s,  925.49/s)  LR: 4.894e-04  Data: 0.010 (0.012)
Train: 28 [ 650/1251 ( 52%)]  Loss:  4.074036 (4.0545)  Time: 1.098s,  932.98/s  (1.107s,  925.31/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [ 700/1251 ( 56%)]  Loss:  4.057967 (4.0548)  Time: 1.101s,  929.96/s  (1.107s,  925.43/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [ 750/1251 ( 60%)]  Loss:  4.279884 (4.0688)  Time: 1.096s,  934.31/s  (1.108s,  924.26/s)  LR: 4.894e-04  Data: 0.010 (0.012)
Train: 28 [ 800/1251 ( 64%)]  Loss:  4.158796 (4.0741)  Time: 1.097s,  933.18/s  (1.108s,  924.56/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [ 850/1251 ( 68%)]  Loss:  3.878539 (4.0633)  Time: 1.198s,  854.69/s  (1.108s,  924.38/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [ 900/1251 ( 72%)]  Loss:  3.943337 (4.0570)  Time: 1.108s,  923.97/s  (1.108s,  923.94/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [ 950/1251 ( 76%)]  Loss:  4.028269 (4.0555)  Time: 1.095s,  935.40/s  (1.108s,  924.26/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [1000/1251 ( 80%)]  Loss:  4.171264 (4.0610)  Time: 1.102s,  929.57/s  (1.108s,  923.79/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [1050/1251 ( 84%)]  Loss:  3.997833 (4.0582)  Time: 1.096s,  934.27/s  (1.108s,  923.84/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [1100/1251 ( 88%)]  Loss:  3.727661 (4.0438)  Time: 1.097s,  933.24/s  (1.109s,  923.75/s)  LR: 4.894e-04  Data: 0.012 (0.012)
Train: 28 [1150/1251 ( 92%)]  Loss:  4.039638 (4.0436)  Time: 1.097s,  933.69/s  (1.108s,  923.81/s)  LR: 4.894e-04  Data: 0.011 (0.012)
Train: 28 [1200/1251 ( 96%)]  Loss:  3.992948 (4.0416)  Time: 1.101s,  930.10/s  (1.109s,  923.34/s)  LR: 4.894e-04  Data: 0.014 (0.012)
Train: 28 [1250/1251 (100%)]  Loss:  4.041842 (4.0416)  Time: 1.081s,  947.70/s  (1.109s,  923.58/s)  LR: 4.894e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.255 (3.255)  Loss:  0.8446 (0.8446)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.8621 (1.4651)  Acc@1: 81.7217 (67.6620)  Acc@5: 95.2830 (88.3920)
Test (EMA): [   0/48]  Time: 3.054 (3.054)  Loss:  1.1455 (1.1455)  Acc@1: 75.8789 (75.8789)  Acc@5: 92.9688 (92.9688)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  1.3187 (2.0520)  Acc@1: 75.1179 (57.3160)  Acc@5: 88.4434 (80.7420)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 31.52999994873047)

Train: 29 [   0/1251 (  0%)]  Loss:  4.271166 (4.2712)  Time: 1.129s,  906.87/s  (1.129s,  906.87/s)  LR: 4.887e-04  Data: 0.028 (0.028)
Train: 29 [  50/1251 (  4%)]  Loss:  3.823943 (4.0476)  Time: 1.098s,  932.68/s  (1.113s,  920.23/s)  LR: 4.887e-04  Data: 0.014 (0.012)
Train: 29 [ 100/1251 (  8%)]  Loss:  3.972920 (4.0227)  Time: 1.099s,  931.81/s  (1.108s,  924.43/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 150/1251 ( 12%)]  Loss:  3.951696 (4.0049)  Time: 1.096s,  934.69/s  (1.110s,  922.39/s)  LR: 4.887e-04  Data: 0.012 (0.012)
Train: 29 [ 200/1251 ( 16%)]  Loss:  3.791130 (3.9622)  Time: 1.098s,  932.84/s  (1.107s,  924.61/s)  LR: 4.887e-04  Data: 0.013 (0.012)
Train: 29 [ 250/1251 ( 20%)]  Loss:  3.444146 (3.8758)  Time: 1.099s,  931.53/s  (1.109s,  923.43/s)  LR: 4.887e-04  Data: 0.013 (0.012)
Train: 29 [ 300/1251 ( 24%)]  Loss:  4.332617 (3.9411)  Time: 1.119s,  914.85/s  (1.110s,  922.92/s)  LR: 4.887e-04  Data: 0.009 (0.012)
Train: 29 [ 350/1251 ( 28%)]  Loss:  4.349036 (3.9921)  Time: 1.098s,  932.81/s  (1.110s,  922.93/s)  LR: 4.887e-04  Data: 0.014 (0.012)
Train: 29 [ 400/1251 ( 32%)]  Loss:  4.229092 (4.0184)  Time: 1.123s,  911.81/s  (1.112s,  921.13/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 450/1251 ( 36%)]  Loss:  4.018087 (4.0184)  Time: 1.097s,  933.83/s  (1.113s,  920.37/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 500/1251 ( 40%)]  Loss:  4.053871 (4.0216)  Time: 1.099s,  931.63/s  (1.112s,  920.50/s)  LR: 4.887e-04  Data: 0.015 (0.012)
Train: 29 [ 550/1251 ( 44%)]  Loss:  3.734508 (3.9977)  Time: 1.095s,  935.17/s  (1.112s,  921.18/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 600/1251 ( 48%)]  Loss:  3.942013 (3.9934)  Time: 1.097s,  933.51/s  (1.113s,  920.39/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 650/1251 ( 52%)]  Loss:  4.307153 (4.0158)  Time: 1.096s,  934.64/s  (1.112s,  920.52/s)  LR: 4.887e-04  Data: 0.012 (0.012)
Train: 29 [ 700/1251 ( 56%)]  Loss:  4.221647 (4.0295)  Time: 1.098s,  932.55/s  (1.112s,  920.53/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 750/1251 ( 60%)]  Loss:  3.905927 (4.0218)  Time: 1.105s,  926.39/s  (1.112s,  921.13/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [ 800/1251 ( 64%)]  Loss:  3.879259 (4.0134)  Time: 1.197s,  855.64/s  (1.112s,  921.04/s)  LR: 4.887e-04  Data: 0.012 (0.012)
Train: 29 [ 850/1251 ( 68%)]  Loss:  4.200368 (4.0238)  Time: 1.100s,  931.00/s  (1.111s,  921.53/s)  LR: 4.887e-04  Data: 0.013 (0.012)
Train: 29 [ 900/1251 ( 72%)]  Loss:  4.449522 (4.0462)  Time: 1.097s,  933.17/s  (1.111s,  921.95/s)  LR: 4.887e-04  Data: 0.010 (0.012)
Train: 29 [ 950/1251 ( 76%)]  Loss:  4.203197 (4.0541)  Time: 1.096s,  934.73/s  (1.111s,  921.63/s)  LR: 4.887e-04  Data: 0.012 (0.012)
Train: 29 [1000/1251 ( 80%)]  Loss:  4.068745 (4.0548)  Time: 1.124s,  911.00/s  (1.111s,  921.68/s)  LR: 4.887e-04  Data: 0.012 (0.012)
Train: 29 [1050/1251 ( 84%)]  Loss:  4.106628 (4.0571)  Time: 1.096s,  934.42/s  (1.111s,  921.68/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [1100/1251 ( 88%)]  Loss:  4.047161 (4.0567)  Time: 1.129s,  906.77/s  (1.111s,  921.95/s)  LR: 4.887e-04  Data: 0.011 (0.012)
Train: 29 [1150/1251 ( 92%)]  Loss:  4.041053 (4.0560)  Time: 1.124s,  910.90/s  (1.111s,  921.98/s)  LR: 4.887e-04  Data: 0.012 (0.012)
Train: 29 [1200/1251 ( 96%)]  Loss:  3.756730 (4.0441)  Time: 1.098s,  932.35/s  (1.111s,  922.02/s)  LR: 4.887e-04  Data: 0.014 (0.012)
Train: 29 [1250/1251 (100%)]  Loss:  4.000326 (4.0424)  Time: 1.080s,  947.89/s  (1.111s,  921.92/s)  LR: 4.887e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.186 (3.186)  Loss:  0.7880 (0.7880)  Acc@1: 84.5703 (84.5703)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.9010 (1.4360)  Acc@1: 80.0708 (67.8680)  Acc@5: 94.5755 (88.7080)
Test (EMA): [   0/48]  Time: 3.154 (3.154)  Loss:  1.0835 (1.0835)  Acc@1: 77.1484 (77.1484)  Acc@5: 93.4570 (93.4570)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  1.2427 (1.9564)  Acc@1: 76.6509 (58.9400)  Acc@5: 90.0943 (82.0820)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 35.75400003662109)

Train: 30 [   0/1251 (  0%)]  Loss:  3.865566 (3.8656)  Time: 1.108s,  924.35/s  (1.108s,  924.35/s)  LR: 4.879e-04  Data: 0.024 (0.024)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 30 [  50/1251 (  4%)]  Loss:  4.345013 (4.1053)  Time: 1.104s,  927.29/s  (1.102s,  929.49/s)  LR: 4.879e-04  Data: 0.010 (0.012)
Train: 30 [ 100/1251 (  8%)]  Loss:  4.215045 (4.1419)  Time: 1.096s,  934.00/s  (1.108s,  924.25/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 150/1251 ( 12%)]  Loss:  3.833304 (4.0647)  Time: 1.097s,  933.06/s  (1.105s,  926.72/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 200/1251 ( 16%)]  Loss:  3.412374 (3.9343)  Time: 1.095s,  935.06/s  (1.105s,  927.11/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [ 250/1251 ( 20%)]  Loss:  4.001264 (3.9454)  Time: 1.097s,  933.57/s  (1.105s,  926.83/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 300/1251 ( 24%)]  Loss:  4.265615 (3.9912)  Time: 1.096s,  934.04/s  (1.106s,  926.15/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [ 350/1251 ( 28%)]  Loss:  4.277796 (4.0270)  Time: 1.098s,  932.69/s  (1.106s,  925.81/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 400/1251 ( 32%)]  Loss:  4.214216 (4.0478)  Time: 1.097s,  933.36/s  (1.105s,  926.36/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 450/1251 ( 36%)]  Loss:  3.914382 (4.0345)  Time: 1.107s,  925.00/s  (1.106s,  925.71/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 500/1251 ( 40%)]  Loss:  3.573390 (3.9925)  Time: 1.099s,  931.85/s  (1.106s,  926.09/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 550/1251 ( 44%)]  Loss:  4.052343 (3.9975)  Time: 1.121s,  913.13/s  (1.107s,  925.24/s)  LR: 4.879e-04  Data: 0.010 (0.012)
Train: 30 [ 600/1251 ( 48%)]  Loss:  4.186860 (4.0121)  Time: 1.095s,  935.07/s  (1.108s,  924.48/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [ 650/1251 ( 52%)]  Loss:  4.368022 (4.0375)  Time: 1.104s,  927.64/s  (1.108s,  924.16/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [ 700/1251 ( 56%)]  Loss:  3.550738 (4.0051)  Time: 1.099s,  931.70/s  (1.109s,  923.61/s)  LR: 4.879e-04  Data: 0.014 (0.012)
Train: 30 [ 750/1251 ( 60%)]  Loss:  3.836154 (3.9945)  Time: 1.098s,  932.55/s  (1.109s,  923.42/s)  LR: 4.879e-04  Data: 0.013 (0.012)
Train: 30 [ 800/1251 ( 64%)]  Loss:  4.309847 (4.0131)  Time: 1.107s,  925.20/s  (1.109s,  923.11/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [ 850/1251 ( 68%)]  Loss:  3.692964 (3.9953)  Time: 1.097s,  933.24/s  (1.109s,  923.63/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [ 900/1251 ( 72%)]  Loss:  4.327808 (4.0128)  Time: 1.098s,  932.80/s  (1.108s,  923.85/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [ 950/1251 ( 76%)]  Loss:  3.997963 (4.0120)  Time: 1.107s,  925.13/s  (1.108s,  923.92/s)  LR: 4.879e-04  Data: 0.010 (0.012)
Train: 30 [1000/1251 ( 80%)]  Loss:  3.908296 (4.0071)  Time: 1.095s,  935.09/s  (1.108s,  923.88/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [1050/1251 ( 84%)]  Loss:  3.907662 (4.0026)  Time: 1.098s,  932.86/s  (1.108s,  923.83/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [1100/1251 ( 88%)]  Loss:  3.845878 (3.9958)  Time: 1.126s,  909.62/s  (1.109s,  923.46/s)  LR: 4.879e-04  Data: 0.012 (0.012)
Train: 30 [1150/1251 ( 92%)]  Loss:  4.207746 (4.0046)  Time: 1.094s,  935.84/s  (1.108s,  923.78/s)  LR: 4.879e-04  Data: 0.010 (0.012)
Train: 30 [1200/1251 ( 96%)]  Loss:  3.896697 (4.0003)  Time: 1.108s,  923.91/s  (1.108s,  923.77/s)  LR: 4.879e-04  Data: 0.011 (0.012)
Train: 30 [1250/1251 (100%)]  Loss:  3.655161 (3.9870)  Time: 1.083s,  945.63/s  (1.109s,  923.52/s)  LR: 4.879e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.318 (3.318)  Loss:  0.8144 (0.8144)  Acc@1: 84.1797 (84.1797)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.7936 (1.4082)  Acc@1: 81.2500 (68.1680)  Acc@5: 94.6934 (88.7860)
Test (EMA): [   0/48]  Time: 3.167 (3.167)  Loss:  1.0279 (1.0279)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.0430 (94.0430)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  1.1765 (1.8702)  Acc@1: 77.3585 (60.5220)  Acc@5: 90.5660 (83.3580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 39.48599999145508)

Train: 31 [   0/1251 (  0%)]  Loss:  4.295253 (4.2953)  Time: 1.413s,  724.51/s  (1.413s,  724.51/s)  LR: 4.871e-04  Data: 0.188 (0.188)
Train: 31 [  50/1251 (  4%)]  Loss:  3.955250 (4.1253)  Time: 1.100s,  930.51/s  (1.122s,  912.59/s)  LR: 4.871e-04  Data: 0.011 (0.015)
Train: 31 [ 100/1251 (  8%)]  Loss:  3.903236 (4.0512)  Time: 1.121s,  913.09/s  (1.113s,  919.82/s)  LR: 4.871e-04  Data: 0.010 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 31 [ 150/1251 ( 12%)]  Loss:  3.869334 (4.0058)  Time: 1.190s,  860.31/s  (1.111s,  921.29/s)  LR: 4.871e-04  Data: 0.010 (0.013)
Train: 31 [ 200/1251 ( 16%)]  Loss:  4.083369 (4.0213)  Time: 1.097s,  933.47/s  (1.111s,  921.82/s)  LR: 4.871e-04  Data: 0.012 (0.013)
Train: 31 [ 250/1251 ( 20%)]  Loss:  4.011783 (4.0197)  Time: 1.097s,  933.38/s  (1.110s,  922.28/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 300/1251 ( 24%)]  Loss:  4.135364 (4.0362)  Time: 1.097s,  933.22/s  (1.111s,  921.54/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 350/1251 ( 28%)]  Loss:  3.717582 (3.9964)  Time: 1.133s,  903.85/s  (1.110s,  922.86/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [ 400/1251 ( 32%)]  Loss:  4.331213 (4.0336)  Time: 1.096s,  934.11/s  (1.110s,  922.51/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 450/1251 ( 36%)]  Loss:  4.357420 (4.0660)  Time: 1.096s,  933.89/s  (1.109s,  923.04/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 500/1251 ( 40%)]  Loss:  4.053426 (4.0648)  Time: 1.103s,  928.20/s  (1.109s,  923.38/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 550/1251 ( 44%)]  Loss:  3.931646 (4.0537)  Time: 1.104s,  927.72/s  (1.108s,  923.94/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [ 600/1251 ( 48%)]  Loss:  4.053720 (4.0537)  Time: 1.101s,  930.17/s  (1.109s,  923.56/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [ 650/1251 ( 52%)]  Loss:  3.986131 (4.0489)  Time: 1.136s,  901.02/s  (1.109s,  923.68/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 700/1251 ( 56%)]  Loss:  3.900043 (4.0390)  Time: 1.100s,  931.19/s  (1.110s,  922.87/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 750/1251 ( 60%)]  Loss:  3.985793 (4.0357)  Time: 1.097s,  933.13/s  (1.110s,  922.91/s)  LR: 4.871e-04  Data: 0.010 (0.012)
Train: 31 [ 800/1251 ( 64%)]  Loss:  3.882162 (4.0266)  Time: 1.122s,  912.69/s  (1.109s,  923.17/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [ 850/1251 ( 68%)]  Loss:  3.778961 (4.0129)  Time: 1.097s,  933.50/s  (1.109s,  923.20/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 900/1251 ( 72%)]  Loss:  3.996682 (4.0120)  Time: 1.096s,  933.92/s  (1.109s,  923.40/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [ 950/1251 ( 76%)]  Loss:  4.215801 (4.0222)  Time: 1.098s,  932.28/s  (1.109s,  923.37/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [1000/1251 ( 80%)]  Loss:  3.976920 (4.0201)  Time: 1.101s,  930.34/s  (1.109s,  923.64/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [1050/1251 ( 84%)]  Loss:  3.982612 (4.0183)  Time: 1.128s,  907.74/s  (1.109s,  923.63/s)  LR: 4.871e-04  Data: 0.012 (0.012)
Train: 31 [1100/1251 ( 88%)]  Loss:  3.820383 (4.0097)  Time: 1.196s,  856.42/s  (1.109s,  923.24/s)  LR: 4.871e-04  Data: 0.015 (0.012)
Train: 31 [1150/1251 ( 92%)]  Loss:  3.994253 (4.0091)  Time: 1.107s,  925.44/s  (1.109s,  923.37/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [1200/1251 ( 96%)]  Loss:  3.839294 (4.0023)  Time: 1.103s,  928.47/s  (1.109s,  923.64/s)  LR: 4.871e-04  Data: 0.011 (0.012)
Train: 31 [1250/1251 (100%)]  Loss:  3.917946 (3.9991)  Time: 1.172s,  873.90/s  (1.109s,  923.41/s)  LR: 4.871e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.304 (3.304)  Loss:  0.8322 (0.8322)  Acc@1: 83.2031 (83.2031)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.7991 (1.3889)  Acc@1: 82.3113 (69.0920)  Acc@5: 94.6934 (89.2980)
Test (EMA): [   0/48]  Time: 3.126 (3.126)  Loss:  0.9805 (0.9805)  Acc@1: 79.7852 (79.7852)  Acc@5: 94.2383 (94.2383)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  1.1175 (1.7935)  Acc@1: 78.4198 (62.0460)  Acc@5: 91.0377 (84.3400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 42.835999995117184)

Train: 32 [   0/1251 (  0%)]  Loss:  4.120175 (4.1202)  Time: 1.106s,  926.06/s  (1.106s,  926.06/s)  LR: 4.862e-04  Data: 0.024 (0.024)
Train: 32 [  50/1251 (  4%)]  Loss:  3.678958 (3.8996)  Time: 1.095s,  934.87/s  (1.109s,  923.72/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 100/1251 (  8%)]  Loss:  3.718153 (3.8391)  Time: 1.100s,  931.26/s  (1.108s,  924.08/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [ 150/1251 ( 12%)]  Loss:  3.945670 (3.8657)  Time: 1.099s,  931.82/s  (1.107s,  925.30/s)  LR: 4.862e-04  Data: 0.014 (0.012)
Train: 32 [ 200/1251 ( 16%)]  Loss:  3.668761 (3.8263)  Time: 1.103s,  928.73/s  (1.107s,  924.93/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [ 250/1251 ( 20%)]  Loss:  4.024393 (3.8594)  Time: 1.097s,  933.61/s  (1.109s,  923.28/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [ 300/1251 ( 24%)]  Loss:  3.873627 (3.8614)  Time: 1.100s,  930.93/s  (1.108s,  924.48/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 350/1251 ( 28%)]  Loss:  4.148838 (3.8973)  Time: 1.136s,  901.04/s  (1.109s,  923.45/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 400/1251 ( 32%)]  Loss:  3.907766 (3.8985)  Time: 1.106s,  925.58/s  (1.109s,  923.47/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 450/1251 ( 36%)]  Loss:  3.655262 (3.8742)  Time: 1.132s,  904.62/s  (1.110s,  922.88/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [ 500/1251 ( 40%)]  Loss:  3.868461 (3.8736)  Time: 1.098s,  932.60/s  (1.109s,  923.47/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 550/1251 ( 44%)]  Loss:  3.748879 (3.8632)  Time: 1.102s,  928.88/s  (1.109s,  923.14/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [ 600/1251 ( 48%)]  Loss:  3.791516 (3.8577)  Time: 1.132s,  904.47/s  (1.109s,  923.74/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 650/1251 ( 52%)]  Loss:  4.081567 (3.8737)  Time: 1.241s,  825.21/s  (1.109s,  923.73/s)  LR: 4.862e-04  Data: 0.016 (0.012)
Train: 32 [ 700/1251 ( 56%)]  Loss:  4.231700 (3.8976)  Time: 1.122s,  912.68/s  (1.109s,  923.35/s)  LR: 4.862e-04  Data: 0.010 (0.012)
Train: 32 [ 750/1251 ( 60%)]  Loss:  4.042103 (3.9066)  Time: 1.111s,  921.33/s  (1.109s,  923.28/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 800/1251 ( 64%)]  Loss:  3.792306 (3.8999)  Time: 1.098s,  932.83/s  (1.109s,  923.49/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 850/1251 ( 68%)]  Loss:  3.814970 (3.8952)  Time: 1.108s,  924.52/s  (1.109s,  923.58/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [ 900/1251 ( 72%)]  Loss:  3.939222 (3.8975)  Time: 1.109s,  923.12/s  (1.109s,  923.40/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [ 950/1251 ( 76%)]  Loss:  4.089995 (3.9071)  Time: 1.100s,  931.19/s  (1.109s,  923.60/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [1000/1251 ( 80%)]  Loss:  4.016539 (3.9123)  Time: 1.104s,  927.35/s  (1.109s,  923.18/s)  LR: 4.862e-04  Data: 0.014 (0.012)
Train: 32 [1050/1251 ( 84%)]  Loss:  3.998191 (3.9162)  Time: 1.121s,  913.28/s  (1.109s,  923.13/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [1100/1251 ( 88%)]  Loss:  4.020995 (3.9208)  Time: 1.100s,  930.63/s  (1.110s,  922.89/s)  LR: 4.862e-04  Data: 0.012 (0.012)
Train: 32 [1150/1251 ( 92%)]  Loss:  4.174134 (3.9313)  Time: 1.096s,  934.10/s  (1.109s,  923.08/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [1200/1251 ( 96%)]  Loss:  3.615641 (3.9187)  Time: 1.097s,  933.57/s  (1.110s,  922.67/s)  LR: 4.862e-04  Data: 0.011 (0.012)
Train: 32 [1250/1251 (100%)]  Loss:  3.908404 (3.9183)  Time: 1.080s,  947.93/s  (1.110s,  922.63/s)  LR: 4.862e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.223 (3.223)  Loss:  0.7789 (0.7789)  Acc@1: 84.4727 (84.4727)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.8364 (1.3862)  Acc@1: 81.6038 (69.3960)  Acc@5: 94.6934 (89.3900)
Test (EMA): [   0/48]  Time: 3.152 (3.152)  Loss:  0.9345 (0.9345)  Acc@1: 80.8594 (80.8594)  Acc@5: 94.6289 (94.6289)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  1.0626 (1.7237)  Acc@1: 79.3632 (63.2900)  Acc@5: 91.7453 (85.1880)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 46.05799982910156)

Train: 33 [   0/1251 (  0%)]  Loss:  3.692509 (3.6925)  Time: 1.111s,  921.30/s  (1.111s,  921.30/s)  LR: 4.854e-04  Data: 0.026 (0.026)
Train: 33 [  50/1251 (  4%)]  Loss:  3.952650 (3.8226)  Time: 1.111s,  921.69/s  (1.107s,  924.68/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 100/1251 (  8%)]  Loss:  3.871588 (3.8389)  Time: 1.096s,  934.06/s  (1.105s,  927.08/s)  LR: 4.854e-04  Data: 0.012 (0.012)
Train: 33 [ 150/1251 ( 12%)]  Loss:  3.891551 (3.8521)  Time: 1.097s,  933.15/s  (1.104s,  927.83/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 200/1251 ( 16%)]  Loss:  3.554881 (3.7926)  Time: 1.097s,  933.60/s  (1.107s,  925.33/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 33 [ 250/1251 ( 20%)]  Loss:  4.224439 (3.8646)  Time: 1.096s,  934.04/s  (1.107s,  924.62/s)  LR: 4.854e-04  Data: 0.014 (0.012)
Train: 33 [ 300/1251 ( 24%)]  Loss:  3.755218 (3.8490)  Time: 1.098s,  932.37/s  (1.107s,  924.76/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 350/1251 ( 28%)]  Loss:  4.057292 (3.8750)  Time: 1.098s,  932.19/s  (1.106s,  925.58/s)  LR: 4.854e-04  Data: 0.012 (0.012)
Train: 33 [ 400/1251 ( 32%)]  Loss:  3.743901 (3.8604)  Time: 1.099s,  932.02/s  (1.107s,  925.14/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 450/1251 ( 36%)]  Loss:  3.978493 (3.8723)  Time: 1.096s,  934.04/s  (1.106s,  925.57/s)  LR: 4.854e-04  Data: 0.012 (0.012)
Train: 33 [ 500/1251 ( 40%)]  Loss:  4.077382 (3.8909)  Time: 1.134s,  903.33/s  (1.107s,  925.09/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 550/1251 ( 44%)]  Loss:  3.900315 (3.8917)  Time: 1.104s,  927.70/s  (1.107s,  925.26/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 600/1251 ( 48%)]  Loss:  3.917953 (3.8937)  Time: 1.097s,  933.66/s  (1.107s,  925.20/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 650/1251 ( 52%)]  Loss:  3.937146 (3.8968)  Time: 1.123s,  911.73/s  (1.107s,  924.86/s)  LR: 4.854e-04  Data: 0.013 (0.012)
Train: 33 [ 700/1251 ( 56%)]  Loss:  4.002248 (3.9038)  Time: 1.106s,  925.92/s  (1.107s,  925.09/s)  LR: 4.854e-04  Data: 0.012 (0.012)
Train: 33 [ 750/1251 ( 60%)]  Loss:  3.809190 (3.8979)  Time: 1.096s,  934.30/s  (1.107s,  924.72/s)  LR: 4.854e-04  Data: 0.013 (0.012)
Train: 33 [ 800/1251 ( 64%)]  Loss:  3.833301 (3.8941)  Time: 1.122s,  913.00/s  (1.108s,  924.53/s)  LR: 4.854e-04  Data: 0.012 (0.012)
Train: 33 [ 850/1251 ( 68%)]  Loss:  4.117934 (3.9066)  Time: 1.134s,  902.62/s  (1.108s,  923.99/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 900/1251 ( 72%)]  Loss:  4.003685 (3.9117)  Time: 1.098s,  932.92/s  (1.108s,  924.28/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [ 950/1251 ( 76%)]  Loss:  3.851705 (3.9087)  Time: 1.136s,  901.40/s  (1.108s,  924.30/s)  LR: 4.854e-04  Data: 0.012 (0.012)
Train: 33 [1000/1251 ( 80%)]  Loss:  3.981880 (3.9122)  Time: 1.102s,  928.91/s  (1.108s,  924.19/s)  LR: 4.854e-04  Data: 0.010 (0.012)
Train: 33 [1050/1251 ( 84%)]  Loss:  4.061080 (3.9189)  Time: 1.096s,  934.40/s  (1.108s,  923.99/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [1100/1251 ( 88%)]  Loss:  4.105387 (3.9270)  Time: 1.100s,  931.12/s  (1.108s,  924.01/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [1150/1251 ( 92%)]  Loss:  4.294055 (3.9423)  Time: 1.118s,  915.89/s  (1.108s,  924.02/s)  LR: 4.854e-04  Data: 0.011 (0.012)
Train: 33 [1200/1251 ( 96%)]  Loss:  3.849125 (3.9386)  Time: 1.116s,  917.86/s  (1.108s,  924.32/s)  LR: 4.854e-04  Data: 0.010 (0.012)
Train: 33 [1250/1251 (100%)]  Loss:  3.526874 (3.9228)  Time: 1.082s,  946.16/s  (1.108s,  924.43/s)  LR: 4.854e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.244 (3.244)  Loss:  0.6738 (0.6738)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.7759 (1.3218)  Acc@1: 82.6651 (69.4360)  Acc@5: 94.6934 (89.6040)
Test (EMA): [   0/48]  Time: 3.192 (3.192)  Loss:  0.8930 (0.8930)  Acc@1: 81.5430 (81.5430)  Acc@5: 95.0195 (95.0195)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  1.0136 (1.6610)  Acc@1: 80.0708 (64.2900)  Acc@5: 92.3349 (85.9180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 48.76200002685547)

Train: 34 [   0/1251 (  0%)]  Loss:  3.970570 (3.9706)  Time: 1.118s,  915.80/s  (1.118s,  915.80/s)  LR: 4.845e-04  Data: 0.033 (0.033)
Train: 34 [  50/1251 (  4%)]  Loss:  3.472506 (3.7215)  Time: 1.098s,  933.01/s  (1.104s,  927.64/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 100/1251 (  8%)]  Loss:  3.770495 (3.7379)  Time: 1.099s,  932.12/s  (1.104s,  927.26/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 150/1251 ( 12%)]  Loss:  4.494088 (3.9269)  Time: 1.093s,  936.52/s  (1.107s,  925.11/s)  LR: 4.845e-04  Data: 0.010 (0.012)
Train: 34 [ 200/1251 ( 16%)]  Loss:  4.038573 (3.9492)  Time: 1.102s,  929.54/s  (1.108s,  924.12/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 250/1251 ( 20%)]  Loss:  3.890097 (3.9394)  Time: 1.100s,  930.50/s  (1.110s,  922.77/s)  LR: 4.845e-04  Data: 0.013 (0.012)
Train: 34 [ 300/1251 ( 24%)]  Loss:  4.112899 (3.9642)  Time: 1.096s,  934.46/s  (1.109s,  923.62/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 350/1251 ( 28%)]  Loss:  4.225402 (3.9968)  Time: 1.101s,  930.38/s  (1.108s,  924.00/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 400/1251 ( 32%)]  Loss:  4.269881 (4.0272)  Time: 1.098s,  932.79/s  (1.108s,  924.54/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 450/1251 ( 36%)]  Loss:  3.473684 (3.9718)  Time: 1.099s,  932.12/s  (1.108s,  924.33/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 500/1251 ( 40%)]  Loss:  3.957577 (3.9705)  Time: 1.096s,  934.69/s  (1.107s,  924.97/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 550/1251 ( 44%)]  Loss:  3.665420 (3.9451)  Time: 1.110s,  922.84/s  (1.107s,  925.00/s)  LR: 4.845e-04  Data: 0.013 (0.012)
Train: 34 [ 600/1251 ( 48%)]  Loss:  3.932119 (3.9441)  Time: 1.098s,  932.64/s  (1.107s,  925.04/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 650/1251 ( 52%)]  Loss:  4.076732 (3.9536)  Time: 1.097s,  933.34/s  (1.107s,  925.30/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 700/1251 ( 56%)]  Loss:  4.034102 (3.9589)  Time: 1.096s,  933.88/s  (1.107s,  925.29/s)  LR: 4.845e-04  Data: 0.014 (0.012)
Train: 34 [ 750/1251 ( 60%)]  Loss:  4.123511 (3.9692)  Time: 1.101s,  930.03/s  (1.107s,  925.42/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 800/1251 ( 64%)]  Loss:  3.797400 (3.9591)  Time: 1.119s,  915.30/s  (1.107s,  925.33/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [ 850/1251 ( 68%)]  Loss:  3.896417 (3.9556)  Time: 1.097s,  933.66/s  (1.107s,  925.42/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 900/1251 ( 72%)]  Loss:  3.800757 (3.9475)  Time: 1.096s,  934.48/s  (1.107s,  925.28/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [ 950/1251 ( 76%)]  Loss:  4.246799 (3.9625)  Time: 1.098s,  932.89/s  (1.106s,  925.55/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [1000/1251 ( 80%)]  Loss:  3.766900 (3.9531)  Time: 1.099s,  931.68/s  (1.106s,  925.58/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [1050/1251 ( 84%)]  Loss:  3.547550 (3.9347)  Time: 1.191s,  859.46/s  (1.106s,  925.77/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 34 [1100/1251 ( 88%)]  Loss:  3.881697 (3.9324)  Time: 1.204s,  850.81/s  (1.106s,  925.74/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [1150/1251 ( 92%)]  Loss:  3.734269 (3.9241)  Time: 1.102s,  929.58/s  (1.106s,  925.94/s)  LR: 4.845e-04  Data: 0.012 (0.012)
Train: 34 [1200/1251 ( 96%)]  Loss:  4.244625 (3.9370)  Time: 1.123s,  911.72/s  (1.106s,  926.10/s)  LR: 4.845e-04  Data: 0.011 (0.012)
Train: 34 [1250/1251 (100%)]  Loss:  4.075121 (3.9423)  Time: 1.084s,  944.62/s  (1.106s,  925.97/s)  LR: 4.845e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.217 (3.217)  Loss:  0.7481 (0.7481)  Acc@1: 84.6680 (84.6680)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.8589 (1.3698)  Acc@1: 81.9576 (69.6620)  Acc@5: 94.4576 (89.6800)
Test (EMA): [   0/48]  Time: 3.169 (3.169)  Loss:  0.8565 (0.8565)  Acc@1: 81.6406 (81.6406)  Acc@5: 95.4102 (95.4102)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.9715 (1.6034)  Acc@1: 80.0708 (65.2800)  Acc@5: 92.4528 (86.6780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 51.258000041503905)

Train: 35 [   0/1251 (  0%)]  Loss:  4.105814 (4.1058)  Time: 1.109s,  923.29/s  (1.109s,  923.29/s)  LR: 4.836e-04  Data: 0.024 (0.024)
Train: 35 [  50/1251 (  4%)]  Loss:  3.804532 (3.9552)  Time: 1.131s,  905.31/s  (1.112s,  920.97/s)  LR: 4.836e-04  Data: 0.010 (0.012)
Train: 35 [ 100/1251 (  8%)]  Loss:  3.874694 (3.9283)  Time: 1.122s,  912.38/s  (1.110s,  922.69/s)  LR: 4.836e-04  Data: 0.013 (0.012)
Train: 35 [ 150/1251 ( 12%)]  Loss:  4.089981 (3.9688)  Time: 1.103s,  928.47/s  (1.107s,  925.39/s)  LR: 4.836e-04  Data: 0.011 (0.012)
Train: 35 [ 200/1251 ( 16%)]  Loss:  4.086979 (3.9924)  Time: 1.100s,  931.28/s  (1.109s,  923.55/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [ 250/1251 ( 20%)]  Loss:  4.221783 (4.0306)  Time: 1.101s,  930.03/s  (1.108s,  923.79/s)  LR: 4.836e-04  Data: 0.010 (0.012)
Train: 35 [ 300/1251 ( 24%)]  Loss:  3.886387 (4.0100)  Time: 1.136s,  901.07/s  (1.109s,  923.00/s)  LR: 4.836e-04  Data: 0.011 (0.012)
Train: 35 [ 350/1251 ( 28%)]  Loss:  4.061737 (4.0165)  Time: 1.099s,  931.92/s  (1.109s,  923.72/s)  LR: 4.836e-04  Data: 0.013 (0.012)
Train: 35 [ 400/1251 ( 32%)]  Loss:  3.742630 (3.9861)  Time: 1.097s,  933.11/s  (1.108s,  923.91/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [ 450/1251 ( 36%)]  Loss:  3.398894 (3.9273)  Time: 1.098s,  932.94/s  (1.107s,  924.80/s)  LR: 4.836e-04  Data: 0.013 (0.012)
Train: 35 [ 500/1251 ( 40%)]  Loss:  4.000514 (3.9340)  Time: 1.102s,  929.17/s  (1.107s,  924.63/s)  LR: 4.836e-04  Data: 0.013 (0.012)
Train: 35 [ 550/1251 ( 44%)]  Loss:  3.773479 (3.9206)  Time: 1.190s,  860.24/s  (1.107s,  925.10/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [ 600/1251 ( 48%)]  Loss:  3.900129 (3.9190)  Time: 1.097s,  933.51/s  (1.106s,  925.56/s)  LR: 4.836e-04  Data: 0.013 (0.012)
Train: 35 [ 650/1251 ( 52%)]  Loss:  3.803073 (3.9108)  Time: 1.132s,  904.44/s  (1.107s,  925.22/s)  LR: 4.836e-04  Data: 0.010 (0.012)
Train: 35 [ 700/1251 ( 56%)]  Loss:  3.785367 (3.9024)  Time: 1.097s,  933.62/s  (1.106s,  925.54/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [ 750/1251 ( 60%)]  Loss:  3.990836 (3.9079)  Time: 1.098s,  932.70/s  (1.107s,  924.94/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [ 800/1251 ( 64%)]  Loss:  3.737824 (3.8979)  Time: 1.103s,  928.24/s  (1.107s,  924.87/s)  LR: 4.836e-04  Data: 0.011 (0.012)
Train: 35 [ 850/1251 ( 68%)]  Loss:  3.985373 (3.9028)  Time: 1.098s,  933.01/s  (1.108s,  924.43/s)  LR: 4.836e-04  Data: 0.010 (0.012)
Train: 35 [ 900/1251 ( 72%)]  Loss:  3.771360 (3.8959)  Time: 1.096s,  934.13/s  (1.107s,  924.67/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [ 950/1251 ( 76%)]  Loss:  3.753134 (3.8887)  Time: 1.103s,  928.12/s  (1.107s,  924.74/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [1000/1251 ( 80%)]  Loss:  3.661555 (3.8779)  Time: 1.104s,  927.35/s  (1.108s,  924.58/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [1050/1251 ( 84%)]  Loss:  3.812358 (3.8749)  Time: 1.192s,  858.86/s  (1.108s,  924.34/s)  LR: 4.836e-04  Data: 0.010 (0.012)
Train: 35 [1100/1251 ( 88%)]  Loss:  3.866677 (3.8746)  Time: 1.097s,  933.77/s  (1.107s,  924.61/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [1150/1251 ( 92%)]  Loss:  3.986182 (3.8792)  Time: 1.097s,  933.09/s  (1.107s,  924.61/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [1200/1251 ( 96%)]  Loss:  3.790445 (3.8757)  Time: 1.096s,  934.13/s  (1.108s,  924.58/s)  LR: 4.836e-04  Data: 0.012 (0.012)
Train: 35 [1250/1251 (100%)]  Loss:  4.070037 (3.8831)  Time: 1.083s,  945.84/s  (1.107s,  924.66/s)  LR: 4.836e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.175 (3.175)  Loss:  0.6943 (0.6943)  Acc@1: 85.1562 (85.1562)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.8101 (1.3058)  Acc@1: 81.1321 (70.0400)  Acc@5: 95.4009 (89.8120)
Test (EMA): [   0/48]  Time: 3.119 (3.119)  Loss:  0.8221 (0.8221)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.4102 (95.4102)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.9309 (1.5501)  Acc@1: 80.6604 (66.3380)  Acc@5: 93.2783 (87.3260)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 53.428000061035156)

Train: 36 [   0/1251 (  0%)]  Loss:  3.924861 (3.9249)  Time: 1.124s,  911.42/s  (1.124s,  911.42/s)  LR: 4.826e-04  Data: 0.023 (0.023)
Train: 36 [  50/1251 (  4%)]  Loss:  3.675115 (3.8000)  Time: 1.103s,  928.69/s  (1.115s,  918.04/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 100/1251 (  8%)]  Loss:  4.175733 (3.9252)  Time: 1.099s,  932.08/s  (1.109s,  923.20/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [ 150/1251 ( 12%)]  Loss:  4.239353 (4.0038)  Time: 1.096s,  934.70/s  (1.111s,  921.84/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 200/1251 ( 16%)]  Loss:  4.146864 (4.0324)  Time: 1.099s,  932.17/s  (1.109s,  922.95/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 250/1251 ( 20%)]  Loss:  3.992110 (4.0257)  Time: 1.121s,  913.47/s  (1.111s,  921.59/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 300/1251 ( 24%)]  Loss:  3.675149 (3.9756)  Time: 1.099s,  931.83/s  (1.109s,  922.96/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [ 350/1251 ( 28%)]  Loss:  4.129967 (3.9949)  Time: 1.096s,  934.66/s  (1.109s,  923.53/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 400/1251 ( 32%)]  Loss:  3.691005 (3.9611)  Time: 1.098s,  932.48/s  (1.108s,  924.12/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 450/1251 ( 36%)]  Loss:  3.993291 (3.9643)  Time: 1.099s,  932.06/s  (1.108s,  923.84/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [ 500/1251 ( 40%)]  Loss:  3.568156 (3.9283)  Time: 1.097s,  933.36/s  (1.108s,  923.83/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [ 550/1251 ( 44%)]  Loss:  3.880005 (3.9243)  Time: 1.097s,  933.49/s  (1.109s,  923.71/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 600/1251 ( 48%)]  Loss:  3.937082 (3.9253)  Time: 1.099s,  931.57/s  (1.108s,  923.99/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 36 [ 650/1251 ( 52%)]  Loss:  3.759326 (3.9134)  Time: 1.097s,  933.43/s  (1.108s,  924.36/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [ 700/1251 ( 56%)]  Loss:  3.826251 (3.9076)  Time: 1.098s,  932.29/s  (1.108s,  924.49/s)  LR: 4.826e-04  Data: 0.013 (0.012)
Train: 36 [ 750/1251 ( 60%)]  Loss:  3.816128 (3.9019)  Time: 1.100s,  931.23/s  (1.107s,  924.71/s)  LR: 4.826e-04  Data: 0.010 (0.012)
Train: 36 [ 800/1251 ( 64%)]  Loss:  3.669020 (3.8882)  Time: 1.099s,  931.40/s  (1.108s,  924.25/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 850/1251 ( 68%)]  Loss:  3.550327 (3.8694)  Time: 1.097s,  933.76/s  (1.108s,  923.85/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [ 900/1251 ( 72%)]  Loss:  3.591581 (3.8548)  Time: 1.103s,  928.22/s  (1.109s,  923.35/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [ 950/1251 ( 76%)]  Loss:  3.963488 (3.8602)  Time: 1.100s,  931.31/s  (1.109s,  923.36/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [1000/1251 ( 80%)]  Loss:  4.020697 (3.8679)  Time: 1.100s,  931.23/s  (1.109s,  923.58/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [1050/1251 ( 84%)]  Loss:  3.851193 (3.8671)  Time: 1.098s,  932.90/s  (1.108s,  923.85/s)  LR: 4.826e-04  Data: 0.012 (0.012)
Train: 36 [1100/1251 ( 88%)]  Loss:  3.894166 (3.8683)  Time: 1.131s,  904.99/s  (1.108s,  924.00/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [1150/1251 ( 92%)]  Loss:  4.022187 (3.8747)  Time: 1.120s,  913.90/s  (1.108s,  924.01/s)  LR: 4.826e-04  Data: 0.011 (0.012)
Train: 36 [1200/1251 ( 96%)]  Loss:  3.842541 (3.8734)  Time: 1.102s,  928.97/s  (1.108s,  924.13/s)  LR: 4.826e-04  Data: 0.013 (0.012)
Train: 36 [1250/1251 (100%)]  Loss:  3.868297 (3.8732)  Time: 1.091s,  938.82/s  (1.108s,  923.96/s)  LR: 4.826e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.301 (3.301)  Loss:  0.7429 (0.7429)  Acc@1: 84.8633 (84.8633)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.7979 (1.3254)  Acc@1: 82.0755 (70.2120)  Acc@5: 95.1651 (90.0580)
Test (EMA): [   0/48]  Time: 3.117 (3.117)  Loss:  0.7914 (0.7914)  Acc@1: 82.5195 (82.5195)  Acc@5: 95.6055 (95.6055)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.8957 (1.5017)  Acc@1: 81.2500 (67.1340)  Acc@5: 93.3962 (87.8920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 55.438000080566404)

Train: 37 [   0/1251 (  0%)]  Loss:  3.765586 (3.7656)  Time: 1.108s,  923.98/s  (1.108s,  923.98/s)  LR: 4.817e-04  Data: 0.025 (0.025)
Train: 37 [  50/1251 (  4%)]  Loss:  3.873729 (3.8197)  Time: 1.098s,  932.37/s  (1.121s,  913.44/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 100/1251 (  8%)]  Loss:  3.595583 (3.7450)  Time: 1.097s,  933.56/s  (1.117s,  916.61/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 150/1251 ( 12%)]  Loss:  3.718759 (3.7384)  Time: 1.097s,  933.71/s  (1.114s,  919.62/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 200/1251 ( 16%)]  Loss:  3.907386 (3.7722)  Time: 1.098s,  933.03/s  (1.113s,  919.94/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 250/1251 ( 20%)]  Loss:  4.116109 (3.8295)  Time: 1.097s,  933.58/s  (1.112s,  921.26/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 300/1251 ( 24%)]  Loss:  3.618561 (3.7994)  Time: 1.105s,  926.45/s  (1.111s,  921.79/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 37 [ 350/1251 ( 28%)]  Loss:  3.837480 (3.8041)  Time: 1.098s,  932.59/s  (1.110s,  922.59/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 400/1251 ( 32%)]  Loss:  3.992281 (3.8251)  Time: 1.105s,  926.61/s  (1.109s,  923.24/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 450/1251 ( 36%)]  Loss:  3.799655 (3.8225)  Time: 1.109s,  922.94/s  (1.109s,  923.69/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 500/1251 ( 40%)]  Loss:  3.842242 (3.8243)  Time: 1.103s,  928.12/s  (1.108s,  923.96/s)  LR: 4.817e-04  Data: 0.010 (0.012)
Train: 37 [ 550/1251 ( 44%)]  Loss:  4.040084 (3.8423)  Time: 1.103s,  928.75/s  (1.108s,  924.19/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 600/1251 ( 48%)]  Loss:  4.021355 (3.8561)  Time: 1.103s,  928.69/s  (1.108s,  924.41/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 650/1251 ( 52%)]  Loss:  4.267213 (3.8854)  Time: 1.097s,  933.15/s  (1.107s,  924.63/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 700/1251 ( 56%)]  Loss:  3.702035 (3.8732)  Time: 1.102s,  929.26/s  (1.107s,  924.64/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 750/1251 ( 60%)]  Loss:  3.757442 (3.8660)  Time: 1.098s,  932.70/s  (1.108s,  924.13/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 800/1251 ( 64%)]  Loss:  3.510948 (3.8451)  Time: 1.097s,  933.43/s  (1.108s,  924.47/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 850/1251 ( 68%)]  Loss:  4.018007 (3.8547)  Time: 1.096s,  934.32/s  (1.108s,  924.17/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [ 900/1251 ( 72%)]  Loss:  3.674232 (3.8452)  Time: 1.097s,  933.69/s  (1.108s,  924.34/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [ 950/1251 ( 76%)]  Loss:  3.950996 (3.8505)  Time: 1.134s,  902.98/s  (1.108s,  923.83/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [1000/1251 ( 80%)]  Loss:  3.891809 (3.8525)  Time: 1.098s,  932.30/s  (1.108s,  924.09/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [1050/1251 ( 84%)]  Loss:  4.150733 (3.8660)  Time: 1.100s,  930.85/s  (1.108s,  924.27/s)  LR: 4.817e-04  Data: 0.013 (0.012)
Train: 37 [1100/1251 ( 88%)]  Loss:  3.655158 (3.8568)  Time: 1.212s,  844.86/s  (1.108s,  924.08/s)  LR: 4.817e-04  Data: 0.012 (0.012)
Train: 37 [1150/1251 ( 92%)]  Loss:  3.918180 (3.8594)  Time: 1.127s,  908.85/s  (1.108s,  924.29/s)  LR: 4.817e-04  Data: 0.011 (0.012)
Train: 37 [1200/1251 ( 96%)]  Loss:  4.059031 (3.8674)  Time: 1.113s,  920.18/s  (1.108s,  924.20/s)  LR: 4.817e-04  Data: 0.013 (0.012)
Train: 37 [1250/1251 (100%)]  Loss:  3.999246 (3.8725)  Time: 1.089s,  940.17/s  (1.108s,  923.78/s)  LR: 4.817e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.332 (3.332)  Loss:  0.6411 (0.6411)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.7915 (1.2692)  Acc@1: 82.6651 (70.6620)  Acc@5: 94.2217 (90.2720)
Test (EMA): [   0/48]  Time: 3.298 (3.298)  Loss:  0.7628 (0.7628)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.5078 (95.5078)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.8653 (1.4575)  Acc@1: 81.7217 (67.9440)  Acc@5: 94.1038 (88.3720)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 57.315999921875)

Train: 38 [   0/1251 (  0%)]  Loss:  3.977229 (3.9772)  Time: 1.124s,  911.18/s  (1.124s,  911.18/s)  LR: 4.807e-04  Data: 0.030 (0.030)
Train: 38 [  50/1251 (  4%)]  Loss:  3.351968 (3.6646)  Time: 1.098s,  932.47/s  (1.110s,  922.33/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 100/1251 (  8%)]  Loss:  3.778824 (3.7027)  Time: 1.129s,  906.88/s  (1.110s,  922.69/s)  LR: 4.807e-04  Data: 0.015 (0.012)
Train: 38 [ 150/1251 ( 12%)]  Loss:  4.020954 (3.7822)  Time: 1.098s,  932.43/s  (1.113s,  919.84/s)  LR: 4.807e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 38 [ 200/1251 ( 16%)]  Loss:  3.958039 (3.8174)  Time: 1.093s,  936.45/s  (1.112s,  920.89/s)  LR: 4.807e-04  Data: 0.010 (0.012)
Train: 38 [ 250/1251 ( 20%)]  Loss:  4.052618 (3.8566)  Time: 1.104s,  927.26/s  (1.112s,  921.06/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 300/1251 ( 24%)]  Loss:  4.145701 (3.8979)  Time: 1.098s,  932.43/s  (1.110s,  922.15/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 350/1251 ( 28%)]  Loss:  3.443214 (3.8411)  Time: 1.105s,  926.80/s  (1.110s,  922.41/s)  LR: 4.807e-04  Data: 0.010 (0.012)
Train: 38 [ 400/1251 ( 32%)]  Loss:  3.809221 (3.8375)  Time: 1.098s,  932.76/s  (1.109s,  923.12/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 450/1251 ( 36%)]  Loss:  3.764700 (3.8302)  Time: 1.213s,  844.47/s  (1.109s,  923.62/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 500/1251 ( 40%)]  Loss:  4.061625 (3.8513)  Time: 1.113s,  920.06/s  (1.110s,  922.54/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 550/1251 ( 44%)]  Loss:  3.828725 (3.8494)  Time: 1.124s,  911.33/s  (1.110s,  922.88/s)  LR: 4.807e-04  Data: 0.012 (0.012)
Train: 38 [ 600/1251 ( 48%)]  Loss:  3.682928 (3.8366)  Time: 1.191s,  859.77/s  (1.110s,  922.45/s)  LR: 4.807e-04  Data: 0.012 (0.012)
Train: 38 [ 650/1251 ( 52%)]  Loss:  3.722209 (3.8284)  Time: 1.120s,  913.88/s  (1.109s,  923.13/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 700/1251 ( 56%)]  Loss:  4.048913 (3.8431)  Time: 1.100s,  931.19/s  (1.109s,  923.11/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 750/1251 ( 60%)]  Loss:  3.957320 (3.8503)  Time: 1.099s,  931.73/s  (1.109s,  923.37/s)  LR: 4.807e-04  Data: 0.016 (0.012)
Train: 38 [ 800/1251 ( 64%)]  Loss:  3.830994 (3.8491)  Time: 1.096s,  934.58/s  (1.109s,  923.35/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 850/1251 ( 68%)]  Loss:  3.833659 (3.8483)  Time: 1.105s,  927.11/s  (1.109s,  923.63/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [ 900/1251 ( 72%)]  Loss:  4.007126 (3.8566)  Time: 1.101s,  930.44/s  (1.108s,  923.80/s)  LR: 4.807e-04  Data: 0.012 (0.012)
Train: 38 [ 950/1251 ( 76%)]  Loss:  4.100893 (3.8688)  Time: 1.097s,  933.17/s  (1.108s,  924.02/s)  LR: 4.807e-04  Data: 0.013 (0.012)
Train: 38 [1000/1251 ( 80%)]  Loss:  3.873033 (3.8690)  Time: 1.130s,  905.84/s  (1.108s,  924.13/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [1050/1251 ( 84%)]  Loss:  4.174164 (3.8829)  Time: 1.098s,  932.62/s  (1.108s,  924.13/s)  LR: 4.807e-04  Data: 0.011 (0.012)
Train: 38 [1100/1251 ( 88%)]  Loss:  3.895943 (3.8835)  Time: 1.096s,  934.08/s  (1.108s,  924.28/s)  LR: 4.807e-04  Data: 0.012 (0.012)
Train: 38 [1150/1251 ( 92%)]  Loss:  4.054902 (3.8906)  Time: 1.103s,  928.14/s  (1.108s,  924.16/s)  LR: 4.807e-04  Data: 0.012 (0.012)
Train: 38 [1200/1251 ( 96%)]  Loss:  3.708754 (3.8833)  Time: 1.096s,  934.47/s  (1.108s,  924.37/s)  LR: 4.807e-04  Data: 0.012 (0.012)
Train: 38 [1250/1251 (100%)]  Loss:  4.056528 (3.8900)  Time: 1.088s,  940.83/s  (1.108s,  924.35/s)  LR: 4.807e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.228 (3.228)  Loss:  0.6702 (0.6702)  Acc@1: 84.7656 (84.7656)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.8173 (1.2667)  Acc@1: 82.7830 (71.0560)  Acc@5: 94.5755 (90.4440)
Test (EMA): [   0/48]  Time: 3.223 (3.223)  Loss:  0.7388 (0.7388)  Acc@1: 83.5938 (83.5938)  Acc@5: 96.0938 (96.0938)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.8376 (1.4172)  Acc@1: 82.0755 (68.6840)  Acc@5: 94.4576 (88.8640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 58.94000007080078)

Train: 39 [   0/1251 (  0%)]  Loss:  3.561439 (3.5614)  Time: 1.193s,  858.69/s  (1.193s,  858.69/s)  LR: 4.796e-04  Data: 0.024 (0.024)
Train: 39 [  50/1251 (  4%)]  Loss:  3.872182 (3.7168)  Time: 1.100s,  931.23/s  (1.108s,  924.54/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [ 100/1251 (  8%)]  Loss:  3.627547 (3.6871)  Time: 1.121s,  913.37/s  (1.109s,  923.09/s)  LR: 4.796e-04  Data: 0.011 (0.012)
Train: 39 [ 150/1251 ( 12%)]  Loss:  3.557635 (3.6547)  Time: 1.104s,  927.49/s  (1.107s,  924.63/s)  LR: 4.796e-04  Data: 0.015 (0.012)
Train: 39 [ 200/1251 ( 16%)]  Loss:  3.949696 (3.7137)  Time: 1.099s,  931.53/s  (1.109s,  923.01/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [ 250/1251 ( 20%)]  Loss:  3.801060 (3.7283)  Time: 1.101s,  930.34/s  (1.107s,  924.85/s)  LR: 4.796e-04  Data: 0.017 (0.012)
Train: 39 [ 300/1251 ( 24%)]  Loss:  3.891874 (3.7516)  Time: 1.195s,  856.94/s  (1.107s,  924.67/s)  LR: 4.796e-04  Data: 0.011 (0.012)
Train: 39 [ 350/1251 ( 28%)]  Loss:  3.635104 (3.7371)  Time: 1.119s,  915.21/s  (1.109s,  923.75/s)  LR: 4.796e-04  Data: 0.010 (0.012)
Train: 39 [ 400/1251 ( 32%)]  Loss:  4.104364 (3.7779)  Time: 1.191s,  859.61/s  (1.108s,  924.23/s)  LR: 4.796e-04  Data: 0.011 (0.012)
Train: 39 [ 450/1251 ( 36%)]  Loss:  3.832823 (3.7834)  Time: 1.115s,  918.53/s  (1.108s,  924.45/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [ 500/1251 ( 40%)]  Loss:  3.796860 (3.7846)  Time: 1.099s,  932.04/s  (1.107s,  925.11/s)  LR: 4.796e-04  Data: 0.013 (0.012)
Train: 39 [ 550/1251 ( 44%)]  Loss:  3.888097 (3.7932)  Time: 1.112s,  920.78/s  (1.108s,  924.30/s)  LR: 4.796e-04  Data: 0.011 (0.012)
Train: 39 [ 600/1251 ( 48%)]  Loss:  3.493455 (3.7702)  Time: 1.120s,  914.41/s  (1.107s,  924.72/s)  LR: 4.796e-04  Data: 0.011 (0.012)
Train: 39 [ 650/1251 ( 52%)]  Loss:  4.137070 (3.7964)  Time: 1.097s,  933.13/s  (1.108s,  924.47/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [ 700/1251 ( 56%)]  Loss:  3.671346 (3.7880)  Time: 1.099s,  931.94/s  (1.107s,  924.61/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [ 750/1251 ( 60%)]  Loss:  3.725485 (3.7841)  Time: 1.101s,  930.36/s  (1.108s,  924.37/s)  LR: 4.796e-04  Data: 0.010 (0.012)
Train: 39 [ 800/1251 ( 64%)]  Loss:  3.559482 (3.7709)  Time: 1.122s,  912.77/s  (1.108s,  924.21/s)  LR: 4.796e-04  Data: 0.013 (0.012)
Train: 39 [ 850/1251 ( 68%)]  Loss:  3.518259 (3.7569)  Time: 1.097s,  933.28/s  (1.108s,  923.85/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [ 900/1251 ( 72%)]  Loss:  3.959970 (3.7676)  Time: 1.098s,  932.47/s  (1.108s,  924.04/s)  LR: 4.796e-04  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 39 [ 950/1251 ( 76%)]  Loss:  3.957971 (3.7771)  Time: 1.119s,  915.36/s  (1.108s,  924.27/s)  LR: 4.796e-04  Data: 0.010 (0.012)
Train: 39 [1000/1251 ( 80%)]  Loss:  3.726227 (3.7747)  Time: 1.097s,  933.07/s  (1.108s,  924.32/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [1050/1251 ( 84%)]  Loss:  3.709979 (3.7717)  Time: 1.096s,  934.17/s  (1.108s,  924.59/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [1100/1251 ( 88%)]  Loss:  3.604309 (3.7644)  Time: 1.098s,  932.71/s  (1.108s,  924.41/s)  LR: 4.796e-04  Data: 0.012 (0.012)
Train: 39 [1150/1251 ( 92%)]  Loss:  4.070801 (3.7772)  Time: 1.101s,  929.96/s  (1.107s,  924.67/s)  LR: 4.796e-04  Data: 0.010 (0.012)
Train: 39 [1200/1251 ( 96%)]  Loss:  3.745795 (3.7760)  Time: 1.100s,  930.79/s  (1.108s,  924.41/s)  LR: 4.796e-04  Data: 0.011 (0.012)
Train: 39 [1250/1251 (100%)]  Loss:  4.122789 (3.7893)  Time: 1.081s,  947.20/s  (1.108s,  924.51/s)  LR: 4.796e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.256 (3.256)  Loss:  0.7040 (0.7040)  Acc@1: 85.0586 (85.0586)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.7825 (1.2648)  Acc@1: 83.0189 (71.3040)  Acc@5: 95.1651 (90.7020)
Test (EMA): [   0/48]  Time: 3.141 (3.141)  Loss:  0.7172 (0.7172)  Acc@1: 84.4727 (84.4727)  Acc@5: 96.2891 (96.2891)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.8157 (1.3811)  Acc@1: 82.0755 (69.4040)  Acc@5: 94.6934 (89.2380)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 60.52199999023438)

Train: 40 [   0/1251 (  0%)]  Loss:  3.861142 (3.8611)  Time: 1.116s,  917.63/s  (1.116s,  917.63/s)  LR: 4.786e-04  Data: 0.027 (0.027)
Train: 40 [  50/1251 (  4%)]  Loss:  3.883876 (3.8725)  Time: 1.127s,  908.39/s  (1.113s,  919.91/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 100/1251 (  8%)]  Loss:  3.558509 (3.7678)  Time: 1.110s,  922.28/s  (1.107s,  924.94/s)  LR: 4.786e-04  Data: 0.016 (0.012)
Train: 40 [ 150/1251 ( 12%)]  Loss:  3.837224 (3.7852)  Time: 1.132s,  904.37/s  (1.110s,  922.20/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 200/1251 ( 16%)]  Loss:  3.799452 (3.7880)  Time: 1.103s,  928.18/s  (1.110s,  922.25/s)  LR: 4.786e-04  Data: 0.010 (0.012)
Train: 40 [ 250/1251 ( 20%)]  Loss:  3.684959 (3.7709)  Time: 1.251s,  818.70/s  (1.112s,  920.82/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 300/1251 ( 24%)]  Loss:  3.550191 (3.7393)  Time: 1.097s,  933.14/s  (1.112s,  920.50/s)  LR: 4.786e-04  Data: 0.013 (0.012)
Train: 40 [ 350/1251 ( 28%)]  Loss:  3.905338 (3.7601)  Time: 1.097s,  933.21/s  (1.112s,  921.02/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 400/1251 ( 32%)]  Loss:  3.716133 (3.7552)  Time: 1.099s,  931.63/s  (1.111s,  921.64/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 450/1251 ( 36%)]  Loss:  3.798568 (3.7595)  Time: 1.096s,  934.65/s  (1.110s,  922.31/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 500/1251 ( 40%)]  Loss:  3.835567 (3.7665)  Time: 1.098s,  933.01/s  (1.111s,  922.11/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 550/1251 ( 44%)]  Loss:  3.888727 (3.7766)  Time: 1.133s,  903.88/s  (1.110s,  922.58/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 600/1251 ( 48%)]  Loss:  3.671036 (3.7685)  Time: 1.134s,  903.12/s  (1.111s,  921.71/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 650/1251 ( 52%)]  Loss:  3.886038 (3.7769)  Time: 1.120s,  914.12/s  (1.111s,  921.96/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 700/1251 ( 56%)]  Loss:  4.102767 (3.7986)  Time: 1.098s,  932.79/s  (1.111s,  921.83/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 750/1251 ( 60%)]  Loss:  3.797224 (3.7985)  Time: 1.098s,  932.55/s  (1.110s,  922.46/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 800/1251 ( 64%)]  Loss:  3.536930 (3.7832)  Time: 1.123s,  911.72/s  (1.110s,  922.22/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 850/1251 ( 68%)]  Loss:  4.030958 (3.7969)  Time: 1.101s,  930.12/s  (1.110s,  922.50/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [ 900/1251 ( 72%)]  Loss:  4.007852 (3.8080)  Time: 1.102s,  928.84/s  (1.110s,  922.55/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [ 950/1251 ( 76%)]  Loss:  3.953043 (3.8153)  Time: 1.101s,  930.24/s  (1.110s,  922.31/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [1000/1251 ( 80%)]  Loss:  3.832278 (3.8161)  Time: 1.095s,  934.92/s  (1.110s,  922.28/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [1050/1251 ( 84%)]  Loss:  4.202218 (3.8336)  Time: 1.100s,  931.25/s  (1.111s,  922.07/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [1100/1251 ( 88%)]  Loss:  3.898052 (3.8364)  Time: 1.100s,  930.79/s  (1.110s,  922.34/s)  LR: 4.786e-04  Data: 0.011 (0.012)
Train: 40 [1150/1251 ( 92%)]  Loss:  3.938737 (3.8407)  Time: 1.097s,  933.06/s  (1.110s,  922.15/s)  LR: 4.786e-04  Data: 0.013 (0.012)
Train: 40 [1200/1251 ( 96%)]  Loss:  3.790314 (3.8387)  Time: 1.097s,  933.83/s  (1.110s,  922.23/s)  LR: 4.786e-04  Data: 0.012 (0.012)
Train: 40 [1250/1251 (100%)]  Loss:  3.825550 (3.8382)  Time: 1.079s,  948.63/s  (1.111s,  922.09/s)  LR: 4.786e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.226 (3.226)  Loss:  0.6724 (0.6724)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.7573 (1.2599)  Acc@1: 82.7830 (71.1320)  Acc@5: 95.1651 (90.5660)
Test (EMA): [   0/48]  Time: 4.038 (4.038)  Loss:  0.6989 (0.6989)  Acc@1: 84.7656 (84.7656)  Acc@5: 96.4844 (96.4844)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.7968 (1.3484)  Acc@1: 82.5472 (69.9320)  Acc@5: 94.8113 (89.5600)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 62.04599993408203)

Train: 41 [   0/1251 (  0%)]  Loss:  3.745922 (3.7459)  Time: 1.112s,  921.03/s  (1.112s,  921.03/s)  LR: 4.775e-04  Data: 0.023 (0.023)
Train: 41 [  50/1251 (  4%)]  Loss:  3.801160 (3.7735)  Time: 1.133s,  904.08/s  (1.105s,  926.97/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 100/1251 (  8%)]  Loss:  3.956150 (3.8344)  Time: 1.131s,  905.06/s  (1.114s,  919.42/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 150/1251 ( 12%)]  Loss:  3.947954 (3.8628)  Time: 1.122s,  913.06/s  (1.113s,  920.30/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 200/1251 ( 16%)]  Loss:  3.925873 (3.8754)  Time: 1.118s,  916.03/s  (1.115s,  918.60/s)  LR: 4.775e-04  Data: 0.010 (0.012)
Train: 41 [ 250/1251 ( 20%)]  Loss:  3.952228 (3.8882)  Time: 1.118s,  915.89/s  (1.116s,  917.65/s)  LR: 4.775e-04  Data: 0.011 (0.011)
Train: 41 [ 300/1251 ( 24%)]  Loss:  4.296902 (3.9466)  Time: 1.122s,  912.95/s  (1.116s,  917.24/s)  LR: 4.775e-04  Data: 0.014 (0.012)
Train: 41 [ 350/1251 ( 28%)]  Loss:  3.879585 (3.9382)  Time: 1.120s,  913.88/s  (1.115s,  918.16/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [ 400/1251 ( 32%)]  Loss:  3.786518 (3.9214)  Time: 1.198s,  854.67/s  (1.114s,  918.99/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 450/1251 ( 36%)]  Loss:  4.165223 (3.9458)  Time: 1.095s,  934.77/s  (1.114s,  919.26/s)  LR: 4.775e-04  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 41 [ 500/1251 ( 40%)]  Loss:  3.645223 (3.9184)  Time: 1.095s,  934.75/s  (1.112s,  920.54/s)  LR: 4.775e-04  Data: 0.012 (0.011)
Train: 41 [ 550/1251 ( 44%)]  Loss:  3.582677 (3.8905)  Time: 1.098s,  932.71/s  (1.113s,  920.13/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [ 600/1251 ( 48%)]  Loss:  4.251289 (3.9182)  Time: 1.131s,  905.30/s  (1.112s,  920.70/s)  LR: 4.775e-04  Data: 0.010 (0.012)
Train: 41 [ 650/1251 ( 52%)]  Loss:  3.903999 (3.9172)  Time: 1.095s,  935.14/s  (1.112s,  920.96/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 700/1251 ( 56%)]  Loss:  3.709141 (3.9033)  Time: 1.097s,  933.82/s  (1.112s,  921.21/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [ 750/1251 ( 60%)]  Loss:  3.731829 (3.8926)  Time: 1.096s,  934.50/s  (1.112s,  921.16/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [ 800/1251 ( 64%)]  Loss:  3.953184 (3.8962)  Time: 1.097s,  933.24/s  (1.111s,  921.44/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 850/1251 ( 68%)]  Loss:  3.848424 (3.8935)  Time: 1.192s,  859.17/s  (1.112s,  921.24/s)  LR: 4.775e-04  Data: 0.010 (0.012)
Train: 41 [ 900/1251 ( 72%)]  Loss:  3.714372 (3.8841)  Time: 1.103s,  928.62/s  (1.111s,  921.32/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [ 950/1251 ( 76%)]  Loss:  3.634020 (3.8716)  Time: 1.100s,  930.61/s  (1.111s,  921.61/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [1000/1251 ( 80%)]  Loss:  3.814770 (3.8689)  Time: 1.101s,  929.76/s  (1.111s,  921.92/s)  LR: 4.775e-04  Data: 0.010 (0.012)
Train: 41 [1050/1251 ( 84%)]  Loss:  3.610059 (3.8571)  Time: 1.096s,  933.91/s  (1.111s,  921.98/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [1100/1251 ( 88%)]  Loss:  4.023193 (3.8643)  Time: 1.094s,  936.04/s  (1.111s,  922.03/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [1150/1251 ( 92%)]  Loss:  3.857625 (3.8641)  Time: 1.099s,  931.96/s  (1.110s,  922.12/s)  LR: 4.775e-04  Data: 0.012 (0.012)
Train: 41 [1200/1251 ( 96%)]  Loss:  3.908651 (3.8658)  Time: 1.095s,  935.15/s  (1.111s,  921.85/s)  LR: 4.775e-04  Data: 0.011 (0.012)
Train: 41 [1250/1251 (100%)]  Loss:  4.088182 (3.8744)  Time: 1.081s,  947.20/s  (1.111s,  922.03/s)  LR: 4.775e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.249 (3.249)  Loss:  0.6546 (0.6546)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.7829 (1.2484)  Acc@1: 82.9009 (71.6860)  Acc@5: 95.6368 (90.8400)
Test (EMA): [   0/48]  Time: 3.135 (3.135)  Loss:  0.6826 (0.6826)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.4844 (96.4844)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.7767 (1.3179)  Acc@1: 82.5472 (70.4160)  Acc@5: 94.8113 (89.8340)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 63.29000008544922)

Train: 42 [   0/1251 (  0%)]  Loss:  3.529830 (3.5298)  Time: 1.122s,  912.56/s  (1.122s,  912.56/s)  LR: 4.764e-04  Data: 0.042 (0.042)
Train: 42 [  50/1251 (  4%)]  Loss:  4.130165 (3.8300)  Time: 1.097s,  933.19/s  (1.114s,  919.56/s)  LR: 4.764e-04  Data: 0.013 (0.012)
Train: 42 [ 100/1251 (  8%)]  Loss:  3.523338 (3.7278)  Time: 1.095s,  934.95/s  (1.108s,  923.83/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 150/1251 ( 12%)]  Loss:  3.687846 (3.7178)  Time: 1.101s,  929.78/s  (1.107s,  924.71/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 200/1251 ( 16%)]  Loss:  3.826267 (3.7395)  Time: 1.099s,  931.77/s  (1.105s,  926.52/s)  LR: 4.764e-04  Data: 0.017 (0.012)
Train: 42 [ 250/1251 ( 20%)]  Loss:  3.684698 (3.7304)  Time: 1.097s,  933.85/s  (1.107s,  925.14/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 300/1251 ( 24%)]  Loss:  3.930220 (3.7589)  Time: 1.123s,  911.59/s  (1.108s,  924.14/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 350/1251 ( 28%)]  Loss:  4.093075 (3.8007)  Time: 1.103s,  928.29/s  (1.107s,  925.18/s)  LR: 4.764e-04  Data: 0.012 (0.012)
Train: 42 [ 400/1251 ( 32%)]  Loss:  3.767266 (3.7970)  Time: 1.097s,  933.35/s  (1.107s,  924.62/s)  LR: 4.764e-04  Data: 0.012 (0.012)
Train: 42 [ 450/1251 ( 36%)]  Loss:  3.581383 (3.7754)  Time: 1.097s,  933.20/s  (1.107s,  925.30/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 500/1251 ( 40%)]  Loss:  3.829983 (3.7804)  Time: 1.095s,  935.14/s  (1.107s,  924.72/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 550/1251 ( 44%)]  Loss:  3.923983 (3.7923)  Time: 1.097s,  933.64/s  (1.107s,  925.16/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 600/1251 ( 48%)]  Loss:  3.978276 (3.8066)  Time: 1.096s,  934.11/s  (1.108s,  924.58/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 650/1251 ( 52%)]  Loss:  3.905401 (3.8137)  Time: 1.134s,  903.35/s  (1.108s,  924.58/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 700/1251 ( 56%)]  Loss:  3.815684 (3.8138)  Time: 1.095s,  935.39/s  (1.109s,  923.69/s)  LR: 4.764e-04  Data: 0.012 (0.012)
Train: 42 [ 750/1251 ( 60%)]  Loss:  3.530635 (3.7961)  Time: 1.098s,  932.49/s  (1.108s,  924.02/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 800/1251 ( 64%)]  Loss:  3.961555 (3.8059)  Time: 1.196s,  856.17/s  (1.108s,  924.06/s)  LR: 4.764e-04  Data: 0.013 (0.012)
Train: 42 [ 850/1251 ( 68%)]  Loss:  4.072149 (3.8207)  Time: 1.104s,  927.52/s  (1.108s,  924.32/s)  LR: 4.764e-04  Data: 0.012 (0.012)
Train: 42 [ 900/1251 ( 72%)]  Loss:  4.212018 (3.8413)  Time: 1.098s,  932.27/s  (1.107s,  924.62/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [ 950/1251 ( 76%)]  Loss:  3.717229 (3.8350)  Time: 1.100s,  930.86/s  (1.108s,  924.60/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [1000/1251 ( 80%)]  Loss:  3.919233 (3.8391)  Time: 1.130s,  906.54/s  (1.107s,  924.93/s)  LR: 4.764e-04  Data: 0.018 (0.012)
Train: 42 [1050/1251 ( 84%)]  Loss:  3.897186 (3.8417)  Time: 1.098s,  932.45/s  (1.107s,  924.80/s)  LR: 4.764e-04  Data: 0.012 (0.012)
Train: 42 [1100/1251 ( 88%)]  Loss:  3.817753 (3.8407)  Time: 1.095s,  935.45/s  (1.107s,  925.11/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [1150/1251 ( 92%)]  Loss:  3.822632 (3.8399)  Time: 1.098s,  932.69/s  (1.107s,  925.07/s)  LR: 4.764e-04  Data: 0.011 (0.012)
Train: 42 [1200/1251 ( 96%)]  Loss:  3.977869 (3.8454)  Time: 1.100s,  930.79/s  (1.107s,  924.93/s)  LR: 4.764e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 42 [1250/1251 (100%)]  Loss:  4.113891 (3.8558)  Time: 1.080s,  948.45/s  (1.107s,  924.86/s)  LR: 4.764e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.236 (3.236)  Loss:  0.6778 (0.6778)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.7323 (1.2364)  Acc@1: 84.1981 (71.7640)  Acc@5: 96.1085 (90.9380)
Test (EMA): [   0/48]  Time: 3.165 (3.165)  Loss:  0.6671 (0.6671)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.7773 (96.7773)
Test (EMA): [  48/48]  Time: 0.229 (0.412)  Loss:  0.7581 (1.2898)  Acc@1: 83.0189 (70.9260)  Acc@5: 94.6934 (90.1640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 64.29000000488281)

Train: 43 [   0/1251 (  0%)]  Loss:  3.453012 (3.4530)  Time: 1.106s,  925.48/s  (1.106s,  925.48/s)  LR: 4.753e-04  Data: 0.025 (0.025)
Train: 43 [  50/1251 (  4%)]  Loss:  4.106999 (3.7800)  Time: 1.108s,  924.51/s  (1.110s,  922.41/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 100/1251 (  8%)]  Loss:  3.552680 (3.7042)  Time: 1.092s,  938.14/s  (1.114s,  919.52/s)  LR: 4.753e-04  Data: 0.009 (0.012)
Train: 43 [ 150/1251 ( 12%)]  Loss:  3.842607 (3.7388)  Time: 1.095s,  934.79/s  (1.110s,  922.79/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 200/1251 ( 16%)]  Loss:  3.997071 (3.7905)  Time: 1.091s,  938.71/s  (1.109s,  923.50/s)  LR: 4.753e-04  Data: 0.009 (0.012)
Train: 43 [ 250/1251 ( 20%)]  Loss:  3.337578 (3.7150)  Time: 1.095s,  935.27/s  (1.108s,  923.80/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 300/1251 ( 24%)]  Loss:  3.857140 (3.7353)  Time: 1.133s,  904.09/s  (1.107s,  924.63/s)  LR: 4.753e-04  Data: 0.011 (0.012)
Train: 43 [ 350/1251 ( 28%)]  Loss:  3.935651 (3.7603)  Time: 1.135s,  902.24/s  (1.110s,  922.58/s)  LR: 4.753e-04  Data: 0.011 (0.012)
Train: 43 [ 400/1251 ( 32%)]  Loss:  3.610744 (3.7437)  Time: 1.104s,  927.67/s  (1.110s,  922.46/s)  LR: 4.753e-04  Data: 0.014 (0.012)
Train: 43 [ 450/1251 ( 36%)]  Loss:  3.391690 (3.7085)  Time: 1.123s,  911.78/s  (1.111s,  921.84/s)  LR: 4.753e-04  Data: 0.011 (0.012)
Train: 43 [ 500/1251 ( 40%)]  Loss:  4.109477 (3.7450)  Time: 1.096s,  934.55/s  (1.112s,  921.04/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 550/1251 ( 44%)]  Loss:  3.620175 (3.7346)  Time: 1.143s,  895.86/s  (1.112s,  920.93/s)  LR: 4.753e-04  Data: 0.010 (0.012)
Train: 43 [ 600/1251 ( 48%)]  Loss:  4.037983 (3.7579)  Time: 1.122s,  912.55/s  (1.112s,  920.56/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 650/1251 ( 52%)]  Loss:  3.700169 (3.7538)  Time: 1.096s,  934.10/s  (1.113s,  920.38/s)  LR: 4.753e-04  Data: 0.010 (0.012)
Train: 43 [ 700/1251 ( 56%)]  Loss:  3.818345 (3.7581)  Time: 1.096s,  934.40/s  (1.112s,  920.97/s)  LR: 4.753e-04  Data: 0.011 (0.012)
Train: 43 [ 750/1251 ( 60%)]  Loss:  3.730805 (3.7564)  Time: 1.100s,  930.97/s  (1.112s,  920.46/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 800/1251 ( 64%)]  Loss:  3.729313 (3.7548)  Time: 1.097s,  933.54/s  (1.112s,  920.73/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [ 850/1251 ( 68%)]  Loss:  3.840252 (3.7595)  Time: 1.103s,  928.67/s  (1.112s,  921.21/s)  LR: 4.753e-04  Data: 0.010 (0.012)
Train: 43 [ 900/1251 ( 72%)]  Loss:  3.902186 (3.7670)  Time: 1.123s,  911.44/s  (1.112s,  921.27/s)  LR: 4.753e-04  Data: 0.011 (0.012)
Train: 43 [ 950/1251 ( 76%)]  Loss:  3.809741 (3.7692)  Time: 1.100s,  930.69/s  (1.111s,  921.78/s)  LR: 4.753e-04  Data: 0.013 (0.012)
Train: 43 [1000/1251 ( 80%)]  Loss:  3.810090 (3.7711)  Time: 1.097s,  933.14/s  (1.111s,  921.71/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [1050/1251 ( 84%)]  Loss:  3.984286 (3.7808)  Time: 1.123s,  911.63/s  (1.111s,  921.97/s)  LR: 4.753e-04  Data: 0.010 (0.012)
Train: 43 [1100/1251 ( 88%)]  Loss:  3.844300 (3.7836)  Time: 1.119s,  914.77/s  (1.111s,  921.70/s)  LR: 4.753e-04  Data: 0.010 (0.012)
Train: 43 [1150/1251 ( 92%)]  Loss:  3.713149 (3.7806)  Time: 1.099s,  931.56/s  (1.111s,  921.73/s)  LR: 4.753e-04  Data: 0.012 (0.012)
Train: 43 [1200/1251 ( 96%)]  Loss:  4.043148 (3.7911)  Time: 1.098s,  932.82/s  (1.111s,  921.60/s)  LR: 4.753e-04  Data: 0.013 (0.012)
Train: 43 [1250/1251 (100%)]  Loss:  3.543792 (3.7816)  Time: 1.106s,  925.96/s  (1.111s,  921.76/s)  LR: 4.753e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.209 (3.209)  Loss:  0.6596 (0.6596)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.7248 (1.2112)  Acc@1: 84.4340 (71.7060)  Acc@5: 95.2830 (91.0340)
Test (EMA): [   0/48]  Time: 3.130 (3.130)  Loss:  0.6528 (0.6528)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.8750 (96.8750)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.7436 (1.2646)  Acc@1: 83.1368 (71.3400)  Acc@5: 94.9292 (90.4740)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 65.28000000488281)

Train: 44 [   0/1251 (  0%)]  Loss:  3.221275 (3.2213)  Time: 1.108s,  924.39/s  (1.108s,  924.39/s)  LR: 4.742e-04  Data: 0.024 (0.024)
Train: 44 [  50/1251 (  4%)]  Loss:  3.669470 (3.4454)  Time: 1.125s,  910.62/s  (1.112s,  920.74/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [ 100/1251 (  8%)]  Loss:  3.616122 (3.5023)  Time: 1.101s,  929.69/s  (1.108s,  924.23/s)  LR: 4.742e-04  Data: 0.013 (0.012)
Train: 44 [ 150/1251 ( 12%)]  Loss:  4.125641 (3.6581)  Time: 1.092s,  938.05/s  (1.108s,  924.17/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [ 200/1251 ( 16%)]  Loss:  3.862522 (3.6990)  Time: 1.101s,  930.31/s  (1.108s,  924.38/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [ 250/1251 ( 20%)]  Loss:  3.447478 (3.6571)  Time: 1.102s,  928.97/s  (1.108s,  924.59/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [ 300/1251 ( 24%)]  Loss:  3.675947 (3.6598)  Time: 1.097s,  933.82/s  (1.109s,  923.65/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [ 350/1251 ( 28%)]  Loss:  3.890397 (3.6886)  Time: 1.095s,  935.30/s  (1.108s,  924.38/s)  LR: 4.742e-04  Data: 0.013 (0.012)
Train: 44 [ 400/1251 ( 32%)]  Loss:  4.015778 (3.7250)  Time: 1.098s,  932.18/s  (1.108s,  923.91/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [ 450/1251 ( 36%)]  Loss:  3.464211 (3.6989)  Time: 1.103s,  928.15/s  (1.107s,  924.75/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [ 500/1251 ( 40%)]  Loss:  3.612609 (3.6910)  Time: 1.108s,  924.48/s  (1.107s,  924.63/s)  LR: 4.742e-04  Data: 0.023 (0.012)
Train: 44 [ 550/1251 ( 44%)]  Loss:  3.669538 (3.6892)  Time: 1.096s,  934.34/s  (1.107s,  925.23/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [ 600/1251 ( 48%)]  Loss:  3.675390 (3.6882)  Time: 1.193s,  858.35/s  (1.108s,  924.45/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [ 650/1251 ( 52%)]  Loss:  3.690130 (3.6883)  Time: 1.121s,  913.38/s  (1.108s,  924.16/s)  LR: 4.742e-04  Data: 0.010 (0.012)
Train: 44 [ 700/1251 ( 56%)]  Loss:  4.149571 (3.7191)  Time: 1.187s,  862.41/s  (1.109s,  923.68/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 44 [ 750/1251 ( 60%)]  Loss:  4.072451 (3.7412)  Time: 1.127s,  908.77/s  (1.109s,  923.17/s)  LR: 4.742e-04  Data: 0.018 (0.012)
Train: 44 [ 800/1251 ( 64%)]  Loss:  3.630604 (3.7347)  Time: 1.097s,  933.26/s  (1.109s,  923.00/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [ 850/1251 ( 68%)]  Loss:  3.914808 (3.7447)  Time: 1.121s,  913.84/s  (1.110s,  922.46/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [ 900/1251 ( 72%)]  Loss:  3.502839 (3.7319)  Time: 1.118s,  915.71/s  (1.110s,  922.85/s)  LR: 4.742e-04  Data: 0.010 (0.012)
Train: 44 [ 950/1251 ( 76%)]  Loss:  3.622877 (3.7265)  Time: 1.099s,  931.40/s  (1.110s,  922.39/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [1000/1251 ( 80%)]  Loss:  3.807443 (3.7303)  Time: 1.099s,  931.84/s  (1.110s,  922.49/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [1050/1251 ( 84%)]  Loss:  3.819023 (3.7344)  Time: 1.133s,  903.89/s  (1.110s,  922.34/s)  LR: 4.742e-04  Data: 0.012 (0.012)
Train: 44 [1100/1251 ( 88%)]  Loss:  3.762271 (3.7356)  Time: 1.098s,  932.21/s  (1.110s,  922.73/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [1150/1251 ( 92%)]  Loss:  3.724370 (3.7351)  Time: 1.125s,  910.51/s  (1.110s,  922.59/s)  LR: 4.742e-04  Data: 0.009 (0.012)
Train: 44 [1200/1251 ( 96%)]  Loss:  3.652254 (3.7318)  Time: 1.100s,  931.15/s  (1.110s,  922.61/s)  LR: 4.742e-04  Data: 0.011 (0.012)
Train: 44 [1250/1251 (100%)]  Loss:  3.965718 (3.7408)  Time: 1.079s,  948.69/s  (1.110s,  922.54/s)  LR: 4.742e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.255 (3.255)  Loss:  0.6528 (0.6528)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.7331 (1.2291)  Acc@1: 84.1981 (71.9400)  Acc@5: 95.7547 (91.1280)
Test (EMA): [   0/48]  Time: 3.188 (3.188)  Loss:  0.6391 (0.6391)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.8750 (96.8750)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.7314 (1.2426)  Acc@1: 83.4906 (71.7960)  Acc@5: 95.2830 (90.6740)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 66.33799987304687)

Train: 45 [   0/1251 (  0%)]  Loss:  3.738344 (3.7383)  Time: 1.110s,  922.91/s  (1.110s,  922.91/s)  LR: 4.730e-04  Data: 0.026 (0.026)
Train: 45 [  50/1251 (  4%)]  Loss:  3.639560 (3.6890)  Time: 1.096s,  934.47/s  (1.102s,  929.60/s)  LR: 4.730e-04  Data: 0.011 (0.012)
Train: 45 [ 100/1251 (  8%)]  Loss:  4.117452 (3.8318)  Time: 1.097s,  933.11/s  (1.102s,  929.27/s)  LR: 4.730e-04  Data: 0.014 (0.012)
Train: 45 [ 150/1251 ( 12%)]  Loss:  3.486219 (3.7454)  Time: 1.123s,  911.71/s  (1.109s,  923.19/s)  LR: 4.730e-04  Data: 0.016 (0.012)
Train: 45 [ 200/1251 ( 16%)]  Loss:  3.769303 (3.7502)  Time: 1.110s,  922.89/s  (1.107s,  924.63/s)  LR: 4.730e-04  Data: 0.014 (0.012)
Train: 45 [ 250/1251 ( 20%)]  Loss:  3.534498 (3.7142)  Time: 1.137s,  900.93/s  (1.109s,  923.34/s)  LR: 4.730e-04  Data: 0.011 (0.012)
Train: 45 [ 300/1251 ( 24%)]  Loss:  3.796105 (3.7259)  Time: 1.096s,  934.22/s  (1.109s,  923.65/s)  LR: 4.730e-04  Data: 0.013 (0.012)
Train: 45 [ 350/1251 ( 28%)]  Loss:  3.629323 (3.7139)  Time: 1.132s,  904.25/s  (1.109s,  923.13/s)  LR: 4.730e-04  Data: 0.010 (0.012)
Train: 45 [ 400/1251 ( 32%)]  Loss:  3.722301 (3.7148)  Time: 1.099s,  932.16/s  (1.109s,  923.62/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [ 450/1251 ( 36%)]  Loss:  3.751570 (3.7185)  Time: 1.120s,  914.07/s  (1.110s,  922.69/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [ 500/1251 ( 40%)]  Loss:  3.889899 (3.7341)  Time: 1.096s,  934.32/s  (1.109s,  922.96/s)  LR: 4.730e-04  Data: 0.011 (0.012)
Train: 45 [ 550/1251 ( 44%)]  Loss:  3.987169 (3.7551)  Time: 1.103s,  928.35/s  (1.110s,  922.88/s)  LR: 4.730e-04  Data: 0.010 (0.012)
Train: 45 [ 600/1251 ( 48%)]  Loss:  3.876534 (3.7645)  Time: 1.102s,  928.91/s  (1.109s,  923.50/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [ 650/1251 ( 52%)]  Loss:  3.915679 (3.7753)  Time: 1.099s,  931.39/s  (1.109s,  923.21/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [ 700/1251 ( 56%)]  Loss:  3.507890 (3.7575)  Time: 1.096s,  934.24/s  (1.109s,  923.19/s)  LR: 4.730e-04  Data: 0.011 (0.012)
Train: 45 [ 750/1251 ( 60%)]  Loss:  3.500317 (3.7414)  Time: 1.210s,  846.57/s  (1.109s,  923.46/s)  LR: 4.730e-04  Data: 0.010 (0.012)
Train: 45 [ 800/1251 ( 64%)]  Loss:  3.839216 (3.7471)  Time: 1.099s,  931.58/s  (1.109s,  923.43/s)  LR: 4.730e-04  Data: 0.014 (0.012)
Train: 45 [ 850/1251 ( 68%)]  Loss:  4.095013 (3.7665)  Time: 1.093s,  936.58/s  (1.109s,  923.75/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [ 900/1251 ( 72%)]  Loss:  3.281173 (3.7409)  Time: 1.097s,  933.06/s  (1.109s,  923.67/s)  LR: 4.730e-04  Data: 0.010 (0.012)
Train: 45 [ 950/1251 ( 76%)]  Loss:  3.784717 (3.7431)  Time: 1.092s,  937.89/s  (1.108s,  924.06/s)  LR: 4.730e-04  Data: 0.010 (0.012)
Train: 45 [1000/1251 ( 80%)]  Loss:  3.790004 (3.7453)  Time: 1.130s,  906.52/s  (1.109s,  923.75/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [1050/1251 ( 84%)]  Loss:  3.744099 (3.7453)  Time: 1.101s,  930.29/s  (1.108s,  924.04/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [1100/1251 ( 88%)]  Loss:  3.985301 (3.7557)  Time: 1.183s,  865.33/s  (1.109s,  923.68/s)  LR: 4.730e-04  Data: 0.011 (0.012)
Train: 45 [1150/1251 ( 92%)]  Loss:  3.919029 (3.7625)  Time: 1.102s,  929.30/s  (1.109s,  923.53/s)  LR: 4.730e-04  Data: 0.013 (0.012)
Train: 45 [1200/1251 ( 96%)]  Loss:  3.895145 (3.7678)  Time: 1.098s,  932.97/s  (1.109s,  923.73/s)  LR: 4.730e-04  Data: 0.012 (0.012)
Train: 45 [1250/1251 (100%)]  Loss:  3.894897 (3.7727)  Time: 1.080s,  948.04/s  (1.109s,  923.74/s)  LR: 4.730e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.202 (3.202)  Loss:  0.6394 (0.6394)  Acc@1: 86.1328 (86.1328)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.400)  Loss:  0.7580 (1.2401)  Acc@1: 83.8443 (72.1080)  Acc@5: 95.2830 (91.1180)
Test (EMA): [   0/48]  Time: 3.245 (3.245)  Loss:  0.6252 (0.6252)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.8750 (96.8750)
Test (EMA): [  48/48]  Time: 0.230 (0.408)  Loss:  0.7202 (1.2215)  Acc@1: 83.6085 (72.1580)  Acc@5: 95.4009 (90.9300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 67.134)

Train: 46 [   0/1251 (  0%)]  Loss:  3.610311 (3.6103)  Time: 1.131s,  905.63/s  (1.131s,  905.63/s)  LR: 4.718e-04  Data: 0.028 (0.028)
Train: 46 [  50/1251 (  4%)]  Loss:  3.767987 (3.6891)  Time: 1.097s,  933.24/s  (1.108s,  924.30/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [ 100/1251 (  8%)]  Loss:  3.556280 (3.6449)  Time: 1.123s,  912.11/s  (1.109s,  923.71/s)  LR: 4.718e-04  Data: 0.013 (0.012)
Train: 46 [ 150/1251 ( 12%)]  Loss:  3.576749 (3.6278)  Time: 1.123s,  911.56/s  (1.108s,  924.01/s)  LR: 4.718e-04  Data: 0.017 (0.012)
Train: 46 [ 200/1251 ( 16%)]  Loss:  3.714168 (3.6451)  Time: 1.095s,  934.90/s  (1.112s,  920.69/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [ 250/1251 ( 20%)]  Loss:  3.955003 (3.6967)  Time: 1.107s,  925.44/s  (1.111s,  921.36/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 46 [ 300/1251 ( 24%)]  Loss:  3.525427 (3.6723)  Time: 1.097s,  933.44/s  (1.112s,  920.64/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Train: 46 [ 350/1251 ( 28%)]  Loss:  3.631367 (3.6672)  Time: 1.125s,  910.56/s  (1.111s,  922.00/s)  LR: 4.718e-04  Data: 0.014 (0.012)
Train: 46 [ 400/1251 ( 32%)]  Loss:  3.918941 (3.6951)  Time: 1.105s,  926.68/s  (1.111s,  921.77/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Train: 46 [ 450/1251 ( 36%)]  Loss:  3.496244 (3.6752)  Time: 1.097s,  933.09/s  (1.110s,  922.73/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [ 500/1251 ( 40%)]  Loss:  3.495888 (3.6589)  Time: 1.097s,  933.18/s  (1.110s,  922.12/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [ 550/1251 ( 44%)]  Loss:  3.747132 (3.6663)  Time: 1.097s,  933.15/s  (1.110s,  922.12/s)  LR: 4.718e-04  Data: 0.013 (0.012)
Train: 46 [ 600/1251 ( 48%)]  Loss:  3.759264 (3.6734)  Time: 1.189s,  860.91/s  (1.110s,  922.38/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Train: 46 [ 650/1251 ( 52%)]  Loss:  3.321930 (3.6483)  Time: 1.100s,  930.61/s  (1.110s,  922.60/s)  LR: 4.718e-04  Data: 0.018 (0.012)
Train: 46 [ 700/1251 ( 56%)]  Loss:  3.734323 (3.6541)  Time: 1.121s,  913.65/s  (1.110s,  922.71/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Train: 46 [ 750/1251 ( 60%)]  Loss:  3.849027 (3.6663)  Time: 1.101s,  929.93/s  (1.111s,  922.10/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Train: 46 [ 800/1251 ( 64%)]  Loss:  3.422143 (3.6519)  Time: 1.190s,  860.60/s  (1.110s,  922.49/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [ 850/1251 ( 68%)]  Loss:  3.452435 (3.6408)  Time: 1.096s,  934.32/s  (1.110s,  922.57/s)  LR: 4.718e-04  Data: 0.011 (0.012)
Train: 46 [ 900/1251 ( 72%)]  Loss:  3.880806 (3.6534)  Time: 1.097s,  933.36/s  (1.109s,  923.01/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [ 950/1251 ( 76%)]  Loss:  3.589777 (3.6503)  Time: 1.108s,  924.57/s  (1.110s,  922.80/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [1000/1251 ( 80%)]  Loss:  3.799061 (3.6573)  Time: 1.131s,  905.60/s  (1.109s,  923.19/s)  LR: 4.718e-04  Data: 0.017 (0.012)
Train: 46 [1050/1251 ( 84%)]  Loss:  3.764397 (3.6622)  Time: 1.122s,  912.57/s  (1.110s,  922.80/s)  LR: 4.718e-04  Data: 0.010 (0.012)
Train: 46 [1100/1251 ( 88%)]  Loss:  3.854450 (3.6706)  Time: 1.103s,  928.50/s  (1.109s,  922.97/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [1150/1251 ( 92%)]  Loss:  3.465173 (3.6620)  Time: 1.098s,  932.58/s  (1.109s,  923.02/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [1200/1251 ( 96%)]  Loss:  3.516589 (3.6562)  Time: 1.096s,  934.03/s  (1.109s,  923.22/s)  LR: 4.718e-04  Data: 0.012 (0.012)
Train: 46 [1250/1251 (100%)]  Loss:  3.875674 (3.6646)  Time: 1.081s,  947.25/s  (1.109s,  923.39/s)  LR: 4.718e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.251 (3.251)  Loss:  0.6500 (0.6500)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.7577 (1.2197)  Acc@1: 83.6085 (72.4060)  Acc@5: 95.4009 (91.3140)
Test (EMA): [   0/48]  Time: 3.253 (3.253)  Loss:  0.6134 (0.6134)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.9727 (96.9727)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.7095 (1.2027)  Acc@1: 84.0802 (72.4840)  Acc@5: 95.7547 (91.1200)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 67.94399994628907)

Train: 47 [   0/1251 (  0%)]  Loss:  3.582851 (3.5829)  Time: 1.207s,  848.43/s  (1.207s,  848.43/s)  LR: 4.706e-04  Data: 0.029 (0.029)
Train: 47 [  50/1251 (  4%)]  Loss:  3.875757 (3.7293)  Time: 1.131s,  905.22/s  (1.111s,  922.03/s)  LR: 4.706e-04  Data: 0.013 (0.012)
Train: 47 [ 100/1251 (  8%)]  Loss:  3.549366 (3.6693)  Time: 1.097s,  933.79/s  (1.108s,  924.25/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 150/1251 ( 12%)]  Loss:  3.584480 (3.6481)  Time: 1.092s,  937.42/s  (1.111s,  921.98/s)  LR: 4.706e-04  Data: 0.010 (0.012)
Train: 47 [ 200/1251 ( 16%)]  Loss:  3.535673 (3.6256)  Time: 1.116s,  917.42/s  (1.108s,  924.35/s)  LR: 4.706e-04  Data: 0.011 (0.012)
Train: 47 [ 250/1251 ( 20%)]  Loss:  3.693546 (3.6369)  Time: 1.096s,  933.97/s  (1.108s,  923.87/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 300/1251 ( 24%)]  Loss:  3.562037 (3.6262)  Time: 1.095s,  934.79/s  (1.107s,  924.82/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 350/1251 ( 28%)]  Loss:  3.856496 (3.6550)  Time: 1.093s,  936.65/s  (1.108s,  924.54/s)  LR: 4.706e-04  Data: 0.010 (0.012)
Train: 47 [ 400/1251 ( 32%)]  Loss:  3.861938 (3.6780)  Time: 1.097s,  933.80/s  (1.107s,  924.70/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 450/1251 ( 36%)]  Loss:  4.081407 (3.7184)  Time: 1.124s,  910.76/s  (1.108s,  924.20/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 500/1251 ( 40%)]  Loss:  3.919960 (3.7367)  Time: 1.122s,  912.27/s  (1.108s,  924.38/s)  LR: 4.706e-04  Data: 0.013 (0.012)
Train: 47 [ 550/1251 ( 44%)]  Loss:  3.313756 (3.7014)  Time: 1.099s,  931.63/s  (1.108s,  924.07/s)  LR: 4.706e-04  Data: 0.010 (0.012)
Train: 47 [ 600/1251 ( 48%)]  Loss:  3.426217 (3.6803)  Time: 1.101s,  930.00/s  (1.108s,  924.53/s)  LR: 4.706e-04  Data: 0.011 (0.012)
Train: 47 [ 650/1251 ( 52%)]  Loss:  3.873255 (3.6941)  Time: 1.097s,  933.59/s  (1.107s,  924.79/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 700/1251 ( 56%)]  Loss:  3.942503 (3.7106)  Time: 1.128s,  908.17/s  (1.108s,  924.12/s)  LR: 4.706e-04  Data: 0.013 (0.012)
Train: 47 [ 750/1251 ( 60%)]  Loss:  3.438327 (3.6936)  Time: 1.098s,  932.32/s  (1.108s,  924.45/s)  LR: 4.706e-04  Data: 0.010 (0.012)
Train: 47 [ 800/1251 ( 64%)]  Loss:  3.860407 (3.7034)  Time: 1.109s,  923.12/s  (1.108s,  924.02/s)  LR: 4.706e-04  Data: 0.011 (0.012)
Train: 47 [ 850/1251 ( 68%)]  Loss:  3.741632 (3.7055)  Time: 1.096s,  934.38/s  (1.108s,  924.21/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 900/1251 ( 72%)]  Loss:  3.823843 (3.7118)  Time: 1.099s,  931.58/s  (1.108s,  924.08/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [ 950/1251 ( 76%)]  Loss:  3.574378 (3.7049)  Time: 1.099s,  931.54/s  (1.108s,  924.33/s)  LR: 4.706e-04  Data: 0.010 (0.012)
Train: 47 [1000/1251 ( 80%)]  Loss:  3.619549 (3.7008)  Time: 1.100s,  930.76/s  (1.108s,  924.37/s)  LR: 4.706e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 47 [1050/1251 ( 84%)]  Loss:  3.610127 (3.6967)  Time: 1.103s,  928.45/s  (1.107s,  924.63/s)  LR: 4.706e-04  Data: 0.014 (0.012)
Train: 47 [1100/1251 ( 88%)]  Loss:  3.756699 (3.6993)  Time: 1.096s,  933.97/s  (1.107s,  924.70/s)  LR: 4.706e-04  Data: 0.012 (0.012)
Train: 47 [1150/1251 ( 92%)]  Loss:  3.702081 (3.6994)  Time: 1.095s,  935.35/s  (1.107s,  924.82/s)  LR: 4.706e-04  Data: 0.011 (0.012)
Train: 47 [1200/1251 ( 96%)]  Loss:  3.281990 (3.6827)  Time: 1.093s,  937.10/s  (1.107s,  925.02/s)  LR: 4.706e-04  Data: 0.010 (0.012)
Train: 47 [1250/1251 (100%)]  Loss:  3.761196 (3.6857)  Time: 1.086s,  943.21/s  (1.107s,  924.64/s)  LR: 4.706e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.179 (3.179)  Loss:  0.5907 (0.5907)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6869 (1.1952)  Acc@1: 84.0802 (72.3800)  Acc@5: 96.5802 (91.2800)
Test (EMA): [   0/48]  Time: 3.121 (3.121)  Loss:  0.6026 (0.6026)  Acc@1: 86.8164 (86.8164)  Acc@5: 97.1680 (97.1680)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.7002 (1.1854)  Acc@1: 84.1981 (72.7840)  Acc@5: 95.7547 (91.3020)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 68.68399997070313)

Train: 48 [   0/1251 (  0%)]  Loss:  3.422563 (3.4226)  Time: 1.132s,  904.38/s  (1.132s,  904.38/s)  LR: 4.694e-04  Data: 0.026 (0.026)
Train: 48 [  50/1251 (  4%)]  Loss:  4.020060 (3.7213)  Time: 1.100s,  931.15/s  (1.103s,  928.75/s)  LR: 4.694e-04  Data: 0.015 (0.012)
Train: 48 [ 100/1251 (  8%)]  Loss:  3.680707 (3.7078)  Time: 1.102s,  928.89/s  (1.110s,  922.47/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 150/1251 ( 12%)]  Loss:  3.763840 (3.7218)  Time: 1.105s,  927.00/s  (1.107s,  925.35/s)  LR: 4.694e-04  Data: 0.014 (0.012)
Train: 48 [ 200/1251 ( 16%)]  Loss:  3.456205 (3.6687)  Time: 1.098s,  932.43/s  (1.107s,  924.77/s)  LR: 4.694e-04  Data: 0.012 (0.012)
Train: 48 [ 250/1251 ( 20%)]  Loss:  3.843698 (3.6978)  Time: 1.096s,  934.51/s  (1.106s,  925.97/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 300/1251 ( 24%)]  Loss:  3.820163 (3.7153)  Time: 1.103s,  928.60/s  (1.107s,  925.30/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 350/1251 ( 28%)]  Loss:  3.416662 (3.6780)  Time: 1.099s,  931.38/s  (1.106s,  925.71/s)  LR: 4.694e-04  Data: 0.013 (0.012)
Train: 48 [ 400/1251 ( 32%)]  Loss:  3.677389 (3.6779)  Time: 1.097s,  933.72/s  (1.107s,  924.81/s)  LR: 4.694e-04  Data: 0.012 (0.012)
Train: 48 [ 450/1251 ( 36%)]  Loss:  3.205857 (3.6307)  Time: 1.097s,  933.54/s  (1.107s,  925.24/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 500/1251 ( 40%)]  Loss:  3.338989 (3.6042)  Time: 1.194s,  857.38/s  (1.107s,  925.19/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 550/1251 ( 44%)]  Loss:  3.486711 (3.5944)  Time: 1.102s,  929.01/s  (1.107s,  924.64/s)  LR: 4.694e-04  Data: 0.012 (0.012)
Train: 48 [ 600/1251 ( 48%)]  Loss:  3.947510 (3.6216)  Time: 1.097s,  933.06/s  (1.107s,  924.95/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 650/1251 ( 52%)]  Loss:  3.484237 (3.6118)  Time: 1.096s,  934.52/s  (1.107s,  924.68/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 700/1251 ( 56%)]  Loss:  3.860800 (3.6284)  Time: 1.111s,  922.04/s  (1.107s,  925.04/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 750/1251 ( 60%)]  Loss:  3.536823 (3.6226)  Time: 1.126s,  909.14/s  (1.107s,  924.73/s)  LR: 4.694e-04  Data: 0.010 (0.012)
Train: 48 [ 800/1251 ( 64%)]  Loss:  4.010682 (3.6455)  Time: 1.108s,  924.50/s  (1.108s,  924.36/s)  LR: 4.694e-04  Data: 0.012 (0.012)
Train: 48 [ 850/1251 ( 68%)]  Loss:  3.570830 (3.6413)  Time: 1.094s,  935.91/s  (1.108s,  923.96/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 900/1251 ( 72%)]  Loss:  4.065340 (3.6636)  Time: 1.097s,  933.46/s  (1.108s,  924.37/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [ 950/1251 ( 76%)]  Loss:  3.798853 (3.6704)  Time: 1.122s,  912.83/s  (1.108s,  924.14/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [1000/1251 ( 80%)]  Loss:  3.854931 (3.6792)  Time: 1.098s,  932.43/s  (1.108s,  923.96/s)  LR: 4.694e-04  Data: 0.013 (0.012)
Train: 48 [1050/1251 ( 84%)]  Loss:  3.468810 (3.6696)  Time: 1.096s,  934.47/s  (1.108s,  923.94/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [1100/1251 ( 88%)]  Loss:  3.601062 (3.6666)  Time: 1.133s,  903.72/s  (1.108s,  923.86/s)  LR: 4.694e-04  Data: 0.010 (0.012)
Train: 48 [1150/1251 ( 92%)]  Loss:  3.820033 (3.6730)  Time: 1.222s,  838.05/s  (1.108s,  923.90/s)  LR: 4.694e-04  Data: 0.011 (0.012)
Train: 48 [1200/1251 ( 96%)]  Loss:  3.869721 (3.6809)  Time: 1.093s,  936.57/s  (1.109s,  923.63/s)  LR: 4.694e-04  Data: 0.010 (0.012)
Train: 48 [1250/1251 (100%)]  Loss:  3.936294 (3.6907)  Time: 1.084s,  945.06/s  (1.109s,  923.73/s)  LR: 4.694e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.281 (3.281)  Loss:  0.6592 (0.6592)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.6657 (1.1844)  Acc@1: 84.4340 (72.8220)  Acc@5: 95.7547 (91.4800)
Test (EMA): [   0/48]  Time: 3.056 (3.056)  Loss:  0.5913 (0.5913)  Acc@1: 86.9141 (86.9141)  Acc@5: 97.3633 (97.3633)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.6900 (1.1691)  Acc@1: 84.3160 (73.0980)  Acc@5: 95.8726 (91.4640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 69.40399997070313)

Train: 49 [   0/1251 (  0%)]  Loss:  3.416350 (3.4163)  Time: 1.134s,  903.22/s  (1.134s,  903.22/s)  LR: 4.681e-04  Data: 0.027 (0.027)
Train: 49 [  50/1251 (  4%)]  Loss:  3.748998 (3.5827)  Time: 1.096s,  934.41/s  (1.110s,  922.23/s)  LR: 4.681e-04  Data: 0.011 (0.012)
Train: 49 [ 100/1251 (  8%)]  Loss:  3.558854 (3.5747)  Time: 1.167s,  877.74/s  (1.110s,  922.18/s)  LR: 4.681e-04  Data: 0.010 (0.012)
Train: 49 [ 150/1251 ( 12%)]  Loss:  3.957754 (3.6705)  Time: 1.096s,  934.27/s  (1.109s,  923.70/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [ 200/1251 ( 16%)]  Loss:  3.935115 (3.7234)  Time: 1.117s,  916.41/s  (1.107s,  924.89/s)  LR: 4.681e-04  Data: 0.010 (0.012)
Train: 49 [ 250/1251 ( 20%)]  Loss:  3.460082 (3.6795)  Time: 1.097s,  933.62/s  (1.109s,  923.35/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [ 300/1251 ( 24%)]  Loss:  3.440252 (3.6453)  Time: 1.099s,  931.88/s  (1.109s,  922.99/s)  LR: 4.681e-04  Data: 0.013 (0.012)
Train: 49 [ 350/1251 ( 28%)]  Loss:  3.543374 (3.6326)  Time: 1.101s,  930.42/s  (1.109s,  923.07/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [ 400/1251 ( 32%)]  Loss:  3.732385 (3.6437)  Time: 1.097s,  933.76/s  (1.109s,  923.47/s)  LR: 4.681e-04  Data: 0.011 (0.012)
Train: 49 [ 450/1251 ( 36%)]  Loss:  3.499248 (3.6292)  Time: 1.192s,  858.77/s  (1.109s,  923.00/s)  LR: 4.681e-04  Data: 0.011 (0.012)
Train: 49 [ 500/1251 ( 40%)]  Loss:  3.777923 (3.6428)  Time: 1.096s,  934.25/s  (1.109s,  923.57/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 49 [ 550/1251 ( 44%)]  Loss:  3.696895 (3.6473)  Time: 1.098s,  932.53/s  (1.108s,  923.99/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [ 600/1251 ( 48%)]  Loss:  3.591292 (3.6430)  Time: 1.092s,  937.53/s  (1.109s,  923.60/s)  LR: 4.681e-04  Data: 0.010 (0.012)
Train: 49 [ 650/1251 ( 52%)]  Loss:  3.818509 (3.6555)  Time: 1.092s,  937.61/s  (1.109s,  923.65/s)  LR: 4.681e-04  Data: 0.009 (0.012)
Train: 49 [ 700/1251 ( 56%)]  Loss:  3.889227 (3.6711)  Time: 1.096s,  934.13/s  (1.109s,  923.72/s)  LR: 4.681e-04  Data: 0.013 (0.012)
Train: 49 [ 750/1251 ( 60%)]  Loss:  3.462876 (3.6581)  Time: 1.101s,  930.30/s  (1.109s,  923.35/s)  LR: 4.681e-04  Data: 0.014 (0.012)
Train: 49 [ 800/1251 ( 64%)]  Loss:  3.473733 (3.6472)  Time: 1.097s,  933.73/s  (1.109s,  923.13/s)  LR: 4.681e-04  Data: 0.010 (0.012)
Train: 49 [ 850/1251 ( 68%)]  Loss:  3.583328 (3.6437)  Time: 1.097s,  933.07/s  (1.109s,  923.64/s)  LR: 4.681e-04  Data: 0.011 (0.012)
Train: 49 [ 900/1251 ( 72%)]  Loss:  3.911614 (3.6578)  Time: 1.101s,  929.75/s  (1.109s,  923.67/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [ 950/1251 ( 76%)]  Loss:  3.488990 (3.6493)  Time: 1.098s,  932.82/s  (1.108s,  923.82/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [1000/1251 ( 80%)]  Loss:  3.367543 (3.6359)  Time: 1.097s,  933.12/s  (1.109s,  923.66/s)  LR: 4.681e-04  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Train: 49 [1050/1251 ( 84%)]  Loss:  4.305152 (3.6663)  Time: 1.098s,  932.35/s  (1.108s,  923.84/s)  LR: 4.681e-04  Data: 0.013 (0.012)
Train: 49 [1100/1251 ( 88%)]  Loss:  3.686152 (3.6672)  Time: 1.240s,  825.97/s  (1.108s,  923.86/s)  LR: 4.681e-04  Data: 0.011 (0.012)
Train: 49 [1150/1251 ( 92%)]  Loss:  3.872171 (3.6757)  Time: 1.098s,  932.44/s  (1.108s,  923.92/s)  LR: 4.681e-04  Data: 0.012 (0.012)
Train: 49 [1200/1251 ( 96%)]  Loss:  3.842695 (3.6824)  Time: 1.098s,  933.01/s  (1.108s,  924.23/s)  LR: 4.681e-04  Data: 0.011 (0.012)
Train: 49 [1250/1251 (100%)]  Loss:  3.933325 (3.6921)  Time: 1.083s,  945.78/s  (1.108s,  924.21/s)  LR: 4.681e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.353 (3.353)  Loss:  0.6384 (0.6384)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.7140 (1.1873)  Acc@1: 83.7264 (72.6800)  Acc@5: 96.6981 (91.5180)
Test (EMA): [   0/48]  Time: 3.190 (3.190)  Loss:  0.5822 (0.5822)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.6806 (1.1542)  Acc@1: 84.4340 (73.4060)  Acc@5: 95.8726 (91.6220)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 69.93199991699218)

Train: 50 [   0/1251 (  0%)]  Loss:  3.734300 (3.7343)  Time: 1.103s,  928.22/s  (1.103s,  928.22/s)  LR: 4.668e-04  Data: 0.024 (0.024)
Train: 50 [  50/1251 (  4%)]  Loss:  3.490545 (3.6124)  Time: 1.099s,  932.13/s  (1.100s,  931.31/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 100/1251 (  8%)]  Loss:  3.964938 (3.7299)  Time: 1.122s,  912.57/s  (1.104s,  927.26/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 150/1251 ( 12%)]  Loss:  3.349957 (3.6349)  Time: 1.098s,  932.32/s  (1.103s,  928.78/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 200/1251 ( 16%)]  Loss:  3.370850 (3.5821)  Time: 1.102s,  928.89/s  (1.105s,  926.99/s)  LR: 4.668e-04  Data: 0.013 (0.012)
Train: 50 [ 250/1251 ( 20%)]  Loss:  3.489020 (3.5666)  Time: 1.102s,  929.42/s  (1.104s,  927.50/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 300/1251 ( 24%)]  Loss:  3.701424 (3.5859)  Time: 1.098s,  932.84/s  (1.107s,  925.02/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 350/1251 ( 28%)]  Loss:  3.867647 (3.6211)  Time: 1.158s,  883.98/s  (1.106s,  925.57/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 400/1251 ( 32%)]  Loss:  3.619794 (3.6209)  Time: 1.095s,  935.33/s  (1.106s,  925.51/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 450/1251 ( 36%)]  Loss:  3.897223 (3.6486)  Time: 1.099s,  931.33/s  (1.106s,  926.03/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 500/1251 ( 40%)]  Loss:  3.880964 (3.6697)  Time: 1.105s,  926.43/s  (1.106s,  926.06/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 550/1251 ( 44%)]  Loss:  3.439085 (3.6505)  Time: 1.098s,  932.26/s  (1.106s,  926.10/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 600/1251 ( 48%)]  Loss:  3.369175 (3.6288)  Time: 1.113s,  919.90/s  (1.105s,  926.41/s)  LR: 4.668e-04  Data: 0.010 (0.012)
Train: 50 [ 650/1251 ( 52%)]  Loss:  3.493207 (3.6192)  Time: 1.097s,  933.84/s  (1.106s,  925.92/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 700/1251 ( 56%)]  Loss:  3.507278 (3.6117)  Time: 1.097s,  933.73/s  (1.106s,  925.68/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 750/1251 ( 60%)]  Loss:  4.151731 (3.6454)  Time: 1.097s,  933.03/s  (1.106s,  925.53/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 800/1251 ( 64%)]  Loss:  3.959668 (3.6639)  Time: 1.097s,  933.42/s  (1.106s,  925.87/s)  LR: 4.668e-04  Data: 0.013 (0.012)
Train: 50 [ 850/1251 ( 68%)]  Loss:  4.025696 (3.6840)  Time: 1.135s,  902.24/s  (1.107s,  925.38/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [ 900/1251 ( 72%)]  Loss:  3.567453 (3.6779)  Time: 1.096s,  934.08/s  (1.107s,  924.75/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [ 950/1251 ( 76%)]  Loss:  3.713028 (3.6796)  Time: 1.097s,  933.29/s  (1.107s,  924.61/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [1000/1251 ( 80%)]  Loss:  3.657440 (3.6786)  Time: 1.098s,  932.98/s  (1.108s,  924.52/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [1050/1251 ( 84%)]  Loss:  3.410170 (3.6664)  Time: 1.128s,  907.97/s  (1.107s,  924.76/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [1100/1251 ( 88%)]  Loss:  3.871281 (3.6753)  Time: 1.100s,  930.85/s  (1.108s,  924.50/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [1150/1251 ( 92%)]  Loss:  3.714167 (3.6769)  Time: 1.097s,  933.55/s  (1.108s,  924.56/s)  LR: 4.668e-04  Data: 0.012 (0.012)
Train: 50 [1200/1251 ( 96%)]  Loss:  4.124343 (3.6948)  Time: 1.099s,  932.14/s  (1.108s,  923.96/s)  LR: 4.668e-04  Data: 0.011 (0.012)
Train: 50 [1250/1251 (100%)]  Loss:  3.860947 (3.7012)  Time: 1.081s,  947.71/s  (1.108s,  924.10/s)  LR: 4.668e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.227 (3.227)  Loss:  0.5954 (0.5954)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.7114 (1.1748)  Acc@1: 84.3160 (72.9840)  Acc@5: 96.1085 (91.5540)
Test (EMA): [   0/48]  Time: 3.183 (3.183)  Loss:  0.5744 (0.5744)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.6725 (1.1405)  Acc@1: 84.3160 (73.7020)  Acc@5: 95.8726 (91.7540)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.41600004638671)

Train: 51 [   0/1251 (  0%)]  Loss:  3.495480 (3.4955)  Time: 1.108s,  924.55/s  (1.108s,  924.55/s)  LR: 4.655e-04  Data: 0.025 (0.025)
Train: 51 [  50/1251 (  4%)]  Loss:  3.411444 (3.4535)  Time: 1.096s,  934.43/s  (1.118s,  916.23/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 100/1251 (  8%)]  Loss:  3.711672 (3.5395)  Time: 1.120s,  913.91/s  (1.109s,  923.20/s)  LR: 4.655e-04  Data: 0.012 (0.012)
Train: 51 [ 150/1251 ( 12%)]  Loss:  4.047252 (3.6665)  Time: 1.100s,  931.12/s  (1.110s,  922.54/s)  LR: 4.655e-04  Data: 0.012 (0.012)
Train: 51 [ 200/1251 ( 16%)]  Loss:  3.589468 (3.6511)  Time: 1.105s,  926.80/s  (1.108s,  924.45/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 250/1251 ( 20%)]  Loss:  3.824095 (3.6799)  Time: 1.096s,  934.50/s  (1.110s,  922.42/s)  LR: 4.655e-04  Data: 0.010 (0.012)
Train: 51 [ 300/1251 ( 24%)]  Loss:  3.929292 (3.7155)  Time: 1.128s,  907.82/s  (1.111s,  921.45/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 350/1251 ( 28%)]  Loss:  3.808667 (3.7272)  Time: 1.196s,  856.54/s  (1.111s,  921.81/s)  LR: 4.655e-04  Data: 0.012 (0.012)
Train: 51 [ 400/1251 ( 32%)]  Loss:  3.920923 (3.7487)  Time: 1.095s,  935.29/s  (1.110s,  922.65/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 450/1251 ( 36%)]  Loss:  3.670535 (3.7409)  Time: 1.129s,  907.04/s  (1.109s,  923.25/s)  LR: 4.655e-04  Data: 0.010 (0.012)
Train: 51 [ 500/1251 ( 40%)]  Loss:  3.677633 (3.7351)  Time: 1.099s,  931.79/s  (1.109s,  923.42/s)  LR: 4.655e-04  Data: 0.012 (0.012)
Train: 51 [ 550/1251 ( 44%)]  Loss:  3.598221 (3.7237)  Time: 1.104s,  927.51/s  (1.108s,  923.97/s)  LR: 4.655e-04  Data: 0.012 (0.012)
Train: 51 [ 600/1251 ( 48%)]  Loss:  3.465655 (3.7039)  Time: 1.097s,  933.84/s  (1.109s,  923.77/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 650/1251 ( 52%)]  Loss:  3.738547 (3.7063)  Time: 1.100s,  931.30/s  (1.108s,  924.09/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 700/1251 ( 56%)]  Loss:  3.496207 (3.6923)  Time: 1.101s,  929.89/s  (1.108s,  923.84/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 750/1251 ( 60%)]  Loss:  3.513987 (3.6812)  Time: 1.101s,  930.17/s  (1.108s,  924.34/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 800/1251 ( 64%)]  Loss:  3.865745 (3.6920)  Time: 1.252s,  817.88/s  (1.108s,  924.16/s)  LR: 4.655e-04  Data: 0.013 (0.012)
Train: 51 [ 850/1251 ( 68%)]  Loss:  3.796482 (3.6979)  Time: 1.097s,  933.59/s  (1.108s,  924.23/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [ 900/1251 ( 72%)]  Loss:  3.755088 (3.7009)  Time: 1.096s,  934.59/s  (1.108s,  924.07/s)  LR: 4.655e-04  Data: 0.013 (0.012)
Train: 51 [ 950/1251 ( 76%)]  Loss:  3.701564 (3.7009)  Time: 1.099s,  932.11/s  (1.108s,  924.26/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [1000/1251 ( 80%)]  Loss:  3.726243 (3.7021)  Time: 1.111s,  921.74/s  (1.108s,  924.54/s)  LR: 4.655e-04  Data: 0.013 (0.012)
Train: 51 [1050/1251 ( 84%)]  Loss:  3.880085 (3.7102)  Time: 1.096s,  934.52/s  (1.108s,  924.11/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [1100/1251 ( 88%)]  Loss:  3.580673 (3.7046)  Time: 1.099s,  931.71/s  (1.108s,  924.14/s)  LR: 4.655e-04  Data: 0.016 (0.012)
Train: 51 [1150/1251 ( 92%)]  Loss:  3.970387 (3.7156)  Time: 1.120s,  914.00/s  (1.108s,  924.09/s)  LR: 4.655e-04  Data: 0.011 (0.012)
Train: 51 [1200/1251 ( 96%)]  Loss:  3.541815 (3.7087)  Time: 1.096s,  934.01/s  (1.108s,  924.37/s)  LR: 4.655e-04  Data: 0.013 (0.012)
Train: 51 [1250/1251 (100%)]  Loss:  3.791926 (3.7119)  Time: 1.109s,  923.71/s  (1.108s,  924.06/s)  LR: 4.655e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.210 (3.210)  Loss:  0.6160 (0.6160)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.7362 (1.2072)  Acc@1: 84.3160 (72.6700)  Acc@5: 95.5189 (91.4300)
Test (EMA): [   0/48]  Time: 3.116 (3.116)  Loss:  0.5666 (0.5666)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.3633 (97.3633)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.6659 (1.1276)  Acc@1: 84.7877 (73.9740)  Acc@5: 95.8726 (91.9000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.9260001220703)

Train: 52 [   0/1251 (  0%)]  Loss:  4.015336 (4.0153)  Time: 1.099s,  931.96/s  (1.099s,  931.96/s)  LR: 4.642e-04  Data: 0.020 (0.020)
Train: 52 [  50/1251 (  4%)]  Loss:  3.263216 (3.6393)  Time: 1.097s,  933.07/s  (1.100s,  931.28/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 100/1251 (  8%)]  Loss:  3.615537 (3.6314)  Time: 1.098s,  932.63/s  (1.105s,  926.63/s)  LR: 4.642e-04  Data: 0.013 (0.012)
Train: 52 [ 150/1251 ( 12%)]  Loss:  3.370718 (3.5662)  Time: 1.101s,  930.31/s  (1.104s,  927.48/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 200/1251 ( 16%)]  Loss:  3.661563 (3.5853)  Time: 1.097s,  933.21/s  (1.105s,  926.88/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [ 250/1251 ( 20%)]  Loss:  3.762424 (3.6148)  Time: 1.123s,  912.19/s  (1.105s,  927.00/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 300/1251 ( 24%)]  Loss:  3.592198 (3.6116)  Time: 1.255s,  815.85/s  (1.107s,  925.02/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 350/1251 ( 28%)]  Loss:  3.808715 (3.6362)  Time: 1.097s,  933.33/s  (1.107s,  925.34/s)  LR: 4.642e-04  Data: 0.010 (0.012)
Train: 52 [ 400/1251 ( 32%)]  Loss:  3.853054 (3.6603)  Time: 1.102s,  929.45/s  (1.106s,  925.52/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [ 450/1251 ( 36%)]  Loss:  3.605475 (3.6548)  Time: 1.098s,  932.72/s  (1.107s,  925.11/s)  LR: 4.642e-04  Data: 0.011 (0.011)
Train: 52 [ 500/1251 ( 40%)]  Loss:  3.645832 (3.6540)  Time: 1.097s,  933.24/s  (1.106s,  925.78/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 550/1251 ( 44%)]  Loss:  4.016370 (3.6842)  Time: 1.102s,  929.42/s  (1.107s,  925.31/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 600/1251 ( 48%)]  Loss:  3.704537 (3.6858)  Time: 1.098s,  932.31/s  (1.106s,  925.72/s)  LR: 4.642e-04  Data: 0.013 (0.012)
Train: 52 [ 650/1251 ( 52%)]  Loss:  3.760002 (3.6911)  Time: 1.132s,  904.98/s  (1.107s,  925.42/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 700/1251 ( 56%)]  Loss:  3.614077 (3.6859)  Time: 1.132s,  904.29/s  (1.107s,  925.08/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [ 750/1251 ( 60%)]  Loss:  3.877054 (3.6979)  Time: 1.102s,  929.19/s  (1.107s,  924.65/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [ 800/1251 ( 64%)]  Loss:  3.801373 (3.7040)  Time: 1.099s,  932.02/s  (1.107s,  925.02/s)  LR: 4.642e-04  Data: 0.011 (0.011)
Train: 52 [ 850/1251 ( 68%)]  Loss:  3.885735 (3.7141)  Time: 1.134s,  903.15/s  (1.107s,  924.64/s)  LR: 4.642e-04  Data: 0.011 (0.011)
Train: 52 [ 900/1251 ( 72%)]  Loss:  3.871458 (3.7224)  Time: 1.096s,  933.96/s  (1.108s,  924.39/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [ 950/1251 ( 76%)]  Loss:  3.486530 (3.7106)  Time: 1.095s,  935.10/s  (1.108s,  924.52/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [1000/1251 ( 80%)]  Loss:  3.532686 (3.7021)  Time: 1.100s,  931.02/s  (1.108s,  924.56/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [1050/1251 ( 84%)]  Loss:  4.077969 (3.7192)  Time: 1.103s,  928.44/s  (1.108s,  924.58/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [1100/1251 ( 88%)]  Loss:  3.289360 (3.7005)  Time: 1.099s,  931.64/s  (1.108s,  924.54/s)  LR: 4.642e-04  Data: 0.012 (0.012)
Train: 52 [1150/1251 ( 92%)]  Loss:  3.630793 (3.6976)  Time: 1.097s,  933.21/s  (1.107s,  924.83/s)  LR: 4.642e-04  Data: 0.013 (0.012)
Train: 52 [1200/1251 ( 96%)]  Loss:  3.740806 (3.6993)  Time: 1.098s,  932.76/s  (1.107s,  924.62/s)  LR: 4.642e-04  Data: 0.011 (0.012)
Train: 52 [1250/1251 (100%)]  Loss:  3.980496 (3.7101)  Time: 1.081s,  947.10/s  (1.107s,  924.63/s)  LR: 4.642e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.250 (3.250)  Loss:  0.6130 (0.6130)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.6914 (1.1718)  Acc@1: 84.7877 (73.1840)  Acc@5: 96.1085 (91.7120)
Test (EMA): [   0/48]  Time: 3.119 (3.119)  Loss:  0.5587 (0.5587)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.6600 (1.1158)  Acc@1: 84.7877 (74.1860)  Acc@5: 95.8726 (92.0080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 71.34000004394531)

Train: 53 [   0/1251 (  0%)]  Loss:  4.110070 (4.1101)  Time: 1.106s,  925.60/s  (1.106s,  925.60/s)  LR: 4.628e-04  Data: 0.024 (0.024)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 53 [  50/1251 (  4%)]  Loss:  3.823779 (3.9669)  Time: 1.122s,  912.44/s  (1.116s,  917.62/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [ 100/1251 (  8%)]  Loss:  3.471501 (3.8018)  Time: 1.097s,  933.33/s  (1.108s,  924.34/s)  LR: 4.628e-04  Data: 0.010 (0.012)
Train: 53 [ 150/1251 ( 12%)]  Loss:  3.777674 (3.7958)  Time: 1.097s,  933.36/s  (1.110s,  922.33/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [ 200/1251 ( 16%)]  Loss:  3.769089 (3.7904)  Time: 1.096s,  933.91/s  (1.107s,  924.63/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [ 250/1251 ( 20%)]  Loss:  3.651860 (3.7673)  Time: 1.095s,  935.04/s  (1.108s,  924.01/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [ 300/1251 ( 24%)]  Loss:  3.426506 (3.7186)  Time: 1.096s,  933.95/s  (1.108s,  923.84/s)  LR: 4.628e-04  Data: 0.010 (0.012)
Train: 53 [ 350/1251 ( 28%)]  Loss:  3.356284 (3.6733)  Time: 1.127s,  908.46/s  (1.108s,  924.32/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [ 400/1251 ( 32%)]  Loss:  3.608297 (3.6661)  Time: 1.122s,  912.64/s  (1.109s,  923.70/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [ 450/1251 ( 36%)]  Loss:  3.689351 (3.6684)  Time: 1.186s,  863.59/s  (1.109s,  923.68/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [ 500/1251 ( 40%)]  Loss:  3.507676 (3.6538)  Time: 1.106s,  926.15/s  (1.109s,  923.33/s)  LR: 4.628e-04  Data: 0.013 (0.012)
Train: 53 [ 550/1251 ( 44%)]  Loss:  3.771019 (3.6636)  Time: 1.124s,  910.97/s  (1.110s,  922.53/s)  LR: 4.628e-04  Data: 0.013 (0.012)
Train: 53 [ 600/1251 ( 48%)]  Loss:  3.476576 (3.6492)  Time: 1.124s,  911.13/s  (1.111s,  921.88/s)  LR: 4.628e-04  Data: 0.014 (0.012)
Train: 53 [ 650/1251 ( 52%)]  Loss:  3.420603 (3.6329)  Time: 1.098s,  932.97/s  (1.110s,  922.51/s)  LR: 4.628e-04  Data: 0.014 (0.012)
Train: 53 [ 700/1251 ( 56%)]  Loss:  3.817906 (3.6452)  Time: 1.097s,  933.48/s  (1.110s,  922.47/s)  LR: 4.628e-04  Data: 0.010 (0.012)
Train: 53 [ 750/1251 ( 60%)]  Loss:  3.457101 (3.6335)  Time: 1.121s,  913.79/s  (1.109s,  923.07/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [ 800/1251 ( 64%)]  Loss:  4.044840 (3.6577)  Time: 1.097s,  933.74/s  (1.109s,  923.23/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [ 850/1251 ( 68%)]  Loss:  3.779351 (3.6644)  Time: 1.096s,  933.94/s  (1.109s,  923.63/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [ 900/1251 ( 72%)]  Loss:  3.802543 (3.6717)  Time: 1.099s,  931.35/s  (1.109s,  923.66/s)  LR: 4.628e-04  Data: 0.010 (0.012)
Train: 53 [ 950/1251 ( 76%)]  Loss:  3.430134 (3.6596)  Time: 1.101s,  929.99/s  (1.108s,  923.83/s)  LR: 4.628e-04  Data: 0.011 (0.012)
Train: 53 [1000/1251 ( 80%)]  Loss:  3.446348 (3.6495)  Time: 1.107s,  924.68/s  (1.108s,  923.97/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [1050/1251 ( 84%)]  Loss:  3.913983 (3.6615)  Time: 1.122s,  912.95/s  (1.109s,  923.50/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [1100/1251 ( 88%)]  Loss:  3.468953 (3.6531)  Time: 1.191s,  859.97/s  (1.109s,  923.14/s)  LR: 4.628e-04  Data: 0.010 (0.012)
Train: 53 [1150/1251 ( 92%)]  Loss:  3.992367 (3.6672)  Time: 1.095s,  934.98/s  (1.109s,  923.09/s)  LR: 4.628e-04  Data: 0.012 (0.012)
Train: 53 [1200/1251 ( 96%)]  Loss:  3.688711 (3.6681)  Time: 1.096s,  934.11/s  (1.109s,  922.94/s)  LR: 4.628e-04  Data: 0.013 (0.012)
Train: 53 [1250/1251 (100%)]  Loss:  3.620603 (3.6663)  Time: 1.077s,  950.44/s  (1.110s,  922.87/s)  LR: 4.628e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.255 (3.255)  Loss:  0.5993 (0.5993)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.7097 (1.1701)  Acc@1: 83.4906 (73.1960)  Acc@5: 96.2264 (91.8260)
Test (EMA): [   0/48]  Time: 3.126 (3.126)  Loss:  0.5519 (0.5519)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.230 (0.403)  Loss:  0.6543 (1.1052)  Acc@1: 84.7877 (74.3660)  Acc@5: 95.8726 (92.1240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.79599993896484)

Train: 54 [   0/1251 (  0%)]  Loss:  3.585734 (3.5857)  Time: 1.101s,  930.26/s  (1.101s,  930.26/s)  LR: 4.615e-04  Data: 0.020 (0.020)
Train: 54 [  50/1251 (  4%)]  Loss:  3.414488 (3.5001)  Time: 1.098s,  932.32/s  (1.101s,  929.70/s)  LR: 4.615e-04  Data: 0.014 (0.012)
Train: 54 [ 100/1251 (  8%)]  Loss:  3.677156 (3.5591)  Time: 1.097s,  933.47/s  (1.107s,  925.38/s)  LR: 4.615e-04  Data: 0.012 (0.012)
Train: 54 [ 150/1251 ( 12%)]  Loss:  3.734071 (3.6029)  Time: 1.107s,  925.34/s  (1.107s,  924.85/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 200/1251 ( 16%)]  Loss:  3.623431 (3.6070)  Time: 1.095s,  935.11/s  (1.109s,  923.33/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 250/1251 ( 20%)]  Loss:  3.429283 (3.5774)  Time: 1.122s,  912.78/s  (1.108s,  923.84/s)  LR: 4.615e-04  Data: 0.010 (0.012)
Train: 54 [ 300/1251 ( 24%)]  Loss:  3.308454 (3.5389)  Time: 1.097s,  933.74/s  (1.109s,  923.42/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 350/1251 ( 28%)]  Loss:  3.479565 (3.5315)  Time: 1.097s,  933.12/s  (1.110s,  922.57/s)  LR: 4.615e-04  Data: 0.013 (0.012)
Train: 54 [ 400/1251 ( 32%)]  Loss:  3.848854 (3.5668)  Time: 1.104s,  927.53/s  (1.110s,  922.86/s)  LR: 4.615e-04  Data: 0.010 (0.012)
Train: 54 [ 450/1251 ( 36%)]  Loss:  3.395510 (3.5497)  Time: 1.129s,  907.28/s  (1.110s,  922.61/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 500/1251 ( 40%)]  Loss:  3.588017 (3.5531)  Time: 1.103s,  928.20/s  (1.109s,  923.16/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 550/1251 ( 44%)]  Loss:  3.115983 (3.5167)  Time: 1.104s,  927.41/s  (1.110s,  922.93/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 600/1251 ( 48%)]  Loss:  3.816721 (3.5398)  Time: 1.098s,  932.58/s  (1.109s,  923.32/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [ 650/1251 ( 52%)]  Loss:  3.967699 (3.5704)  Time: 1.102s,  929.53/s  (1.109s,  923.06/s)  LR: 4.615e-04  Data: 0.013 (0.012)
Train: 54 [ 700/1251 ( 56%)]  Loss:  3.506913 (3.5661)  Time: 1.103s,  928.43/s  (1.109s,  923.00/s)  LR: 4.615e-04  Data: 0.014 (0.012)
Train: 54 [ 750/1251 ( 60%)]  Loss:  3.762092 (3.5784)  Time: 1.099s,  932.07/s  (1.109s,  923.26/s)  LR: 4.615e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 54 [ 800/1251 ( 64%)]  Loss:  3.713381 (3.5863)  Time: 1.096s,  933.97/s  (1.109s,  923.56/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 54 [ 850/1251 ( 68%)]  Loss:  3.801524 (3.5983)  Time: 1.104s,  927.89/s  (1.109s,  923.65/s)  LR: 4.615e-04  Data: 0.010 (0.012)
Train: 54 [ 900/1251 ( 72%)]  Loss:  3.315147 (3.5834)  Time: 1.096s,  934.48/s  (1.108s,  923.78/s)  LR: 4.615e-04  Data: 0.012 (0.012)
Train: 54 [ 950/1251 ( 76%)]  Loss:  3.485456 (3.5785)  Time: 1.124s,  911.20/s  (1.108s,  924.00/s)  LR: 4.615e-04  Data: 0.013 (0.012)
Train: 54 [1000/1251 ( 80%)]  Loss:  3.351709 (3.5677)  Time: 1.129s,  907.19/s  (1.109s,  923.21/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [1050/1251 ( 84%)]  Loss:  3.411846 (3.5606)  Time: 1.098s,  932.97/s  (1.109s,  923.54/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [1100/1251 ( 88%)]  Loss:  3.468461 (3.5566)  Time: 1.098s,  932.43/s  (1.109s,  923.63/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [1150/1251 ( 92%)]  Loss:  3.752687 (3.5648)  Time: 1.096s,  934.11/s  (1.108s,  923.88/s)  LR: 4.615e-04  Data: 0.011 (0.012)
Train: 54 [1200/1251 ( 96%)]  Loss:  3.510356 (3.5626)  Time: 1.095s,  935.30/s  (1.108s,  923.78/s)  LR: 4.615e-04  Data: 0.010 (0.012)
Train: 54 [1250/1251 (100%)]  Loss:  3.425571 (3.5573)  Time: 1.093s,  936.59/s  (1.108s,  923.98/s)  LR: 4.615e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.266 (3.266)  Loss:  0.6650 (0.6650)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.7059 (1.1761)  Acc@1: 84.3160 (73.0080)  Acc@5: 95.6368 (91.6040)
Test (EMA): [   0/48]  Time: 3.267 (3.267)  Loss:  0.5461 (0.5461)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.6503 (1.0954)  Acc@1: 84.5519 (74.6020)  Acc@5: 95.8726 (92.2180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 72.15799999023437)

Train: 55 [   0/1251 (  0%)]  Loss:  3.928554 (3.9286)  Time: 1.102s,  929.24/s  (1.102s,  929.24/s)  LR: 4.601e-04  Data: 0.020 (0.020)
Train: 55 [  50/1251 (  4%)]  Loss:  3.708220 (3.8184)  Time: 1.132s,  904.81/s  (1.118s,  916.18/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 100/1251 (  8%)]  Loss:  3.716110 (3.7843)  Time: 1.112s,  921.07/s  (1.113s,  920.45/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 150/1251 ( 12%)]  Loss:  3.394868 (3.6869)  Time: 1.099s,  931.60/s  (1.112s,  920.73/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 200/1251 ( 16%)]  Loss:  3.498925 (3.6493)  Time: 1.101s,  929.99/s  (1.111s,  921.96/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 250/1251 ( 20%)]  Loss:  3.542888 (3.6316)  Time: 1.099s,  931.59/s  (1.111s,  922.10/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 300/1251 ( 24%)]  Loss:  3.466466 (3.6080)  Time: 1.099s,  931.98/s  (1.111s,  921.88/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 350/1251 ( 28%)]  Loss:  3.781451 (3.6297)  Time: 1.100s,  931.12/s  (1.110s,  922.39/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 400/1251 ( 32%)]  Loss:  3.860178 (3.6553)  Time: 1.120s,  914.56/s  (1.110s,  922.71/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 450/1251 ( 36%)]  Loss:  3.674981 (3.6573)  Time: 1.097s,  933.54/s  (1.109s,  923.47/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 500/1251 ( 40%)]  Loss:  3.843117 (3.6742)  Time: 1.100s,  930.99/s  (1.109s,  922.97/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 550/1251 ( 44%)]  Loss:  3.596689 (3.6677)  Time: 1.105s,  927.09/s  (1.109s,  923.07/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 600/1251 ( 48%)]  Loss:  3.628719 (3.6647)  Time: 1.095s,  935.15/s  (1.110s,  922.35/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 650/1251 ( 52%)]  Loss:  4.026598 (3.6906)  Time: 1.097s,  933.62/s  (1.109s,  923.00/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 700/1251 ( 56%)]  Loss:  3.679401 (3.6898)  Time: 1.096s,  934.60/s  (1.109s,  923.11/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [ 750/1251 ( 60%)]  Loss:  3.559106 (3.6816)  Time: 1.116s,  917.97/s  (1.109s,  923.10/s)  LR: 4.601e-04  Data: 0.010 (0.012)
Train: 55 [ 800/1251 ( 64%)]  Loss:  3.865566 (3.6925)  Time: 1.096s,  934.00/s  (1.109s,  923.05/s)  LR: 4.601e-04  Data: 0.010 (0.012)
Train: 55 [ 850/1251 ( 68%)]  Loss:  3.369845 (3.6745)  Time: 1.098s,  932.48/s  (1.109s,  923.33/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 900/1251 ( 72%)]  Loss:  3.924511 (3.6877)  Time: 1.097s,  933.51/s  (1.109s,  923.62/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [ 950/1251 ( 76%)]  Loss:  3.713892 (3.6890)  Time: 1.101s,  929.67/s  (1.109s,  923.62/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [1000/1251 ( 80%)]  Loss:  3.596681 (3.6846)  Time: 1.101s,  929.94/s  (1.109s,  923.66/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [1050/1251 ( 84%)]  Loss:  3.438552 (3.6734)  Time: 1.131s,  905.31/s  (1.109s,  923.38/s)  LR: 4.601e-04  Data: 0.011 (0.012)
Train: 55 [1100/1251 ( 88%)]  Loss:  3.266486 (3.6557)  Time: 1.122s,  913.02/s  (1.109s,  923.51/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [1150/1251 ( 92%)]  Loss:  3.613977 (3.6540)  Time: 1.103s,  928.76/s  (1.109s,  923.45/s)  LR: 4.601e-04  Data: 0.010 (0.012)
Train: 55 [1200/1251 ( 96%)]  Loss:  3.608999 (3.6522)  Time: 1.097s,  933.86/s  (1.108s,  923.78/s)  LR: 4.601e-04  Data: 0.012 (0.012)
Train: 55 [1250/1251 (100%)]  Loss:  3.553856 (3.6484)  Time: 1.168s,  876.38/s  (1.109s,  923.74/s)  LR: 4.601e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.236 (3.236)  Loss:  0.6210 (0.6210)  Acc@1: 87.3047 (87.3047)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.6901 (1.1675)  Acc@1: 85.0236 (73.7580)  Acc@5: 96.4623 (92.0020)
Test (EMA): [   0/48]  Time: 3.117 (3.117)  Loss:  0.5411 (0.5411)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.3633 (97.3633)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.6457 (1.0861)  Acc@1: 84.7877 (74.7680)  Acc@5: 95.7547 (92.3400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 72.48399993652343)

Train: 56 [   0/1251 (  0%)]  Loss:  3.956799 (3.9568)  Time: 1.137s,  900.81/s  (1.137s,  900.81/s)  LR: 4.586e-04  Data: 0.029 (0.029)
Train: 56 [  50/1251 (  4%)]  Loss:  3.622689 (3.7897)  Time: 1.120s,  914.24/s  (1.117s,  916.33/s)  LR: 4.586e-04  Data: 0.012 (0.012)
Train: 56 [ 100/1251 (  8%)]  Loss:  3.629740 (3.7364)  Time: 1.096s,  933.89/s  (1.114s,  919.51/s)  LR: 4.586e-04  Data: 0.011 (0.011)
Train: 56 [ 150/1251 ( 12%)]  Loss:  3.442342 (3.6629)  Time: 1.096s,  933.89/s  (1.111s,  921.89/s)  LR: 4.586e-04  Data: 0.012 (0.011)
Train: 56 [ 200/1251 ( 16%)]  Loss:  3.516355 (3.6336)  Time: 1.097s,  933.40/s  (1.109s,  923.52/s)  LR: 4.586e-04  Data: 0.013 (0.012)
Train: 56 [ 250/1251 ( 20%)]  Loss:  3.670358 (3.6397)  Time: 1.094s,  935.74/s  (1.109s,  923.58/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 300/1251 ( 24%)]  Loss:  3.784100 (3.6603)  Time: 1.108s,  924.03/s  (1.108s,  924.15/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 350/1251 ( 28%)]  Loss:  3.961760 (3.6980)  Time: 1.102s,  929.33/s  (1.108s,  924.08/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 400/1251 ( 32%)]  Loss:  3.774639 (3.7065)  Time: 1.097s,  933.53/s  (1.107s,  924.81/s)  LR: 4.586e-04  Data: 0.013 (0.012)
Train: 56 [ 450/1251 ( 36%)]  Loss:  3.604754 (3.6964)  Time: 1.095s,  935.29/s  (1.107s,  924.73/s)  LR: 4.586e-04  Data: 0.013 (0.012)
Train: 56 [ 500/1251 ( 40%)]  Loss:  3.543703 (3.6825)  Time: 1.096s,  934.03/s  (1.106s,  925.52/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 550/1251 ( 44%)]  Loss:  3.497523 (3.6671)  Time: 1.102s,  928.91/s  (1.107s,  924.77/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 600/1251 ( 48%)]  Loss:  3.515898 (3.6554)  Time: 1.097s,  933.06/s  (1.107s,  925.10/s)  LR: 4.586e-04  Data: 0.012 (0.012)
Train: 56 [ 650/1251 ( 52%)]  Loss:  3.889324 (3.6721)  Time: 1.193s,  858.65/s  (1.107s,  924.93/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 700/1251 ( 56%)]  Loss:  3.906374 (3.6878)  Time: 1.104s,  927.84/s  (1.107s,  925.13/s)  LR: 4.586e-04  Data: 0.010 (0.012)
Train: 56 [ 750/1251 ( 60%)]  Loss:  3.773106 (3.6931)  Time: 1.103s,  928.65/s  (1.107s,  925.34/s)  LR: 4.586e-04  Data: 0.014 (0.012)
Train: 56 [ 800/1251 ( 64%)]  Loss:  3.640769 (3.6900)  Time: 1.098s,  932.43/s  (1.107s,  925.33/s)  LR: 4.586e-04  Data: 0.012 (0.012)
Train: 56 [ 850/1251 ( 68%)]  Loss:  3.451869 (3.6768)  Time: 1.095s,  935.19/s  (1.106s,  925.62/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 900/1251 ( 72%)]  Loss:  3.530901 (3.6691)  Time: 1.097s,  933.17/s  (1.106s,  925.73/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [ 950/1251 ( 76%)]  Loss:  3.799266 (3.6756)  Time: 1.097s,  933.56/s  (1.106s,  926.03/s)  LR: 4.586e-04  Data: 0.013 (0.012)
Train: 56 [1000/1251 ( 80%)]  Loss:  3.711777 (3.6773)  Time: 1.097s,  933.73/s  (1.106s,  925.72/s)  LR: 4.586e-04  Data: 0.013 (0.012)
Train: 56 [1050/1251 ( 84%)]  Loss:  3.584197 (3.6731)  Time: 1.095s,  934.91/s  (1.106s,  925.85/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [1100/1251 ( 88%)]  Loss:  3.687172 (3.6737)  Time: 1.098s,  932.80/s  (1.106s,  925.76/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [1150/1251 ( 92%)]  Loss:  3.851310 (3.6811)  Time: 1.096s,  934.13/s  (1.106s,  926.03/s)  LR: 4.586e-04  Data: 0.010 (0.012)
Train: 56 [1200/1251 ( 96%)]  Loss:  3.388475 (3.6694)  Time: 1.097s,  933.23/s  (1.106s,  925.97/s)  LR: 4.586e-04  Data: 0.011 (0.012)
Train: 56 [1250/1251 (100%)]  Loss:  3.704201 (3.6707)  Time: 1.084s,  944.29/s  (1.106s,  926.07/s)  LR: 4.586e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.204 (3.204)  Loss:  0.6232 (0.6232)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.7012 (1.1608)  Acc@1: 84.5519 (73.3300)  Acc@5: 95.4009 (91.9340)
Test (EMA): [   0/48]  Time: 3.082 (3.082)  Loss:  0.5358 (0.5358)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.5586 (97.5586)
Test (EMA): [  48/48]  Time: 0.229 (0.412)  Loss:  0.6415 (1.0771)  Acc@1: 84.6698 (74.9960)  Acc@5: 95.8726 (92.4180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 72.78399998779297)

Train: 57 [   0/1251 (  0%)]  Loss:  3.636799 (3.6368)  Time: 1.103s,  928.09/s  (1.103s,  928.09/s)  LR: 4.572e-04  Data: 0.020 (0.020)
Train: 57 [  50/1251 (  4%)]  Loss:  3.258619 (3.4477)  Time: 1.096s,  934.66/s  (1.105s,  926.58/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [ 100/1251 (  8%)]  Loss:  3.303367 (3.3996)  Time: 1.097s,  933.55/s  (1.107s,  925.24/s)  LR: 4.572e-04  Data: 0.013 (0.012)
Train: 57 [ 150/1251 ( 12%)]  Loss:  3.364758 (3.3909)  Time: 1.132s,  904.35/s  (1.110s,  922.21/s)  LR: 4.572e-04  Data: 0.012 (0.012)
Train: 57 [ 200/1251 ( 16%)]  Loss:  3.711352 (3.4550)  Time: 1.096s,  934.18/s  (1.111s,  921.44/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [ 250/1251 ( 20%)]  Loss:  3.741739 (3.5028)  Time: 1.097s,  933.23/s  (1.110s,  922.59/s)  LR: 4.572e-04  Data: 0.011 (0.011)
Train: 57 [ 300/1251 ( 24%)]  Loss:  3.382634 (3.4856)  Time: 1.098s,  932.56/s  (1.110s,  922.29/s)  LR: 4.572e-04  Data: 0.012 (0.011)
Train: 57 [ 350/1251 ( 28%)]  Loss:  3.561979 (3.4952)  Time: 1.097s,  933.83/s  (1.109s,  923.44/s)  LR: 4.572e-04  Data: 0.013 (0.011)
Train: 57 [ 400/1251 ( 32%)]  Loss:  3.808138 (3.5299)  Time: 1.092s,  937.75/s  (1.109s,  923.62/s)  LR: 4.572e-04  Data: 0.010 (0.012)
Train: 57 [ 450/1251 ( 36%)]  Loss:  3.466288 (3.5236)  Time: 1.096s,  934.52/s  (1.108s,  924.29/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [ 500/1251 ( 40%)]  Loss:  3.643007 (3.5344)  Time: 1.123s,  912.20/s  (1.108s,  924.04/s)  LR: 4.572e-04  Data: 0.012 (0.012)
Train: 57 [ 550/1251 ( 44%)]  Loss:  3.457396 (3.5280)  Time: 1.098s,  932.37/s  (1.107s,  924.61/s)  LR: 4.572e-04  Data: 0.012 (0.012)
Train: 57 [ 600/1251 ( 48%)]  Loss:  3.740059 (3.5443)  Time: 1.191s,  860.04/s  (1.107s,  924.61/s)  LR: 4.572e-04  Data: 0.010 (0.012)
Train: 57 [ 650/1251 ( 52%)]  Loss:  3.943809 (3.5729)  Time: 1.098s,  932.70/s  (1.108s,  924.06/s)  LR: 4.572e-04  Data: 0.015 (0.012)
Train: 57 [ 700/1251 ( 56%)]  Loss:  3.805332 (3.5884)  Time: 1.120s,  914.30/s  (1.108s,  923.92/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [ 750/1251 ( 60%)]  Loss:  3.131102 (3.5598)  Time: 1.096s,  933.96/s  (1.108s,  923.85/s)  LR: 4.572e-04  Data: 0.012 (0.012)
Train: 57 [ 800/1251 ( 64%)]  Loss:  3.905713 (3.5801)  Time: 1.097s,  933.79/s  (1.109s,  923.54/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [ 850/1251 ( 68%)]  Loss:  3.489716 (3.5751)  Time: 1.094s,  936.08/s  (1.109s,  923.74/s)  LR: 4.572e-04  Data: 0.014 (0.012)
Train: 57 [ 900/1251 ( 72%)]  Loss:  3.747100 (3.5842)  Time: 1.109s,  923.71/s  (1.108s,  924.07/s)  LR: 4.572e-04  Data: 0.012 (0.012)
Train: 57 [ 950/1251 ( 76%)]  Loss:  3.536303 (3.5818)  Time: 1.122s,  912.97/s  (1.108s,  923.87/s)  LR: 4.572e-04  Data: 0.012 (0.012)
Train: 57 [1000/1251 ( 80%)]  Loss:  3.876618 (3.5958)  Time: 1.097s,  933.13/s  (1.108s,  924.02/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [1050/1251 ( 84%)]  Loss:  3.634875 (3.5976)  Time: 1.097s,  933.25/s  (1.108s,  924.17/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 57 [1100/1251 ( 88%)]  Loss:  3.987253 (3.6145)  Time: 1.131s,  905.09/s  (1.108s,  923.85/s)  LR: 4.572e-04  Data: 0.014 (0.012)
Train: 57 [1150/1251 ( 92%)]  Loss:  3.437269 (3.6071)  Time: 1.133s,  903.92/s  (1.109s,  923.66/s)  LR: 4.572e-04  Data: 0.011 (0.012)
Train: 57 [1200/1251 ( 96%)]  Loss:  3.716401 (3.6115)  Time: 1.098s,  932.94/s  (1.109s,  923.44/s)  LR: 4.572e-04  Data: 0.010 (0.012)
Train: 57 [1250/1251 (100%)]  Loss:  3.474272 (3.6062)  Time: 1.104s,  927.59/s  (1.109s,  923.36/s)  LR: 4.572e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.224 (3.224)  Loss:  0.6079 (0.6079)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6795 (1.1605)  Acc@1: 85.1415 (73.6580)  Acc@5: 97.2877 (92.0620)
Test (EMA): [   0/48]  Time: 3.261 (3.261)  Loss:  0.5320 (0.5320)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.6386 (1.0693)  Acc@1: 84.5519 (75.0740)  Acc@5: 95.8726 (92.5280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 73.0980000390625)

Train: 58 [   0/1251 (  0%)]  Loss:  3.859556 (3.8596)  Time: 1.108s,  924.07/s  (1.108s,  924.07/s)  LR: 4.557e-04  Data: 0.026 (0.026)
Train: 58 [  50/1251 (  4%)]  Loss:  3.181147 (3.5204)  Time: 1.097s,  933.32/s  (1.113s,  920.36/s)  LR: 4.557e-04  Data: 0.012 (0.012)
Train: 58 [ 100/1251 (  8%)]  Loss:  3.563905 (3.5349)  Time: 1.111s,  921.62/s  (1.109s,  923.10/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 150/1251 ( 12%)]  Loss:  3.809530 (3.6035)  Time: 1.122s,  912.51/s  (1.110s,  922.41/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 200/1251 ( 16%)]  Loss:  3.445203 (3.5719)  Time: 1.100s,  930.69/s  (1.108s,  924.29/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 250/1251 ( 20%)]  Loss:  3.409155 (3.5447)  Time: 1.099s,  931.37/s  (1.108s,  923.86/s)  LR: 4.557e-04  Data: 0.012 (0.012)
Train: 58 [ 300/1251 ( 24%)]  Loss:  3.746938 (3.5736)  Time: 1.096s,  934.14/s  (1.108s,  923.94/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 350/1251 ( 28%)]  Loss:  3.751329 (3.5958)  Time: 1.099s,  931.80/s  (1.109s,  923.37/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 400/1251 ( 32%)]  Loss:  3.931330 (3.6331)  Time: 1.123s,  911.71/s  (1.109s,  923.63/s)  LR: 4.557e-04  Data: 0.012 (0.012)
Train: 58 [ 450/1251 ( 36%)]  Loss:  3.633900 (3.6332)  Time: 1.100s,  931.12/s  (1.108s,  923.83/s)  LR: 4.557e-04  Data: 0.012 (0.012)
Train: 58 [ 500/1251 ( 40%)]  Loss:  3.614171 (3.6315)  Time: 1.125s,  910.32/s  (1.108s,  924.33/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 550/1251 ( 44%)]  Loss:  3.650730 (3.6331)  Time: 1.196s,  855.97/s  (1.109s,  923.69/s)  LR: 4.557e-04  Data: 0.012 (0.012)
Train: 58 [ 600/1251 ( 48%)]  Loss:  3.268524 (3.6050)  Time: 1.103s,  928.78/s  (1.109s,  923.68/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 650/1251 ( 52%)]  Loss:  3.561935 (3.6020)  Time: 1.100s,  930.63/s  (1.109s,  923.47/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 700/1251 ( 56%)]  Loss:  3.450761 (3.5919)  Time: 1.098s,  933.02/s  (1.109s,  923.68/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 750/1251 ( 60%)]  Loss:  3.824435 (3.6064)  Time: 1.131s,  905.51/s  (1.108s,  923.95/s)  LR: 4.557e-04  Data: 0.010 (0.012)
Train: 58 [ 800/1251 ( 64%)]  Loss:  3.852885 (3.6209)  Time: 1.100s,  930.70/s  (1.108s,  924.07/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 850/1251 ( 68%)]  Loss:  3.104718 (3.5922)  Time: 1.107s,  925.40/s  (1.108s,  924.44/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [ 900/1251 ( 72%)]  Loss:  3.599796 (3.5926)  Time: 1.102s,  929.35/s  (1.108s,  924.26/s)  LR: 4.557e-04  Data: 0.009 (0.012)
Train: 58 [ 950/1251 ( 76%)]  Loss:  3.558335 (3.5909)  Time: 1.097s,  933.18/s  (1.108s,  924.27/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [1000/1251 ( 80%)]  Loss:  3.655534 (3.5940)  Time: 1.103s,  928.70/s  (1.108s,  924.21/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [1050/1251 ( 84%)]  Loss:  3.481837 (3.5889)  Time: 1.121s,  913.38/s  (1.108s,  924.44/s)  LR: 4.557e-04  Data: 0.012 (0.012)
Train: 58 [1100/1251 ( 88%)]  Loss:  3.442952 (3.5825)  Time: 1.193s,  858.00/s  (1.108s,  924.43/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [1150/1251 ( 92%)]  Loss:  3.594261 (3.5830)  Time: 1.097s,  933.75/s  (1.107s,  924.67/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [1200/1251 ( 96%)]  Loss:  3.961565 (3.5982)  Time: 1.103s,  928.31/s  (1.107s,  924.69/s)  LR: 4.557e-04  Data: 0.011 (0.012)
Train: 58 [1250/1251 (100%)]  Loss:  3.528439 (3.5955)  Time: 1.105s,  926.58/s  (1.108s,  924.46/s)  LR: 4.557e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.299 (3.299)  Loss:  0.5447 (0.5447)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6744 (1.1324)  Acc@1: 84.4340 (73.7240)  Acc@5: 96.2264 (91.9860)
Test (EMA): [   0/48]  Time: 3.341 (3.341)  Loss:  0.5278 (0.5278)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.6350 (1.0621)  Acc@1: 84.4340 (75.2000)  Acc@5: 95.8726 (92.5780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 73.4059999609375)

Train: 59 [   0/1251 (  0%)]  Loss:  3.333599 (3.3336)  Time: 1.137s,  900.86/s  (1.137s,  900.86/s)  LR: 4.542e-04  Data: 0.030 (0.030)
Train: 59 [  50/1251 (  4%)]  Loss:  3.516561 (3.4251)  Time: 1.097s,  933.70/s  (1.109s,  922.96/s)  LR: 4.542e-04  Data: 0.014 (0.012)
Train: 59 [ 100/1251 (  8%)]  Loss:  3.480497 (3.4436)  Time: 1.095s,  935.35/s  (1.109s,  923.61/s)  LR: 4.542e-04  Data: 0.010 (0.012)
Train: 59 [ 150/1251 ( 12%)]  Loss:  3.389355 (3.4300)  Time: 1.119s,  915.20/s  (1.111s,  922.04/s)  LR: 4.542e-04  Data: 0.010 (0.012)
Train: 59 [ 200/1251 ( 16%)]  Loss:  3.805797 (3.5052)  Time: 1.098s,  932.85/s  (1.110s,  922.12/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [ 250/1251 ( 20%)]  Loss:  3.685865 (3.5353)  Time: 1.128s,  908.08/s  (1.111s,  921.89/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [ 300/1251 ( 24%)]  Loss:  3.443460 (3.5222)  Time: 1.096s,  934.71/s  (1.111s,  921.43/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [ 350/1251 ( 28%)]  Loss:  3.641074 (3.5370)  Time: 1.133s,  903.84/s  (1.112s,  920.85/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [ 400/1251 ( 32%)]  Loss:  3.759866 (3.5618)  Time: 1.103s,  928.04/s  (1.114s,  919.12/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [ 450/1251 ( 36%)]  Loss:  3.513207 (3.5569)  Time: 1.098s,  932.18/s  (1.113s,  920.23/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [ 500/1251 ( 40%)]  Loss:  3.723208 (3.5720)  Time: 1.097s,  933.54/s  (1.112s,  920.91/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [ 550/1251 ( 44%)]  Loss:  3.546860 (3.5699)  Time: 1.126s,  909.43/s  (1.111s,  921.64/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 59 [ 600/1251 ( 48%)]  Loss:  3.558796 (3.5691)  Time: 1.097s,  933.48/s  (1.111s,  921.93/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [ 650/1251 ( 52%)]  Loss:  3.818436 (3.5869)  Time: 1.116s,  917.91/s  (1.111s,  922.01/s)  LR: 4.542e-04  Data: 0.010 (0.012)
Train: 59 [ 700/1251 ( 56%)]  Loss:  3.196079 (3.5608)  Time: 1.189s,  861.30/s  (1.111s,  922.05/s)  LR: 4.542e-04  Data: 0.013 (0.012)
Train: 59 [ 750/1251 ( 60%)]  Loss:  3.580086 (3.5620)  Time: 1.121s,  913.66/s  (1.110s,  922.17/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [ 800/1251 ( 64%)]  Loss:  3.782354 (3.5750)  Time: 1.212s,  844.84/s  (1.111s,  922.10/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [ 850/1251 ( 68%)]  Loss:  3.787098 (3.5868)  Time: 1.098s,  932.44/s  (1.110s,  922.45/s)  LR: 4.542e-04  Data: 0.010 (0.012)
Train: 59 [ 900/1251 ( 72%)]  Loss:  3.653384 (3.5903)  Time: 1.095s,  934.75/s  (1.110s,  922.44/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [ 950/1251 ( 76%)]  Loss:  3.364451 (3.5790)  Time: 1.098s,  932.42/s  (1.110s,  922.61/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [1000/1251 ( 80%)]  Loss:  3.295873 (3.5655)  Time: 1.097s,  933.66/s  (1.110s,  922.73/s)  LR: 4.542e-04  Data: 0.012 (0.012)
Train: 59 [1050/1251 ( 84%)]  Loss:  3.464084 (3.5609)  Time: 1.123s,  912.01/s  (1.110s,  922.73/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [1100/1251 ( 88%)]  Loss:  3.806497 (3.5716)  Time: 1.095s,  935.57/s  (1.109s,  922.98/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [1150/1251 ( 92%)]  Loss:  3.813514 (3.5817)  Time: 1.097s,  933.43/s  (1.109s,  923.01/s)  LR: 4.542e-04  Data: 0.013 (0.012)
Train: 59 [1200/1251 ( 96%)]  Loss:  3.684165 (3.5858)  Time: 1.134s,  902.78/s  (1.110s,  922.63/s)  LR: 4.542e-04  Data: 0.011 (0.012)
Train: 59 [1250/1251 (100%)]  Loss:  3.593606 (3.5861)  Time: 1.104s,  927.33/s  (1.111s,  921.90/s)  LR: 4.542e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.303 (3.303)  Loss:  0.6358 (0.6358)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.8389 (1.1929)  Acc@1: 82.5472 (73.9420)  Acc@5: 95.5189 (92.2520)
Test (EMA): [   0/48]  Time: 3.117 (3.117)  Loss:  0.5215 (0.5215)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.3633 (97.3633)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.6326 (1.0552)  Acc@1: 84.6698 (75.3920)  Acc@5: 95.8726 (92.6460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 73.70199990966798)

Train: 60 [   0/1251 (  0%)]  Loss:  3.243541 (3.2435)  Time: 1.101s,  929.72/s  (1.101s,  929.72/s)  LR: 4.527e-04  Data: 0.020 (0.020)
Train: 60 [  50/1251 (  4%)]  Loss:  3.367947 (3.3057)  Time: 1.222s,  838.16/s  (1.113s,  920.11/s)  LR: 4.527e-04  Data: 0.010 (0.012)
Train: 60 [ 100/1251 (  8%)]  Loss:  3.813278 (3.4749)  Time: 1.127s,  908.73/s  (1.110s,  922.27/s)  LR: 4.527e-04  Data: 0.011 (0.011)
Train: 60 [ 150/1251 ( 12%)]  Loss:  3.614087 (3.5097)  Time: 1.096s,  933.99/s  (1.109s,  923.20/s)  LR: 4.527e-04  Data: 0.011 (0.011)
Train: 60 [ 200/1251 ( 16%)]  Loss:  3.867878 (3.5813)  Time: 1.100s,  930.52/s  (1.108s,  924.41/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 250/1251 ( 20%)]  Loss:  3.694352 (3.6002)  Time: 1.101s,  929.88/s  (1.109s,  923.38/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 300/1251 ( 24%)]  Loss:  3.056612 (3.5225)  Time: 1.099s,  932.00/s  (1.108s,  923.98/s)  LR: 4.527e-04  Data: 0.015 (0.012)
Train: 60 [ 350/1251 ( 28%)]  Loss:  3.388950 (3.5058)  Time: 1.116s,  917.80/s  (1.109s,  923.64/s)  LR: 4.527e-04  Data: 0.013 (0.012)
Train: 60 [ 400/1251 ( 32%)]  Loss:  4.017006 (3.5626)  Time: 1.097s,  933.66/s  (1.108s,  924.35/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 450/1251 ( 36%)]  Loss:  3.695390 (3.5759)  Time: 1.131s,  905.27/s  (1.109s,  923.54/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 500/1251 ( 40%)]  Loss:  3.459605 (3.5653)  Time: 1.103s,  928.07/s  (1.109s,  923.17/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [ 550/1251 ( 44%)]  Loss:  3.451719 (3.5559)  Time: 1.102s,  929.04/s  (1.110s,  922.72/s)  LR: 4.527e-04  Data: 0.010 (0.012)
Train: 60 [ 600/1251 ( 48%)]  Loss:  3.678591 (3.5653)  Time: 1.097s,  933.52/s  (1.109s,  923.03/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 650/1251 ( 52%)]  Loss:  3.286640 (3.5454)  Time: 1.099s,  931.76/s  (1.109s,  923.32/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 700/1251 ( 56%)]  Loss:  3.513948 (3.5433)  Time: 1.104s,  927.63/s  (1.109s,  923.09/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 750/1251 ( 60%)]  Loss:  3.608817 (3.5474)  Time: 1.098s,  932.26/s  (1.109s,  923.58/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [ 800/1251 ( 64%)]  Loss:  3.634038 (3.5525)  Time: 1.097s,  933.20/s  (1.109s,  923.37/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 850/1251 ( 68%)]  Loss:  3.543724 (3.5520)  Time: 1.129s,  906.71/s  (1.109s,  923.13/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [ 900/1251 ( 72%)]  Loss:  3.442896 (3.5463)  Time: 1.098s,  932.42/s  (1.110s,  922.92/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [ 950/1251 ( 76%)]  Loss:  3.064539 (3.5222)  Time: 1.098s,  932.45/s  (1.109s,  923.23/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [1000/1251 ( 80%)]  Loss:  3.924854 (3.5414)  Time: 1.123s,  912.20/s  (1.110s,  922.68/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [1050/1251 ( 84%)]  Loss:  3.367725 (3.5335)  Time: 1.098s,  932.71/s  (1.110s,  922.93/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [1100/1251 ( 88%)]  Loss:  3.612559 (3.5369)  Time: 1.097s,  933.77/s  (1.109s,  923.28/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [1150/1251 ( 92%)]  Loss:  3.665852 (3.5423)  Time: 1.094s,  935.63/s  (1.109s,  923.11/s)  LR: 4.527e-04  Data: 0.011 (0.012)
Train: 60 [1200/1251 ( 96%)]  Loss:  3.797435 (3.5525)  Time: 1.100s,  931.04/s  (1.109s,  923.31/s)  LR: 4.527e-04  Data: 0.012 (0.012)
Train: 60 [1250/1251 (100%)]  Loss:  3.403500 (3.5467)  Time: 1.088s,  941.09/s  (1.109s,  923.04/s)  LR: 4.527e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.293 (3.293)  Loss:  0.6204 (0.6204)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.7330 (1.1694)  Acc@1: 84.4340 (73.7440)  Acc@5: 95.9906 (92.0700)
Test (EMA): [   0/48]  Time: 3.148 (3.148)  Loss:  0.5170 (0.5170)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.4609 (97.4609)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.6292 (1.0485)  Acc@1: 84.7877 (75.5160)  Acc@5: 95.8726 (92.7480)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 73.97399998535157)

Train: 61 [   0/1251 (  0%)]  Loss:  3.512636 (3.5126)  Time: 1.105s,  926.41/s  (1.105s,  926.41/s)  LR: 4.512e-04  Data: 0.022 (0.022)
Train: 61 [  50/1251 (  4%)]  Loss:  3.811313 (3.6620)  Time: 1.200s,  853.58/s  (1.109s,  923.59/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 61 [ 100/1251 (  8%)]  Loss:  3.607388 (3.6438)  Time: 1.097s,  933.45/s  (1.107s,  924.87/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 150/1251 ( 12%)]  Loss:  3.405553 (3.5842)  Time: 1.106s,  926.04/s  (1.105s,  926.48/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 200/1251 ( 16%)]  Loss:  3.750085 (3.6174)  Time: 1.113s,  920.15/s  (1.107s,  924.76/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 250/1251 ( 20%)]  Loss:  3.674896 (3.6270)  Time: 1.103s,  928.75/s  (1.106s,  925.46/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 300/1251 ( 24%)]  Loss:  3.650278 (3.6303)  Time: 1.097s,  933.83/s  (1.107s,  924.84/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 350/1251 ( 28%)]  Loss:  3.550945 (3.6204)  Time: 1.096s,  934.17/s  (1.107s,  925.11/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 400/1251 ( 32%)]  Loss:  3.627662 (3.6212)  Time: 1.097s,  933.33/s  (1.107s,  925.26/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Train: 61 [ 450/1251 ( 36%)]  Loss:  3.517693 (3.6108)  Time: 1.101s,  930.07/s  (1.106s,  925.62/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Train: 61 [ 500/1251 ( 40%)]  Loss:  3.764338 (3.6248)  Time: 1.101s,  930.07/s  (1.106s,  925.79/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 550/1251 ( 44%)]  Loss:  3.234387 (3.5923)  Time: 1.098s,  933.00/s  (1.106s,  925.82/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 600/1251 ( 48%)]  Loss:  3.769532 (3.6059)  Time: 1.100s,  931.19/s  (1.106s,  925.88/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Train: 61 [ 650/1251 ( 52%)]  Loss:  3.564163 (3.6029)  Time: 1.101s,  930.37/s  (1.106s,  925.78/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Train: 61 [ 700/1251 ( 56%)]  Loss:  3.701919 (3.6095)  Time: 1.097s,  933.87/s  (1.106s,  926.06/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [ 750/1251 ( 60%)]  Loss:  3.415390 (3.5974)  Time: 1.133s,  903.84/s  (1.106s,  925.83/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 61 [ 800/1251 ( 64%)]  Loss:  3.909852 (3.6158)  Time: 1.130s,  906.00/s  (1.107s,  925.18/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Train: 61 [ 850/1251 ( 68%)]  Loss:  3.655690 (3.6180)  Time: 1.098s,  932.34/s  (1.107s,  924.73/s)  LR: 4.512e-04  Data: 0.015 (0.012)
Train: 61 [ 900/1251 ( 72%)]  Loss:  3.439212 (3.6086)  Time: 1.097s,  933.32/s  (1.107s,  924.74/s)  LR: 4.512e-04  Data: 0.010 (0.012)
Train: 61 [ 950/1251 ( 76%)]  Loss:  3.836091 (3.6200)  Time: 1.191s,  859.84/s  (1.108s,  924.48/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [1000/1251 ( 80%)]  Loss:  3.363393 (3.6077)  Time: 1.135s,  902.11/s  (1.107s,  924.67/s)  LR: 4.512e-04  Data: 0.012 (0.012)
Train: 61 [1050/1251 ( 84%)]  Loss:  3.372969 (3.5971)  Time: 1.124s,  910.72/s  (1.108s,  924.25/s)  LR: 4.512e-04  Data: 0.010 (0.012)
Train: 61 [1100/1251 ( 88%)]  Loss:  3.788503 (3.6054)  Time: 1.101s,  930.12/s  (1.108s,  924.25/s)  LR: 4.512e-04  Data: 0.010 (0.012)
Train: 61 [1150/1251 ( 92%)]  Loss:  3.806127 (3.6138)  Time: 1.093s,  937.23/s  (1.108s,  924.58/s)  LR: 4.512e-04  Data: 0.010 (0.012)
Train: 61 [1200/1251 ( 96%)]  Loss:  3.809925 (3.6216)  Time: 1.129s,  906.67/s  (1.108s,  924.29/s)  LR: 4.512e-04  Data: 0.011 (0.012)
Train: 61 [1250/1251 (100%)]  Loss:  3.857800 (3.6307)  Time: 1.083s,  945.27/s  (1.108s,  924.38/s)  LR: 4.512e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.267 (3.267)  Loss:  0.6250 (0.6250)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.230 (0.397)  Loss:  0.6824 (1.1567)  Acc@1: 85.1415 (74.0160)  Acc@5: 96.8160 (92.2060)
Test (EMA): [   0/48]  Time: 3.275 (3.275)  Loss:  0.5132 (0.5132)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.5586 (97.5586)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.6258 (1.0421)  Acc@1: 85.2594 (75.6380)  Acc@5: 95.9906 (92.8280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 74.18599998535156)

Train: 62 [   0/1251 (  0%)]  Loss:  3.736450 (3.7364)  Time: 1.102s,  928.86/s  (1.102s,  928.86/s)  LR: 4.496e-04  Data: 0.023 (0.023)
Train: 62 [  50/1251 (  4%)]  Loss:  3.886626 (3.8115)  Time: 1.100s,  931.24/s  (1.109s,  923.72/s)  LR: 4.496e-04  Data: 0.011 (0.011)
Train: 62 [ 100/1251 (  8%)]  Loss:  3.691291 (3.7715)  Time: 1.098s,  932.42/s  (1.111s,  921.49/s)  LR: 4.496e-04  Data: 0.011 (0.011)
Train: 62 [ 150/1251 ( 12%)]  Loss:  3.102119 (3.6041)  Time: 1.096s,  934.02/s  (1.111s,  921.95/s)  LR: 4.496e-04  Data: 0.012 (0.011)
Train: 62 [ 200/1251 ( 16%)]  Loss:  3.587008 (3.6007)  Time: 1.090s,  939.20/s  (1.108s,  923.90/s)  LR: 4.496e-04  Data: 0.010 (0.012)
Train: 62 [ 250/1251 ( 20%)]  Loss:  3.535107 (3.5898)  Time: 1.098s,  932.26/s  (1.109s,  923.47/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 300/1251 ( 24%)]  Loss:  3.996701 (3.6479)  Time: 1.103s,  927.98/s  (1.109s,  923.15/s)  LR: 4.496e-04  Data: 0.020 (0.012)
Train: 62 [ 350/1251 ( 28%)]  Loss:  3.375305 (3.6138)  Time: 1.095s,  934.98/s  (1.109s,  923.26/s)  LR: 4.496e-04  Data: 0.010 (0.012)
Train: 62 [ 400/1251 ( 32%)]  Loss:  3.619334 (3.6144)  Time: 1.099s,  931.61/s  (1.109s,  923.40/s)  LR: 4.496e-04  Data: 0.010 (0.012)
Train: 62 [ 450/1251 ( 36%)]  Loss:  3.788379 (3.6318)  Time: 1.098s,  932.63/s  (1.108s,  924.12/s)  LR: 4.496e-04  Data: 0.012 (0.012)
Train: 62 [ 500/1251 ( 40%)]  Loss:  3.612741 (3.6301)  Time: 1.100s,  930.89/s  (1.108s,  924.37/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 550/1251 ( 44%)]  Loss:  3.260462 (3.5993)  Time: 1.097s,  933.27/s  (1.107s,  924.91/s)  LR: 4.496e-04  Data: 0.012 (0.012)
Train: 62 [ 600/1251 ( 48%)]  Loss:  3.634352 (3.6020)  Time: 1.096s,  934.63/s  (1.107s,  924.86/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 650/1251 ( 52%)]  Loss:  3.585643 (3.6008)  Time: 1.097s,  933.59/s  (1.107s,  925.21/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 700/1251 ( 56%)]  Loss:  3.387456 (3.5866)  Time: 1.100s,  930.66/s  (1.107s,  924.78/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 750/1251 ( 60%)]  Loss:  3.320952 (3.5700)  Time: 1.100s,  930.56/s  (1.107s,  925.14/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 800/1251 ( 64%)]  Loss:  3.697628 (3.5775)  Time: 1.100s,  931.09/s  (1.107s,  924.69/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 850/1251 ( 68%)]  Loss:  3.301597 (3.5622)  Time: 1.098s,  932.24/s  (1.107s,  924.96/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 900/1251 ( 72%)]  Loss:  3.704126 (3.5696)  Time: 1.179s,  868.45/s  (1.107s,  924.76/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [ 950/1251 ( 76%)]  Loss:  3.457744 (3.5641)  Time: 1.104s,  927.90/s  (1.107s,  924.88/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [1000/1251 ( 80%)]  Loss:  3.494635 (3.5607)  Time: 1.095s,  934.80/s  (1.107s,  925.02/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [1050/1251 ( 84%)]  Loss:  3.596626 (3.5624)  Time: 1.098s,  932.32/s  (1.107s,  925.10/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [1100/1251 ( 88%)]  Loss:  3.604158 (3.5642)  Time: 1.097s,  933.51/s  (1.107s,  925.43/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [1150/1251 ( 92%)]  Loss:  3.809134 (3.5744)  Time: 1.097s,  933.39/s  (1.106s,  925.50/s)  LR: 4.496e-04  Data: 0.012 (0.012)
Train: 62 [1200/1251 ( 96%)]  Loss:  3.697650 (3.5793)  Time: 1.098s,  932.91/s  (1.106s,  925.72/s)  LR: 4.496e-04  Data: 0.011 (0.012)
Train: 62 [1250/1251 (100%)]  Loss:  3.529256 (3.5774)  Time: 1.083s,  945.44/s  (1.107s,  925.26/s)  LR: 4.496e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.237 (3.237)  Loss:  0.6188 (0.6188)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6682 (1.1490)  Acc@1: 84.7877 (73.8180)  Acc@5: 96.4623 (92.1040)
Test (EMA): [   0/48]  Time: 3.208 (3.208)  Loss:  0.5099 (0.5099)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.6562 (97.6562)
Test (EMA): [  48/48]  Time: 0.229 (0.414)  Loss:  0.6224 (1.0367)  Acc@1: 85.3774 (75.7500)  Acc@5: 96.1085 (92.9140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 74.36599998535156)

Train: 63 [   0/1251 (  0%)]  Loss:  3.327263 (3.3273)  Time: 1.105s,  926.33/s  (1.105s,  926.33/s)  LR: 4.481e-04  Data: 0.023 (0.023)
Train: 63 [  50/1251 (  4%)]  Loss:  3.634683 (3.4810)  Time: 1.105s,  926.91/s  (1.101s,  930.15/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 100/1251 (  8%)]  Loss:  3.516571 (3.4928)  Time: 1.097s,  933.52/s  (1.106s,  925.45/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 150/1251 ( 12%)]  Loss:  3.794694 (3.5683)  Time: 1.097s,  933.58/s  (1.106s,  926.15/s)  LR: 4.481e-04  Data: 0.013 (0.012)
Train: 63 [ 200/1251 ( 16%)]  Loss:  3.752774 (3.6052)  Time: 1.098s,  932.71/s  (1.106s,  926.27/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 63 [ 250/1251 ( 20%)]  Loss:  3.524060 (3.5917)  Time: 1.104s,  927.29/s  (1.105s,  926.98/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 300/1251 ( 24%)]  Loss:  3.502299 (3.5789)  Time: 1.133s,  903.97/s  (1.107s,  925.32/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 350/1251 ( 28%)]  Loss:  3.437674 (3.5613)  Time: 1.100s,  930.62/s  (1.107s,  924.88/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 400/1251 ( 32%)]  Loss:  3.764673 (3.5839)  Time: 1.104s,  927.95/s  (1.107s,  925.11/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 450/1251 ( 36%)]  Loss:  3.501276 (3.5756)  Time: 1.105s,  926.43/s  (1.107s,  924.88/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 500/1251 ( 40%)]  Loss:  3.760218 (3.5924)  Time: 1.121s,  913.16/s  (1.107s,  924.66/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 63 [ 550/1251 ( 44%)]  Loss:  3.292577 (3.5674)  Time: 1.103s,  928.22/s  (1.107s,  924.71/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 600/1251 ( 48%)]  Loss:  3.486017 (3.5611)  Time: 1.098s,  932.75/s  (1.107s,  924.76/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 650/1251 ( 52%)]  Loss:  3.672406 (3.5691)  Time: 1.097s,  933.75/s  (1.107s,  924.63/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 700/1251 ( 56%)]  Loss:  3.241072 (3.5472)  Time: 1.101s,  930.23/s  (1.107s,  925.07/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 750/1251 ( 60%)]  Loss:  4.033054 (3.5776)  Time: 1.096s,  934.19/s  (1.107s,  924.82/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 800/1251 ( 64%)]  Loss:  3.638497 (3.5812)  Time: 1.101s,  929.84/s  (1.107s,  924.84/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [ 850/1251 ( 68%)]  Loss:  3.681602 (3.5867)  Time: 1.187s,  862.47/s  (1.107s,  924.90/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [ 900/1251 ( 72%)]  Loss:  3.811393 (3.5986)  Time: 1.092s,  937.98/s  (1.107s,  925.23/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 63 [ 950/1251 ( 76%)]  Loss:  3.663297 (3.6018)  Time: 1.119s,  914.96/s  (1.107s,  925.21/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 63 [1000/1251 ( 80%)]  Loss:  3.483043 (3.5961)  Time: 1.102s,  929.51/s  (1.107s,  925.04/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [1050/1251 ( 84%)]  Loss:  3.475467 (3.5907)  Time: 1.096s,  933.98/s  (1.107s,  925.20/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [1100/1251 ( 88%)]  Loss:  3.859686 (3.6024)  Time: 1.121s,  913.17/s  (1.107s,  924.92/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 63 [1150/1251 ( 92%)]  Loss:  3.737481 (3.6080)  Time: 1.096s,  933.89/s  (1.107s,  925.03/s)  LR: 4.481e-04  Data: 0.010 (0.012)
Train: 63 [1200/1251 ( 96%)]  Loss:  3.374739 (3.5987)  Time: 1.099s,  931.99/s  (1.107s,  925.08/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 63 [1250/1251 (100%)]  Loss:  3.743978 (3.6042)  Time: 1.079s,  948.74/s  (1.107s,  925.08/s)  LR: 4.481e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.272 (3.272)  Loss:  0.5538 (0.5538)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.7069 (1.1390)  Acc@1: 83.4906 (73.9100)  Acc@5: 96.6981 (92.1320)
Test (EMA): [   0/48]  Time: 3.068 (3.068)  Loss:  0.5056 (0.5056)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.6198 (1.0310)  Acc@1: 85.4953 (75.8600)  Acc@5: 96.1085 (92.9900)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 74.60200001220703)

Train: 64 [   0/1251 (  0%)]  Loss:  3.526358 (3.5264)  Time: 1.111s,  922.10/s  (1.111s,  922.10/s)  LR: 4.465e-04  Data: 0.028 (0.028)
Train: 64 [  50/1251 (  4%)]  Loss:  3.478106 (3.5022)  Time: 1.098s,  932.95/s  (1.110s,  922.29/s)  LR: 4.465e-04  Data: 0.010 (0.012)
Train: 64 [ 100/1251 (  8%)]  Loss:  3.592059 (3.5322)  Time: 1.100s,  931.20/s  (1.106s,  925.84/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 150/1251 ( 12%)]  Loss:  3.374938 (3.4929)  Time: 1.098s,  932.68/s  (1.107s,  924.92/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 200/1251 ( 16%)]  Loss:  3.781467 (3.5506)  Time: 1.103s,  928.00/s  (1.108s,  923.99/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 250/1251 ( 20%)]  Loss:  3.503881 (3.5428)  Time: 1.098s,  932.54/s  (1.108s,  923.88/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 300/1251 ( 24%)]  Loss:  3.685833 (3.5632)  Time: 1.101s,  929.84/s  (1.108s,  924.31/s)  LR: 4.465e-04  Data: 0.011 (0.011)
Train: 64 [ 350/1251 ( 28%)]  Loss:  4.025130 (3.6210)  Time: 1.100s,  930.59/s  (1.107s,  925.07/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 400/1251 ( 32%)]  Loss:  3.834432 (3.6447)  Time: 1.134s,  902.63/s  (1.107s,  924.61/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 450/1251 ( 36%)]  Loss:  3.734552 (3.6537)  Time: 1.100s,  930.95/s  (1.108s,  924.53/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 500/1251 ( 40%)]  Loss:  3.307394 (3.6222)  Time: 1.097s,  933.43/s  (1.109s,  923.75/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 550/1251 ( 44%)]  Loss:  3.570055 (3.6179)  Time: 1.096s,  934.56/s  (1.108s,  924.39/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 600/1251 ( 48%)]  Loss:  3.536441 (3.6116)  Time: 1.097s,  933.35/s  (1.108s,  924.26/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 650/1251 ( 52%)]  Loss:  3.941180 (3.6351)  Time: 1.095s,  934.94/s  (1.108s,  924.55/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 700/1251 ( 56%)]  Loss:  3.364559 (3.6171)  Time: 1.096s,  934.30/s  (1.108s,  924.58/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 750/1251 ( 60%)]  Loss:  3.632494 (3.6181)  Time: 1.098s,  932.71/s  (1.108s,  924.58/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [ 800/1251 ( 64%)]  Loss:  3.587977 (3.6163)  Time: 1.191s,  859.94/s  (1.108s,  924.22/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 850/1251 ( 68%)]  Loss:  3.395790 (3.6040)  Time: 1.100s,  931.10/s  (1.108s,  924.46/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 900/1251 ( 72%)]  Loss:  3.393991 (3.5930)  Time: 1.123s,  911.99/s  (1.108s,  923.89/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [ 950/1251 ( 76%)]  Loss:  3.369992 (3.5818)  Time: 1.097s,  933.65/s  (1.108s,  923.98/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [1000/1251 ( 80%)]  Loss:  3.400128 (3.5732)  Time: 1.135s,  901.82/s  (1.109s,  923.72/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 64 [1050/1251 ( 84%)]  Loss:  3.798878 (3.5834)  Time: 1.099s,  931.39/s  (1.108s,  923.84/s)  LR: 4.465e-04  Data: 0.011 (0.012)
Train: 64 [1100/1251 ( 88%)]  Loss:  3.453454 (3.5778)  Time: 1.104s,  927.69/s  (1.108s,  923.95/s)  LR: 4.465e-04  Data: 0.010 (0.012)
Train: 64 [1150/1251 ( 92%)]  Loss:  3.521243 (3.5754)  Time: 1.100s,  930.70/s  (1.108s,  923.86/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [1200/1251 ( 96%)]  Loss:  3.662237 (3.5789)  Time: 1.098s,  932.66/s  (1.108s,  924.06/s)  LR: 4.465e-04  Data: 0.012 (0.012)
Train: 64 [1250/1251 (100%)]  Loss:  3.682597 (3.5829)  Time: 1.080s,  948.04/s  (1.108s,  923.86/s)  LR: 4.465e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.322 (3.322)  Loss:  0.6223 (0.6223)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6692 (1.1291)  Acc@1: 85.3774 (74.1680)  Acc@5: 96.3443 (92.3100)
Test (EMA): [   0/48]  Time: 3.099 (3.099)  Loss:  0.5018 (0.5018)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.6184 (1.0256)  Acc@1: 85.4953 (75.9400)  Acc@5: 96.2264 (93.0620)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 74.76799998535157)

Train: 65 [   0/1251 (  0%)]  Loss:  3.691525 (3.6915)  Time: 1.132s,  904.96/s  (1.132s,  904.96/s)  LR: 4.448e-04  Data: 0.025 (0.025)
Train: 65 [  50/1251 (  4%)]  Loss:  3.428711 (3.5601)  Time: 1.099s,  931.87/s  (1.101s,  929.64/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 100/1251 (  8%)]  Loss:  3.158204 (3.4261)  Time: 1.135s,  902.46/s  (1.108s,  924.05/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 150/1251 ( 12%)]  Loss:  3.321036 (3.3999)  Time: 1.122s,  912.53/s  (1.109s,  923.55/s)  LR: 4.448e-04  Data: 0.016 (0.012)
Train: 65 [ 200/1251 ( 16%)]  Loss:  3.655249 (3.4509)  Time: 1.097s,  933.39/s  (1.109s,  923.28/s)  LR: 4.448e-04  Data: 0.010 (0.012)
Train: 65 [ 250/1251 ( 20%)]  Loss:  3.091365 (3.3910)  Time: 1.097s,  933.63/s  (1.108s,  923.86/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [ 300/1251 ( 24%)]  Loss:  3.558961 (3.4150)  Time: 1.096s,  934.09/s  (1.108s,  924.43/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 350/1251 ( 28%)]  Loss:  3.522936 (3.4285)  Time: 1.098s,  932.39/s  (1.108s,  924.37/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 400/1251 ( 32%)]  Loss:  3.104985 (3.3926)  Time: 1.100s,  931.21/s  (1.107s,  924.67/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [ 450/1251 ( 36%)]  Loss:  3.768650 (3.4302)  Time: 1.095s,  935.28/s  (1.108s,  924.26/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [ 500/1251 ( 40%)]  Loss:  3.778745 (3.4619)  Time: 1.098s,  932.72/s  (1.107s,  924.83/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 550/1251 ( 44%)]  Loss:  3.379217 (3.4550)  Time: 1.101s,  930.16/s  (1.108s,  923.89/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 600/1251 ( 48%)]  Loss:  3.552677 (3.4625)  Time: 1.099s,  931.37/s  (1.108s,  924.28/s)  LR: 4.448e-04  Data: 0.013 (0.012)
Train: 65 [ 650/1251 ( 52%)]  Loss:  3.710109 (3.4802)  Time: 1.103s,  928.13/s  (1.108s,  923.95/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [ 700/1251 ( 56%)]  Loss:  3.708248 (3.4954)  Time: 1.097s,  933.34/s  (1.108s,  923.91/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 750/1251 ( 60%)]  Loss:  3.577641 (3.5005)  Time: 1.106s,  925.48/s  (1.109s,  923.38/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [ 800/1251 ( 64%)]  Loss:  3.933199 (3.5260)  Time: 1.095s,  935.33/s  (1.109s,  923.16/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [ 850/1251 ( 68%)]  Loss:  3.788618 (3.5406)  Time: 1.098s,  933.01/s  (1.109s,  923.46/s)  LR: 4.448e-04  Data: 0.014 (0.012)
Train: 65 [ 900/1251 ( 72%)]  Loss:  3.949110 (3.5621)  Time: 1.096s,  933.97/s  (1.109s,  923.66/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [ 950/1251 ( 76%)]  Loss:  3.557833 (3.5619)  Time: 1.131s,  905.42/s  (1.109s,  923.49/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [1000/1251 ( 80%)]  Loss:  3.803046 (3.5733)  Time: 1.097s,  933.46/s  (1.109s,  923.07/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [1050/1251 ( 84%)]  Loss:  3.669844 (3.5777)  Time: 1.096s,  934.61/s  (1.109s,  923.45/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [1100/1251 ( 88%)]  Loss:  3.507240 (3.5747)  Time: 1.101s,  929.91/s  (1.109s,  923.40/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [1150/1251 ( 92%)]  Loss:  3.717673 (3.5806)  Time: 1.102s,  929.28/s  (1.109s,  923.69/s)  LR: 4.448e-04  Data: 0.012 (0.012)
Train: 65 [1200/1251 ( 96%)]  Loss:  3.603608 (3.5815)  Time: 1.098s,  932.52/s  (1.109s,  923.57/s)  LR: 4.448e-04  Data: 0.011 (0.012)
Train: 65 [1250/1251 (100%)]  Loss:  3.246619 (3.5687)  Time: 1.080s,  948.47/s  (1.108s,  923.77/s)  LR: 4.448e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.245 (3.245)  Loss:  0.6278 (0.6278)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.6820 (1.1331)  Acc@1: 84.9057 (74.1400)  Acc@5: 97.1698 (92.3800)
Test (EMA): [   0/48]  Time: 3.197 (3.197)  Loss:  0.4986 (0.4986)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.6165 (1.0203)  Acc@1: 85.3774 (76.0360)  Acc@5: 96.3443 (93.0860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 74.99600006347656)

Train: 66 [   0/1251 (  0%)]  Loss:  3.487566 (3.4876)  Time: 1.103s,  928.22/s  (1.103s,  928.22/s)  LR: 4.432e-04  Data: 0.020 (0.020)
Train: 66 [  50/1251 (  4%)]  Loss:  3.775019 (3.6313)  Time: 1.198s,  854.66/s  (1.113s,  920.16/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Train: 66 [ 100/1251 (  8%)]  Loss:  3.522996 (3.5952)  Time: 1.102s,  929.60/s  (1.113s,  920.22/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Train: 66 [ 150/1251 ( 12%)]  Loss:  3.617587 (3.6008)  Time: 1.123s,  912.12/s  (1.111s,  921.77/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 66 [ 200/1251 ( 16%)]  Loss:  3.898132 (3.6603)  Time: 1.098s,  932.52/s  (1.108s,  924.07/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [ 250/1251 ( 20%)]  Loss:  3.591267 (3.6488)  Time: 1.111s,  921.33/s  (1.107s,  924.75/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Train: 66 [ 300/1251 ( 24%)]  Loss:  3.662291 (3.6507)  Time: 1.096s,  934.68/s  (1.111s,  921.76/s)  LR: 4.432e-04  Data: 0.013 (0.012)
Train: 66 [ 350/1251 ( 28%)]  Loss:  3.668791 (3.6530)  Time: 1.099s,  931.68/s  (1.110s,  922.32/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Train: 66 [ 400/1251 ( 32%)]  Loss:  3.766647 (3.6656)  Time: 1.098s,  932.21/s  (1.110s,  922.14/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [ 450/1251 ( 36%)]  Loss:  3.606376 (3.6597)  Time: 1.101s,  930.13/s  (1.110s,  922.13/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [ 500/1251 ( 40%)]  Loss:  3.530685 (3.6479)  Time: 1.095s,  935.49/s  (1.110s,  922.19/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [ 550/1251 ( 44%)]  Loss:  3.609765 (3.6448)  Time: 1.097s,  933.33/s  (1.110s,  922.80/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [ 600/1251 ( 48%)]  Loss:  3.562598 (3.6384)  Time: 1.100s,  930.49/s  (1.110s,  922.50/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [ 650/1251 ( 52%)]  Loss:  3.505107 (3.6289)  Time: 1.097s,  933.50/s  (1.110s,  922.75/s)  LR: 4.432e-04  Data: 0.013 (0.012)
Train: 66 [ 700/1251 ( 56%)]  Loss:  3.664867 (3.6313)  Time: 1.125s,  910.21/s  (1.110s,  922.22/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Train: 66 [ 750/1251 ( 60%)]  Loss:  3.660387 (3.6331)  Time: 1.098s,  932.88/s  (1.110s,  922.27/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [ 800/1251 ( 64%)]  Loss:  3.230516 (3.6094)  Time: 1.105s,  926.62/s  (1.110s,  922.71/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [ 850/1251 ( 68%)]  Loss:  3.354092 (3.5953)  Time: 1.103s,  928.18/s  (1.109s,  922.97/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [ 900/1251 ( 72%)]  Loss:  3.049100 (3.5665)  Time: 1.116s,  917.42/s  (1.109s,  923.24/s)  LR: 4.432e-04  Data: 0.010 (0.012)
Train: 66 [ 950/1251 ( 76%)]  Loss:  3.404759 (3.5584)  Time: 1.104s,  927.31/s  (1.109s,  923.25/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [1000/1251 ( 80%)]  Loss:  3.504507 (3.5559)  Time: 1.098s,  932.41/s  (1.109s,  923.68/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [1050/1251 ( 84%)]  Loss:  4.013163 (3.5766)  Time: 1.108s,  924.58/s  (1.109s,  923.65/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [1100/1251 ( 88%)]  Loss:  3.782492 (3.5856)  Time: 1.095s,  935.50/s  (1.108s,  924.06/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [1150/1251 ( 92%)]  Loss:  3.539669 (3.5837)  Time: 1.099s,  931.80/s  (1.108s,  924.00/s)  LR: 4.432e-04  Data: 0.012 (0.012)
Train: 66 [1200/1251 ( 96%)]  Loss:  3.810337 (3.5927)  Time: 1.102s,  929.49/s  (1.108s,  924.31/s)  LR: 4.432e-04  Data: 0.011 (0.012)
Train: 66 [1250/1251 (100%)]  Loss:  3.476182 (3.5883)  Time: 1.165s,  879.15/s  (1.108s,  924.29/s)  LR: 4.432e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.227 (3.227)  Loss:  0.5860 (0.5860)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.230 (0.407)  Loss:  0.6932 (1.1415)  Acc@1: 84.7877 (74.0680)  Acc@5: 96.6981 (92.1680)
Test (EMA): [   0/48]  Time: 3.292 (3.292)  Loss:  0.4960 (0.4960)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.6151 (1.0153)  Acc@1: 85.7311 (76.1580)  Acc@5: 96.6981 (93.1580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 75.07400001220704)

Train: 67 [   0/1251 (  0%)]  Loss:  3.532173 (3.5322)  Time: 1.129s,  906.69/s  (1.129s,  906.69/s)  LR: 4.415e-04  Data: 0.022 (0.022)
Train: 67 [  50/1251 (  4%)]  Loss:  3.515674 (3.5239)  Time: 1.129s,  907.35/s  (1.103s,  928.67/s)  LR: 4.415e-04  Data: 0.010 (0.012)
Train: 67 [ 100/1251 (  8%)]  Loss:  3.561548 (3.5365)  Time: 1.188s,  861.88/s  (1.107s,  925.01/s)  LR: 4.415e-04  Data: 0.011 (0.011)
Train: 67 [ 150/1251 ( 12%)]  Loss:  3.695690 (3.5763)  Time: 1.132s,  904.61/s  (1.108s,  923.81/s)  LR: 4.415e-04  Data: 0.012 (0.011)
Train: 67 [ 200/1251 ( 16%)]  Loss:  3.638742 (3.5888)  Time: 1.098s,  932.91/s  (1.107s,  925.15/s)  LR: 4.415e-04  Data: 0.012 (0.011)
Train: 67 [ 250/1251 ( 20%)]  Loss:  3.527202 (3.5785)  Time: 1.134s,  902.77/s  (1.107s,  924.89/s)  LR: 4.415e-04  Data: 0.010 (0.011)
Train: 67 [ 300/1251 ( 24%)]  Loss:  3.782208 (3.6076)  Time: 1.094s,  935.69/s  (1.106s,  925.55/s)  LR: 4.415e-04  Data: 0.011 (0.011)
Train: 67 [ 350/1251 ( 28%)]  Loss:  3.808226 (3.6327)  Time: 1.098s,  932.68/s  (1.106s,  925.58/s)  LR: 4.415e-04  Data: 0.012 (0.011)
Train: 67 [ 400/1251 ( 32%)]  Loss:  3.934052 (3.6662)  Time: 1.133s,  904.12/s  (1.106s,  925.63/s)  LR: 4.415e-04  Data: 0.011 (0.011)
Train: 67 [ 450/1251 ( 36%)]  Loss:  3.705237 (3.6701)  Time: 1.102s,  929.48/s  (1.107s,  925.19/s)  LR: 4.415e-04  Data: 0.011 (0.011)
Train: 67 [ 500/1251 ( 40%)]  Loss:  3.751186 (3.6774)  Time: 1.102s,  929.42/s  (1.106s,  925.78/s)  LR: 4.415e-04  Data: 0.011 (0.011)
Train: 67 [ 550/1251 ( 44%)]  Loss:  3.928698 (3.6984)  Time: 1.097s,  933.33/s  (1.106s,  925.64/s)  LR: 4.415e-04  Data: 0.014 (0.012)
Train: 67 [ 600/1251 ( 48%)]  Loss:  3.463729 (3.6803)  Time: 1.117s,  916.45/s  (1.106s,  925.97/s)  LR: 4.415e-04  Data: 0.012 (0.012)
Train: 67 [ 650/1251 ( 52%)]  Loss:  3.823383 (3.6906)  Time: 1.097s,  933.35/s  (1.106s,  925.78/s)  LR: 4.415e-04  Data: 0.011 (0.012)
Train: 67 [ 700/1251 ( 56%)]  Loss:  3.607227 (3.6850)  Time: 1.099s,  932.09/s  (1.106s,  925.97/s)  LR: 4.415e-04  Data: 0.012 (0.012)
Train: 67 [ 750/1251 ( 60%)]  Loss:  3.655351 (3.6831)  Time: 1.098s,  932.87/s  (1.106s,  925.94/s)  LR: 4.415e-04  Data: 0.014 (0.012)
Train: 67 [ 800/1251 ( 64%)]  Loss:  3.423630 (3.6679)  Time: 1.098s,  932.82/s  (1.106s,  925.50/s)  LR: 4.415e-04  Data: 0.012 (0.012)
Train: 67 [ 850/1251 ( 68%)]  Loss:  3.602373 (3.6642)  Time: 1.097s,  933.52/s  (1.107s,  925.43/s)  LR: 4.415e-04  Data: 0.011 (0.012)
Train: 67 [ 900/1251 ( 72%)]  Loss:  3.558836 (3.6587)  Time: 1.116s,  917.83/s  (1.107s,  925.41/s)  LR: 4.415e-04  Data: 0.011 (0.012)
Train: 67 [ 950/1251 ( 76%)]  Loss:  3.536082 (3.6526)  Time: 1.096s,  933.94/s  (1.106s,  925.59/s)  LR: 4.415e-04  Data: 0.013 (0.012)
Train: 67 [1000/1251 ( 80%)]  Loss:  3.569213 (3.6486)  Time: 1.098s,  932.78/s  (1.107s,  925.30/s)  LR: 4.415e-04  Data: 0.012 (0.012)
Train: 67 [1050/1251 ( 84%)]  Loss:  3.375342 (3.6362)  Time: 1.097s,  933.34/s  (1.106s,  925.57/s)  LR: 4.415e-04  Data: 0.011 (0.012)
Train: 67 [1100/1251 ( 88%)]  Loss:  3.741481 (3.6408)  Time: 1.193s,  858.69/s  (1.107s,  925.35/s)  LR: 4.415e-04  Data: 0.014 (0.012)
Train: 67 [1150/1251 ( 92%)]  Loss:  3.424128 (3.6317)  Time: 1.100s,  931.30/s  (1.107s,  925.43/s)  LR: 4.415e-04  Data: 0.012 (0.012)
Train: 67 [1200/1251 ( 96%)]  Loss:  3.336876 (3.6199)  Time: 1.096s,  934.48/s  (1.107s,  925.42/s)  LR: 4.415e-04  Data: 0.012 (0.012)
Train: 67 [1250/1251 (100%)]  Loss:  3.548237 (3.6172)  Time: 1.091s,  938.98/s  (1.107s,  925.37/s)  LR: 4.415e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.190 (3.190)  Loss:  0.6652 (0.6652)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.6998 (1.1564)  Acc@1: 85.4953 (74.4580)  Acc@5: 97.0519 (92.5480)
Test (EMA): [   0/48]  Time: 3.120 (3.120)  Loss:  0.4927 (0.4927)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.6128 (1.0109)  Acc@1: 85.6132 (76.2920)  Acc@5: 96.6981 (93.2120)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 75.1999999609375)

Train: 68 [   0/1251 (  0%)]  Loss:  3.557033 (3.5570)  Time: 1.129s,  906.91/s  (1.129s,  906.91/s)  LR: 4.399e-04  Data: 0.022 (0.022)
Train: 68 [  50/1251 (  4%)]  Loss:  3.399550 (3.4783)  Time: 1.207s,  848.21/s  (1.114s,  919.51/s)  LR: 4.399e-04  Data: 0.016 (0.012)
Train: 68 [ 100/1251 (  8%)]  Loss:  3.370274 (3.4423)  Time: 1.098s,  932.33/s  (1.118s,  916.26/s)  LR: 4.399e-04  Data: 0.014 (0.012)
Train: 68 [ 150/1251 ( 12%)]  Loss:  3.336027 (3.4157)  Time: 1.098s,  932.97/s  (1.113s,  920.09/s)  LR: 4.399e-04  Data: 0.011 (0.011)
Train: 68 [ 200/1251 ( 16%)]  Loss:  3.627353 (3.4580)  Time: 1.125s,  909.98/s  (1.111s,  921.32/s)  LR: 4.399e-04  Data: 0.016 (0.012)
Train: 68 [ 250/1251 ( 20%)]  Loss:  3.746469 (3.5061)  Time: 1.201s,  852.52/s  (1.111s,  921.31/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 300/1251 ( 24%)]  Loss:  3.377378 (3.4877)  Time: 1.096s,  934.22/s  (1.112s,  921.24/s)  LR: 4.399e-04  Data: 0.012 (0.012)
Train: 68 [ 350/1251 ( 28%)]  Loss:  3.903505 (3.5397)  Time: 1.096s,  934.10/s  (1.110s,  922.77/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 400/1251 ( 32%)]  Loss:  3.709992 (3.5586)  Time: 1.097s,  933.20/s  (1.109s,  923.08/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 450/1251 ( 36%)]  Loss:  3.941367 (3.5969)  Time: 1.130s,  906.46/s  (1.109s,  923.39/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 500/1251 ( 40%)]  Loss:  3.553464 (3.5929)  Time: 1.130s,  906.39/s  (1.110s,  922.65/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 550/1251 ( 44%)]  Loss:  3.651919 (3.5979)  Time: 1.094s,  936.03/s  (1.110s,  922.70/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 600/1251 ( 48%)]  Loss:  3.445644 (3.5862)  Time: 1.206s,  848.95/s  (1.110s,  922.21/s)  LR: 4.399e-04  Data: 0.013 (0.012)
Train: 68 [ 650/1251 ( 52%)]  Loss:  3.404504 (3.5732)  Time: 1.107s,  924.70/s  (1.110s,  922.23/s)  LR: 4.399e-04  Data: 0.012 (0.012)
Train: 68 [ 700/1251 ( 56%)]  Loss:  3.766802 (3.5861)  Time: 1.111s,  921.32/s  (1.110s,  922.36/s)  LR: 4.399e-04  Data: 0.012 (0.012)
Train: 68 [ 750/1251 ( 60%)]  Loss:  3.346613 (3.5711)  Time: 1.097s,  933.44/s  (1.110s,  922.14/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 800/1251 ( 64%)]  Loss:  3.401901 (3.5612)  Time: 1.100s,  930.69/s  (1.110s,  922.51/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 850/1251 ( 68%)]  Loss:  3.542776 (3.5601)  Time: 1.098s,  932.70/s  (1.110s,  922.74/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [ 900/1251 ( 72%)]  Loss:  3.329454 (3.5480)  Time: 1.130s,  906.31/s  (1.109s,  923.04/s)  LR: 4.399e-04  Data: 0.012 (0.012)
Train: 68 [ 950/1251 ( 76%)]  Loss:  3.846676 (3.5629)  Time: 1.120s,  914.66/s  (1.110s,  922.92/s)  LR: 4.399e-04  Data: 0.010 (0.012)
Train: 68 [1000/1251 ( 80%)]  Loss:  3.723401 (3.5706)  Time: 1.120s,  914.03/s  (1.111s,  922.11/s)  LR: 4.399e-04  Data: 0.013 (0.012)
Train: 68 [1050/1251 ( 84%)]  Loss:  3.834407 (3.5826)  Time: 1.095s,  935.37/s  (1.110s,  922.13/s)  LR: 4.399e-04  Data: 0.013 (0.012)
Train: 68 [1100/1251 ( 88%)]  Loss:  3.359792 (3.5729)  Time: 1.097s,  933.71/s  (1.110s,  922.57/s)  LR: 4.399e-04  Data: 0.013 (0.012)
Train: 68 [1150/1251 ( 92%)]  Loss:  3.309458 (3.5619)  Time: 1.093s,  936.53/s  (1.110s,  922.68/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [1200/1251 ( 96%)]  Loss:  3.817987 (3.5721)  Time: 1.100s,  931.15/s  (1.110s,  922.74/s)  LR: 4.399e-04  Data: 0.011 (0.012)
Train: 68 [1250/1251 (100%)]  Loss:  3.851506 (3.5829)  Time: 1.166s,  878.07/s  (1.110s,  922.42/s)  LR: 4.399e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.305 (3.305)  Loss:  0.5132 (0.5132)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6505 (1.1044)  Acc@1: 85.1415 (74.4860)  Acc@5: 96.8160 (92.3900)
Test (EMA): [   0/48]  Time: 3.080 (3.080)  Loss:  0.4894 (0.4894)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.6109 (1.0066)  Acc@1: 85.6132 (76.3900)  Acc@5: 96.6981 (93.3220)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 75.39200006347656)

Train: 69 [   0/1251 (  0%)]  Loss:  3.197706 (3.1977)  Time: 1.101s,  929.80/s  (1.101s,  929.80/s)  LR: 4.382e-04  Data: 0.020 (0.020)
Train: 69 [  50/1251 (  4%)]  Loss:  3.508173 (3.3529)  Time: 1.094s,  935.81/s  (1.119s,  915.14/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [ 100/1251 (  8%)]  Loss:  3.629076 (3.4450)  Time: 1.117s,  916.87/s  (1.113s,  920.12/s)  LR: 4.382e-04  Data: 0.010 (0.012)
Train: 69 [ 150/1251 ( 12%)]  Loss:  3.501853 (3.4592)  Time: 1.096s,  933.91/s  (1.111s,  921.70/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [ 200/1251 ( 16%)]  Loss:  3.404923 (3.4483)  Time: 1.209s,  846.96/s  (1.110s,  922.54/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [ 250/1251 ( 20%)]  Loss:  3.438102 (3.4466)  Time: 1.100s,  931.07/s  (1.111s,  921.50/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [ 300/1251 ( 24%)]  Loss:  3.646019 (3.4751)  Time: 1.102s,  929.37/s  (1.109s,  923.01/s)  LR: 4.382e-04  Data: 0.010 (0.012)
Train: 69 [ 350/1251 ( 28%)]  Loss:  3.314173 (3.4550)  Time: 1.096s,  934.71/s  (1.110s,  922.67/s)  LR: 4.382e-04  Data: 0.012 (0.012)
Train: 69 [ 400/1251 ( 32%)]  Loss:  3.136423 (3.4196)  Time: 1.096s,  934.38/s  (1.109s,  923.09/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 69 [ 450/1251 ( 36%)]  Loss:  3.634271 (3.4411)  Time: 1.125s,  909.98/s  (1.109s,  923.25/s)  LR: 4.382e-04  Data: 0.012 (0.011)
Train: 69 [ 500/1251 ( 40%)]  Loss:  3.488764 (3.4454)  Time: 1.100s,  930.74/s  (1.109s,  923.49/s)  LR: 4.382e-04  Data: 0.012 (0.012)
Train: 69 [ 550/1251 ( 44%)]  Loss:  3.366001 (3.4388)  Time: 1.180s,  867.89/s  (1.109s,  923.10/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 69 [ 600/1251 ( 48%)]  Loss:  3.662425 (3.4560)  Time: 1.097s,  933.82/s  (1.109s,  923.22/s)  LR: 4.382e-04  Data: 0.014 (0.012)
Train: 69 [ 650/1251 ( 52%)]  Loss:  3.664492 (3.4709)  Time: 1.098s,  932.61/s  (1.109s,  923.50/s)  LR: 4.382e-04  Data: 0.012 (0.012)
Train: 69 [ 700/1251 ( 56%)]  Loss:  3.491155 (3.4722)  Time: 1.098s,  932.21/s  (1.109s,  923.38/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [ 750/1251 ( 60%)]  Loss:  3.653463 (3.4836)  Time: 1.095s,  935.09/s  (1.109s,  923.71/s)  LR: 4.382e-04  Data: 0.012 (0.012)
Train: 69 [ 800/1251 ( 64%)]  Loss:  3.392193 (3.4782)  Time: 1.097s,  933.71/s  (1.109s,  923.36/s)  LR: 4.382e-04  Data: 0.012 (0.012)
Train: 69 [ 850/1251 ( 68%)]  Loss:  3.770796 (3.4944)  Time: 1.095s,  935.08/s  (1.108s,  923.84/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [ 900/1251 ( 72%)]  Loss:  3.590253 (3.4995)  Time: 1.098s,  932.34/s  (1.109s,  923.54/s)  LR: 4.382e-04  Data: 0.010 (0.012)
Train: 69 [ 950/1251 ( 76%)]  Loss:  3.574050 (3.5032)  Time: 1.095s,  934.96/s  (1.109s,  923.65/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [1000/1251 ( 80%)]  Loss:  3.503926 (3.5032)  Time: 1.165s,  878.75/s  (1.109s,  923.75/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [1050/1251 ( 84%)]  Loss:  3.639556 (3.5094)  Time: 1.115s,  918.41/s  (1.109s,  923.27/s)  LR: 4.382e-04  Data: 0.012 (0.012)
Train: 69 [1100/1251 ( 88%)]  Loss:  3.361390 (3.5030)  Time: 1.090s,  939.30/s  (1.109s,  923.08/s)  LR: 4.382e-04  Data: 0.009 (0.012)
Train: 69 [1150/1251 ( 92%)]  Loss:  3.366870 (3.4973)  Time: 1.101s,  929.89/s  (1.109s,  923.21/s)  LR: 4.382e-04  Data: 0.010 (0.012)
Train: 69 [1200/1251 ( 96%)]  Loss:  3.317995 (3.4902)  Time: 1.107s,  925.42/s  (1.109s,  923.28/s)  LR: 4.382e-04  Data: 0.011 (0.012)
Train: 69 [1250/1251 (100%)]  Loss:  3.395742 (3.4865)  Time: 1.080s,  947.86/s  (1.109s,  923.16/s)  LR: 4.382e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.228 (3.228)  Loss:  0.5744 (0.5744)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.6611 (1.1052)  Acc@1: 85.3774 (74.5300)  Acc@5: 96.2264 (92.5080)
Test (EMA): [   0/48]  Time: 3.208 (3.208)  Loss:  0.4874 (0.4874)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.6096 (1.0024)  Acc@1: 85.7311 (76.5460)  Acc@5: 96.9340 (93.3400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 75.51599998535156)

Train: 70 [   0/1251 (  0%)]  Loss:  3.571660 (3.5717)  Time: 1.103s,  928.75/s  (1.103s,  928.75/s)  LR: 4.364e-04  Data: 0.023 (0.023)
Train: 70 [  50/1251 (  4%)]  Loss:  3.454610 (3.5131)  Time: 1.106s,  925.78/s  (1.102s,  928.90/s)  LR: 4.364e-04  Data: 0.010 (0.012)
Train: 70 [ 100/1251 (  8%)]  Loss:  3.672864 (3.5664)  Time: 1.097s,  933.50/s  (1.109s,  923.48/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 150/1251 ( 12%)]  Loss:  3.716331 (3.6039)  Time: 1.209s,  846.95/s  (1.108s,  924.57/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 200/1251 ( 16%)]  Loss:  3.280448 (3.5392)  Time: 1.094s,  935.86/s  (1.111s,  921.64/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 250/1251 ( 20%)]  Loss:  3.637898 (3.5556)  Time: 1.099s,  931.66/s  (1.110s,  922.39/s)  LR: 4.364e-04  Data: 0.011 (0.011)
Train: 70 [ 300/1251 ( 24%)]  Loss:  3.098957 (3.4904)  Time: 1.101s,  929.69/s  (1.110s,  922.87/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 350/1251 ( 28%)]  Loss:  3.627813 (3.5076)  Time: 1.097s,  933.38/s  (1.108s,  923.91/s)  LR: 4.364e-04  Data: 0.012 (0.012)
Train: 70 [ 400/1251 ( 32%)]  Loss:  3.972758 (3.5593)  Time: 1.121s,  913.65/s  (1.109s,  923.31/s)  LR: 4.364e-04  Data: 0.012 (0.012)
Train: 70 [ 450/1251 ( 36%)]  Loss:  3.254276 (3.5288)  Time: 1.103s,  928.64/s  (1.108s,  924.04/s)  LR: 4.364e-04  Data: 0.012 (0.012)
Train: 70 [ 500/1251 ( 40%)]  Loss:  3.360437 (3.5135)  Time: 1.092s,  937.66/s  (1.108s,  923.92/s)  LR: 4.364e-04  Data: 0.010 (0.012)
Train: 70 [ 550/1251 ( 44%)]  Loss:  3.509760 (3.5132)  Time: 1.096s,  933.99/s  (1.107s,  924.70/s)  LR: 4.364e-04  Data: 0.012 (0.012)
Train: 70 [ 600/1251 ( 48%)]  Loss:  3.697380 (3.5273)  Time: 1.095s,  935.54/s  (1.107s,  925.21/s)  LR: 4.364e-04  Data: 0.012 (0.012)
Train: 70 [ 650/1251 ( 52%)]  Loss:  3.331962 (3.5134)  Time: 1.121s,  913.38/s  (1.107s,  924.88/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 700/1251 ( 56%)]  Loss:  3.324342 (3.5008)  Time: 1.095s,  935.57/s  (1.107s,  924.70/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 750/1251 ( 60%)]  Loss:  3.464484 (3.4985)  Time: 1.098s,  932.39/s  (1.108s,  924.52/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 800/1251 ( 64%)]  Loss:  3.323653 (3.4882)  Time: 1.096s,  934.47/s  (1.107s,  924.93/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [ 850/1251 ( 68%)]  Loss:  3.419463 (3.4844)  Time: 1.104s,  927.82/s  (1.107s,  924.76/s)  LR: 4.364e-04  Data: 0.010 (0.012)
Train: 70 [ 900/1251 ( 72%)]  Loss:  3.836686 (3.5029)  Time: 1.097s,  933.11/s  (1.107s,  925.16/s)  LR: 4.364e-04  Data: 0.012 (0.012)
Train: 70 [ 950/1251 ( 76%)]  Loss:  3.438473 (3.4997)  Time: 1.197s,  855.69/s  (1.107s,  925.10/s)  LR: 4.364e-04  Data: 0.014 (0.012)
Train: 70 [1000/1251 ( 80%)]  Loss:  3.508008 (3.5001)  Time: 1.096s,  934.03/s  (1.107s,  925.10/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [1050/1251 ( 84%)]  Loss:  3.183475 (3.4857)  Time: 1.193s,  858.42/s  (1.107s,  924.99/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [1100/1251 ( 88%)]  Loss:  3.796802 (3.4992)  Time: 1.099s,  931.70/s  (1.107s,  925.00/s)  LR: 4.364e-04  Data: 0.013 (0.012)
Train: 70 [1150/1251 ( 92%)]  Loss:  3.506049 (3.4995)  Time: 1.129s,  906.85/s  (1.107s,  925.08/s)  LR: 4.364e-04  Data: 0.010 (0.012)
Train: 70 [1200/1251 ( 96%)]  Loss:  3.542048 (3.5012)  Time: 1.099s,  931.55/s  (1.107s,  925.23/s)  LR: 4.364e-04  Data: 0.011 (0.012)
Train: 70 [1250/1251 (100%)]  Loss:  3.694545 (3.5087)  Time: 1.083s,  945.95/s  (1.107s,  925.37/s)  LR: 4.364e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.310 (3.310)  Loss:  0.6234 (0.6234)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.7209 (1.1628)  Acc@1: 84.5519 (74.5820)  Acc@5: 97.1698 (92.4280)
Test (EMA): [   0/48]  Time: 3.236 (3.236)  Loss:  0.4851 (0.4851)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.6062 (0.9984)  Acc@1: 85.7311 (76.6020)  Acc@5: 97.1698 (93.4000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 75.63800006103516)

Train: 71 [   0/1251 (  0%)]  Loss:  3.473174 (3.4732)  Time: 1.102s,  929.25/s  (1.102s,  929.25/s)  LR: 4.347e-04  Data: 0.020 (0.020)
Train: 71 [  50/1251 (  4%)]  Loss:  3.662831 (3.5680)  Time: 1.100s,  930.71/s  (1.116s,  917.34/s)  LR: 4.347e-04  Data: 0.013 (0.011)
Train: 71 [ 100/1251 (  8%)]  Loss:  3.326008 (3.4873)  Time: 1.208s,  847.84/s  (1.110s,  922.22/s)  LR: 4.347e-04  Data: 0.011 (0.011)
Train: 71 [ 150/1251 ( 12%)]  Loss:  3.532584 (3.4986)  Time: 1.097s,  933.10/s  (1.111s,  921.64/s)  LR: 4.347e-04  Data: 0.011 (0.011)
Train: 71 [ 200/1251 ( 16%)]  Loss:  3.308750 (3.4607)  Time: 1.096s,  934.38/s  (1.108s,  924.05/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 250/1251 ( 20%)]  Loss:  3.660843 (3.4940)  Time: 1.098s,  932.41/s  (1.109s,  923.71/s)  LR: 4.347e-04  Data: 0.012 (0.012)
Train: 71 [ 300/1251 ( 24%)]  Loss:  3.405987 (3.4815)  Time: 1.096s,  934.29/s  (1.107s,  924.86/s)  LR: 4.347e-04  Data: 0.012 (0.012)
Train: 71 [ 350/1251 ( 28%)]  Loss:  3.877141 (3.5309)  Time: 1.100s,  930.62/s  (1.107s,  924.83/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 400/1251 ( 32%)]  Loss:  3.336184 (3.5093)  Time: 1.132s,  904.95/s  (1.107s,  925.10/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 450/1251 ( 36%)]  Loss:  3.397690 (3.4981)  Time: 1.100s,  931.17/s  (1.107s,  924.62/s)  LR: 4.347e-04  Data: 0.014 (0.012)
Train: 71 [ 500/1251 ( 40%)]  Loss:  3.494070 (3.4978)  Time: 1.099s,  931.99/s  (1.107s,  925.04/s)  LR: 4.347e-04  Data: 0.014 (0.012)
Train: 71 [ 550/1251 ( 44%)]  Loss:  3.674128 (3.5124)  Time: 1.104s,  927.94/s  (1.107s,  924.98/s)  LR: 4.347e-04  Data: 0.010 (0.012)
Train: 71 [ 600/1251 ( 48%)]  Loss:  3.277402 (3.4944)  Time: 1.097s,  933.74/s  (1.108s,  924.21/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 650/1251 ( 52%)]  Loss:  3.314157 (3.4815)  Time: 1.098s,  932.50/s  (1.108s,  924.56/s)  LR: 4.347e-04  Data: 0.012 (0.012)
Train: 71 [ 700/1251 ( 56%)]  Loss:  3.461883 (3.4802)  Time: 1.096s,  934.27/s  (1.108s,  924.29/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 750/1251 ( 60%)]  Loss:  3.634931 (3.4899)  Time: 1.098s,  932.45/s  (1.107s,  924.76/s)  LR: 4.347e-04  Data: 0.010 (0.012)
Train: 71 [ 800/1251 ( 64%)]  Loss:  3.187744 (3.4721)  Time: 1.101s,  929.66/s  (1.108s,  924.27/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 850/1251 ( 68%)]  Loss:  3.753047 (3.4877)  Time: 1.104s,  927.61/s  (1.108s,  924.33/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [ 900/1251 ( 72%)]  Loss:  3.548937 (3.4909)  Time: 1.098s,  932.26/s  (1.108s,  923.97/s)  LR: 4.347e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 71 [ 950/1251 ( 76%)]  Loss:  3.603364 (3.4965)  Time: 1.097s,  933.66/s  (1.109s,  923.63/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [1000/1251 ( 80%)]  Loss:  3.603041 (3.5016)  Time: 1.216s,  842.06/s  (1.109s,  923.37/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [1050/1251 ( 84%)]  Loss:  3.259099 (3.4906)  Time: 1.096s,  934.47/s  (1.109s,  923.43/s)  LR: 4.347e-04  Data: 0.011 (0.012)
Train: 71 [1100/1251 ( 88%)]  Loss:  3.552045 (3.4933)  Time: 1.107s,  925.27/s  (1.109s,  923.31/s)  LR: 4.347e-04  Data: 0.010 (0.012)
Train: 71 [1150/1251 ( 92%)]  Loss:  3.899673 (3.5102)  Time: 1.137s,  900.27/s  (1.109s,  923.28/s)  LR: 4.347e-04  Data: 0.010 (0.012)
Train: 71 [1200/1251 ( 96%)]  Loss:  3.717376 (3.5185)  Time: 1.095s,  934.78/s  (1.109s,  923.47/s)  LR: 4.347e-04  Data: 0.012 (0.012)
Train: 71 [1250/1251 (100%)]  Loss:  3.321072 (3.5109)  Time: 1.089s,  940.26/s  (1.109s,  923.46/s)  LR: 4.347e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.266 (3.266)  Loss:  0.5300 (0.5300)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6395 (1.1049)  Acc@1: 85.1415 (74.7740)  Acc@5: 97.1698 (92.6520)
Test (EMA): [   0/48]  Time: 3.123 (3.123)  Loss:  0.4820 (0.4820)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.6041 (0.9942)  Acc@1: 85.4953 (76.6160)  Acc@5: 97.0519 (93.4180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 75.74999998291015)

Train: 72 [   0/1251 (  0%)]  Loss:  3.446623 (3.4466)  Time: 1.103s,  928.55/s  (1.103s,  928.55/s)  LR: 4.329e-04  Data: 0.020 (0.020)
Train: 72 [  50/1251 (  4%)]  Loss:  3.584947 (3.5158)  Time: 1.095s,  935.13/s  (1.100s,  930.51/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 100/1251 (  8%)]  Loss:  3.748508 (3.5934)  Time: 1.100s,  930.84/s  (1.103s,  928.27/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 150/1251 ( 12%)]  Loss:  3.009869 (3.4475)  Time: 1.098s,  932.53/s  (1.103s,  928.44/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [ 200/1251 ( 16%)]  Loss:  3.676999 (3.4934)  Time: 1.103s,  928.36/s  (1.108s,  924.55/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [ 250/1251 ( 20%)]  Loss:  3.657153 (3.5207)  Time: 1.098s,  932.60/s  (1.106s,  925.81/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 300/1251 ( 24%)]  Loss:  3.568826 (3.5276)  Time: 1.099s,  931.45/s  (1.107s,  924.78/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 350/1251 ( 28%)]  Loss:  3.162600 (3.4819)  Time: 1.099s,  931.95/s  (1.106s,  925.49/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [ 400/1251 ( 32%)]  Loss:  3.693066 (3.5054)  Time: 1.103s,  928.31/s  (1.106s,  925.90/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 450/1251 ( 36%)]  Loss:  3.435560 (3.4984)  Time: 1.098s,  933.01/s  (1.105s,  926.38/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 500/1251 ( 40%)]  Loss:  3.620208 (3.5095)  Time: 1.095s,  935.41/s  (1.106s,  926.22/s)  LR: 4.329e-04  Data: 0.010 (0.012)
Train: 72 [ 550/1251 ( 44%)]  Loss:  3.740494 (3.5287)  Time: 1.097s,  933.88/s  (1.106s,  925.67/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 600/1251 ( 48%)]  Loss:  3.568796 (3.5318)  Time: 1.091s,  938.94/s  (1.106s,  925.86/s)  LR: 4.329e-04  Data: 0.010 (0.012)
Train: 72 [ 650/1251 ( 52%)]  Loss:  3.598481 (3.5366)  Time: 1.091s,  938.22/s  (1.107s,  925.39/s)  LR: 4.329e-04  Data: 0.009 (0.012)
Train: 72 [ 700/1251 ( 56%)]  Loss:  3.465284 (3.5318)  Time: 1.097s,  933.85/s  (1.106s,  925.90/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 750/1251 ( 60%)]  Loss:  3.342720 (3.5200)  Time: 1.098s,  932.34/s  (1.106s,  925.87/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [ 800/1251 ( 64%)]  Loss:  3.508674 (3.5193)  Time: 1.098s,  932.96/s  (1.106s,  925.92/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [ 850/1251 ( 68%)]  Loss:  3.774179 (3.5335)  Time: 1.096s,  934.38/s  (1.106s,  925.51/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 900/1251 ( 72%)]  Loss:  3.843626 (3.5498)  Time: 1.097s,  933.26/s  (1.106s,  925.71/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [ 950/1251 ( 76%)]  Loss:  3.709580 (3.5578)  Time: 1.133s,  903.82/s  (1.107s,  925.44/s)  LR: 4.329e-04  Data: 0.010 (0.012)
Train: 72 [1000/1251 ( 80%)]  Loss:  3.587197 (3.5592)  Time: 1.096s,  934.29/s  (1.106s,  925.61/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [1050/1251 ( 84%)]  Loss:  3.109883 (3.5388)  Time: 1.095s,  935.06/s  (1.106s,  925.91/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [1100/1251 ( 88%)]  Loss:  3.535390 (3.5386)  Time: 1.101s,  930.07/s  (1.106s,  925.94/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [1150/1251 ( 92%)]  Loss:  3.330233 (3.5300)  Time: 1.099s,  931.82/s  (1.106s,  926.01/s)  LR: 4.329e-04  Data: 0.012 (0.012)
Train: 72 [1200/1251 ( 96%)]  Loss:  3.616195 (3.5334)  Time: 1.134s,  903.24/s  (1.106s,  925.51/s)  LR: 4.329e-04  Data: 0.011 (0.012)
Train: 72 [1250/1251 (100%)]  Loss:  3.315408 (3.5250)  Time: 1.103s,  928.71/s  (1.106s,  925.64/s)  LR: 4.329e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.257 (3.257)  Loss:  0.5642 (0.5642)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6454 (1.1039)  Acc@1: 85.8491 (74.8340)  Acc@5: 96.6981 (92.7120)
Test (EMA): [   0/48]  Time: 3.016 (3.016)  Loss:  0.4790 (0.4790)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.6030 (0.9904)  Acc@1: 85.7311 (76.7160)  Acc@5: 97.1698 (93.4600)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 75.86000003417969)

Train: 73 [   0/1251 (  0%)]  Loss:  3.567572 (3.5676)  Time: 1.107s,  925.27/s  (1.107s,  925.27/s)  LR: 4.311e-04  Data: 0.024 (0.024)
Train: 73 [  50/1251 (  4%)]  Loss:  3.481044 (3.5243)  Time: 1.102s,  929.07/s  (1.108s,  924.10/s)  LR: 4.311e-04  Data: 0.011 (0.012)
Train: 73 [ 100/1251 (  8%)]  Loss:  3.582195 (3.5436)  Time: 1.101s,  930.19/s  (1.104s,  927.29/s)  LR: 4.311e-04  Data: 0.013 (0.012)
Train: 73 [ 150/1251 ( 12%)]  Loss:  3.835845 (3.6167)  Time: 1.099s,  931.91/s  (1.107s,  925.38/s)  LR: 4.311e-04  Data: 0.013 (0.012)
Train: 73 [ 200/1251 ( 16%)]  Loss:  3.359567 (3.5652)  Time: 1.096s,  934.00/s  (1.106s,  925.55/s)  LR: 4.311e-04  Data: 0.010 (0.012)
Train: 73 [ 250/1251 ( 20%)]  Loss:  3.572126 (3.5664)  Time: 1.204s,  850.44/s  (1.108s,  924.43/s)  LR: 4.311e-04  Data: 0.011 (0.012)
Train: 73 [ 300/1251 ( 24%)]  Loss:  3.449144 (3.5496)  Time: 1.103s,  928.78/s  (1.108s,  924.43/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [ 350/1251 ( 28%)]  Loss:  3.610264 (3.5572)  Time: 1.194s,  857.46/s  (1.108s,  923.85/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [ 400/1251 ( 32%)]  Loss:  3.421137 (3.5421)  Time: 1.123s,  912.12/s  (1.110s,  922.89/s)  LR: 4.311e-04  Data: 0.013 (0.011)
Train: 73 [ 450/1251 ( 36%)]  Loss:  3.214694 (3.5094)  Time: 1.132s,  904.60/s  (1.110s,  922.93/s)  LR: 4.311e-04  Data: 0.010 (0.011)
Train: 73 [ 500/1251 ( 40%)]  Loss:  3.369595 (3.4967)  Time: 1.136s,  901.35/s  (1.110s,  922.39/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [ 550/1251 ( 44%)]  Loss:  3.440112 (3.4919)  Time: 1.096s,  934.15/s  (1.110s,  922.64/s)  LR: 4.311e-04  Data: 0.011 (0.011)
Train: 73 [ 600/1251 ( 48%)]  Loss:  3.586936 (3.4992)  Time: 1.097s,  933.10/s  (1.110s,  922.24/s)  LR: 4.311e-04  Data: 0.010 (0.011)
Train: 73 [ 650/1251 ( 52%)]  Loss:  3.536421 (3.5019)  Time: 1.097s,  933.38/s  (1.109s,  922.96/s)  LR: 4.311e-04  Data: 0.011 (0.011)
Train: 73 [ 700/1251 ( 56%)]  Loss:  3.705380 (3.5155)  Time: 1.091s,  938.50/s  (1.110s,  922.65/s)  LR: 4.311e-04  Data: 0.009 (0.011)
Train: 73 [ 750/1251 ( 60%)]  Loss:  3.275867 (3.5005)  Time: 1.096s,  934.22/s  (1.110s,  922.89/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [ 800/1251 ( 64%)]  Loss:  3.878856 (3.5228)  Time: 1.104s,  927.72/s  (1.110s,  922.69/s)  LR: 4.311e-04  Data: 0.010 (0.011)
Train: 73 [ 850/1251 ( 68%)]  Loss:  3.617567 (3.5280)  Time: 1.095s,  935.10/s  (1.109s,  923.15/s)  LR: 4.311e-04  Data: 0.011 (0.011)
Train: 73 [ 900/1251 ( 72%)]  Loss:  3.541531 (3.5287)  Time: 1.108s,  924.37/s  (1.109s,  923.18/s)  LR: 4.311e-04  Data: 0.014 (0.011)
Train: 73 [ 950/1251 ( 76%)]  Loss:  3.481604 (3.5264)  Time: 1.097s,  933.26/s  (1.109s,  923.39/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [1000/1251 ( 80%)]  Loss:  3.452994 (3.5229)  Time: 1.099s,  931.59/s  (1.109s,  923.54/s)  LR: 4.311e-04  Data: 0.011 (0.011)
Train: 73 [1050/1251 ( 84%)]  Loss:  3.005577 (3.4994)  Time: 1.095s,  934.90/s  (1.108s,  923.95/s)  LR: 4.311e-04  Data: 0.011 (0.011)
Train: 73 [1100/1251 ( 88%)]  Loss:  3.576695 (3.5027)  Time: 1.091s,  938.20/s  (1.108s,  924.11/s)  LR: 4.311e-04  Data: 0.010 (0.011)
Train: 73 [1150/1251 ( 92%)]  Loss:  3.745938 (3.5129)  Time: 1.098s,  932.95/s  (1.108s,  924.00/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [1200/1251 ( 96%)]  Loss:  3.539382 (3.5139)  Time: 1.096s,  934.45/s  (1.108s,  923.88/s)  LR: 4.311e-04  Data: 0.012 (0.011)
Train: 73 [1250/1251 (100%)]  Loss:  3.716766 (3.5217)  Time: 1.079s,  948.89/s  (1.109s,  923.50/s)  LR: 4.311e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.169 (3.169)  Loss:  0.5427 (0.5427)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.398)  Loss:  0.6840 (1.1017)  Acc@1: 85.4953 (74.9600)  Acc@5: 96.4623 (92.6720)
Test (EMA): [   0/48]  Time: 3.172 (3.172)  Loss:  0.4782 (0.4782)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.6012 (0.9869)  Acc@1: 85.6132 (76.7920)  Acc@5: 97.1698 (93.5120)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 75.94000003417969)

Train: 74 [   0/1251 (  0%)]  Loss:  3.574042 (3.5740)  Time: 1.137s,  900.45/s  (1.137s,  900.45/s)  LR: 4.293e-04  Data: 0.030 (0.030)
Train: 74 [  50/1251 (  4%)]  Loss:  3.570921 (3.5725)  Time: 1.121s,  913.82/s  (1.121s,  913.86/s)  LR: 4.293e-04  Data: 0.012 (0.012)
Train: 74 [ 100/1251 (  8%)]  Loss:  3.567528 (3.5708)  Time: 1.096s,  934.33/s  (1.117s,  916.99/s)  LR: 4.293e-04  Data: 0.011 (0.012)
Train: 74 [ 150/1251 ( 12%)]  Loss:  3.187207 (3.4749)  Time: 1.096s,  933.90/s  (1.111s,  921.40/s)  LR: 4.293e-04  Data: 0.011 (0.012)
Train: 74 [ 200/1251 ( 16%)]  Loss:  3.718733 (3.5237)  Time: 1.208s,  847.35/s  (1.112s,  921.07/s)  LR: 4.293e-04  Data: 0.012 (0.012)
Train: 74 [ 250/1251 ( 20%)]  Loss:  3.377848 (3.4994)  Time: 1.097s,  933.43/s  (1.112s,  921.24/s)  LR: 4.293e-04  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 74 [ 300/1251 ( 24%)]  Loss:  3.277909 (3.4677)  Time: 1.097s,  933.49/s  (1.111s,  921.31/s)  LR: 4.293e-04  Data: 0.011 (0.012)
Train: 74 [ 350/1251 ( 28%)]  Loss:  3.711047 (3.4982)  Time: 1.116s,  917.37/s  (1.110s,  922.44/s)  LR: 4.293e-04  Data: 0.012 (0.012)
Train: 74 [ 400/1251 ( 32%)]  Loss:  3.705856 (3.5212)  Time: 1.098s,  932.78/s  (1.110s,  922.73/s)  LR: 4.293e-04  Data: 0.010 (0.012)
Train: 74 [ 450/1251 ( 36%)]  Loss:  3.647123 (3.5338)  Time: 1.097s,  933.68/s  (1.110s,  922.91/s)  LR: 4.293e-04  Data: 0.012 (0.012)
Train: 74 [ 500/1251 ( 40%)]  Loss:  3.378974 (3.5197)  Time: 1.094s,  936.33/s  (1.109s,  923.10/s)  LR: 4.293e-04  Data: 0.010 (0.012)
Train: 74 [ 550/1251 ( 44%)]  Loss:  3.590519 (3.5256)  Time: 1.109s,  923.27/s  (1.109s,  923.42/s)  LR: 4.293e-04  Data: 0.010 (0.012)
Train: 74 [ 600/1251 ( 48%)]  Loss:  3.515062 (3.5248)  Time: 1.125s,  910.56/s  (1.109s,  923.73/s)  LR: 4.293e-04  Data: 0.011 (0.012)
Train: 74 [ 650/1251 ( 52%)]  Loss:  3.703925 (3.5376)  Time: 1.097s,  933.55/s  (1.109s,  923.59/s)  LR: 4.293e-04  Data: 0.012 (0.012)
Train: 74 [ 700/1251 ( 56%)]  Loss:  3.333156 (3.5240)  Time: 1.095s,  934.75/s  (1.108s,  923.85/s)  LR: 4.293e-04  Data: 0.011 (0.012)
Train: 74 [ 750/1251 ( 60%)]  Loss:  3.487400 (3.5217)  Time: 1.104s,  927.28/s  (1.109s,  923.25/s)  LR: 4.293e-04  Data: 0.011 (0.011)
Train: 74 [ 800/1251 ( 64%)]  Loss:  3.545399 (3.5231)  Time: 1.095s,  935.38/s  (1.108s,  923.77/s)  LR: 4.293e-04  Data: 0.012 (0.011)
Train: 74 [ 850/1251 ( 68%)]  Loss:  3.546916 (3.5244)  Time: 1.136s,  901.54/s  (1.109s,  923.76/s)  LR: 4.293e-04  Data: 0.012 (0.011)
Train: 74 [ 900/1251 ( 72%)]  Loss:  3.504031 (3.5233)  Time: 1.121s,  913.81/s  (1.109s,  923.58/s)  LR: 4.293e-04  Data: 0.011 (0.011)
Train: 74 [ 950/1251 ( 76%)]  Loss:  3.782882 (3.5363)  Time: 1.103s,  928.18/s  (1.109s,  923.69/s)  LR: 4.293e-04  Data: 0.010 (0.011)
Train: 74 [1000/1251 ( 80%)]  Loss:  3.596106 (3.5392)  Time: 1.105s,  926.87/s  (1.109s,  923.39/s)  LR: 4.293e-04  Data: 0.011 (0.011)
Train: 74 [1050/1251 ( 84%)]  Loss:  3.931356 (3.5570)  Time: 1.103s,  928.76/s  (1.109s,  923.60/s)  LR: 4.293e-04  Data: 0.012 (0.011)
Train: 74 [1100/1251 ( 88%)]  Loss:  3.613158 (3.5594)  Time: 1.094s,  935.69/s  (1.109s,  923.57/s)  LR: 4.293e-04  Data: 0.011 (0.011)
Train: 74 [1150/1251 ( 92%)]  Loss:  3.193184 (3.5442)  Time: 1.097s,  933.28/s  (1.108s,  923.86/s)  LR: 4.293e-04  Data: 0.012 (0.012)
Train: 74 [1200/1251 ( 96%)]  Loss:  3.376969 (3.5375)  Time: 1.103s,  928.55/s  (1.109s,  923.75/s)  LR: 4.293e-04  Data: 0.011 (0.012)
Train: 74 [1250/1251 (100%)]  Loss:  3.540581 (3.5376)  Time: 1.079s,  948.77/s  (1.108s,  923.97/s)  LR: 4.293e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.419 (3.419)  Loss:  0.5536 (0.5536)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.230 (0.404)  Loss:  0.6857 (1.0960)  Acc@1: 85.1415 (74.8420)  Acc@5: 96.5802 (92.5120)
Test (EMA): [   0/48]  Time: 3.204 (3.204)  Loss:  0.4775 (0.4775)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.401)  Loss:  0.5999 (0.9831)  Acc@1: 85.7311 (76.8560)  Acc@5: 97.1698 (93.5500)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 76.03599998291016)

Train: 75 [   0/1251 (  0%)]  Loss:  3.248163 (3.2482)  Time: 1.115s,  918.41/s  (1.115s,  918.41/s)  LR: 4.275e-04  Data: 0.031 (0.031)
Train: 75 [  50/1251 (  4%)]  Loss:  3.452087 (3.3501)  Time: 1.100s,  931.05/s  (1.112s,  920.72/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 100/1251 (  8%)]  Loss:  3.311935 (3.3374)  Time: 1.096s,  934.19/s  (1.106s,  926.09/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [ 150/1251 ( 12%)]  Loss:  3.819090 (3.4578)  Time: 1.223s,  837.11/s  (1.110s,  922.49/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 200/1251 ( 16%)]  Loss:  3.879852 (3.5422)  Time: 1.102s,  928.92/s  (1.108s,  924.00/s)  LR: 4.275e-04  Data: 0.013 (0.012)
Train: 75 [ 250/1251 ( 20%)]  Loss:  3.434306 (3.5242)  Time: 1.117s,  916.40/s  (1.110s,  922.55/s)  LR: 4.275e-04  Data: 0.009 (0.012)
Train: 75 [ 300/1251 ( 24%)]  Loss:  3.839823 (3.5693)  Time: 1.108s,  924.53/s  (1.110s,  922.43/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 350/1251 ( 28%)]  Loss:  3.654733 (3.5800)  Time: 1.122s,  912.69/s  (1.110s,  922.36/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [ 400/1251 ( 32%)]  Loss:  3.718096 (3.5953)  Time: 1.123s,  911.80/s  (1.111s,  921.80/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [ 450/1251 ( 36%)]  Loss:  3.275702 (3.5634)  Time: 1.096s,  934.64/s  (1.111s,  921.51/s)  LR: 4.275e-04  Data: 0.010 (0.012)
Train: 75 [ 500/1251 ( 40%)]  Loss:  3.559299 (3.5630)  Time: 1.096s,  934.23/s  (1.111s,  921.64/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [ 550/1251 ( 44%)]  Loss:  3.767545 (3.5801)  Time: 1.096s,  934.00/s  (1.111s,  921.89/s)  LR: 4.275e-04  Data: 0.013 (0.012)
Train: 75 [ 600/1251 ( 48%)]  Loss:  3.749171 (3.5931)  Time: 1.096s,  934.43/s  (1.111s,  921.90/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [ 650/1251 ( 52%)]  Loss:  3.338403 (3.5749)  Time: 1.096s,  934.71/s  (1.110s,  922.64/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 700/1251 ( 56%)]  Loss:  3.361386 (3.5606)  Time: 1.094s,  936.22/s  (1.110s,  922.90/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 750/1251 ( 60%)]  Loss:  3.294647 (3.5440)  Time: 1.098s,  932.34/s  (1.109s,  923.39/s)  LR: 4.275e-04  Data: 0.014 (0.012)
Train: 75 [ 800/1251 ( 64%)]  Loss:  3.279731 (3.5285)  Time: 1.097s,  933.75/s  (1.109s,  923.17/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [ 850/1251 ( 68%)]  Loss:  3.189199 (3.5096)  Time: 1.131s,  905.16/s  (1.109s,  923.30/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 900/1251 ( 72%)]  Loss:  3.422071 (3.5050)  Time: 1.097s,  933.34/s  (1.109s,  923.38/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [ 950/1251 ( 76%)]  Loss:  3.236274 (3.4916)  Time: 1.100s,  931.00/s  (1.109s,  923.48/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [1000/1251 ( 80%)]  Loss:  3.662963 (3.4997)  Time: 1.207s,  848.39/s  (1.109s,  923.74/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [1050/1251 ( 84%)]  Loss:  3.757675 (3.5115)  Time: 1.098s,  932.29/s  (1.108s,  923.96/s)  LR: 4.275e-04  Data: 0.011 (0.012)
Train: 75 [1100/1251 ( 88%)]  Loss:  3.443256 (3.5085)  Time: 1.092s,  937.74/s  (1.108s,  924.22/s)  LR: 4.275e-04  Data: 0.010 (0.012)
Train: 75 [1150/1251 ( 92%)]  Loss:  3.430096 (3.5052)  Time: 1.119s,  914.96/s  (1.108s,  924.03/s)  LR: 4.275e-04  Data: 0.010 (0.012)
Train: 75 [1200/1251 ( 96%)]  Loss:  3.498724 (3.5050)  Time: 1.144s,  895.04/s  (1.108s,  924.09/s)  LR: 4.275e-04  Data: 0.012 (0.012)
Train: 75 [1250/1251 (100%)]  Loss:  3.361839 (3.4995)  Time: 1.082s,  946.24/s  (1.109s,  923.53/s)  LR: 4.275e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.201 (3.201)  Loss:  0.5623 (0.5623)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.6710 (1.0879)  Acc@1: 84.5519 (75.1280)  Acc@5: 96.8160 (92.7200)
Test (EMA): [   0/48]  Time: 3.131 (3.131)  Loss:  0.4767 (0.4767)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5979 (0.9797)  Acc@1: 85.7311 (76.9440)  Acc@5: 97.1698 (93.6280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 76.15800000732422)

Train: 76 [   0/1251 (  0%)]  Loss:  3.550883 (3.5509)  Time: 1.112s,  920.60/s  (1.112s,  920.60/s)  LR: 4.257e-04  Data: 0.023 (0.023)
Train: 76 [  50/1251 (  4%)]  Loss:  3.381282 (3.4661)  Time: 1.097s,  933.41/s  (1.101s,  929.79/s)  LR: 4.257e-04  Data: 0.012 (0.012)
Train: 76 [ 100/1251 (  8%)]  Loss:  3.352755 (3.4283)  Time: 1.197s,  855.49/s  (1.107s,  924.89/s)  LR: 4.257e-04  Data: 0.011 (0.012)
Train: 76 [ 150/1251 ( 12%)]  Loss:  3.136303 (3.3553)  Time: 1.103s,  928.48/s  (1.110s,  922.67/s)  LR: 4.257e-04  Data: 0.010 (0.012)
Train: 76 [ 200/1251 ( 16%)]  Loss:  3.616646 (3.4076)  Time: 1.108s,  924.55/s  (1.110s,  922.31/s)  LR: 4.257e-04  Data: 0.012 (0.012)
Train: 76 [ 250/1251 ( 20%)]  Loss:  3.531492 (3.4282)  Time: 1.099s,  932.04/s  (1.108s,  923.87/s)  LR: 4.257e-04  Data: 0.011 (0.012)
Train: 76 [ 300/1251 ( 24%)]  Loss:  3.552942 (3.4460)  Time: 1.098s,  932.93/s  (1.108s,  924.39/s)  LR: 4.257e-04  Data: 0.013 (0.012)
Train: 76 [ 350/1251 ( 28%)]  Loss:  3.855857 (3.4973)  Time: 1.096s,  934.73/s  (1.107s,  924.74/s)  LR: 4.257e-04  Data: 0.012 (0.012)
Train: 76 [ 400/1251 ( 32%)]  Loss:  3.566365 (3.5049)  Time: 1.097s,  933.08/s  (1.107s,  924.99/s)  LR: 4.257e-04  Data: 0.011 (0.012)
Train: 76 [ 450/1251 ( 36%)]  Loss:  3.430421 (3.4975)  Time: 1.122s,  912.33/s  (1.108s,  923.99/s)  LR: 4.257e-04  Data: 0.012 (0.012)
Train: 76 [ 500/1251 ( 40%)]  Loss:  3.588230 (3.5057)  Time: 1.104s,  927.74/s  (1.108s,  924.53/s)  LR: 4.257e-04  Data: 0.010 (0.011)
Train: 76 [ 550/1251 ( 44%)]  Loss:  3.129133 (3.4744)  Time: 1.091s,  938.68/s  (1.108s,  924.01/s)  LR: 4.257e-04  Data: 0.010 (0.011)
Train: 76 [ 600/1251 ( 48%)]  Loss:  3.144602 (3.4490)  Time: 1.102s,  929.16/s  (1.108s,  924.51/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [ 650/1251 ( 52%)]  Loss:  3.566385 (3.4574)  Time: 1.099s,  931.66/s  (1.108s,  924.48/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [ 700/1251 ( 56%)]  Loss:  3.713264 (3.4744)  Time: 1.129s,  906.69/s  (1.107s,  924.70/s)  LR: 4.257e-04  Data: 0.011 (0.012)
Train: 76 [ 750/1251 ( 60%)]  Loss:  3.646739 (3.4852)  Time: 1.138s,  899.48/s  (1.109s,  923.74/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [ 800/1251 ( 64%)]  Loss:  3.634452 (3.4940)  Time: 1.097s,  933.70/s  (1.109s,  923.72/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [ 850/1251 ( 68%)]  Loss:  3.537574 (3.4964)  Time: 1.096s,  934.33/s  (1.108s,  923.94/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [ 900/1251 ( 72%)]  Loss:  3.747086 (3.5096)  Time: 1.120s,  914.00/s  (1.108s,  923.90/s)  LR: 4.257e-04  Data: 0.012 (0.011)
Train: 76 [ 950/1251 ( 76%)]  Loss:  3.545490 (3.5114)  Time: 1.182s,  866.33/s  (1.108s,  923.99/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [1000/1251 ( 80%)]  Loss:  3.643243 (3.5177)  Time: 1.096s,  934.58/s  (1.108s,  924.07/s)  LR: 4.257e-04  Data: 0.012 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 76 [1050/1251 ( 84%)]  Loss:  3.527652 (3.5181)  Time: 1.104s,  927.84/s  (1.108s,  924.31/s)  LR: 4.257e-04  Data: 0.011 (0.012)
Train: 76 [1100/1251 ( 88%)]  Loss:  3.609832 (3.5221)  Time: 1.102s,  929.18/s  (1.108s,  923.91/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [1150/1251 ( 92%)]  Loss:  3.476768 (3.5202)  Time: 1.132s,  904.79/s  (1.108s,  923.81/s)  LR: 4.257e-04  Data: 0.010 (0.011)
Train: 76 [1200/1251 ( 96%)]  Loss:  3.661118 (3.5259)  Time: 1.103s,  928.12/s  (1.109s,  923.71/s)  LR: 4.257e-04  Data: 0.011 (0.011)
Train: 76 [1250/1251 (100%)]  Loss:  3.225558 (3.5143)  Time: 1.080s,  947.85/s  (1.108s,  923.96/s)  LR: 4.257e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.179 (3.179)  Loss:  0.5595 (0.5595)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6746 (1.0813)  Acc@1: 84.1981 (74.9420)  Acc@5: 96.3443 (92.5980)
Test (EMA): [   0/48]  Time: 3.081 (3.081)  Loss:  0.4758 (0.4758)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5963 (0.9766)  Acc@1: 85.7311 (77.0580)  Acc@5: 97.1698 (93.6340)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 76.29200008544922)

Train: 77 [   0/1251 (  0%)]  Loss:  3.316184 (3.3162)  Time: 1.101s,  930.33/s  (1.101s,  930.33/s)  LR: 4.238e-04  Data: 0.020 (0.020)
Train: 77 [  50/1251 (  4%)]  Loss:  3.491802 (3.4040)  Time: 1.105s,  926.51/s  (1.111s,  921.69/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 100/1251 (  8%)]  Loss:  3.605770 (3.4713)  Time: 1.129s,  906.65/s  (1.114s,  919.55/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [ 150/1251 ( 12%)]  Loss:  3.418421 (3.4580)  Time: 1.097s,  933.59/s  (1.115s,  918.27/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 200/1251 ( 16%)]  Loss:  3.660324 (3.4985)  Time: 1.097s,  933.76/s  (1.111s,  921.45/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 250/1251 ( 20%)]  Loss:  3.504942 (3.4996)  Time: 1.097s,  933.40/s  (1.110s,  922.71/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 300/1251 ( 24%)]  Loss:  3.338695 (3.4766)  Time: 1.097s,  933.27/s  (1.110s,  922.63/s)  LR: 4.238e-04  Data: 0.010 (0.012)
Train: 77 [ 350/1251 ( 28%)]  Loss:  3.208669 (3.4431)  Time: 1.134s,  903.06/s  (1.111s,  922.02/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [ 400/1251 ( 32%)]  Loss:  3.555600 (3.4556)  Time: 1.098s,  933.00/s  (1.112s,  920.70/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 450/1251 ( 36%)]  Loss:  3.325446 (3.4426)  Time: 1.103s,  928.65/s  (1.112s,  920.88/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 500/1251 ( 40%)]  Loss:  3.169418 (3.4178)  Time: 1.097s,  933.33/s  (1.113s,  920.21/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [ 550/1251 ( 44%)]  Loss:  3.602839 (3.4332)  Time: 1.101s,  930.40/s  (1.113s,  920.43/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [ 600/1251 ( 48%)]  Loss:  3.726075 (3.4557)  Time: 1.097s,  933.80/s  (1.112s,  920.51/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 650/1251 ( 52%)]  Loss:  3.348547 (3.4481)  Time: 1.101s,  930.47/s  (1.112s,  921.20/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 700/1251 ( 56%)]  Loss:  3.105852 (3.4252)  Time: 1.098s,  932.19/s  (1.111s,  921.44/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [ 750/1251 ( 60%)]  Loss:  3.591110 (3.4356)  Time: 1.096s,  934.21/s  (1.111s,  921.97/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 800/1251 ( 64%)]  Loss:  3.629610 (3.4470)  Time: 1.110s,  922.35/s  (1.111s,  921.70/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [ 850/1251 ( 68%)]  Loss:  3.597528 (3.4554)  Time: 1.098s,  932.83/s  (1.111s,  921.55/s)  LR: 4.238e-04  Data: 0.013 (0.012)
Train: 77 [ 900/1251 ( 72%)]  Loss:  3.787779 (3.4729)  Time: 1.097s,  933.57/s  (1.111s,  921.83/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [ 950/1251 ( 76%)]  Loss:  3.287370 (3.4636)  Time: 1.092s,  938.09/s  (1.111s,  921.71/s)  LR: 4.238e-04  Data: 0.010 (0.012)
Train: 77 [1000/1251 ( 80%)]  Loss:  2.978492 (3.4405)  Time: 1.096s,  934.02/s  (1.111s,  922.00/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [1050/1251 ( 84%)]  Loss:  3.520707 (3.4441)  Time: 1.099s,  932.17/s  (1.110s,  922.11/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [1100/1251 ( 88%)]  Loss:  3.269885 (3.4366)  Time: 1.096s,  934.67/s  (1.110s,  922.48/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [1150/1251 ( 92%)]  Loss:  3.320682 (3.4317)  Time: 1.128s,  908.16/s  (1.111s,  922.03/s)  LR: 4.238e-04  Data: 0.011 (0.012)
Train: 77 [1200/1251 ( 96%)]  Loss:  3.351607 (3.4285)  Time: 1.095s,  934.89/s  (1.111s,  921.92/s)  LR: 4.238e-04  Data: 0.012 (0.012)
Train: 77 [1250/1251 (100%)]  Loss:  3.896534 (3.4465)  Time: 1.136s,  901.08/s  (1.111s,  921.98/s)  LR: 4.238e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.280 (3.280)  Loss:  0.5531 (0.5531)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.230 (0.400)  Loss:  0.7211 (1.1214)  Acc@1: 83.8443 (74.8160)  Acc@5: 96.4623 (92.6680)
Test (EMA): [   0/48]  Time: 3.199 (3.199)  Loss:  0.4750 (0.4750)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5961 (0.9738)  Acc@1: 85.6132 (77.0740)  Acc@5: 97.2877 (93.7120)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 76.3899999560547)

Train: 78 [   0/1251 (  0%)]  Loss:  3.435856 (3.4359)  Time: 1.103s,  928.34/s  (1.103s,  928.34/s)  LR: 4.219e-04  Data: 0.023 (0.023)
Train: 78 [  50/1251 (  4%)]  Loss:  3.716619 (3.5762)  Time: 1.101s,  929.94/s  (1.103s,  928.45/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 100/1251 (  8%)]  Loss:  3.300197 (3.4842)  Time: 1.098s,  932.73/s  (1.105s,  926.57/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 150/1251 ( 12%)]  Loss:  3.616760 (3.5174)  Time: 1.096s,  934.19/s  (1.104s,  927.27/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 200/1251 ( 16%)]  Loss:  3.550773 (3.5240)  Time: 1.127s,  908.43/s  (1.106s,  925.65/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 250/1251 ( 20%)]  Loss:  3.499189 (3.5199)  Time: 1.103s,  928.11/s  (1.111s,  921.79/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 300/1251 ( 24%)]  Loss:  3.608886 (3.5326)  Time: 1.096s,  934.00/s  (1.110s,  922.73/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 350/1251 ( 28%)]  Loss:  3.435324 (3.5205)  Time: 1.124s,  910.85/s  (1.110s,  922.51/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 400/1251 ( 32%)]  Loss:  3.495935 (3.5177)  Time: 1.095s,  935.40/s  (1.109s,  923.29/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 450/1251 ( 36%)]  Loss:  3.466014 (3.5126)  Time: 1.098s,  932.39/s  (1.109s,  923.41/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 500/1251 ( 40%)]  Loss:  3.186690 (3.4829)  Time: 1.097s,  933.78/s  (1.108s,  923.93/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 550/1251 ( 44%)]  Loss:  3.341903 (3.4712)  Time: 1.095s,  935.11/s  (1.109s,  923.36/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 600/1251 ( 48%)]  Loss:  3.148857 (3.4464)  Time: 1.131s,  905.30/s  (1.108s,  923.91/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 650/1251 ( 52%)]  Loss:  3.748901 (3.4680)  Time: 1.101s,  929.87/s  (1.109s,  923.77/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 700/1251 ( 56%)]  Loss:  3.668781 (3.4814)  Time: 1.097s,  933.57/s  (1.108s,  924.22/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [ 750/1251 ( 60%)]  Loss:  3.745019 (3.4979)  Time: 1.130s,  906.16/s  (1.108s,  923.78/s)  LR: 4.219e-04  Data: 0.013 (0.012)
Train: 78 [ 800/1251 ( 64%)]  Loss:  3.364249 (3.4900)  Time: 1.116s,  917.21/s  (1.109s,  923.12/s)  LR: 4.219e-04  Data: 0.010 (0.012)
Train: 78 [ 850/1251 ( 68%)]  Loss:  3.680530 (3.5006)  Time: 1.099s,  931.81/s  (1.109s,  923.19/s)  LR: 4.219e-04  Data: 0.010 (0.012)
Train: 78 [ 900/1251 ( 72%)]  Loss:  3.274948 (3.4887)  Time: 1.099s,  932.17/s  (1.109s,  923.27/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [ 950/1251 ( 76%)]  Loss:  3.389560 (3.4837)  Time: 1.121s,  913.81/s  (1.109s,  923.51/s)  LR: 4.219e-04  Data: 0.010 (0.012)
Train: 78 [1000/1251 ( 80%)]  Loss:  3.646884 (3.4915)  Time: 1.098s,  932.68/s  (1.109s,  923.51/s)  LR: 4.219e-04  Data: 0.011 (0.012)
Train: 78 [1050/1251 ( 84%)]  Loss:  3.543272 (3.4939)  Time: 1.098s,  932.66/s  (1.109s,  923.43/s)  LR: 4.219e-04  Data: 0.014 (0.012)
Train: 78 [1100/1251 ( 88%)]  Loss:  3.694566 (3.5026)  Time: 1.091s,  938.39/s  (1.109s,  923.41/s)  LR: 4.219e-04  Data: 0.010 (0.012)
Train: 78 [1150/1251 ( 92%)]  Loss:  3.391664 (3.4980)  Time: 1.095s,  934.97/s  (1.109s,  923.64/s)  LR: 4.219e-04  Data: 0.012 (0.012)
Train: 78 [1200/1251 ( 96%)]  Loss:  3.563202 (3.5006)  Time: 1.097s,  933.53/s  (1.109s,  923.71/s)  LR: 4.219e-04  Data: 0.010 (0.012)
Train: 78 [1250/1251 (100%)]  Loss:  3.438875 (3.4982)  Time: 1.083s,  945.29/s  (1.109s,  923.63/s)  LR: 4.219e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.346 (3.346)  Loss:  0.6123 (0.6123)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.6187 (1.0857)  Acc@1: 86.0849 (75.2580)  Acc@5: 97.4057 (92.9220)
Test (EMA): [   0/48]  Time: 3.239 (3.239)  Loss:  0.4730 (0.4730)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.6562 (97.6562)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.5946 (0.9706)  Acc@1: 85.8491 (77.1380)  Acc@5: 97.4057 (93.7600)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 76.54600000732422)

Train: 79 [   0/1251 (  0%)]  Loss:  3.385446 (3.3854)  Time: 1.137s,  900.99/s  (1.137s,  900.99/s)  LR: 4.200e-04  Data: 0.029 (0.029)
Train: 79 [  50/1251 (  4%)]  Loss:  3.505886 (3.4457)  Time: 1.192s,  859.25/s  (1.110s,  922.71/s)  LR: 4.200e-04  Data: 0.012 (0.012)
Train: 79 [ 100/1251 (  8%)]  Loss:  3.778430 (3.5566)  Time: 1.099s,  932.14/s  (1.113s,  920.07/s)  LR: 4.200e-04  Data: 0.014 (0.012)
Train: 79 [ 150/1251 ( 12%)]  Loss:  3.248153 (3.4795)  Time: 1.097s,  933.37/s  (1.109s,  922.98/s)  LR: 4.200e-04  Data: 0.013 (0.012)
Train: 79 [ 200/1251 ( 16%)]  Loss:  3.417702 (3.4671)  Time: 1.096s,  933.88/s  (1.111s,  921.84/s)  LR: 4.200e-04  Data: 0.014 (0.012)
Train: 79 [ 250/1251 ( 20%)]  Loss:  3.303440 (3.4398)  Time: 1.198s,  854.83/s  (1.111s,  921.76/s)  LR: 4.200e-04  Data: 0.012 (0.012)
Train: 79 [ 300/1251 ( 24%)]  Loss:  3.432231 (3.4388)  Time: 1.097s,  933.58/s  (1.110s,  922.82/s)  LR: 4.200e-04  Data: 0.012 (0.012)
Train: 79 [ 350/1251 ( 28%)]  Loss:  3.506711 (3.4472)  Time: 1.103s,  928.45/s  (1.109s,  923.13/s)  LR: 4.200e-04  Data: 0.010 (0.012)
Train: 79 [ 400/1251 ( 32%)]  Loss:  3.608082 (3.4651)  Time: 1.100s,  930.80/s  (1.109s,  922.97/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [ 450/1251 ( 36%)]  Loss:  3.690506 (3.4877)  Time: 1.111s,  921.73/s  (1.110s,  922.80/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [ 500/1251 ( 40%)]  Loss:  3.533535 (3.4918)  Time: 1.098s,  932.84/s  (1.110s,  922.93/s)  LR: 4.200e-04  Data: 0.012 (0.012)
Train: 79 [ 550/1251 ( 44%)]  Loss:  3.426577 (3.4864)  Time: 1.099s,  931.84/s  (1.109s,  923.57/s)  LR: 4.200e-04  Data: 0.010 (0.012)
Train: 79 [ 600/1251 ( 48%)]  Loss:  3.788050 (3.5096)  Time: 1.091s,  938.91/s  (1.109s,  923.53/s)  LR: 4.200e-04  Data: 0.009 (0.012)
Train: 79 [ 650/1251 ( 52%)]  Loss:  3.419268 (3.5031)  Time: 1.097s,  933.49/s  (1.109s,  922.99/s)  LR: 4.200e-04  Data: 0.012 (0.012)
Train: 79 [ 700/1251 ( 56%)]  Loss:  3.402171 (3.4964)  Time: 1.133s,  903.43/s  (1.109s,  923.00/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [ 750/1251 ( 60%)]  Loss:  3.585764 (3.5020)  Time: 1.097s,  933.29/s  (1.110s,  922.76/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [ 800/1251 ( 64%)]  Loss:  3.220721 (3.4855)  Time: 1.095s,  934.85/s  (1.109s,  923.00/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [ 850/1251 ( 68%)]  Loss:  3.598298 (3.4917)  Time: 1.096s,  934.21/s  (1.109s,  923.22/s)  LR: 4.200e-04  Data: 0.013 (0.012)
Train: 79 [ 900/1251 ( 72%)]  Loss:  3.553743 (3.4950)  Time: 1.096s,  934.07/s  (1.109s,  923.37/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [ 950/1251 ( 76%)]  Loss:  3.468517 (3.4937)  Time: 1.101s,  929.93/s  (1.109s,  923.25/s)  LR: 4.200e-04  Data: 0.010 (0.012)
Train: 79 [1000/1251 ( 80%)]  Loss:  3.352317 (3.4869)  Time: 1.095s,  935.36/s  (1.109s,  923.56/s)  LR: 4.200e-04  Data: 0.012 (0.012)
Train: 79 [1050/1251 ( 84%)]  Loss:  3.303873 (3.4786)  Time: 1.096s,  934.71/s  (1.109s,  923.37/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [1100/1251 ( 88%)]  Loss:  3.957641 (3.4994)  Time: 1.121s,  913.53/s  (1.109s,  923.03/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Train: 79 [1150/1251 ( 92%)]  Loss:  3.278161 (3.4902)  Time: 1.133s,  903.99/s  (1.110s,  922.84/s)  LR: 4.200e-04  Data: 0.013 (0.012)
Train: 79 [1200/1251 ( 96%)]  Loss:  3.638468 (3.4961)  Time: 1.131s,  905.79/s  (1.109s,  922.97/s)  LR: 4.200e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 79 [1250/1251 (100%)]  Loss:  3.510540 (3.4967)  Time: 1.050s,  974.92/s  (1.109s,  923.00/s)  LR: 4.200e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.292 (3.292)  Loss:  0.5259 (0.5259)  Acc@1: 89.3555 (89.3555)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6375 (1.0785)  Acc@1: 85.8491 (75.2700)  Acc@5: 97.5236 (92.8760)
Test (EMA): [   0/48]  Time: 3.149 (3.149)  Loss:  0.4723 (0.4723)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5925 (0.9676)  Acc@1: 85.7311 (77.2040)  Acc@5: 97.4057 (93.8040)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 76.60200000732422)

Train: 80 [   0/1251 (  0%)]  Loss:  3.582058 (3.5821)  Time: 1.106s,  925.79/s  (1.106s,  925.79/s)  LR: 4.181e-04  Data: 0.021 (0.021)
Train: 80 [  50/1251 (  4%)]  Loss:  3.723448 (3.6528)  Time: 1.096s,  934.66/s  (1.103s,  928.42/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [ 100/1251 (  8%)]  Loss:  3.726089 (3.6772)  Time: 1.102s,  928.88/s  (1.112s,  921.01/s)  LR: 4.181e-04  Data: 0.010 (0.012)
Train: 80 [ 150/1251 ( 12%)]  Loss:  3.411262 (3.6107)  Time: 1.135s,  902.11/s  (1.112s,  920.50/s)  LR: 4.181e-04  Data: 0.014 (0.011)
Train: 80 [ 200/1251 ( 16%)]  Loss:  3.294191 (3.5474)  Time: 1.195s,  856.93/s  (1.114s,  919.44/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [ 250/1251 ( 20%)]  Loss:  3.554115 (3.5485)  Time: 1.101s,  929.99/s  (1.113s,  919.89/s)  LR: 4.181e-04  Data: 0.010 (0.011)
Train: 80 [ 300/1251 ( 24%)]  Loss:  3.959883 (3.6073)  Time: 1.120s,  914.12/s  (1.113s,  920.22/s)  LR: 4.181e-04  Data: 0.012 (0.011)
Train: 80 [ 350/1251 ( 28%)]  Loss:  3.534304 (3.5982)  Time: 1.100s,  930.79/s  (1.113s,  920.22/s)  LR: 4.181e-04  Data: 0.012 (0.011)
Train: 80 [ 400/1251 ( 32%)]  Loss:  3.607821 (3.5992)  Time: 1.094s,  936.15/s  (1.112s,  920.92/s)  LR: 4.181e-04  Data: 0.011 (0.011)
Train: 80 [ 450/1251 ( 36%)]  Loss:  3.560428 (3.5954)  Time: 1.096s,  934.24/s  (1.111s,  921.38/s)  LR: 4.181e-04  Data: 0.011 (0.011)
Train: 80 [ 500/1251 ( 40%)]  Loss:  3.580841 (3.5940)  Time: 1.098s,  932.53/s  (1.110s,  922.32/s)  LR: 4.181e-04  Data: 0.013 (0.012)
Train: 80 [ 550/1251 ( 44%)]  Loss:  3.289020 (3.5686)  Time: 1.129s,  906.88/s  (1.110s,  922.16/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [ 600/1251 ( 48%)]  Loss:  3.508903 (3.5640)  Time: 1.098s,  932.80/s  (1.110s,  922.25/s)  LR: 4.181e-04  Data: 0.014 (0.012)
Train: 80 [ 650/1251 ( 52%)]  Loss:  3.465973 (3.5570)  Time: 1.092s,  937.32/s  (1.110s,  922.61/s)  LR: 4.181e-04  Data: 0.010 (0.011)
Train: 80 [ 700/1251 ( 56%)]  Loss:  3.270217 (3.5379)  Time: 1.101s,  930.01/s  (1.110s,  922.76/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [ 750/1251 ( 60%)]  Loss:  3.273951 (3.5214)  Time: 1.095s,  935.44/s  (1.109s,  923.00/s)  LR: 4.181e-04  Data: 0.010 (0.012)
Train: 80 [ 800/1251 ( 64%)]  Loss:  3.543195 (3.5227)  Time: 1.097s,  933.42/s  (1.109s,  923.27/s)  LR: 4.181e-04  Data: 0.013 (0.012)
Train: 80 [ 850/1251 ( 68%)]  Loss:  3.163133 (3.5027)  Time: 1.096s,  934.45/s  (1.109s,  923.74/s)  LR: 4.181e-04  Data: 0.012 (0.012)
Train: 80 [ 900/1251 ( 72%)]  Loss:  3.510691 (3.5031)  Time: 1.097s,  933.20/s  (1.109s,  923.67/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [ 950/1251 ( 76%)]  Loss:  3.588377 (3.5074)  Time: 1.097s,  933.10/s  (1.108s,  923.96/s)  LR: 4.181e-04  Data: 0.012 (0.012)
Train: 80 [1000/1251 ( 80%)]  Loss:  3.360374 (3.5004)  Time: 1.120s,  913.99/s  (1.108s,  923.87/s)  LR: 4.181e-04  Data: 0.012 (0.012)
Train: 80 [1050/1251 ( 84%)]  Loss:  3.778513 (3.5130)  Time: 1.132s,  904.81/s  (1.108s,  923.84/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [1100/1251 ( 88%)]  Loss:  3.413197 (3.5087)  Time: 1.128s,  907.70/s  (1.108s,  923.79/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [1150/1251 ( 92%)]  Loss:  4.006891 (3.5295)  Time: 1.143s,  896.16/s  (1.109s,  923.26/s)  LR: 4.181e-04  Data: 0.011 (0.012)
Train: 80 [1200/1251 ( 96%)]  Loss:  3.692600 (3.5360)  Time: 1.098s,  933.01/s  (1.109s,  923.41/s)  LR: 4.181e-04  Data: 0.014 (0.012)
Train: 80 [1250/1251 (100%)]  Loss:  3.285564 (3.5263)  Time: 1.081s,  947.20/s  (1.109s,  923.45/s)  LR: 4.181e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.191 (3.191)  Loss:  0.5513 (0.5513)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6701 (1.0861)  Acc@1: 85.0236 (75.1700)  Acc@5: 97.5236 (92.7600)
Test (EMA): [   0/48]  Time: 3.116 (3.116)  Loss:  0.4709 (0.4709)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5899 (0.9647)  Acc@1: 85.9670 (77.3140)  Acc@5: 97.5236 (93.8120)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 76.61600003417969)

Train: 81 [   0/1251 (  0%)]  Loss:  3.457248 (3.4572)  Time: 1.102s,  929.45/s  (1.102s,  929.45/s)  LR: 4.162e-04  Data: 0.020 (0.020)
Train: 81 [  50/1251 (  4%)]  Loss:  3.440843 (3.4490)  Time: 1.095s,  935.05/s  (1.104s,  927.20/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [ 100/1251 (  8%)]  Loss:  3.406960 (3.4350)  Time: 1.097s,  933.23/s  (1.105s,  926.53/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [ 150/1251 ( 12%)]  Loss:  3.818567 (3.5309)  Time: 1.192s,  858.72/s  (1.105s,  926.35/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [ 200/1251 ( 16%)]  Loss:  3.587460 (3.5422)  Time: 1.101s,  930.31/s  (1.106s,  925.97/s)  LR: 4.162e-04  Data: 0.010 (0.012)
Train: 81 [ 250/1251 ( 20%)]  Loss:  3.562274 (3.5456)  Time: 1.098s,  933.01/s  (1.106s,  926.16/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [ 300/1251 ( 24%)]  Loss:  3.721549 (3.5707)  Time: 1.102s,  929.61/s  (1.106s,  925.58/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [ 350/1251 ( 28%)]  Loss:  3.477836 (3.5591)  Time: 1.097s,  933.58/s  (1.105s,  926.40/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [ 400/1251 ( 32%)]  Loss:  3.684125 (3.5730)  Time: 1.096s,  934.25/s  (1.108s,  924.30/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [ 450/1251 ( 36%)]  Loss:  3.968832 (3.6126)  Time: 1.106s,  926.03/s  (1.108s,  924.55/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [ 500/1251 ( 40%)]  Loss:  3.920291 (3.6405)  Time: 1.096s,  934.19/s  (1.107s,  924.66/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [ 550/1251 ( 44%)]  Loss:  3.556716 (3.6336)  Time: 1.097s,  933.27/s  (1.108s,  924.26/s)  LR: 4.162e-04  Data: 0.014 (0.012)
Train: 81 [ 600/1251 ( 48%)]  Loss:  3.712878 (3.6397)  Time: 1.121s,  913.60/s  (1.109s,  923.12/s)  LR: 4.162e-04  Data: 0.010 (0.012)
Train: 81 [ 650/1251 ( 52%)]  Loss:  3.564841 (3.6343)  Time: 1.135s,  902.55/s  (1.110s,  922.31/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [ 700/1251 ( 56%)]  Loss:  3.135588 (3.6011)  Time: 1.103s,  928.28/s  (1.110s,  922.60/s)  LR: 4.162e-04  Data: 0.013 (0.012)
Train: 81 [ 750/1251 ( 60%)]  Loss:  3.309191 (3.5828)  Time: 1.106s,  925.51/s  (1.109s,  922.95/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [ 800/1251 ( 64%)]  Loss:  3.426156 (3.5736)  Time: 1.091s,  938.53/s  (1.110s,  922.89/s)  LR: 4.162e-04  Data: 0.009 (0.012)
Train: 81 [ 850/1251 ( 68%)]  Loss:  3.691877 (3.5802)  Time: 1.101s,  930.33/s  (1.110s,  922.85/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 81 [ 900/1251 ( 72%)]  Loss:  3.627003 (3.5826)  Time: 1.100s,  930.86/s  (1.109s,  923.18/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [ 950/1251 ( 76%)]  Loss:  3.240913 (3.5656)  Time: 1.098s,  932.65/s  (1.110s,  922.83/s)  LR: 4.162e-04  Data: 0.013 (0.012)
Train: 81 [1000/1251 ( 80%)]  Loss:  3.377474 (3.5566)  Time: 1.092s,  937.91/s  (1.109s,  923.12/s)  LR: 4.162e-04  Data: 0.010 (0.012)
Train: 81 [1050/1251 ( 84%)]  Loss:  3.835131 (3.5693)  Time: 1.094s,  935.76/s  (1.109s,  923.03/s)  LR: 4.162e-04  Data: 0.011 (0.012)
Train: 81 [1100/1251 ( 88%)]  Loss:  3.739644 (3.5767)  Time: 1.132s,  904.56/s  (1.110s,  922.55/s)  LR: 4.162e-04  Data: 0.012 (0.012)
Train: 81 [1150/1251 ( 92%)]  Loss:  3.527473 (3.5746)  Time: 1.120s,  914.67/s  (1.110s,  922.52/s)  LR: 4.162e-04  Data: 0.013 (0.012)
Train: 81 [1200/1251 ( 96%)]  Loss:  3.395877 (3.5675)  Time: 1.119s,  914.73/s  (1.111s,  921.79/s)  LR: 4.162e-04  Data: 0.010 (0.012)
Train: 81 [1250/1251 (100%)]  Loss:  3.774433 (3.5754)  Time: 1.078s,  950.19/s  (1.111s,  921.53/s)  LR: 4.162e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.228 (3.228)  Loss:  0.5818 (0.5818)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6769 (1.1015)  Acc@1: 85.1415 (75.2820)  Acc@5: 97.4057 (92.8060)
Test (EMA): [   0/48]  Time: 3.228 (3.228)  Loss:  0.4697 (0.4697)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5872 (0.9619)  Acc@1: 86.0849 (77.2960)  Acc@5: 97.5236 (93.8540)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 76.71600000732421)

Train: 82 [   0/1251 (  0%)]  Loss:  3.533370 (3.5334)  Time: 1.102s,  929.41/s  (1.102s,  929.41/s)  LR: 4.142e-04  Data: 0.020 (0.020)
Train: 82 [  50/1251 (  4%)]  Loss:  3.694861 (3.6141)  Time: 1.189s,  861.09/s  (1.115s,  918.32/s)  LR: 4.142e-04  Data: 0.010 (0.011)
Train: 82 [ 100/1251 (  8%)]  Loss:  3.767914 (3.6654)  Time: 1.193s,  858.66/s  (1.111s,  921.48/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 150/1251 ( 12%)]  Loss:  3.702423 (3.6746)  Time: 1.102s,  929.30/s  (1.114s,  919.36/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 200/1251 ( 16%)]  Loss:  3.805217 (3.7008)  Time: 1.122s,  912.91/s  (1.113s,  919.76/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 250/1251 ( 20%)]  Loss:  3.651422 (3.6925)  Time: 1.107s,  925.31/s  (1.113s,  919.80/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 300/1251 ( 24%)]  Loss:  3.189841 (3.6207)  Time: 1.099s,  931.80/s  (1.111s,  921.44/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 350/1251 ( 28%)]  Loss:  3.360803 (3.5882)  Time: 1.099s,  931.49/s  (1.110s,  922.14/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 400/1251 ( 32%)]  Loss:  3.579933 (3.5873)  Time: 1.098s,  932.78/s  (1.112s,  921.15/s)  LR: 4.142e-04  Data: 0.014 (0.012)
Train: 82 [ 450/1251 ( 36%)]  Loss:  3.474653 (3.5760)  Time: 1.194s,  857.48/s  (1.112s,  921.25/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 500/1251 ( 40%)]  Loss:  2.840363 (3.5092)  Time: 1.095s,  935.11/s  (1.111s,  921.83/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 550/1251 ( 44%)]  Loss:  3.375710 (3.4980)  Time: 1.098s,  933.00/s  (1.111s,  921.95/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 600/1251 ( 48%)]  Loss:  3.331936 (3.4853)  Time: 1.097s,  933.53/s  (1.111s,  922.01/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 650/1251 ( 52%)]  Loss:  3.330146 (3.4742)  Time: 1.096s,  934.65/s  (1.110s,  922.33/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 700/1251 ( 56%)]  Loss:  3.720550 (3.4906)  Time: 1.102s,  929.22/s  (1.110s,  922.41/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [ 750/1251 ( 60%)]  Loss:  3.663527 (3.5014)  Time: 1.129s,  906.99/s  (1.110s,  922.67/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 800/1251 ( 64%)]  Loss:  3.540344 (3.5037)  Time: 1.101s,  930.44/s  (1.110s,  922.61/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 850/1251 ( 68%)]  Loss:  3.548433 (3.5062)  Time: 1.101s,  929.81/s  (1.109s,  923.01/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 900/1251 ( 72%)]  Loss:  3.689630 (3.5158)  Time: 1.106s,  926.25/s  (1.110s,  922.93/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [ 950/1251 ( 76%)]  Loss:  3.551261 (3.5176)  Time: 1.103s,  928.11/s  (1.109s,  923.27/s)  LR: 4.142e-04  Data: 0.012 (0.012)
Train: 82 [1000/1251 ( 80%)]  Loss:  3.618921 (3.5224)  Time: 1.097s,  933.32/s  (1.110s,  922.86/s)  LR: 4.142e-04  Data: 0.013 (0.012)
Train: 82 [1050/1251 ( 84%)]  Loss:  3.544951 (3.5235)  Time: 1.097s,  933.25/s  (1.109s,  923.20/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [1100/1251 ( 88%)]  Loss:  3.357606 (3.5163)  Time: 1.099s,  931.67/s  (1.109s,  923.37/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [1150/1251 ( 92%)]  Loss:  3.604836 (3.5199)  Time: 1.097s,  933.38/s  (1.109s,  923.39/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [1200/1251 ( 96%)]  Loss:  3.179781 (3.5063)  Time: 1.111s,  922.08/s  (1.109s,  923.30/s)  LR: 4.142e-04  Data: 0.011 (0.012)
Train: 82 [1250/1251 (100%)]  Loss:  3.844187 (3.5193)  Time: 1.079s,  948.71/s  (1.109s,  923.15/s)  LR: 4.142e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.269 (3.269)  Loss:  0.5337 (0.5337)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6945 (1.0703)  Acc@1: 84.5519 (75.5680)  Acc@5: 96.5802 (92.9080)
Test (EMA): [   0/48]  Time: 3.202 (3.202)  Loss:  0.4681 (0.4681)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5861 (0.9593)  Acc@1: 86.0849 (77.3820)  Acc@5: 97.6415 (93.9060)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 76.79199995605468)

Train: 83 [   0/1251 (  0%)]  Loss:  3.298765 (3.2988)  Time: 1.105s,  926.54/s  (1.105s,  926.54/s)  LR: 4.123e-04  Data: 0.023 (0.023)
Train: 83 [  50/1251 (  4%)]  Loss:  3.417774 (3.3583)  Time: 1.205s,  849.95/s  (1.112s,  920.95/s)  LR: 4.123e-04  Data: 0.016 (0.012)
Train: 83 [ 100/1251 (  8%)]  Loss:  3.307652 (3.3414)  Time: 1.101s,  930.16/s  (1.112s,  920.54/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 150/1251 ( 12%)]  Loss:  3.656662 (3.4202)  Time: 1.099s,  931.48/s  (1.113s,  920.36/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [ 200/1251 ( 16%)]  Loss:  3.341320 (3.4044)  Time: 1.117s,  916.50/s  (1.114s,  918.81/s)  LR: 4.123e-04  Data: 0.010 (0.012)
Train: 83 [ 250/1251 ( 20%)]  Loss:  3.504355 (3.4211)  Time: 1.125s,  910.57/s  (1.113s,  920.09/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 300/1251 ( 24%)]  Loss:  3.349073 (3.4108)  Time: 1.095s,  935.03/s  (1.114s,  919.61/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [ 350/1251 ( 28%)]  Loss:  3.768962 (3.4556)  Time: 1.098s,  932.74/s  (1.112s,  921.08/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 83 [ 400/1251 ( 32%)]  Loss:  3.435560 (3.4533)  Time: 1.096s,  934.62/s  (1.112s,  920.99/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [ 450/1251 ( 36%)]  Loss:  3.457952 (3.4538)  Time: 1.101s,  929.73/s  (1.111s,  921.90/s)  LR: 4.123e-04  Data: 0.016 (0.012)
Train: 83 [ 500/1251 ( 40%)]  Loss:  3.310678 (3.4408)  Time: 1.098s,  932.55/s  (1.110s,  922.34/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 550/1251 ( 44%)]  Loss:  3.582289 (3.4526)  Time: 1.126s,  909.10/s  (1.111s,  922.04/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [ 600/1251 ( 48%)]  Loss:  3.590372 (3.4632)  Time: 1.137s,  900.54/s  (1.110s,  922.56/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [ 650/1251 ( 52%)]  Loss:  3.456068 (3.4627)  Time: 1.194s,  857.74/s  (1.110s,  922.73/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 700/1251 ( 56%)]  Loss:  3.625045 (3.4735)  Time: 1.101s,  930.12/s  (1.109s,  923.25/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 750/1251 ( 60%)]  Loss:  3.327156 (3.4644)  Time: 1.099s,  931.93/s  (1.109s,  923.16/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 800/1251 ( 64%)]  Loss:  3.598792 (3.4723)  Time: 1.097s,  933.31/s  (1.109s,  923.58/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [ 850/1251 ( 68%)]  Loss:  3.521026 (3.4750)  Time: 1.119s,  914.73/s  (1.109s,  923.42/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 900/1251 ( 72%)]  Loss:  3.308222 (3.4662)  Time: 1.133s,  903.68/s  (1.110s,  922.80/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [ 950/1251 ( 76%)]  Loss:  3.533821 (3.4696)  Time: 1.097s,  933.21/s  (1.109s,  923.01/s)  LR: 4.123e-04  Data: 0.013 (0.012)
Train: 83 [1000/1251 ( 80%)]  Loss:  3.566659 (3.4742)  Time: 1.126s,  909.72/s  (1.110s,  922.91/s)  LR: 4.123e-04  Data: 0.013 (0.012)
Train: 83 [1050/1251 ( 84%)]  Loss:  3.289020 (3.4658)  Time: 1.096s,  934.00/s  (1.110s,  922.32/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [1100/1251 ( 88%)]  Loss:  3.542524 (3.4691)  Time: 1.126s,  909.28/s  (1.110s,  922.41/s)  LR: 4.123e-04  Data: 0.011 (0.012)
Train: 83 [1150/1251 ( 92%)]  Loss:  3.671915 (3.4776)  Time: 1.132s,  904.45/s  (1.111s,  922.03/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [1200/1251 ( 96%)]  Loss:  3.422501 (3.4754)  Time: 1.097s,  933.65/s  (1.110s,  922.18/s)  LR: 4.123e-04  Data: 0.012 (0.012)
Train: 83 [1250/1251 (100%)]  Loss:  3.701269 (3.4841)  Time: 1.082s,  946.24/s  (1.110s,  922.39/s)  LR: 4.123e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.239 (3.239)  Loss:  0.5414 (0.5414)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.7035 (1.0810)  Acc@1: 84.6698 (75.4420)  Acc@5: 96.5802 (92.9960)
Test (EMA): [   0/48]  Time: 3.113 (3.113)  Loss:  0.4668 (0.4668)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5858 (0.9566)  Acc@1: 86.2028 (77.4840)  Acc@5: 97.7594 (93.9480)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 76.85600000732421)

Train: 84 [   0/1251 (  0%)]  Loss:  3.257507 (3.2575)  Time: 1.100s,  930.58/s  (1.100s,  930.58/s)  LR: 4.103e-04  Data: 0.019 (0.019)
Train: 84 [  50/1251 (  4%)]  Loss:  3.482076 (3.3698)  Time: 1.097s,  933.17/s  (1.108s,  924.33/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 100/1251 (  8%)]  Loss:  3.274521 (3.3380)  Time: 1.102s,  928.83/s  (1.109s,  922.94/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 150/1251 ( 12%)]  Loss:  3.690677 (3.4262)  Time: 1.096s,  934.45/s  (1.108s,  924.29/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 200/1251 ( 16%)]  Loss:  3.473653 (3.4357)  Time: 1.101s,  929.83/s  (1.106s,  925.98/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 250/1251 ( 20%)]  Loss:  3.385514 (3.4273)  Time: 1.126s,  909.10/s  (1.107s,  924.72/s)  LR: 4.103e-04  Data: 0.014 (0.012)
Train: 84 [ 300/1251 ( 24%)]  Loss:  3.305487 (3.4099)  Time: 1.098s,  932.97/s  (1.108s,  924.08/s)  LR: 4.103e-04  Data: 0.014 (0.012)
Train: 84 [ 350/1251 ( 28%)]  Loss:  3.747981 (3.4522)  Time: 1.097s,  933.77/s  (1.108s,  923.85/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 400/1251 ( 32%)]  Loss:  3.578133 (3.4662)  Time: 1.099s,  931.90/s  (1.108s,  924.22/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 450/1251 ( 36%)]  Loss:  3.755065 (3.4951)  Time: 1.097s,  933.72/s  (1.108s,  924.46/s)  LR: 4.103e-04  Data: 0.012 (0.012)
Train: 84 [ 500/1251 ( 40%)]  Loss:  3.295623 (3.4769)  Time: 1.121s,  913.62/s  (1.107s,  924.84/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 550/1251 ( 44%)]  Loss:  3.517258 (3.4803)  Time: 1.100s,  931.04/s  (1.107s,  925.42/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 600/1251 ( 48%)]  Loss:  3.129848 (3.4533)  Time: 1.099s,  931.33/s  (1.107s,  925.19/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 650/1251 ( 52%)]  Loss:  3.597993 (3.4637)  Time: 1.123s,  911.55/s  (1.107s,  925.19/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 700/1251 ( 56%)]  Loss:  3.812397 (3.4869)  Time: 1.120s,  914.11/s  (1.108s,  924.51/s)  LR: 4.103e-04  Data: 0.012 (0.012)
Train: 84 [ 750/1251 ( 60%)]  Loss:  2.983527 (3.4555)  Time: 1.119s,  914.79/s  (1.108s,  924.41/s)  LR: 4.103e-04  Data: 0.010 (0.012)
Train: 84 [ 800/1251 ( 64%)]  Loss:  3.573554 (3.4624)  Time: 1.096s,  934.26/s  (1.108s,  924.16/s)  LR: 4.103e-04  Data: 0.012 (0.012)
Train: 84 [ 850/1251 ( 68%)]  Loss:  3.271109 (3.4518)  Time: 1.096s,  934.48/s  (1.108s,  924.33/s)  LR: 4.103e-04  Data: 0.012 (0.012)
Train: 84 [ 900/1251 ( 72%)]  Loss:  3.679870 (3.4638)  Time: 1.120s,  914.04/s  (1.108s,  924.42/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [ 950/1251 ( 76%)]  Loss:  3.301865 (3.4557)  Time: 1.097s,  933.53/s  (1.108s,  924.38/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [1000/1251 ( 80%)]  Loss:  3.258533 (3.4463)  Time: 1.195s,  856.60/s  (1.108s,  924.51/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [1050/1251 ( 84%)]  Loss:  3.296027 (3.4395)  Time: 1.105s,  926.62/s  (1.108s,  924.49/s)  LR: 4.103e-04  Data: 0.013 (0.012)
Train: 84 [1100/1251 ( 88%)]  Loss:  3.484257 (3.4414)  Time: 1.123s,  911.98/s  (1.108s,  924.46/s)  LR: 4.103e-04  Data: 0.012 (0.012)
Train: 84 [1150/1251 ( 92%)]  Loss:  3.531261 (3.4452)  Time: 1.102s,  929.17/s  (1.108s,  924.52/s)  LR: 4.103e-04  Data: 0.011 (0.012)
Train: 84 [1200/1251 ( 96%)]  Loss:  3.266691 (3.4380)  Time: 1.110s,  922.51/s  (1.107s,  924.69/s)  LR: 4.103e-04  Data: 0.010 (0.012)
Train: 84 [1250/1251 (100%)]  Loss:  3.552484 (3.4424)  Time: 1.082s,  946.47/s  (1.108s,  924.55/s)  LR: 4.103e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.207 (3.207)  Loss:  0.5362 (0.5362)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.7010 (1.0763)  Acc@1: 84.1981 (75.7340)  Acc@5: 96.1085 (93.0200)
Test (EMA): [   0/48]  Time: 3.151 (3.151)  Loss:  0.4659 (0.4659)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5860 (0.9542)  Acc@1: 86.2028 (77.5480)  Acc@5: 97.7594 (93.9880)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 76.94400000732422)

Train: 85 [   0/1251 (  0%)]  Loss:  3.442825 (3.4428)  Time: 1.102s,  929.39/s  (1.102s,  929.39/s)  LR: 4.083e-04  Data: 0.020 (0.020)
Train: 85 [  50/1251 (  4%)]  Loss:  3.946316 (3.6946)  Time: 1.098s,  932.18/s  (1.104s,  927.66/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 100/1251 (  8%)]  Loss:  3.497764 (3.6290)  Time: 1.101s,  929.87/s  (1.111s,  922.02/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 150/1251 ( 12%)]  Loss:  3.432037 (3.5797)  Time: 1.095s,  934.96/s  (1.108s,  924.20/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 200/1251 ( 16%)]  Loss:  3.419195 (3.5476)  Time: 1.102s,  929.60/s  (1.108s,  924.01/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 250/1251 ( 20%)]  Loss:  3.524976 (3.5439)  Time: 1.120s,  914.36/s  (1.107s,  924.99/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 300/1251 ( 24%)]  Loss:  3.166706 (3.4900)  Time: 1.097s,  933.82/s  (1.108s,  924.52/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 350/1251 ( 28%)]  Loss:  3.436529 (3.4833)  Time: 1.096s,  934.30/s  (1.107s,  924.92/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 400/1251 ( 32%)]  Loss:  3.279996 (3.4607)  Time: 1.126s,  909.61/s  (1.106s,  925.52/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 450/1251 ( 36%)]  Loss:  3.230750 (3.4377)  Time: 1.099s,  932.15/s  (1.106s,  925.71/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 500/1251 ( 40%)]  Loss:  3.448952 (3.4387)  Time: 1.096s,  934.48/s  (1.106s,  925.77/s)  LR: 4.083e-04  Data: 0.010 (0.012)
Train: 85 [ 550/1251 ( 44%)]  Loss:  3.442236 (3.4390)  Time: 1.096s,  934.73/s  (1.106s,  925.73/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 600/1251 ( 48%)]  Loss:  3.609933 (3.4522)  Time: 1.121s,  913.34/s  (1.106s,  925.90/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 650/1251 ( 52%)]  Loss:  3.544317 (3.4588)  Time: 1.113s,  919.91/s  (1.107s,  924.76/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 700/1251 ( 56%)]  Loss:  3.554653 (3.4651)  Time: 1.098s,  932.36/s  (1.107s,  924.67/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 750/1251 ( 60%)]  Loss:  3.441273 (3.4637)  Time: 1.096s,  934.44/s  (1.108s,  924.52/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [ 800/1251 ( 64%)]  Loss:  3.518102 (3.4669)  Time: 1.100s,  930.99/s  (1.107s,  924.78/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 850/1251 ( 68%)]  Loss:  3.646189 (3.4768)  Time: 1.097s,  933.34/s  (1.107s,  924.68/s)  LR: 4.083e-04  Data: 0.014 (0.012)
Train: 85 [ 900/1251 ( 72%)]  Loss:  3.342238 (3.4697)  Time: 1.096s,  934.33/s  (1.107s,  924.81/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [ 950/1251 ( 76%)]  Loss:  3.225465 (3.4575)  Time: 1.122s,  912.77/s  (1.107s,  924.64/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [1000/1251 ( 80%)]  Loss:  3.645341 (3.4665)  Time: 1.099s,  931.64/s  (1.107s,  924.62/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [1050/1251 ( 84%)]  Loss:  3.656240 (3.4751)  Time: 1.184s,  864.63/s  (1.107s,  924.70/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [1100/1251 ( 88%)]  Loss:  3.618978 (3.4813)  Time: 1.096s,  934.02/s  (1.107s,  924.75/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [1150/1251 ( 92%)]  Loss:  3.434813 (3.4794)  Time: 1.097s,  933.63/s  (1.107s,  924.76/s)  LR: 4.083e-04  Data: 0.011 (0.012)
Train: 85 [1200/1251 ( 96%)]  Loss:  3.388382 (3.4758)  Time: 1.098s,  932.77/s  (1.107s,  924.84/s)  LR: 4.083e-04  Data: 0.012 (0.012)
Train: 85 [1250/1251 (100%)]  Loss:  3.525059 (3.4777)  Time: 1.082s,  946.33/s  (1.107s,  925.03/s)  LR: 4.083e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.294 (3.294)  Loss:  0.5622 (0.5622)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.399)  Loss:  0.6627 (1.0695)  Acc@1: 85.4953 (75.2980)  Acc@5: 96.8160 (92.9220)
Test (EMA): [   0/48]  Time: 3.212 (3.212)  Loss:  0.4646 (0.4646)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5855 (0.9518)  Acc@1: 86.2028 (77.6040)  Acc@5: 97.6415 (93.9920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 77.05800000732422)

Train: 86 [   0/1251 (  0%)]  Loss:  3.657937 (3.6579)  Time: 1.100s,  930.58/s  (1.100s,  930.58/s)  LR: 4.062e-04  Data: 0.019 (0.019)
Train: 86 [  50/1251 (  4%)]  Loss:  3.579577 (3.6188)  Time: 1.092s,  938.00/s  (1.123s,  911.98/s)  LR: 4.062e-04  Data: 0.010 (0.011)
Train: 86 [ 100/1251 (  8%)]  Loss:  3.499501 (3.5790)  Time: 1.094s,  936.40/s  (1.111s,  921.68/s)  LR: 4.062e-04  Data: 0.010 (0.011)
Train: 86 [ 150/1251 ( 12%)]  Loss:  3.442260 (3.5448)  Time: 1.094s,  935.75/s  (1.111s,  921.75/s)  LR: 4.062e-04  Data: 0.009 (0.011)
Train: 86 [ 200/1251 ( 16%)]  Loss:  3.384998 (3.5129)  Time: 1.097s,  933.73/s  (1.108s,  924.02/s)  LR: 4.062e-04  Data: 0.013 (0.012)
Train: 86 [ 250/1251 ( 20%)]  Loss:  3.579472 (3.5240)  Time: 1.110s,  922.35/s  (1.108s,  923.88/s)  LR: 4.062e-04  Data: 0.011 (0.011)
Train: 86 [ 300/1251 ( 24%)]  Loss:  3.458778 (3.5146)  Time: 1.098s,  932.32/s  (1.108s,  924.57/s)  LR: 4.062e-04  Data: 0.011 (0.011)
Train: 86 [ 350/1251 ( 28%)]  Loss:  3.437214 (3.5050)  Time: 1.191s,  860.00/s  (1.109s,  923.21/s)  LR: 4.062e-04  Data: 0.012 (0.011)
Train: 86 [ 400/1251 ( 32%)]  Loss:  3.757959 (3.5331)  Time: 1.097s,  933.24/s  (1.109s,  923.66/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [ 450/1251 ( 36%)]  Loss:  3.553497 (3.5351)  Time: 1.103s,  928.57/s  (1.110s,  922.92/s)  LR: 4.062e-04  Data: 0.011 (0.011)
Train: 86 [ 500/1251 ( 40%)]  Loss:  3.522586 (3.5340)  Time: 1.096s,  934.15/s  (1.109s,  923.25/s)  LR: 4.062e-04  Data: 0.012 (0.011)
Train: 86 [ 550/1251 ( 44%)]  Loss:  3.695069 (3.5474)  Time: 1.115s,  918.68/s  (1.109s,  923.61/s)  LR: 4.062e-04  Data: 0.014 (0.012)
Train: 86 [ 600/1251 ( 48%)]  Loss:  3.455267 (3.5403)  Time: 1.097s,  933.28/s  (1.109s,  923.38/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 86 [ 650/1251 ( 52%)]  Loss:  3.620130 (3.5460)  Time: 1.104s,  927.22/s  (1.109s,  923.32/s)  LR: 4.062e-04  Data: 0.010 (0.012)
Train: 86 [ 700/1251 ( 56%)]  Loss:  3.468005 (3.5408)  Time: 1.123s,  911.91/s  (1.109s,  923.36/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [ 750/1251 ( 60%)]  Loss:  3.766775 (3.5549)  Time: 1.097s,  933.40/s  (1.109s,  923.64/s)  LR: 4.062e-04  Data: 0.012 (0.012)
Train: 86 [ 800/1251 ( 64%)]  Loss:  3.336955 (3.5421)  Time: 1.096s,  934.04/s  (1.108s,  923.79/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [ 850/1251 ( 68%)]  Loss:  3.524114 (3.5411)  Time: 1.098s,  932.43/s  (1.108s,  924.16/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [ 900/1251 ( 72%)]  Loss:  3.575011 (3.5429)  Time: 1.125s,  910.37/s  (1.108s,  923.80/s)  LR: 4.062e-04  Data: 0.014 (0.012)
Train: 86 [ 950/1251 ( 76%)]  Loss:  3.568506 (3.5442)  Time: 1.106s,  925.86/s  (1.108s,  923.78/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [1000/1251 ( 80%)]  Loss:  3.573333 (3.5456)  Time: 1.097s,  933.22/s  (1.109s,  923.70/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [1050/1251 ( 84%)]  Loss:  3.535826 (3.5451)  Time: 1.100s,  930.67/s  (1.109s,  923.70/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [1100/1251 ( 88%)]  Loss:  3.367826 (3.5374)  Time: 1.097s,  933.75/s  (1.108s,  924.05/s)  LR: 4.062e-04  Data: 0.013 (0.012)
Train: 86 [1150/1251 ( 92%)]  Loss:  3.479527 (3.5350)  Time: 1.097s,  933.29/s  (1.109s,  923.72/s)  LR: 4.062e-04  Data: 0.013 (0.012)
Train: 86 [1200/1251 ( 96%)]  Loss:  3.644739 (3.5394)  Time: 1.095s,  934.83/s  (1.108s,  923.99/s)  LR: 4.062e-04  Data: 0.011 (0.012)
Train: 86 [1250/1251 (100%)]  Loss:  3.354657 (3.5323)  Time: 1.085s,  944.03/s  (1.109s,  923.62/s)  LR: 4.062e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.368 (3.368)  Loss:  0.5547 (0.5547)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.6856 (1.0705)  Acc@1: 85.8491 (75.6040)  Acc@5: 96.3443 (93.0660)
Test (EMA): [   0/48]  Time: 3.187 (3.187)  Loss:  0.4632 (0.4632)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.233 (0.409)  Loss:  0.5841 (0.9501)  Acc@1: 86.0849 (77.7140)  Acc@5: 97.7594 (94.0520)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 77.07400008544921)

Train: 87 [   0/1251 (  0%)]  Loss:  3.425329 (3.4253)  Time: 1.105s,  926.51/s  (1.105s,  926.51/s)  LR: 4.042e-04  Data: 0.020 (0.020)
Train: 87 [  50/1251 (  4%)]  Loss:  3.223398 (3.3244)  Time: 1.124s,  910.65/s  (1.113s,  920.25/s)  LR: 4.042e-04  Data: 0.011 (0.012)
Train: 87 [ 100/1251 (  8%)]  Loss:  3.409266 (3.3527)  Time: 1.096s,  933.96/s  (1.113s,  920.36/s)  LR: 4.042e-04  Data: 0.012 (0.012)
Train: 87 [ 150/1251 ( 12%)]  Loss:  3.363817 (3.3555)  Time: 1.098s,  932.35/s  (1.108s,  924.15/s)  LR: 4.042e-04  Data: 0.011 (0.012)
Train: 87 [ 200/1251 ( 16%)]  Loss:  3.591520 (3.4027)  Time: 1.102s,  928.81/s  (1.110s,  922.80/s)  LR: 4.042e-04  Data: 0.010 (0.012)
Train: 87 [ 250/1251 ( 20%)]  Loss:  3.517148 (3.4217)  Time: 1.097s,  933.07/s  (1.109s,  923.59/s)  LR: 4.042e-04  Data: 0.012 (0.012)
Train: 87 [ 300/1251 ( 24%)]  Loss:  3.640202 (3.4530)  Time: 1.193s,  858.24/s  (1.109s,  923.66/s)  LR: 4.042e-04  Data: 0.012 (0.012)
Train: 87 [ 350/1251 ( 28%)]  Loss:  3.435236 (3.4507)  Time: 1.096s,  934.72/s  (1.108s,  923.86/s)  LR: 4.042e-04  Data: 0.013 (0.011)
Train: 87 [ 400/1251 ( 32%)]  Loss:  3.369809 (3.4417)  Time: 1.096s,  933.98/s  (1.108s,  924.04/s)  LR: 4.042e-04  Data: 0.011 (0.011)
Train: 87 [ 450/1251 ( 36%)]  Loss:  3.416383 (3.4392)  Time: 1.096s,  933.91/s  (1.108s,  923.86/s)  LR: 4.042e-04  Data: 0.012 (0.011)
Train: 87 [ 500/1251 ( 40%)]  Loss:  3.844960 (3.4761)  Time: 1.098s,  932.18/s  (1.108s,  924.10/s)  LR: 4.042e-04  Data: 0.016 (0.011)
Train: 87 [ 550/1251 ( 44%)]  Loss:  3.766249 (3.5003)  Time: 1.106s,  925.66/s  (1.108s,  923.84/s)  LR: 4.042e-04  Data: 0.011 (0.011)
Train: 87 [ 600/1251 ( 48%)]  Loss:  3.296448 (3.4846)  Time: 1.119s,  914.85/s  (1.109s,  923.71/s)  LR: 4.042e-04  Data: 0.013 (0.011)
Train: 87 [ 650/1251 ( 52%)]  Loss:  3.676768 (3.4983)  Time: 1.098s,  932.93/s  (1.109s,  923.45/s)  LR: 4.042e-04  Data: 0.014 (0.011)
Train: 87 [ 700/1251 ( 56%)]  Loss:  3.539858 (3.5011)  Time: 1.097s,  933.06/s  (1.108s,  923.87/s)  LR: 4.042e-04  Data: 0.014 (0.011)
Train: 87 [ 750/1251 ( 60%)]  Loss:  3.611719 (3.5080)  Time: 1.103s,  928.34/s  (1.108s,  924.00/s)  LR: 4.042e-04  Data: 0.012 (0.011)
Train: 87 [ 800/1251 ( 64%)]  Loss:  3.278569 (3.4945)  Time: 1.097s,  933.33/s  (1.108s,  924.33/s)  LR: 4.042e-04  Data: 0.011 (0.011)
Train: 87 [ 850/1251 ( 68%)]  Loss:  3.346502 (3.4863)  Time: 1.110s,  922.19/s  (1.108s,  924.38/s)  LR: 4.042e-04  Data: 0.011 (0.011)
Train: 87 [ 900/1251 ( 72%)]  Loss:  3.460210 (3.4849)  Time: 1.096s,  934.19/s  (1.108s,  924.57/s)  LR: 4.042e-04  Data: 0.013 (0.011)
Train: 87 [ 950/1251 ( 76%)]  Loss:  3.567440 (3.4890)  Time: 1.099s,  932.00/s  (1.107s,  924.65/s)  LR: 4.042e-04  Data: 0.011 (0.011)
Train: 87 [1000/1251 ( 80%)]  Loss:  3.395930 (3.4846)  Time: 1.101s,  929.98/s  (1.108s,  924.36/s)  LR: 4.042e-04  Data: 0.012 (0.011)
Train: 87 [1050/1251 ( 84%)]  Loss:  3.503901 (3.4855)  Time: 1.121s,  913.30/s  (1.108s,  924.31/s)  LR: 4.042e-04  Data: 0.011 (0.011)
Train: 87 [1100/1251 ( 88%)]  Loss:  3.563602 (3.4889)  Time: 1.117s,  916.85/s  (1.108s,  924.00/s)  LR: 4.042e-04  Data: 0.010 (0.011)
Train: 87 [1150/1251 ( 92%)]  Loss:  3.398505 (3.4851)  Time: 1.096s,  934.12/s  (1.108s,  924.11/s)  LR: 4.042e-04  Data: 0.010 (0.011)
Train: 87 [1200/1251 ( 96%)]  Loss:  3.563073 (3.4882)  Time: 1.096s,  934.72/s  (1.109s,  923.66/s)  LR: 4.042e-04  Data: 0.012 (0.011)
Train: 87 [1250/1251 (100%)]  Loss:  3.360519 (3.4833)  Time: 1.103s,  928.39/s  (1.109s,  923.67/s)  LR: 4.042e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.181 (3.181)  Loss:  0.5423 (0.5423)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6547 (1.0545)  Acc@1: 85.2594 (75.9060)  Acc@5: 96.8160 (93.0700)
Test (EMA): [   0/48]  Time: 3.116 (3.116)  Loss:  0.4612 (0.4612)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5832 (0.9479)  Acc@1: 85.9670 (77.7100)  Acc@5: 97.8774 (94.0860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 77.13800005859375)

Train: 88 [   0/1251 (  0%)]  Loss:  3.159614 (3.1596)  Time: 1.108s,  924.56/s  (1.108s,  924.56/s)  LR: 4.021e-04  Data: 0.023 (0.023)
Train: 88 [  50/1251 (  4%)]  Loss:  3.318641 (3.2391)  Time: 1.102s,  928.81/s  (1.114s,  919.51/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 88 [ 100/1251 (  8%)]  Loss:  3.433178 (3.3038)  Time: 1.098s,  932.38/s  (1.108s,  924.46/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 88 [ 150/1251 ( 12%)]  Loss:  3.204530 (3.2790)  Time: 1.126s,  909.15/s  (1.108s,  923.87/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 88 [ 200/1251 ( 16%)]  Loss:  3.488453 (3.3209)  Time: 1.120s,  914.01/s  (1.109s,  923.59/s)  LR: 4.021e-04  Data: 0.012 (0.011)
Train: 88 [ 250/1251 ( 20%)]  Loss:  3.561848 (3.3610)  Time: 1.215s,  842.97/s  (1.111s,  922.03/s)  LR: 4.021e-04  Data: 0.011 (0.011)
Train: 88 [ 300/1251 ( 24%)]  Loss:  3.314447 (3.3544)  Time: 1.103s,  928.26/s  (1.110s,  922.51/s)  LR: 4.021e-04  Data: 0.016 (0.011)
Train: 88 [ 350/1251 ( 28%)]  Loss:  3.712661 (3.3992)  Time: 1.097s,  933.87/s  (1.110s,  922.68/s)  LR: 4.021e-04  Data: 0.012 (0.011)
Train: 88 [ 400/1251 ( 32%)]  Loss:  3.718167 (3.4346)  Time: 1.099s,  931.59/s  (1.110s,  922.40/s)  LR: 4.021e-04  Data: 0.013 (0.011)
Train: 88 [ 450/1251 ( 36%)]  Loss:  3.505244 (3.4417)  Time: 1.097s,  933.11/s  (1.110s,  922.51/s)  LR: 4.021e-04  Data: 0.015 (0.011)
Train: 88 [ 500/1251 ( 40%)]  Loss:  3.414812 (3.4392)  Time: 1.098s,  932.40/s  (1.110s,  922.87/s)  LR: 4.021e-04  Data: 0.011 (0.011)
Train: 88 [ 550/1251 ( 44%)]  Loss:  3.699758 (3.4609)  Time: 1.122s,  912.91/s  (1.109s,  923.14/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 88 [ 600/1251 ( 48%)]  Loss:  3.477858 (3.4622)  Time: 1.098s,  932.93/s  (1.110s,  922.45/s)  LR: 4.021e-04  Data: 0.012 (0.011)
Train: 88 [ 650/1251 ( 52%)]  Loss:  3.188158 (3.4427)  Time: 1.097s,  933.40/s  (1.109s,  923.15/s)  LR: 4.021e-04  Data: 0.010 (0.011)
Train: 88 [ 700/1251 ( 56%)]  Loss:  3.524140 (3.4481)  Time: 1.100s,  930.87/s  (1.109s,  923.15/s)  LR: 4.021e-04  Data: 0.011 (0.011)
Train: 88 [ 750/1251 ( 60%)]  Loss:  3.715157 (3.4648)  Time: 1.125s,  910.56/s  (1.109s,  923.38/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 88 [ 800/1251 ( 64%)]  Loss:  3.584141 (3.4718)  Time: 1.119s,  914.92/s  (1.109s,  923.44/s)  LR: 4.021e-04  Data: 0.010 (0.011)
Train: 88 [ 850/1251 ( 68%)]  Loss:  3.293798 (3.4619)  Time: 1.099s,  931.57/s  (1.109s,  923.30/s)  LR: 4.021e-04  Data: 0.011 (0.011)
Train: 88 [ 900/1251 ( 72%)]  Loss:  3.458271 (3.4617)  Time: 1.198s,  854.87/s  (1.109s,  923.37/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 88 [ 950/1251 ( 76%)]  Loss:  3.573291 (3.4673)  Time: 1.101s,  930.05/s  (1.109s,  923.17/s)  LR: 4.021e-04  Data: 0.010 (0.012)
Train: 88 [1000/1251 ( 80%)]  Loss:  3.475360 (3.4677)  Time: 1.097s,  933.18/s  (1.109s,  923.35/s)  LR: 4.021e-04  Data: 0.012 (0.011)
Train: 88 [1050/1251 ( 84%)]  Loss:  3.606238 (3.4740)  Time: 1.097s,  933.56/s  (1.109s,  923.40/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 88 [1100/1251 ( 88%)]  Loss:  3.613264 (3.4800)  Time: 1.098s,  932.51/s  (1.109s,  923.51/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 88 [1150/1251 ( 92%)]  Loss:  3.686975 (3.4887)  Time: 1.099s,  931.99/s  (1.109s,  923.47/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 88 [1200/1251 ( 96%)]  Loss:  3.464101 (3.4877)  Time: 1.122s,  912.82/s  (1.109s,  923.67/s)  LR: 4.021e-04  Data: 0.011 (0.012)
Train: 88 [1250/1251 (100%)]  Loss:  3.665682 (3.4945)  Time: 1.171s,  874.15/s  (1.109s,  923.32/s)  LR: 4.021e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.254 (3.254)  Loss:  0.5378 (0.5378)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6798 (1.0783)  Acc@1: 85.7311 (75.7580)  Acc@5: 96.6981 (93.1060)
Test (EMA): [   0/48]  Time: 3.142 (3.142)  Loss:  0.4605 (0.4605)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.230 (0.403)  Loss:  0.5822 (0.9457)  Acc@1: 86.3208 (77.7760)  Acc@5: 97.9953 (94.0880)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 77.20400000732423)

Train: 89 [   0/1251 (  0%)]  Loss:  3.720426 (3.7204)  Time: 1.112s,  920.96/s  (1.112s,  920.96/s)  LR: 4.001e-04  Data: 0.022 (0.022)
Train: 89 [  50/1251 (  4%)]  Loss:  3.496090 (3.6083)  Time: 1.097s,  933.32/s  (1.103s,  928.44/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Train: 89 [ 100/1251 (  8%)]  Loss:  3.721123 (3.6459)  Time: 1.099s,  931.98/s  (1.107s,  925.36/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 150/1251 ( 12%)]  Loss:  3.283693 (3.5553)  Time: 1.098s,  932.35/s  (1.105s,  926.68/s)  LR: 4.001e-04  Data: 0.012 (0.011)
Train: 89 [ 200/1251 ( 16%)]  Loss:  3.143339 (3.4729)  Time: 1.092s,  938.04/s  (1.105s,  926.97/s)  LR: 4.001e-04  Data: 0.010 (0.012)
Train: 89 [ 250/1251 ( 20%)]  Loss:  3.262679 (3.4379)  Time: 1.097s,  933.05/s  (1.105s,  926.85/s)  LR: 4.001e-04  Data: 0.013 (0.012)
Train: 89 [ 300/1251 ( 24%)]  Loss:  3.401009 (3.4326)  Time: 1.099s,  932.11/s  (1.106s,  926.06/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 350/1251 ( 28%)]  Loss:  3.208425 (3.4046)  Time: 1.098s,  932.69/s  (1.106s,  925.68/s)  LR: 4.001e-04  Data: 0.013 (0.012)
Train: 89 [ 400/1251 ( 32%)]  Loss:  3.478504 (3.4128)  Time: 1.101s,  929.73/s  (1.106s,  926.23/s)  LR: 4.001e-04  Data: 0.010 (0.012)
Train: 89 [ 450/1251 ( 36%)]  Loss:  3.696149 (3.4411)  Time: 1.098s,  932.47/s  (1.106s,  925.98/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 500/1251 ( 40%)]  Loss:  3.577950 (3.4536)  Time: 1.097s,  933.03/s  (1.105s,  926.50/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 550/1251 ( 44%)]  Loss:  3.414944 (3.4504)  Time: 1.096s,  933.91/s  (1.105s,  926.32/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 600/1251 ( 48%)]  Loss:  3.189018 (3.4303)  Time: 1.097s,  933.21/s  (1.106s,  925.82/s)  LR: 4.001e-04  Data: 0.010 (0.012)
Train: 89 [ 650/1251 ( 52%)]  Loss:  3.613503 (3.4433)  Time: 1.217s,  841.74/s  (1.106s,  925.47/s)  LR: 4.001e-04  Data: 0.010 (0.012)
Train: 89 [ 700/1251 ( 56%)]  Loss:  3.586491 (3.4529)  Time: 1.096s,  934.19/s  (1.106s,  925.68/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 750/1251 ( 60%)]  Loss:  3.729009 (3.4701)  Time: 1.101s,  930.28/s  (1.106s,  925.85/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 800/1251 ( 64%)]  Loss:  3.566039 (3.4758)  Time: 1.097s,  933.20/s  (1.106s,  926.08/s)  LR: 4.001e-04  Data: 0.011 (0.012)
Train: 89 [ 850/1251 ( 68%)]  Loss:  3.550524 (3.4799)  Time: 1.131s,  905.02/s  (1.106s,  925.65/s)  LR: 4.001e-04  Data: 0.011 (0.012)
Train: 89 [ 900/1251 ( 72%)]  Loss:  3.276936 (3.4693)  Time: 1.098s,  932.35/s  (1.106s,  925.70/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [ 950/1251 ( 76%)]  Loss:  3.526939 (3.4721)  Time: 1.093s,  936.48/s  (1.106s,  925.82/s)  LR: 4.001e-04  Data: 0.010 (0.012)
Train: 89 [1000/1251 ( 80%)]  Loss:  3.496791 (3.4733)  Time: 1.095s,  934.75/s  (1.106s,  925.75/s)  LR: 4.001e-04  Data: 0.013 (0.012)
Train: 89 [1050/1251 ( 84%)]  Loss:  3.407467 (3.4703)  Time: 1.096s,  934.16/s  (1.106s,  926.08/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [1100/1251 ( 88%)]  Loss:  3.478155 (3.4707)  Time: 1.098s,  932.91/s  (1.106s,  925.93/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [1150/1251 ( 92%)]  Loss:  3.473985 (3.4708)  Time: 1.097s,  933.67/s  (1.106s,  925.92/s)  LR: 4.001e-04  Data: 0.012 (0.012)
Train: 89 [1200/1251 ( 96%)]  Loss:  3.047941 (3.4539)  Time: 1.097s,  933.68/s  (1.106s,  925.87/s)  LR: 4.001e-04  Data: 0.011 (0.012)
Train: 89 [1250/1251 (100%)]  Loss:  3.272436 (3.4469)  Time: 1.081s,  947.68/s  (1.106s,  925.45/s)  LR: 4.001e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.227 (3.227)  Loss:  0.5806 (0.5806)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.399)  Loss:  0.7461 (1.1080)  Acc@1: 85.6132 (75.7560)  Acc@5: 96.2264 (92.9540)
Test (EMA): [   0/48]  Time: 3.211 (3.211)  Loss:  0.4592 (0.4592)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5827 (0.9437)  Acc@1: 86.4387 (77.8520)  Acc@5: 97.7594 (94.1240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 77.29600003173829)

Train: 90 [   0/1251 (  0%)]  Loss:  3.574151 (3.5742)  Time: 1.104s,  927.38/s  (1.104s,  927.38/s)  LR: 3.980e-04  Data: 0.023 (0.023)
Train: 90 [  50/1251 (  4%)]  Loss:  3.367187 (3.4707)  Time: 1.097s,  933.62/s  (1.108s,  923.86/s)  LR: 3.980e-04  Data: 0.012 (0.012)
Train: 90 [ 100/1251 (  8%)]  Loss:  3.576866 (3.5061)  Time: 1.098s,  932.83/s  (1.107s,  924.84/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 150/1251 ( 12%)]  Loss:  3.579790 (3.5245)  Time: 1.100s,  931.21/s  (1.106s,  925.47/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 200/1251 ( 16%)]  Loss:  3.308671 (3.4813)  Time: 1.119s,  914.98/s  (1.107s,  924.94/s)  LR: 3.980e-04  Data: 0.010 (0.011)
Train: 90 [ 250/1251 ( 20%)]  Loss:  3.375819 (3.4637)  Time: 1.097s,  933.29/s  (1.106s,  925.67/s)  LR: 3.980e-04  Data: 0.012 (0.012)
Train: 90 [ 300/1251 ( 24%)]  Loss:  3.685990 (3.4955)  Time: 1.125s,  909.90/s  (1.107s,  925.17/s)  LR: 3.980e-04  Data: 0.010 (0.012)
Train: 90 [ 350/1251 ( 28%)]  Loss:  3.366420 (3.4794)  Time: 1.098s,  932.58/s  (1.106s,  925.51/s)  LR: 3.980e-04  Data: 0.011 (0.012)
Train: 90 [ 400/1251 ( 32%)]  Loss:  3.632982 (3.4964)  Time: 1.100s,  930.92/s  (1.106s,  925.67/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 450/1251 ( 36%)]  Loss:  3.570328 (3.5038)  Time: 1.103s,  928.50/s  (1.106s,  925.81/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [ 500/1251 ( 40%)]  Loss:  2.736797 (3.4341)  Time: 1.121s,  913.83/s  (1.107s,  925.37/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [ 550/1251 ( 44%)]  Loss:  3.306889 (3.4235)  Time: 1.097s,  933.12/s  (1.106s,  925.84/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 600/1251 ( 48%)]  Loss:  3.615460 (3.4383)  Time: 1.097s,  933.07/s  (1.107s,  925.28/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 650/1251 ( 52%)]  Loss:  3.296651 (3.4281)  Time: 1.102s,  929.63/s  (1.106s,  925.55/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [ 700/1251 ( 56%)]  Loss:  3.293149 (3.4191)  Time: 1.097s,  933.44/s  (1.107s,  925.32/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 750/1251 ( 60%)]  Loss:  3.351541 (3.4149)  Time: 1.097s,  933.19/s  (1.106s,  925.58/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 800/1251 ( 64%)]  Loss:  3.263151 (3.4060)  Time: 1.098s,  932.30/s  (1.106s,  925.72/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [ 850/1251 ( 68%)]  Loss:  3.251471 (3.3974)  Time: 1.099s,  931.55/s  (1.106s,  925.62/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 900/1251 ( 72%)]  Loss:  3.262107 (3.3903)  Time: 1.106s,  925.51/s  (1.106s,  925.72/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [ 950/1251 ( 76%)]  Loss:  3.289146 (3.3852)  Time: 1.097s,  933.56/s  (1.107s,  925.29/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [1000/1251 ( 80%)]  Loss:  3.492574 (3.3903)  Time: 1.133s,  903.63/s  (1.107s,  925.27/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [1050/1251 ( 84%)]  Loss:  3.399284 (3.3907)  Time: 1.108s,  923.82/s  (1.107s,  924.95/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [1100/1251 ( 88%)]  Loss:  3.458950 (3.3937)  Time: 1.098s,  932.84/s  (1.107s,  925.26/s)  LR: 3.980e-04  Data: 0.011 (0.011)
Train: 90 [1150/1251 ( 92%)]  Loss:  3.648485 (3.4043)  Time: 1.122s,  912.89/s  (1.107s,  924.91/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [1200/1251 ( 96%)]  Loss:  3.568683 (3.4109)  Time: 1.099s,  931.69/s  (1.107s,  924.85/s)  LR: 3.980e-04  Data: 0.012 (0.011)
Train: 90 [1250/1251 (100%)]  Loss:  3.623005 (3.4191)  Time: 1.166s,  878.08/s  (1.107s,  924.87/s)  LR: 3.980e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.223 (3.223)  Loss:  0.5545 (0.5545)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.230 (0.405)  Loss:  0.6761 (1.0565)  Acc@1: 85.3774 (75.7940)  Acc@5: 97.4057 (93.1100)
Test (EMA): [   0/48]  Time: 3.295 (3.295)  Loss:  0.4584 (0.4584)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.230 (0.403)  Loss:  0.5829 (0.9420)  Acc@1: 86.3208 (77.8820)  Acc@5: 97.7594 (94.1600)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 77.31400010986329)

Train: 91 [   0/1251 (  0%)]  Loss:  3.392505 (3.3925)  Time: 1.132s,  904.80/s  (1.132s,  904.80/s)  LR: 3.959e-04  Data: 0.022 (0.022)
Train: 91 [  50/1251 (  4%)]  Loss:  3.589282 (3.4909)  Time: 1.106s,  925.92/s  (1.105s,  926.54/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 100/1251 (  8%)]  Loss:  3.372923 (3.4516)  Time: 1.095s,  935.03/s  (1.107s,  925.38/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 150/1251 ( 12%)]  Loss:  3.770392 (3.5313)  Time: 1.124s,  910.91/s  (1.105s,  926.90/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Train: 91 [ 200/1251 ( 16%)]  Loss:  3.687288 (3.5625)  Time: 1.121s,  913.68/s  (1.105s,  926.50/s)  LR: 3.959e-04  Data: 0.010 (0.012)
Train: 91 [ 250/1251 ( 20%)]  Loss:  3.874672 (3.6145)  Time: 1.105s,  926.95/s  (1.106s,  925.72/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 300/1251 ( 24%)]  Loss:  3.091039 (3.5397)  Time: 1.094s,  936.00/s  (1.106s,  925.95/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 350/1251 ( 28%)]  Loss:  3.432846 (3.5264)  Time: 1.096s,  934.21/s  (1.106s,  925.51/s)  LR: 3.959e-04  Data: 0.013 (0.012)
Train: 91 [ 400/1251 ( 32%)]  Loss:  3.396815 (3.5120)  Time: 1.102s,  929.53/s  (1.105s,  926.30/s)  LR: 3.959e-04  Data: 0.013 (0.012)
Train: 91 [ 450/1251 ( 36%)]  Loss:  3.660158 (3.5268)  Time: 1.105s,  926.51/s  (1.106s,  925.69/s)  LR: 3.959e-04  Data: 0.010 (0.012)
Train: 91 [ 500/1251 ( 40%)]  Loss:  3.561953 (3.5300)  Time: 1.103s,  928.40/s  (1.106s,  926.11/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 550/1251 ( 44%)]  Loss:  3.522360 (3.5294)  Time: 1.121s,  913.55/s  (1.107s,  925.40/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Train: 91 [ 600/1251 ( 48%)]  Loss:  3.197678 (3.5038)  Time: 1.094s,  935.61/s  (1.108s,  924.34/s)  LR: 3.959e-04  Data: 0.010 (0.012)
Train: 91 [ 650/1251 ( 52%)]  Loss:  3.303907 (3.4896)  Time: 1.119s,  915.09/s  (1.108s,  923.96/s)  LR: 3.959e-04  Data: 0.010 (0.012)
Train: 91 [ 700/1251 ( 56%)]  Loss:  3.043328 (3.4598)  Time: 1.095s,  934.87/s  (1.108s,  924.04/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Train: 91 [ 750/1251 ( 60%)]  Loss:  3.449402 (3.4592)  Time: 1.097s,  933.73/s  (1.108s,  924.09/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 800/1251 ( 64%)]  Loss:  3.450425 (3.4586)  Time: 1.098s,  932.89/s  (1.109s,  923.63/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Train: 91 [ 850/1251 ( 68%)]  Loss:  3.404026 (3.4556)  Time: 1.096s,  933.96/s  (1.108s,  924.03/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 900/1251 ( 72%)]  Loss:  3.495037 (3.4577)  Time: 1.134s,  903.09/s  (1.108s,  924.02/s)  LR: 3.959e-04  Data: 0.011 (0.012)
Train: 91 [ 950/1251 ( 76%)]  Loss:  3.238565 (3.4467)  Time: 1.098s,  933.02/s  (1.109s,  923.74/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Train: 91 [1000/1251 ( 80%)]  Loss:  3.506895 (3.4496)  Time: 1.097s,  933.48/s  (1.109s,  923.71/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Train: 91 [1050/1251 ( 84%)]  Loss:  3.547431 (3.4540)  Time: 1.102s,  929.22/s  (1.109s,  923.41/s)  LR: 3.959e-04  Data: 0.010 (0.012)
Train: 91 [1100/1251 ( 88%)]  Loss:  3.402422 (3.4518)  Time: 1.104s,  927.86/s  (1.109s,  923.42/s)  LR: 3.959e-04  Data: 0.010 (0.012)
Train: 91 [1150/1251 ( 92%)]  Loss:  3.348310 (3.4475)  Time: 1.098s,  932.70/s  (1.109s,  923.65/s)  LR: 3.959e-04  Data: 0.012 (0.011)
Train: 91 [1200/1251 ( 96%)]  Loss:  3.534458 (3.4510)  Time: 1.098s,  932.78/s  (1.109s,  923.75/s)  LR: 3.959e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 91 [1250/1251 (100%)]  Loss:  3.589999 (3.4563)  Time: 1.111s,  922.09/s  (1.109s,  923.63/s)  LR: 3.959e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.225 (3.225)  Loss:  0.5337 (0.5337)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6784 (1.0470)  Acc@1: 85.3774 (76.1780)  Acc@5: 96.4623 (93.2800)
Test (EMA): [   0/48]  Time: 3.073 (3.073)  Loss:  0.4577 (0.4577)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5825 (0.9397)  Acc@1: 86.2028 (77.9120)  Acc@5: 97.7594 (94.2020)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 77.38200003173829)

Train: 92 [   0/1251 (  0%)]  Loss:  3.816511 (3.8165)  Time: 1.119s,  915.39/s  (1.119s,  915.39/s)  LR: 3.938e-04  Data: 0.024 (0.024)
Train: 92 [  50/1251 (  4%)]  Loss:  3.678727 (3.7476)  Time: 1.101s,  930.23/s  (1.106s,  925.81/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 100/1251 (  8%)]  Loss:  3.629416 (3.7082)  Time: 1.096s,  934.14/s  (1.105s,  926.61/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 150/1251 ( 12%)]  Loss:  3.353539 (3.6195)  Time: 1.121s,  913.49/s  (1.105s,  926.32/s)  LR: 3.938e-04  Data: 0.010 (0.011)
Train: 92 [ 200/1251 ( 16%)]  Loss:  3.455599 (3.5868)  Time: 1.121s,  913.25/s  (1.105s,  926.68/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [ 250/1251 ( 20%)]  Loss:  3.458130 (3.5653)  Time: 1.196s,  856.07/s  (1.106s,  926.23/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 300/1251 ( 24%)]  Loss:  3.124571 (3.5024)  Time: 1.099s,  932.13/s  (1.107s,  924.76/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 350/1251 ( 28%)]  Loss:  3.488344 (3.5006)  Time: 1.120s,  914.20/s  (1.108s,  924.07/s)  LR: 3.938e-04  Data: 0.010 (0.012)
Train: 92 [ 400/1251 ( 32%)]  Loss:  3.534061 (3.5043)  Time: 1.122s,  912.74/s  (1.109s,  923.13/s)  LR: 3.938e-04  Data: 0.010 (0.011)
Train: 92 [ 450/1251 ( 36%)]  Loss:  3.261625 (3.4801)  Time: 1.097s,  933.41/s  (1.108s,  923.83/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 500/1251 ( 40%)]  Loss:  3.532403 (3.4848)  Time: 1.179s,  868.39/s  (1.109s,  923.23/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [ 550/1251 ( 44%)]  Loss:  3.395879 (3.4774)  Time: 1.101s,  929.76/s  (1.108s,  923.94/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [ 600/1251 ( 48%)]  Loss:  3.528120 (3.4813)  Time: 1.202s,  851.92/s  (1.108s,  924.01/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [ 650/1251 ( 52%)]  Loss:  3.497933 (3.4825)  Time: 1.099s,  932.17/s  (1.108s,  924.17/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 700/1251 ( 56%)]  Loss:  3.378626 (3.4756)  Time: 1.106s,  925.85/s  (1.108s,  924.09/s)  LR: 3.938e-04  Data: 0.018 (0.012)
Train: 92 [ 750/1251 ( 60%)]  Loss:  3.355811 (3.4681)  Time: 1.095s,  935.02/s  (1.108s,  924.12/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [ 800/1251 ( 64%)]  Loss:  3.574858 (3.4744)  Time: 1.097s,  933.04/s  (1.108s,  924.00/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [ 850/1251 ( 68%)]  Loss:  3.266924 (3.4628)  Time: 1.097s,  933.30/s  (1.108s,  924.20/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 900/1251 ( 72%)]  Loss:  3.423765 (3.4608)  Time: 1.099s,  931.67/s  (1.108s,  924.42/s)  LR: 3.938e-04  Data: 0.012 (0.012)
Train: 92 [ 950/1251 ( 76%)]  Loss:  3.300108 (3.4527)  Time: 1.104s,  927.80/s  (1.108s,  924.34/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [1000/1251 ( 80%)]  Loss:  3.699640 (3.4645)  Time: 1.100s,  930.97/s  (1.108s,  924.45/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [1050/1251 ( 84%)]  Loss:  3.619682 (3.4716)  Time: 1.097s,  933.71/s  (1.108s,  924.48/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [1100/1251 ( 88%)]  Loss:  3.449860 (3.4706)  Time: 1.093s,  936.69/s  (1.107s,  924.69/s)  LR: 3.938e-04  Data: 0.010 (0.012)
Train: 92 [1150/1251 ( 92%)]  Loss:  3.622679 (3.4770)  Time: 1.095s,  935.07/s  (1.108s,  924.56/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [1200/1251 ( 96%)]  Loss:  3.410660 (3.4743)  Time: 1.099s,  931.67/s  (1.107s,  924.75/s)  LR: 3.938e-04  Data: 0.011 (0.012)
Train: 92 [1250/1251 (100%)]  Loss:  3.291428 (3.4673)  Time: 1.082s,  946.56/s  (1.107s,  924.89/s)  LR: 3.938e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.213 (3.213)  Loss:  0.5168 (0.5168)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6676 (1.0613)  Acc@1: 84.7877 (75.7340)  Acc@5: 97.1698 (93.1560)
Test (EMA): [   0/48]  Time: 3.187 (3.187)  Loss:  0.4562 (0.4562)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5830 (0.9377)  Acc@1: 86.2028 (77.9080)  Acc@5: 97.7594 (94.2120)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 77.48400008300781)

Train: 93 [   0/1251 (  0%)]  Loss:  3.236498 (3.2365)  Time: 1.099s,  931.91/s  (1.099s,  931.91/s)  LR: 3.916e-04  Data: 0.020 (0.020)
Train: 93 [  50/1251 (  4%)]  Loss:  3.541817 (3.3892)  Time: 1.097s,  933.77/s  (1.109s,  923.09/s)  LR: 3.916e-04  Data: 0.011 (0.012)
Train: 93 [ 100/1251 (  8%)]  Loss:  3.630588 (3.4696)  Time: 1.098s,  932.32/s  (1.109s,  923.00/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 150/1251 ( 12%)]  Loss:  3.479949 (3.4722)  Time: 1.126s,  909.67/s  (1.108s,  923.82/s)  LR: 3.916e-04  Data: 0.011 (0.012)
Train: 93 [ 200/1251 ( 16%)]  Loss:  3.683269 (3.5144)  Time: 1.096s,  934.00/s  (1.107s,  925.26/s)  LR: 3.916e-04  Data: 0.011 (0.012)
Train: 93 [ 250/1251 ( 20%)]  Loss:  3.599271 (3.5286)  Time: 1.096s,  934.00/s  (1.109s,  923.73/s)  LR: 3.916e-04  Data: 0.011 (0.012)
Train: 93 [ 300/1251 ( 24%)]  Loss:  3.516565 (3.5269)  Time: 1.106s,  925.84/s  (1.109s,  923.65/s)  LR: 3.916e-04  Data: 0.010 (0.012)
Train: 93 [ 350/1251 ( 28%)]  Loss:  3.669767 (3.5447)  Time: 1.095s,  935.38/s  (1.110s,  922.87/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 400/1251 ( 32%)]  Loss:  3.612551 (3.5523)  Time: 1.097s,  933.73/s  (1.109s,  923.69/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 450/1251 ( 36%)]  Loss:  3.493240 (3.5464)  Time: 1.096s,  934.37/s  (1.108s,  924.17/s)  LR: 3.916e-04  Data: 0.013 (0.012)
Train: 93 [ 500/1251 ( 40%)]  Loss:  3.303789 (3.5243)  Time: 1.103s,  928.53/s  (1.108s,  924.52/s)  LR: 3.916e-04  Data: 0.010 (0.012)
Train: 93 [ 550/1251 ( 44%)]  Loss:  3.323466 (3.5076)  Time: 1.226s,  835.37/s  (1.108s,  924.19/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 600/1251 ( 48%)]  Loss:  3.421125 (3.5009)  Time: 1.099s,  931.73/s  (1.108s,  924.18/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 650/1251 ( 52%)]  Loss:  3.184218 (3.4783)  Time: 1.102s,  928.94/s  (1.108s,  923.84/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 700/1251 ( 56%)]  Loss:  3.390517 (3.4724)  Time: 1.099s,  931.70/s  (1.108s,  923.93/s)  LR: 3.916e-04  Data: 0.010 (0.012)
Train: 93 [ 750/1251 ( 60%)]  Loss:  3.633047 (3.4825)  Time: 1.096s,  934.05/s  (1.108s,  924.31/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 800/1251 ( 64%)]  Loss:  3.365529 (3.4756)  Time: 1.098s,  932.40/s  (1.108s,  924.54/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 850/1251 ( 68%)]  Loss:  3.228650 (3.4619)  Time: 1.104s,  927.20/s  (1.108s,  924.52/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [ 900/1251 ( 72%)]  Loss:  3.539593 (3.4660)  Time: 1.096s,  934.49/s  (1.108s,  924.03/s)  LR: 3.916e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 93 [ 950/1251 ( 76%)]  Loss:  3.545275 (3.4699)  Time: 1.097s,  933.53/s  (1.108s,  924.43/s)  LR: 3.916e-04  Data: 0.014 (0.012)
Train: 93 [1000/1251 ( 80%)]  Loss:  3.271109 (3.4605)  Time: 1.098s,  932.28/s  (1.108s,  924.25/s)  LR: 3.916e-04  Data: 0.014 (0.012)
Train: 93 [1050/1251 ( 84%)]  Loss:  3.528278 (3.4636)  Time: 1.099s,  932.03/s  (1.108s,  924.56/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [1100/1251 ( 88%)]  Loss:  3.238209 (3.4538)  Time: 1.100s,  930.68/s  (1.108s,  924.57/s)  LR: 3.916e-04  Data: 0.010 (0.012)
Train: 93 [1150/1251 ( 92%)]  Loss:  3.370123 (3.4503)  Time: 1.099s,  931.87/s  (1.107s,  924.70/s)  LR: 3.916e-04  Data: 0.012 (0.012)
Train: 93 [1200/1251 ( 96%)]  Loss:  4.037998 (3.4738)  Time: 1.097s,  933.62/s  (1.107s,  924.65/s)  LR: 3.916e-04  Data: 0.011 (0.012)
Train: 93 [1250/1251 (100%)]  Loss:  3.280329 (3.4663)  Time: 1.080s,  948.54/s  (1.108s,  924.41/s)  LR: 3.916e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.222 (3.222)  Loss:  0.6075 (0.6075)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.6419 (1.0863)  Acc@1: 85.4953 (75.5580)  Acc@5: 97.1698 (93.2220)
Test (EMA): [   0/48]  Time: 3.244 (3.244)  Loss:  0.4554 (0.4554)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.6562 (97.6562)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5819 (0.9358)  Acc@1: 85.9670 (77.9780)  Acc@5: 97.7594 (94.2600)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 77.5480000830078)

Train: 94 [   0/1251 (  0%)]  Loss:  3.608090 (3.6081)  Time: 1.114s,  918.83/s  (1.114s,  918.83/s)  LR: 3.895e-04  Data: 0.031 (0.031)
Train: 94 [  50/1251 (  4%)]  Loss:  3.325770 (3.4669)  Time: 1.098s,  932.86/s  (1.105s,  926.42/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 100/1251 (  8%)]  Loss:  3.794432 (3.5761)  Time: 1.104s,  927.64/s  (1.111s,  921.64/s)  LR: 3.895e-04  Data: 0.014 (0.012)
Train: 94 [ 150/1251 ( 12%)]  Loss:  3.380619 (3.5272)  Time: 1.097s,  933.43/s  (1.108s,  924.04/s)  LR: 3.895e-04  Data: 0.013 (0.012)
Train: 94 [ 200/1251 ( 16%)]  Loss:  3.636052 (3.5490)  Time: 1.135s,  902.58/s  (1.108s,  923.93/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 250/1251 ( 20%)]  Loss:  3.619018 (3.5607)  Time: 1.105s,  926.49/s  (1.109s,  923.39/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 300/1251 ( 24%)]  Loss:  3.510211 (3.5535)  Time: 1.102s,  928.81/s  (1.110s,  922.89/s)  LR: 3.895e-04  Data: 0.012 (0.012)
Train: 94 [ 350/1251 ( 28%)]  Loss:  3.262066 (3.5170)  Time: 1.093s,  936.77/s  (1.110s,  922.34/s)  LR: 3.895e-04  Data: 0.010 (0.012)
Train: 94 [ 400/1251 ( 32%)]  Loss:  3.500948 (3.5152)  Time: 1.094s,  935.83/s  (1.110s,  922.77/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 450/1251 ( 36%)]  Loss:  3.824983 (3.5462)  Time: 1.098s,  932.47/s  (1.109s,  923.43/s)  LR: 3.895e-04  Data: 0.014 (0.012)
Train: 94 [ 500/1251 ( 40%)]  Loss:  3.524105 (3.5442)  Time: 1.096s,  933.89/s  (1.108s,  923.85/s)  LR: 3.895e-04  Data: 0.012 (0.012)
Train: 94 [ 550/1251 ( 44%)]  Loss:  3.587167 (3.5478)  Time: 1.093s,  936.78/s  (1.108s,  924.41/s)  LR: 3.895e-04  Data: 0.010 (0.012)
Train: 94 [ 600/1251 ( 48%)]  Loss:  3.639029 (3.5548)  Time: 1.094s,  936.00/s  (1.108s,  924.16/s)  LR: 3.895e-04  Data: 0.010 (0.012)
Train: 94 [ 650/1251 ( 52%)]  Loss:  3.353754 (3.5404)  Time: 1.100s,  930.81/s  (1.108s,  924.27/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 700/1251 ( 56%)]  Loss:  3.397850 (3.5309)  Time: 1.097s,  933.55/s  (1.108s,  924.55/s)  LR: 3.895e-04  Data: 0.012 (0.012)
Train: 94 [ 750/1251 ( 60%)]  Loss:  3.239646 (3.5127)  Time: 1.095s,  934.88/s  (1.108s,  924.30/s)  LR: 3.895e-04  Data: 0.010 (0.012)
Train: 94 [ 800/1251 ( 64%)]  Loss:  3.364923 (3.5040)  Time: 1.097s,  933.46/s  (1.107s,  924.61/s)  LR: 3.895e-04  Data: 0.012 (0.012)
Train: 94 [ 850/1251 ( 68%)]  Loss:  3.676959 (3.5136)  Time: 1.097s,  933.07/s  (1.108s,  924.56/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 900/1251 ( 72%)]  Loss:  3.231027 (3.4988)  Time: 1.102s,  929.25/s  (1.108s,  924.50/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [ 950/1251 ( 76%)]  Loss:  3.192600 (3.4835)  Time: 1.100s,  930.91/s  (1.108s,  924.50/s)  LR: 3.895e-04  Data: 0.013 (0.012)
Train: 94 [1000/1251 ( 80%)]  Loss:  3.365065 (3.4778)  Time: 1.122s,  912.55/s  (1.107s,  924.86/s)  LR: 3.895e-04  Data: 0.012 (0.012)
Train: 94 [1050/1251 ( 84%)]  Loss:  3.472340 (3.4776)  Time: 1.102s,  929.01/s  (1.107s,  924.81/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [1100/1251 ( 88%)]  Loss:  3.666676 (3.4858)  Time: 1.097s,  933.05/s  (1.107s,  924.74/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [1150/1251 ( 92%)]  Loss:  3.367061 (3.4808)  Time: 1.193s,  858.32/s  (1.107s,  924.73/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [1200/1251 ( 96%)]  Loss:  3.390839 (3.4772)  Time: 1.099s,  931.59/s  (1.107s,  924.96/s)  LR: 3.895e-04  Data: 0.011 (0.012)
Train: 94 [1250/1251 (100%)]  Loss:  3.417534 (3.4750)  Time: 1.092s,  937.65/s  (1.107s,  924.91/s)  LR: 3.895e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.343 (3.343)  Loss:  0.5481 (0.5481)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6921 (1.0815)  Acc@1: 85.0236 (75.8140)  Acc@5: 97.2877 (93.1700)
Test (EMA): [   0/48]  Time: 3.296 (3.296)  Loss:  0.4548 (0.4548)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.6562 (97.6562)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5802 (0.9339)  Acc@1: 85.9670 (77.9980)  Acc@5: 97.6415 (94.2740)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 77.60400008300782)

Train: 95 [   0/1251 (  0%)]  Loss:  2.969984 (2.9700)  Time: 1.125s,  910.13/s  (1.125s,  910.13/s)  LR: 3.873e-04  Data: 0.023 (0.023)
Train: 95 [  50/1251 (  4%)]  Loss:  3.452912 (3.2114)  Time: 1.097s,  933.11/s  (1.107s,  925.11/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 100/1251 (  8%)]  Loss:  3.220062 (3.2143)  Time: 1.097s,  933.28/s  (1.108s,  924.19/s)  LR: 3.873e-04  Data: 0.014 (0.012)
Train: 95 [ 150/1251 ( 12%)]  Loss:  3.348573 (3.2479)  Time: 1.120s,  914.18/s  (1.108s,  923.80/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [ 200/1251 ( 16%)]  Loss:  3.571562 (3.3126)  Time: 1.105s,  927.03/s  (1.107s,  924.64/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Train: 95 [ 250/1251 ( 20%)]  Loss:  3.835722 (3.3998)  Time: 1.097s,  933.76/s  (1.107s,  924.75/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 300/1251 ( 24%)]  Loss:  3.506242 (3.4150)  Time: 1.104s,  927.32/s  (1.106s,  925.51/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [ 350/1251 ( 28%)]  Loss:  3.376610 (3.4102)  Time: 1.120s,  914.28/s  (1.107s,  925.34/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Train: 95 [ 400/1251 ( 32%)]  Loss:  3.602509 (3.4316)  Time: 1.207s,  848.19/s  (1.106s,  925.81/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 450/1251 ( 36%)]  Loss:  3.301200 (3.4185)  Time: 1.134s,  902.68/s  (1.107s,  924.73/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 500/1251 ( 40%)]  Loss:  3.526803 (3.4284)  Time: 1.100s,  931.03/s  (1.109s,  923.75/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [ 550/1251 ( 44%)]  Loss:  3.386914 (3.4249)  Time: 1.097s,  933.15/s  (1.108s,  924.20/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Train: 95 [ 600/1251 ( 48%)]  Loss:  3.600390 (3.4384)  Time: 1.119s,  914.97/s  (1.108s,  924.38/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 650/1251 ( 52%)]  Loss:  3.611384 (3.4508)  Time: 1.098s,  932.26/s  (1.108s,  924.48/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 700/1251 ( 56%)]  Loss:  3.400152 (3.4474)  Time: 1.096s,  934.06/s  (1.108s,  923.92/s)  LR: 3.873e-04  Data: 0.012 (0.012)
Train: 95 [ 750/1251 ( 60%)]  Loss:  3.471000 (3.4489)  Time: 1.111s,  921.80/s  (1.108s,  923.96/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Train: 95 [ 800/1251 ( 64%)]  Loss:  3.466125 (3.4499)  Time: 1.106s,  925.48/s  (1.108s,  923.94/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [ 850/1251 ( 68%)]  Loss:  3.610204 (3.4588)  Time: 1.101s,  929.70/s  (1.108s,  924.23/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [ 900/1251 ( 72%)]  Loss:  3.672360 (3.4700)  Time: 1.103s,  928.16/s  (1.108s,  924.26/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [ 950/1251 ( 76%)]  Loss:  3.121622 (3.4526)  Time: 1.097s,  933.63/s  (1.108s,  924.34/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Train: 95 [1000/1251 ( 80%)]  Loss:  3.438225 (3.4519)  Time: 1.095s,  934.89/s  (1.108s,  924.23/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [1050/1251 ( 84%)]  Loss:  3.302665 (3.4451)  Time: 1.118s,  915.81/s  (1.108s,  924.27/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Train: 95 [1100/1251 ( 88%)]  Loss:  3.448079 (3.4453)  Time: 1.096s,  934.05/s  (1.108s,  924.24/s)  LR: 3.873e-04  Data: 0.013 (0.012)
Train: 95 [1150/1251 ( 92%)]  Loss:  3.606185 (3.4520)  Time: 1.097s,  933.52/s  (1.108s,  924.24/s)  LR: 3.873e-04  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 95 [1200/1251 ( 96%)]  Loss:  3.140787 (3.4395)  Time: 1.111s,  921.36/s  (1.108s,  924.35/s)  LR: 3.873e-04  Data: 0.011 (0.012)
Train: 95 [1250/1251 (100%)]  Loss:  3.407442 (3.4383)  Time: 1.081s,  947.13/s  (1.108s,  924.38/s)  LR: 3.873e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.281 (3.281)  Loss:  0.5420 (0.5420)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6304 (1.0529)  Acc@1: 85.9670 (75.9900)  Acc@5: 96.8160 (93.2600)
Test (EMA): [   0/48]  Time: 3.106 (3.106)  Loss:  0.4537 (0.4537)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.5586 (97.5586)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  0.5790 (0.9319)  Acc@1: 86.0849 (78.0720)  Acc@5: 97.6415 (94.3240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 77.71000010986329)

Train: 96 [   0/1251 (  0%)]  Loss:  3.154332 (3.1543)  Time: 1.111s,  921.73/s  (1.111s,  921.73/s)  LR: 3.851e-04  Data: 0.027 (0.027)
Train: 96 [  50/1251 (  4%)]  Loss:  3.384130 (3.2692)  Time: 1.095s,  935.31/s  (1.107s,  925.42/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 100/1251 (  8%)]  Loss:  3.659554 (3.3993)  Time: 1.098s,  932.83/s  (1.106s,  926.05/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 150/1251 ( 12%)]  Loss:  3.216635 (3.3537)  Time: 1.125s,  910.52/s  (1.107s,  924.70/s)  LR: 3.851e-04  Data: 0.012 (0.012)
Train: 96 [ 200/1251 ( 16%)]  Loss:  3.135781 (3.3101)  Time: 1.100s,  930.96/s  (1.107s,  924.71/s)  LR: 3.851e-04  Data: 0.012 (0.012)
Train: 96 [ 250/1251 ( 20%)]  Loss:  3.491032 (3.3402)  Time: 1.099s,  931.55/s  (1.106s,  926.25/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 300/1251 ( 24%)]  Loss:  3.704144 (3.3922)  Time: 1.101s,  930.24/s  (1.106s,  926.26/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 350/1251 ( 28%)]  Loss:  3.315549 (3.3826)  Time: 1.101s,  929.80/s  (1.106s,  926.00/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 400/1251 ( 32%)]  Loss:  3.724772 (3.4207)  Time: 1.097s,  933.57/s  (1.106s,  925.49/s)  LR: 3.851e-04  Data: 0.012 (0.012)
Train: 96 [ 450/1251 ( 36%)]  Loss:  3.458520 (3.4244)  Time: 1.095s,  935.16/s  (1.108s,  924.30/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 500/1251 ( 40%)]  Loss:  3.652287 (3.4452)  Time: 1.097s,  933.12/s  (1.109s,  923.73/s)  LR: 3.851e-04  Data: 0.012 (0.012)
Train: 96 [ 550/1251 ( 44%)]  Loss:  3.496196 (3.4494)  Time: 1.098s,  932.48/s  (1.108s,  923.92/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 600/1251 ( 48%)]  Loss:  3.440677 (3.4487)  Time: 1.098s,  932.83/s  (1.108s,  924.30/s)  LR: 3.851e-04  Data: 0.012 (0.012)
Train: 96 [ 650/1251 ( 52%)]  Loss:  3.580513 (3.4582)  Time: 1.095s,  934.88/s  (1.108s,  924.34/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 700/1251 ( 56%)]  Loss:  3.196901 (3.4407)  Time: 1.129s,  906.68/s  (1.108s,  924.50/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 750/1251 ( 60%)]  Loss:  3.202820 (3.4259)  Time: 1.097s,  933.74/s  (1.108s,  924.01/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 800/1251 ( 64%)]  Loss:  3.306584 (3.4188)  Time: 1.098s,  932.96/s  (1.108s,  924.41/s)  LR: 3.851e-04  Data: 0.012 (0.012)
Train: 96 [ 850/1251 ( 68%)]  Loss:  3.460302 (3.4212)  Time: 1.197s,  855.70/s  (1.108s,  924.44/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 900/1251 ( 72%)]  Loss:  3.214807 (3.4103)  Time: 1.133s,  904.05/s  (1.108s,  924.56/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [ 950/1251 ( 76%)]  Loss:  3.447172 (3.4121)  Time: 1.104s,  927.43/s  (1.108s,  924.39/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [1000/1251 ( 80%)]  Loss:  3.342680 (3.4088)  Time: 1.100s,  930.58/s  (1.108s,  924.11/s)  LR: 3.851e-04  Data: 0.014 (0.012)
Train: 96 [1050/1251 ( 84%)]  Loss:  3.852401 (3.4290)  Time: 1.095s,  934.87/s  (1.108s,  924.18/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [1100/1251 ( 88%)]  Loss:  3.401233 (3.4278)  Time: 1.143s,  895.94/s  (1.108s,  924.21/s)  LR: 3.851e-04  Data: 0.010 (0.012)
Train: 96 [1150/1251 ( 92%)]  Loss:  3.274887 (3.4214)  Time: 1.128s,  907.44/s  (1.108s,  924.23/s)  LR: 3.851e-04  Data: 0.011 (0.012)
Train: 96 [1200/1251 ( 96%)]  Loss:  3.446720 (3.4224)  Time: 1.090s,  939.11/s  (1.108s,  924.05/s)  LR: 3.851e-04  Data: 0.010 (0.012)
Train: 96 [1250/1251 (100%)]  Loss:  3.347476 (3.4195)  Time: 1.079s,  948.96/s  (1.108s,  923.99/s)  LR: 3.851e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.362 (3.362)  Loss:  0.5163 (0.5163)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.6862 (1.0357)  Acc@1: 84.9057 (76.1860)  Acc@5: 96.8160 (93.3600)
Test (EMA): [   0/48]  Time: 3.125 (3.125)  Loss:  0.4534 (0.4534)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.230 (0.410)  Loss:  0.5764 (0.9299)  Acc@1: 86.0849 (78.1440)  Acc@5: 97.5236 (94.3400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 77.71400003173828)

Train: 97 [   0/1251 (  0%)]  Loss:  3.306051 (3.3061)  Time: 1.102s,  929.10/s  (1.102s,  929.10/s)  LR: 3.829e-04  Data: 0.019 (0.019)
Train: 97 [  50/1251 (  4%)]  Loss:  3.209822 (3.2579)  Time: 1.098s,  932.64/s  (1.107s,  925.17/s)  LR: 3.829e-04  Data: 0.012 (0.012)
Train: 97 [ 100/1251 (  8%)]  Loss:  3.620546 (3.3788)  Time: 1.100s,  931.04/s  (1.105s,  926.50/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 150/1251 ( 12%)]  Loss:  3.703650 (3.4600)  Time: 1.103s,  928.45/s  (1.106s,  925.87/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 200/1251 ( 16%)]  Loss:  3.345011 (3.4370)  Time: 1.099s,  932.05/s  (1.105s,  927.03/s)  LR: 3.829e-04  Data: 0.010 (0.012)
Train: 97 [ 250/1251 ( 20%)]  Loss:  3.617355 (3.4671)  Time: 1.095s,  935.02/s  (1.105s,  926.51/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 300/1251 ( 24%)]  Loss:  3.363261 (3.4522)  Time: 1.151s,  889.76/s  (1.105s,  926.60/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 350/1251 ( 28%)]  Loss:  3.452012 (3.4522)  Time: 1.093s,  936.61/s  (1.106s,  926.08/s)  LR: 3.829e-04  Data: 0.010 (0.012)
Train: 97 [ 400/1251 ( 32%)]  Loss:  3.694810 (3.4792)  Time: 1.095s,  935.43/s  (1.106s,  925.75/s)  LR: 3.829e-04  Data: 0.012 (0.012)
Train: 97 [ 450/1251 ( 36%)]  Loss:  3.625682 (3.4938)  Time: 1.121s,  913.20/s  (1.106s,  925.71/s)  LR: 3.829e-04  Data: 0.013 (0.012)
Train: 97 [ 500/1251 ( 40%)]  Loss:  3.144074 (3.4620)  Time: 1.133s,  903.72/s  (1.107s,  925.21/s)  LR: 3.829e-04  Data: 0.012 (0.012)
Train: 97 [ 550/1251 ( 44%)]  Loss:  3.420312 (3.4585)  Time: 1.101s,  929.82/s  (1.108s,  924.44/s)  LR: 3.829e-04  Data: 0.013 (0.012)
Train: 97 [ 600/1251 ( 48%)]  Loss:  3.543061 (3.4650)  Time: 1.120s,  913.89/s  (1.108s,  924.34/s)  LR: 3.829e-04  Data: 0.013 (0.012)
Train: 97 [ 650/1251 ( 52%)]  Loss:  3.532867 (3.4699)  Time: 1.095s,  935.57/s  (1.108s,  924.21/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 700/1251 ( 56%)]  Loss:  3.359232 (3.4625)  Time: 1.093s,  936.44/s  (1.108s,  924.27/s)  LR: 3.829e-04  Data: 0.013 (0.012)
Train: 97 [ 750/1251 ( 60%)]  Loss:  3.747749 (3.4803)  Time: 1.124s,  910.87/s  (1.108s,  924.45/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 800/1251 ( 64%)]  Loss:  3.358761 (3.4732)  Time: 1.165s,  878.87/s  (1.108s,  923.95/s)  LR: 3.829e-04  Data: 0.010 (0.012)
Train: 97 [ 850/1251 ( 68%)]  Loss:  3.344699 (3.4661)  Time: 1.099s,  931.68/s  (1.108s,  923.97/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [ 900/1251 ( 72%)]  Loss:  3.357638 (3.4603)  Time: 1.101s,  930.37/s  (1.108s,  923.80/s)  LR: 3.829e-04  Data: 0.013 (0.012)
Train: 97 [ 950/1251 ( 76%)]  Loss:  3.683090 (3.4715)  Time: 1.119s,  915.41/s  (1.108s,  923.93/s)  LR: 3.829e-04  Data: 0.010 (0.012)
Train: 97 [1000/1251 ( 80%)]  Loss:  3.655750 (3.4803)  Time: 1.119s,  915.10/s  (1.108s,  923.78/s)  LR: 3.829e-04  Data: 0.010 (0.012)
Train: 97 [1050/1251 ( 84%)]  Loss:  3.264963 (3.4705)  Time: 1.095s,  935.41/s  (1.108s,  923.94/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 97 [1100/1251 ( 88%)]  Loss:  3.449834 (3.4696)  Time: 1.125s,  910.49/s  (1.108s,  923.96/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [1150/1251 ( 92%)]  Loss:  3.277718 (3.4616)  Time: 1.098s,  932.35/s  (1.109s,  923.73/s)  LR: 3.829e-04  Data: 0.013 (0.012)
Train: 97 [1200/1251 ( 96%)]  Loss:  3.343177 (3.4568)  Time: 1.097s,  933.20/s  (1.109s,  923.64/s)  LR: 3.829e-04  Data: 0.011 (0.012)
Train: 97 [1250/1251 (100%)]  Loss:  3.589659 (3.4620)  Time: 1.076s,  951.32/s  (1.109s,  923.52/s)  LR: 3.829e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.222 (3.222)  Loss:  0.5526 (0.5526)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.6228 (1.0431)  Acc@1: 85.6132 (76.0420)  Acc@5: 97.0519 (93.2220)
Test (EMA): [   0/48]  Time: 3.225 (3.225)  Loss:  0.4526 (0.4526)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5749 (0.9281)  Acc@1: 86.0849 (78.1900)  Acc@5: 97.5236 (94.3320)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 77.77600013427734)

Train: 98 [   0/1251 (  0%)]  Loss:  3.405323 (3.4053)  Time: 1.114s,  919.40/s  (1.114s,  919.40/s)  LR: 3.807e-04  Data: 0.030 (0.030)
Train: 98 [  50/1251 (  4%)]  Loss:  3.434512 (3.4199)  Time: 1.097s,  933.78/s  (1.109s,  923.33/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 100/1251 (  8%)]  Loss:  3.601444 (3.4804)  Time: 1.100s,  930.60/s  (1.109s,  923.28/s)  LR: 3.807e-04  Data: 0.016 (0.012)
Train: 98 [ 150/1251 ( 12%)]  Loss:  3.266815 (3.4270)  Time: 1.104s,  927.12/s  (1.107s,  925.39/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 200/1251 ( 16%)]  Loss:  3.493938 (3.4404)  Time: 1.098s,  932.79/s  (1.106s,  925.79/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 250/1251 ( 20%)]  Loss:  3.420069 (3.4370)  Time: 1.106s,  925.71/s  (1.106s,  926.11/s)  LR: 3.807e-04  Data: 0.013 (0.012)
Train: 98 [ 300/1251 ( 24%)]  Loss:  3.361197 (3.4262)  Time: 1.097s,  933.21/s  (1.106s,  925.89/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 350/1251 ( 28%)]  Loss:  3.327110 (3.4138)  Time: 1.097s,  933.74/s  (1.107s,  925.21/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 400/1251 ( 32%)]  Loss:  3.156079 (3.3852)  Time: 1.102s,  928.99/s  (1.106s,  925.67/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 450/1251 ( 36%)]  Loss:  3.429537 (3.3896)  Time: 1.096s,  934.09/s  (1.106s,  925.95/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 500/1251 ( 40%)]  Loss:  3.497956 (3.3995)  Time: 1.098s,  932.64/s  (1.106s,  926.06/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 550/1251 ( 44%)]  Loss:  3.258832 (3.3877)  Time: 1.097s,  933.32/s  (1.106s,  925.61/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 600/1251 ( 48%)]  Loss:  3.221447 (3.3749)  Time: 1.130s,  905.85/s  (1.106s,  925.83/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 650/1251 ( 52%)]  Loss:  3.431645 (3.3790)  Time: 1.120s,  914.40/s  (1.106s,  925.62/s)  LR: 3.807e-04  Data: 0.010 (0.012)
Train: 98 [ 700/1251 ( 56%)]  Loss:  3.218216 (3.3683)  Time: 1.121s,  913.64/s  (1.107s,  925.15/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 750/1251 ( 60%)]  Loss:  3.489082 (3.3758)  Time: 1.176s,  870.59/s  (1.107s,  924.80/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 800/1251 ( 64%)]  Loss:  3.210672 (3.3661)  Time: 1.098s,  932.56/s  (1.107s,  924.82/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 850/1251 ( 68%)]  Loss:  3.248081 (3.3596)  Time: 1.101s,  929.78/s  (1.107s,  924.99/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [ 900/1251 ( 72%)]  Loss:  3.358101 (3.3595)  Time: 1.095s,  935.04/s  (1.107s,  924.92/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [ 950/1251 ( 76%)]  Loss:  3.522542 (3.3676)  Time: 1.124s,  910.79/s  (1.107s,  925.02/s)  LR: 3.807e-04  Data: 0.013 (0.012)
Train: 98 [1000/1251 ( 80%)]  Loss:  3.222010 (3.3607)  Time: 1.097s,  933.65/s  (1.107s,  925.11/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [1050/1251 ( 84%)]  Loss:  3.070546 (3.3475)  Time: 1.122s,  912.48/s  (1.107s,  925.04/s)  LR: 3.807e-04  Data: 0.012 (0.012)
Train: 98 [1100/1251 ( 88%)]  Loss:  3.475618 (3.3531)  Time: 1.096s,  934.71/s  (1.108s,  924.55/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [1150/1251 ( 92%)]  Loss:  3.459615 (3.3575)  Time: 1.098s,  932.48/s  (1.108s,  924.51/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [1200/1251 ( 96%)]  Loss:  3.254414 (3.3534)  Time: 1.098s,  932.61/s  (1.108s,  924.60/s)  LR: 3.807e-04  Data: 0.011 (0.012)
Train: 98 [1250/1251 (100%)]  Loss:  3.524285 (3.3600)  Time: 1.080s,  948.53/s  (1.107s,  924.79/s)  LR: 3.807e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.326 (3.326)  Loss:  0.5156 (0.5156)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.5933 (1.0274)  Acc@1: 85.8491 (76.2360)  Acc@5: 97.4057 (93.4000)
Test (EMA): [   0/48]  Time: 3.163 (3.163)  Loss:  0.4505 (0.4505)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5728 (0.9264)  Acc@1: 85.9670 (78.2520)  Acc@5: 97.5236 (94.3200)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 77.85200005615235)

Train: 99 [   0/1251 (  0%)]  Loss:  3.363576 (3.3636)  Time: 1.137s,  900.93/s  (1.137s,  900.93/s)  LR: 3.785e-04  Data: 0.028 (0.028)
Train: 99 [  50/1251 (  4%)]  Loss:  3.053170 (3.2084)  Time: 1.097s,  933.73/s  (1.111s,  921.78/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [ 100/1251 (  8%)]  Loss:  3.737660 (3.3848)  Time: 1.096s,  934.14/s  (1.114s,  919.01/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [ 150/1251 ( 12%)]  Loss:  3.388093 (3.3856)  Time: 1.129s,  907.15/s  (1.115s,  918.71/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [ 200/1251 ( 16%)]  Loss:  3.650931 (3.4387)  Time: 1.111s,  921.42/s  (1.112s,  920.64/s)  LR: 3.785e-04  Data: 0.012 (0.012)
Train: 99 [ 250/1251 ( 20%)]  Loss:  3.551929 (3.4576)  Time: 1.100s,  930.60/s  (1.113s,  919.69/s)  LR: 3.785e-04  Data: 0.018 (0.012)
Train: 99 [ 300/1251 ( 24%)]  Loss:  3.389228 (3.4478)  Time: 1.093s,  936.52/s  (1.112s,  920.55/s)  LR: 3.785e-04  Data: 0.012 (0.012)
Train: 99 [ 350/1251 ( 28%)]  Loss:  3.535167 (3.4587)  Time: 1.100s,  930.67/s  (1.111s,  921.76/s)  LR: 3.785e-04  Data: 0.016 (0.012)
Train: 99 [ 400/1251 ( 32%)]  Loss:  3.070376 (3.4156)  Time: 1.104s,  927.93/s  (1.112s,  920.98/s)  LR: 3.785e-04  Data: 0.010 (0.012)
Train: 99 [ 450/1251 ( 36%)]  Loss:  3.502098 (3.4242)  Time: 1.117s,  916.48/s  (1.112s,  921.02/s)  LR: 3.785e-04  Data: 0.013 (0.012)
Train: 99 [ 500/1251 ( 40%)]  Loss:  2.934581 (3.3797)  Time: 1.124s,  910.73/s  (1.112s,  921.02/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [ 550/1251 ( 44%)]  Loss:  3.793986 (3.4142)  Time: 1.096s,  934.72/s  (1.112s,  920.51/s)  LR: 3.785e-04  Data: 0.010 (0.012)
Train: 99 [ 600/1251 ( 48%)]  Loss:  3.592455 (3.4279)  Time: 1.096s,  934.25/s  (1.113s,  920.44/s)  LR: 3.785e-04  Data: 0.012 (0.012)
Train: 99 [ 650/1251 ( 52%)]  Loss:  3.137197 (3.4072)  Time: 1.097s,  933.45/s  (1.112s,  921.04/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 99 [ 700/1251 ( 56%)]  Loss:  3.425004 (3.4084)  Time: 1.168s,  877.09/s  (1.111s,  921.51/s)  LR: 3.785e-04  Data: 0.010 (0.012)
Train: 99 [ 750/1251 ( 60%)]  Loss:  3.636476 (3.4226)  Time: 1.093s,  936.99/s  (1.111s,  922.01/s)  LR: 3.785e-04  Data: 0.010 (0.012)
Train: 99 [ 800/1251 ( 64%)]  Loss:  3.427933 (3.4229)  Time: 1.096s,  934.44/s  (1.110s,  922.13/s)  LR: 3.785e-04  Data: 0.012 (0.012)
Train: 99 [ 850/1251 ( 68%)]  Loss:  3.878971 (3.4483)  Time: 1.095s,  934.91/s  (1.111s,  921.94/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [ 900/1251 ( 72%)]  Loss:  3.286617 (3.4398)  Time: 1.096s,  934.56/s  (1.110s,  922.21/s)  LR: 3.785e-04  Data: 0.012 (0.012)
Train: 99 [ 950/1251 ( 76%)]  Loss:  3.642713 (3.4499)  Time: 1.102s,  929.00/s  (1.110s,  922.37/s)  LR: 3.785e-04  Data: 0.013 (0.012)
Train: 99 [1000/1251 ( 80%)]  Loss:  3.286569 (3.4421)  Time: 1.096s,  934.25/s  (1.110s,  922.77/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [1050/1251 ( 84%)]  Loss:  3.633737 (3.4508)  Time: 1.097s,  933.79/s  (1.110s,  922.65/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [1100/1251 ( 88%)]  Loss:  3.522566 (3.4540)  Time: 1.121s,  913.28/s  (1.110s,  922.85/s)  LR: 3.785e-04  Data: 0.011 (0.012)
Train: 99 [1150/1251 ( 92%)]  Loss:  3.071069 (3.4380)  Time: 1.098s,  932.67/s  (1.110s,  922.88/s)  LR: 3.785e-04  Data: 0.014 (0.012)
Train: 99 [1200/1251 ( 96%)]  Loss:  3.346191 (3.4343)  Time: 1.097s,  933.36/s  (1.109s,  923.02/s)  LR: 3.785e-04  Data: 0.014 (0.012)
Train: 99 [1250/1251 (100%)]  Loss:  3.170625 (3.4242)  Time: 1.083s,  945.63/s  (1.110s,  922.81/s)  LR: 3.785e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.313 (3.313)  Loss:  0.5324 (0.5324)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6341 (1.0470)  Acc@1: 86.3208 (76.3220)  Acc@5: 97.1698 (93.3920)
Test (EMA): [   0/48]  Time: 3.042 (3.042)  Loss:  0.4483 (0.4483)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5700 (0.9243)  Acc@1: 85.9670 (78.3000)  Acc@5: 97.6415 (94.3560)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 77.88200000488281)

Train: 100 [   0/1251 (  0%)]  Loss:  3.417200 (3.4172)  Time: 1.105s,  926.49/s  (1.105s,  926.49/s)  LR: 3.763e-04  Data: 0.023 (0.023)
Train: 100 [  50/1251 (  4%)]  Loss:  3.524143 (3.4707)  Time: 1.097s,  933.79/s  (1.107s,  924.77/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 100/1251 (  8%)]  Loss:  3.377428 (3.4396)  Time: 1.190s,  860.19/s  (1.109s,  923.52/s)  LR: 3.763e-04  Data: 0.014 (0.012)
Train: 100 [ 150/1251 ( 12%)]  Loss:  3.225206 (3.3860)  Time: 1.105s,  926.48/s  (1.109s,  923.72/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 200/1251 ( 16%)]  Loss:  3.596372 (3.4281)  Time: 1.097s,  933.60/s  (1.108s,  924.00/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 250/1251 ( 20%)]  Loss:  3.380133 (3.4201)  Time: 1.096s,  934.22/s  (1.108s,  924.03/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 300/1251 ( 24%)]  Loss:  3.786606 (3.4724)  Time: 1.103s,  928.56/s  (1.107s,  924.92/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 350/1251 ( 28%)]  Loss:  3.458820 (3.4707)  Time: 1.100s,  930.66/s  (1.107s,  924.94/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 400/1251 ( 32%)]  Loss:  3.490467 (3.4729)  Time: 1.124s,  910.75/s  (1.107s,  925.40/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 450/1251 ( 36%)]  Loss:  3.448428 (3.4705)  Time: 1.103s,  928.36/s  (1.107s,  925.33/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [ 500/1251 ( 40%)]  Loss:  3.466116 (3.4701)  Time: 1.099s,  931.43/s  (1.106s,  925.48/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [ 550/1251 ( 44%)]  Loss:  3.394216 (3.4638)  Time: 1.094s,  935.59/s  (1.106s,  925.55/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [ 600/1251 ( 48%)]  Loss:  3.265032 (3.4485)  Time: 1.098s,  932.25/s  (1.106s,  925.98/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 650/1251 ( 52%)]  Loss:  3.394821 (3.4446)  Time: 1.104s,  927.88/s  (1.106s,  926.01/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 700/1251 ( 56%)]  Loss:  3.277805 (3.4335)  Time: 1.100s,  931.03/s  (1.106s,  926.24/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [ 750/1251 ( 60%)]  Loss:  3.463475 (3.4354)  Time: 1.092s,  937.64/s  (1.105s,  926.42/s)  LR: 3.763e-04  Data: 0.009 (0.012)
Train: 100 [ 800/1251 ( 64%)]  Loss:  3.633373 (3.4470)  Time: 1.098s,  932.30/s  (1.106s,  926.20/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [ 850/1251 ( 68%)]  Loss:  3.593443 (3.4552)  Time: 1.100s,  931.23/s  (1.106s,  926.25/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [ 900/1251 ( 72%)]  Loss:  3.533645 (3.4593)  Time: 1.098s,  932.73/s  (1.106s,  925.64/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [ 950/1251 ( 76%)]  Loss:  3.332356 (3.4530)  Time: 1.098s,  932.41/s  (1.106s,  925.85/s)  LR: 3.763e-04  Data: 0.013 (0.012)
Train: 100 [1000/1251 ( 80%)]  Loss:  3.315717 (3.4464)  Time: 1.100s,  931.14/s  (1.106s,  925.96/s)  LR: 3.763e-04  Data: 0.010 (0.012)
Train: 100 [1050/1251 ( 84%)]  Loss:  3.541214 (3.4507)  Time: 1.097s,  933.72/s  (1.106s,  926.05/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [1100/1251 ( 88%)]  Loss:  3.207795 (3.4402)  Time: 1.096s,  933.92/s  (1.106s,  925.91/s)  LR: 3.763e-04  Data: 0.012 (0.012)
Train: 100 [1150/1251 ( 92%)]  Loss:  3.447907 (3.4405)  Time: 1.094s,  936.23/s  (1.106s,  925.83/s)  LR: 3.763e-04  Data: 0.011 (0.012)
Train: 100 [1200/1251 ( 96%)]  Loss:  3.354315 (3.4370)  Time: 1.099s,  932.05/s  (1.106s,  925.69/s)  LR: 3.763e-04  Data: 0.010 (0.012)
Train: 100 [1250/1251 (100%)]  Loss:  3.334202 (3.4331)  Time: 1.084s,  945.00/s  (1.106s,  925.61/s)  LR: 3.763e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.251 (3.251)  Loss:  0.5293 (0.5293)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6278 (1.0401)  Acc@1: 85.9670 (76.4060)  Acc@5: 97.2877 (93.4060)
Test (EMA): [   0/48]  Time: 3.111 (3.111)  Loss:  0.4459 (0.4459)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5686 (0.9223)  Acc@1: 85.9670 (78.2960)  Acc@5: 97.5236 (94.3860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 77.90799995361328)

Train: 101 [   0/1251 (  0%)]  Loss:  3.225854 (3.2259)  Time: 1.105s,  926.94/s  (1.105s,  926.94/s)  LR: 3.740e-04  Data: 0.020 (0.020)
Train: 101 [  50/1251 (  4%)]  Loss:  3.229406 (3.2276)  Time: 1.187s,  862.55/s  (1.107s,  925.26/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 100/1251 (  8%)]  Loss:  3.430584 (3.2953)  Time: 1.102s,  928.94/s  (1.105s,  926.67/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 150/1251 ( 12%)]  Loss:  3.533839 (3.3549)  Time: 1.098s,  932.74/s  (1.105s,  926.67/s)  LR: 3.740e-04  Data: 0.010 (0.012)
Train: 101 [ 200/1251 ( 16%)]  Loss:  3.045661 (3.2931)  Time: 1.099s,  931.69/s  (1.106s,  926.01/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [ 250/1251 ( 20%)]  Loss:  3.497793 (3.3272)  Time: 1.095s,  935.17/s  (1.105s,  926.53/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 101 [ 300/1251 ( 24%)]  Loss:  3.308668 (3.3245)  Time: 1.097s,  933.19/s  (1.106s,  925.58/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 350/1251 ( 28%)]  Loss:  3.392512 (3.3330)  Time: 1.133s,  903.70/s  (1.106s,  925.62/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 400/1251 ( 32%)]  Loss:  3.437289 (3.3446)  Time: 1.121s,  913.16/s  (1.108s,  923.92/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 450/1251 ( 36%)]  Loss:  3.553493 (3.3655)  Time: 1.097s,  933.27/s  (1.108s,  924.34/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [ 500/1251 ( 40%)]  Loss:  3.103286 (3.3417)  Time: 1.102s,  929.16/s  (1.108s,  924.24/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 550/1251 ( 44%)]  Loss:  3.430130 (3.3490)  Time: 1.093s,  936.73/s  (1.108s,  924.26/s)  LR: 3.740e-04  Data: 0.013 (0.012)
Train: 101 [ 600/1251 ( 48%)]  Loss:  3.660754 (3.3730)  Time: 1.097s,  933.78/s  (1.107s,  924.66/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [ 650/1251 ( 52%)]  Loss:  3.172551 (3.3587)  Time: 1.100s,  930.93/s  (1.107s,  924.66/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 700/1251 ( 56%)]  Loss:  3.414539 (3.3624)  Time: 1.094s,  935.70/s  (1.107s,  924.84/s)  LR: 3.740e-04  Data: 0.010 (0.012)
Train: 101 [ 750/1251 ( 60%)]  Loss:  3.544442 (3.3738)  Time: 1.096s,  934.64/s  (1.107s,  924.70/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [ 800/1251 ( 64%)]  Loss:  3.454921 (3.3786)  Time: 1.097s,  933.16/s  (1.108s,  924.37/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [ 850/1251 ( 68%)]  Loss:  3.455891 (3.3829)  Time: 1.135s,  902.20/s  (1.108s,  924.11/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [ 900/1251 ( 72%)]  Loss:  3.515895 (3.3899)  Time: 1.121s,  913.73/s  (1.109s,  923.75/s)  LR: 3.740e-04  Data: 0.010 (0.012)
Train: 101 [ 950/1251 ( 76%)]  Loss:  3.439045 (3.3923)  Time: 1.095s,  934.86/s  (1.109s,  923.74/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [1000/1251 ( 80%)]  Loss:  3.195278 (3.3829)  Time: 1.175s,  871.62/s  (1.108s,  923.83/s)  LR: 3.740e-04  Data: 0.014 (0.012)
Train: 101 [1050/1251 ( 84%)]  Loss:  3.294246 (3.3789)  Time: 1.197s,  855.69/s  (1.108s,  923.87/s)  LR: 3.740e-04  Data: 0.014 (0.012)
Train: 101 [1100/1251 ( 88%)]  Loss:  3.558717 (3.3867)  Time: 1.100s,  930.98/s  (1.108s,  923.99/s)  LR: 3.740e-04  Data: 0.011 (0.012)
Train: 101 [1150/1251 ( 92%)]  Loss:  3.666521 (3.3984)  Time: 1.124s,  910.96/s  (1.109s,  923.48/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [1200/1251 ( 96%)]  Loss:  3.794697 (3.4142)  Time: 1.098s,  932.50/s  (1.109s,  923.46/s)  LR: 3.740e-04  Data: 0.012 (0.012)
Train: 101 [1250/1251 (100%)]  Loss:  3.485447 (3.4170)  Time: 1.080s,  948.52/s  (1.109s,  923.35/s)  LR: 3.740e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.232 (3.232)  Loss:  0.5449 (0.5449)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6678 (1.0704)  Acc@1: 85.6132 (75.9260)  Acc@5: 97.1698 (93.2180)
Test (EMA): [   0/48]  Time: 3.304 (3.304)  Loss:  0.4439 (0.4439)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.5680 (0.9207)  Acc@1: 85.9670 (78.3600)  Acc@5: 97.5236 (94.4160)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 77.91199995361328)

Train: 102 [   0/1251 (  0%)]  Loss:  3.586854 (3.5869)  Time: 1.123s,  911.53/s  (1.123s,  911.53/s)  LR: 3.717e-04  Data: 0.022 (0.022)
Train: 102 [  50/1251 (  4%)]  Loss:  3.771480 (3.6792)  Time: 1.093s,  937.05/s  (1.105s,  926.48/s)  LR: 3.717e-04  Data: 0.009 (0.012)
Train: 102 [ 100/1251 (  8%)]  Loss:  3.399137 (3.5858)  Time: 1.121s,  913.13/s  (1.109s,  923.71/s)  LR: 3.717e-04  Data: 0.010 (0.012)
Train: 102 [ 150/1251 ( 12%)]  Loss:  3.612859 (3.5926)  Time: 1.097s,  933.06/s  (1.109s,  923.31/s)  LR: 3.717e-04  Data: 0.012 (0.012)
Train: 102 [ 200/1251 ( 16%)]  Loss:  3.299285 (3.5339)  Time: 1.095s,  935.44/s  (1.109s,  923.43/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 250/1251 ( 20%)]  Loss:  2.954220 (3.4373)  Time: 1.097s,  933.58/s  (1.109s,  923.54/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 300/1251 ( 24%)]  Loss:  3.351899 (3.4251)  Time: 1.093s,  937.09/s  (1.110s,  922.31/s)  LR: 3.717e-04  Data: 0.012 (0.012)
Train: 102 [ 350/1251 ( 28%)]  Loss:  3.292576 (3.4085)  Time: 1.098s,  932.32/s  (1.111s,  921.79/s)  LR: 3.717e-04  Data: 0.013 (0.012)
Train: 102 [ 400/1251 ( 32%)]  Loss:  3.332654 (3.4001)  Time: 1.097s,  933.37/s  (1.110s,  922.83/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 450/1251 ( 36%)]  Loss:  3.699866 (3.4301)  Time: 1.099s,  932.07/s  (1.110s,  922.83/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 500/1251 ( 40%)]  Loss:  3.156923 (3.4053)  Time: 1.098s,  932.75/s  (1.109s,  923.65/s)  LR: 3.717e-04  Data: 0.012 (0.012)
Train: 102 [ 550/1251 ( 44%)]  Loss:  3.046411 (3.3753)  Time: 1.100s,  931.33/s  (1.109s,  923.61/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 600/1251 ( 48%)]  Loss:  3.076011 (3.3523)  Time: 1.103s,  928.19/s  (1.108s,  923.97/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 650/1251 ( 52%)]  Loss:  3.424619 (3.3575)  Time: 1.107s,  925.42/s  (1.108s,  924.15/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 700/1251 ( 56%)]  Loss:  3.635468 (3.3760)  Time: 1.102s,  929.30/s  (1.108s,  924.49/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 750/1251 ( 60%)]  Loss:  3.416819 (3.3786)  Time: 1.100s,  930.96/s  (1.108s,  924.44/s)  LR: 3.717e-04  Data: 0.012 (0.012)
Train: 102 [ 800/1251 ( 64%)]  Loss:  3.286675 (3.3732)  Time: 1.129s,  907.07/s  (1.108s,  924.37/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 850/1251 ( 68%)]  Loss:  3.419798 (3.3758)  Time: 1.131s,  905.11/s  (1.108s,  924.41/s)  LR: 3.717e-04  Data: 0.010 (0.012)
Train: 102 [ 900/1251 ( 72%)]  Loss:  3.380713 (3.3760)  Time: 1.103s,  928.29/s  (1.108s,  924.18/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [ 950/1251 ( 76%)]  Loss:  3.560406 (3.3852)  Time: 1.189s,  861.41/s  (1.108s,  924.53/s)  LR: 3.717e-04  Data: 0.013 (0.012)
Train: 102 [1000/1251 ( 80%)]  Loss:  3.548766 (3.3930)  Time: 1.119s,  914.69/s  (1.107s,  924.70/s)  LR: 3.717e-04  Data: 0.012 (0.012)
Train: 102 [1050/1251 ( 84%)]  Loss:  3.305962 (3.3891)  Time: 1.090s,  939.10/s  (1.107s,  924.94/s)  LR: 3.717e-04  Data: 0.010 (0.012)
Train: 102 [1100/1251 ( 88%)]  Loss:  3.690363 (3.4022)  Time: 1.099s,  932.00/s  (1.108s,  924.41/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 102 [1150/1251 ( 92%)]  Loss:  3.402306 (3.4022)  Time: 1.098s,  932.97/s  (1.107s,  924.65/s)  LR: 3.717e-04  Data: 0.012 (0.012)
Train: 102 [1200/1251 ( 96%)]  Loss:  3.497306 (3.4060)  Time: 1.098s,  932.32/s  (1.107s,  924.78/s)  LR: 3.717e-04  Data: 0.011 (0.012)
Train: 102 [1250/1251 (100%)]  Loss:  3.286565 (3.4014)  Time: 1.085s,  943.66/s  (1.107s,  924.76/s)  LR: 3.717e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.290 (3.290)  Loss:  0.5410 (0.5410)  Acc@1: 89.9414 (89.9414)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.6518 (1.0278)  Acc@1: 86.4387 (76.3780)  Acc@5: 96.6981 (93.4200)
Test (EMA): [   0/48]  Time: 3.329 (3.329)  Loss:  0.4426 (0.4426)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5676 (0.9191)  Acc@1: 86.0849 (78.4420)  Acc@5: 97.5236 (94.4320)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 77.97799998046875)

Train: 103 [   0/1251 (  0%)]  Loss:  3.522524 (3.5225)  Time: 1.104s,  927.89/s  (1.104s,  927.89/s)  LR: 3.695e-04  Data: 0.023 (0.023)
Train: 103 [  50/1251 (  4%)]  Loss:  3.682132 (3.6023)  Time: 1.095s,  935.00/s  (1.105s,  926.42/s)  LR: 3.695e-04  Data: 0.012 (0.012)
Train: 103 [ 100/1251 (  8%)]  Loss:  3.404550 (3.5364)  Time: 1.099s,  931.85/s  (1.108s,  924.19/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 150/1251 ( 12%)]  Loss:  3.290759 (3.4750)  Time: 1.096s,  934.37/s  (1.106s,  925.50/s)  LR: 3.695e-04  Data: 0.010 (0.012)
Train: 103 [ 200/1251 ( 16%)]  Loss:  3.619224 (3.5038)  Time: 1.097s,  933.65/s  (1.107s,  925.41/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 250/1251 ( 20%)]  Loss:  3.490043 (3.5015)  Time: 1.134s,  903.11/s  (1.106s,  925.75/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 300/1251 ( 24%)]  Loss:  3.612725 (3.5174)  Time: 1.096s,  934.14/s  (1.108s,  923.92/s)  LR: 3.695e-04  Data: 0.012 (0.012)
Train: 103 [ 350/1251 ( 28%)]  Loss:  3.677236 (3.5374)  Time: 1.108s,  924.00/s  (1.107s,  924.72/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 400/1251 ( 32%)]  Loss:  3.243054 (3.5047)  Time: 1.103s,  928.46/s  (1.109s,  923.50/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 450/1251 ( 36%)]  Loss:  3.584707 (3.5127)  Time: 1.208s,  847.50/s  (1.109s,  922.99/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 500/1251 ( 40%)]  Loss:  3.140450 (3.4789)  Time: 1.106s,  926.00/s  (1.110s,  922.56/s)  LR: 3.695e-04  Data: 0.012 (0.012)
Train: 103 [ 550/1251 ( 44%)]  Loss:  3.439295 (3.4756)  Time: 1.096s,  934.25/s  (1.110s,  922.91/s)  LR: 3.695e-04  Data: 0.010 (0.012)
Train: 103 [ 600/1251 ( 48%)]  Loss:  3.427413 (3.4719)  Time: 1.097s,  933.56/s  (1.109s,  923.42/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 650/1251 ( 52%)]  Loss:  3.717009 (3.4894)  Time: 1.132s,  904.71/s  (1.110s,  922.33/s)  LR: 3.695e-04  Data: 0.011 (0.011)
Train: 103 [ 700/1251 ( 56%)]  Loss:  3.344529 (3.4797)  Time: 1.098s,  933.00/s  (1.110s,  922.75/s)  LR: 3.695e-04  Data: 0.012 (0.011)
Train: 103 [ 750/1251 ( 60%)]  Loss:  3.460428 (3.4785)  Time: 1.096s,  934.47/s  (1.110s,  922.57/s)  LR: 3.695e-04  Data: 0.011 (0.011)
Train: 103 [ 800/1251 ( 64%)]  Loss:  3.439229 (3.4762)  Time: 1.095s,  935.14/s  (1.110s,  922.88/s)  LR: 3.695e-04  Data: 0.012 (0.011)
Train: 103 [ 850/1251 ( 68%)]  Loss:  3.379325 (3.4708)  Time: 1.092s,  937.32/s  (1.110s,  922.72/s)  LR: 3.695e-04  Data: 0.010 (0.011)
Train: 103 [ 900/1251 ( 72%)]  Loss:  3.420788 (3.4682)  Time: 1.133s,  903.94/s  (1.110s,  922.83/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [ 950/1251 ( 76%)]  Loss:  3.367499 (3.4631)  Time: 1.096s,  933.91/s  (1.109s,  922.99/s)  LR: 3.695e-04  Data: 0.011 (0.011)
Train: 103 [1000/1251 ( 80%)]  Loss:  3.374045 (3.4589)  Time: 1.096s,  934.02/s  (1.110s,  922.92/s)  LR: 3.695e-04  Data: 0.013 (0.012)
Train: 103 [1050/1251 ( 84%)]  Loss:  3.324989 (3.4528)  Time: 1.247s,  821.36/s  (1.110s,  922.61/s)  LR: 3.695e-04  Data: 0.013 (0.012)
Train: 103 [1100/1251 ( 88%)]  Loss:  3.711663 (3.4641)  Time: 1.098s,  932.69/s  (1.110s,  922.47/s)  LR: 3.695e-04  Data: 0.012 (0.012)
Train: 103 [1150/1251 ( 92%)]  Loss:  3.187614 (3.4526)  Time: 1.119s,  914.95/s  (1.110s,  922.59/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [1200/1251 ( 96%)]  Loss:  3.695215 (3.4623)  Time: 1.097s,  933.26/s  (1.110s,  922.14/s)  LR: 3.695e-04  Data: 0.011 (0.012)
Train: 103 [1250/1251 (100%)]  Loss:  3.375245 (3.4589)  Time: 1.172s,  873.88/s  (1.110s,  922.28/s)  LR: 3.695e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.261 (3.261)  Loss:  0.5298 (0.5298)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6096 (1.0178)  Acc@1: 86.6745 (76.4700)  Acc@5: 97.2877 (93.4100)
Test (EMA): [   0/48]  Time: 3.090 (3.090)  Loss:  0.4413 (0.4413)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5672 (0.9175)  Acc@1: 86.3208 (78.4920)  Acc@5: 97.6415 (94.4320)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 77.99799998046875)

Train: 104 [   0/1251 (  0%)]  Loss:  3.344472 (3.3445)  Time: 1.104s,  927.57/s  (1.104s,  927.57/s)  LR: 3.672e-04  Data: 0.021 (0.021)
Train: 104 [  50/1251 (  4%)]  Loss:  3.730103 (3.5373)  Time: 1.095s,  935.03/s  (1.105s,  926.56/s)  LR: 3.672e-04  Data: 0.013 (0.012)
Train: 104 [ 100/1251 (  8%)]  Loss:  3.242992 (3.4392)  Time: 1.097s,  933.32/s  (1.105s,  926.52/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 150/1251 ( 12%)]  Loss:  3.158786 (3.3691)  Time: 1.098s,  932.58/s  (1.107s,  924.97/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 200/1251 ( 16%)]  Loss:  3.259230 (3.3471)  Time: 1.100s,  931.22/s  (1.106s,  925.55/s)  LR: 3.672e-04  Data: 0.015 (0.012)
Train: 104 [ 250/1251 ( 20%)]  Loss:  3.565881 (3.3836)  Time: 1.092s,  937.82/s  (1.108s,  924.25/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 300/1251 ( 24%)]  Loss:  3.284903 (3.3695)  Time: 1.103s,  928.23/s  (1.107s,  924.61/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 350/1251 ( 28%)]  Loss:  3.405075 (3.3739)  Time: 1.103s,  928.80/s  (1.108s,  924.16/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 400/1251 ( 32%)]  Loss:  3.389334 (3.3756)  Time: 1.102s,  929.44/s  (1.108s,  924.07/s)  LR: 3.672e-04  Data: 0.013 (0.012)
Train: 104 [ 450/1251 ( 36%)]  Loss:  3.135721 (3.3516)  Time: 1.096s,  934.43/s  (1.108s,  923.89/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 500/1251 ( 40%)]  Loss:  3.282891 (3.3454)  Time: 1.094s,  936.22/s  (1.108s,  924.26/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 550/1251 ( 44%)]  Loss:  3.391050 (3.3492)  Time: 1.097s,  933.67/s  (1.108s,  924.34/s)  LR: 3.672e-04  Data: 0.012 (0.012)
Train: 104 [ 600/1251 ( 48%)]  Loss:  3.197930 (3.3376)  Time: 1.118s,  915.84/s  (1.108s,  924.27/s)  LR: 3.672e-04  Data: 0.012 (0.012)
Train: 104 [ 650/1251 ( 52%)]  Loss:  3.319992 (3.3363)  Time: 1.190s,  860.24/s  (1.108s,  924.55/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 700/1251 ( 56%)]  Loss:  3.403428 (3.3408)  Time: 1.110s,  922.90/s  (1.107s,  924.71/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 750/1251 ( 60%)]  Loss:  3.625813 (3.3586)  Time: 1.098s,  932.65/s  (1.107s,  924.92/s)  LR: 3.672e-04  Data: 0.012 (0.012)
Train: 104 [ 800/1251 ( 64%)]  Loss:  3.353136 (3.3583)  Time: 1.097s,  933.24/s  (1.107s,  924.80/s)  LR: 3.672e-04  Data: 0.012 (0.012)
Train: 104 [ 850/1251 ( 68%)]  Loss:  3.419965 (3.3617)  Time: 1.096s,  934.46/s  (1.107s,  924.91/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 900/1251 ( 72%)]  Loss:  3.575908 (3.3730)  Time: 1.102s,  929.02/s  (1.107s,  924.82/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [ 950/1251 ( 76%)]  Loss:  3.544108 (3.3815)  Time: 1.099s,  931.46/s  (1.107s,  925.12/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [1000/1251 ( 80%)]  Loss:  3.671055 (3.3953)  Time: 1.181s,  866.96/s  (1.107s,  925.12/s)  LR: 3.672e-04  Data: 0.012 (0.012)
Train: 104 [1050/1251 ( 84%)]  Loss:  3.348917 (3.3932)  Time: 1.098s,  932.96/s  (1.107s,  925.44/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [1100/1251 ( 88%)]  Loss:  3.434706 (3.3950)  Time: 1.098s,  933.01/s  (1.106s,  925.56/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [1150/1251 ( 92%)]  Loss:  3.102457 (3.3828)  Time: 1.101s,  929.73/s  (1.106s,  925.68/s)  LR: 3.672e-04  Data: 0.011 (0.012)
Train: 104 [1200/1251 ( 96%)]  Loss:  3.650132 (3.3935)  Time: 1.172s,  873.65/s  (1.106s,  925.62/s)  LR: 3.672e-04  Data: 0.012 (0.012)
Train: 104 [1250/1251 (100%)]  Loss:  3.540048 (3.3992)  Time: 1.080s,  948.42/s  (1.107s,  925.18/s)  LR: 3.672e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.180 (3.180)  Loss:  0.5441 (0.5441)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6266 (1.0482)  Acc@1: 85.4953 (76.4140)  Acc@5: 97.6415 (93.4140)
Test (EMA): [   0/48]  Time: 3.142 (3.142)  Loss:  0.4419 (0.4419)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5677 (0.9162)  Acc@1: 86.3207 (78.5180)  Acc@5: 97.7594 (94.4860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 78.0720001611328)

Train: 105 [   0/1251 (  0%)]  Loss:  3.126465 (3.1265)  Time: 1.103s,  928.63/s  (1.103s,  928.63/s)  LR: 3.649e-04  Data: 0.022 (0.022)
Train: 105 [  50/1251 (  4%)]  Loss:  3.070966 (3.0987)  Time: 1.098s,  932.36/s  (1.103s,  928.11/s)  LR: 3.649e-04  Data: 0.010 (0.012)
Train: 105 [ 100/1251 (  8%)]  Loss:  3.542786 (3.2467)  Time: 1.098s,  932.32/s  (1.106s,  925.58/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 150/1251 ( 12%)]  Loss:  3.619807 (3.3400)  Time: 1.122s,  912.58/s  (1.105s,  926.90/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 200/1251 ( 16%)]  Loss:  3.650553 (3.4021)  Time: 1.105s,  926.29/s  (1.105s,  926.59/s)  LR: 3.649e-04  Data: 0.010 (0.011)
Train: 105 [ 250/1251 ( 20%)]  Loss:  3.519498 (3.4217)  Time: 1.100s,  930.65/s  (1.104s,  927.15/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 300/1251 ( 24%)]  Loss:  3.490772 (3.4315)  Time: 1.098s,  932.47/s  (1.106s,  925.83/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 350/1251 ( 28%)]  Loss:  3.574007 (3.4494)  Time: 1.100s,  931.16/s  (1.105s,  926.42/s)  LR: 3.649e-04  Data: 0.013 (0.012)
Train: 105 [ 400/1251 ( 32%)]  Loss:  3.533104 (3.4587)  Time: 1.100s,  931.03/s  (1.106s,  925.49/s)  LR: 3.649e-04  Data: 0.011 (0.011)
Train: 105 [ 450/1251 ( 36%)]  Loss:  3.694239 (3.4822)  Time: 1.094s,  935.79/s  (1.106s,  925.65/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 500/1251 ( 40%)]  Loss:  3.608367 (3.4937)  Time: 1.100s,  931.28/s  (1.106s,  925.88/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 550/1251 ( 44%)]  Loss:  3.616591 (3.5039)  Time: 1.109s,  923.68/s  (1.106s,  926.26/s)  LR: 3.649e-04  Data: 0.012 (0.011)
Train: 105 [ 600/1251 ( 48%)]  Loss:  3.187575 (3.4796)  Time: 1.199s,  854.25/s  (1.106s,  926.24/s)  LR: 3.649e-04  Data: 0.011 (0.012)
Train: 105 [ 650/1251 ( 52%)]  Loss:  3.305449 (3.4672)  Time: 1.096s,  934.44/s  (1.106s,  926.02/s)  LR: 3.649e-04  Data: 0.011 (0.012)
Train: 105 [ 700/1251 ( 56%)]  Loss:  3.625909 (3.4777)  Time: 1.099s,  931.92/s  (1.106s,  926.15/s)  LR: 3.649e-04  Data: 0.011 (0.012)
Train: 105 [ 750/1251 ( 60%)]  Loss:  3.537249 (3.4815)  Time: 1.098s,  932.94/s  (1.106s,  925.92/s)  LR: 3.649e-04  Data: 0.012 (0.012)
Train: 105 [ 800/1251 ( 64%)]  Loss:  3.325596 (3.4723)  Time: 1.098s,  933.01/s  (1.106s,  925.93/s)  LR: 3.649e-04  Data: 0.013 (0.012)
Train: 105 [ 850/1251 ( 68%)]  Loss:  3.828622 (3.4921)  Time: 1.135s,  902.02/s  (1.107s,  925.20/s)  LR: 3.649e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Train: 105 [ 900/1251 ( 72%)]  Loss:  3.592515 (3.4974)  Time: 1.122s,  912.27/s  (1.107s,  924.74/s)  LR: 3.649e-04  Data: 0.012 (0.012)
Train: 105 [ 950/1251 ( 76%)]  Loss:  3.574370 (3.5012)  Time: 1.121s,  913.77/s  (1.108s,  924.13/s)  LR: 3.649e-04  Data: 0.013 (0.012)
Train: 105 [1000/1251 ( 80%)]  Loss:  3.395076 (3.4962)  Time: 1.103s,  928.18/s  (1.109s,  923.77/s)  LR: 3.649e-04  Data: 0.010 (0.012)
Train: 105 [1050/1251 ( 84%)]  Loss:  3.455962 (3.4943)  Time: 1.094s,  936.06/s  (1.108s,  923.90/s)  LR: 3.649e-04  Data: 0.013 (0.012)
Train: 105 [1100/1251 ( 88%)]  Loss:  3.577425 (3.4980)  Time: 1.098s,  932.24/s  (1.108s,  924.02/s)  LR: 3.649e-04  Data: 0.011 (0.012)
Train: 105 [1150/1251 ( 92%)]  Loss:  3.312553 (3.4902)  Time: 1.097s,  933.80/s  (1.108s,  924.28/s)  LR: 3.649e-04  Data: 0.013 (0.012)
Train: 105 [1200/1251 ( 96%)]  Loss:  3.419443 (3.4874)  Time: 1.099s,  932.17/s  (1.108s,  924.01/s)  LR: 3.649e-04  Data: 0.011 (0.012)
Train: 105 [1250/1251 (100%)]  Loss:  3.334420 (3.4815)  Time: 1.080s,  947.90/s  (1.108s,  924.05/s)  LR: 3.649e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.190 (3.190)  Loss:  0.5143 (0.5143)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.230 (0.399)  Loss:  0.6282 (1.0350)  Acc@1: 86.4387 (76.5660)  Acc@5: 97.0519 (93.5300)
Test (EMA): [   0/48]  Time: 3.231 (3.231)  Loss:  0.4413 (0.4413)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5674 (0.9146)  Acc@1: 86.2028 (78.5540)  Acc@5: 97.7594 (94.4980)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 78.14400016113281)

Train: 106 [   0/1251 (  0%)]  Loss:  3.387155 (3.3872)  Time: 1.107s,  925.36/s  (1.107s,  925.36/s)  LR: 3.625e-04  Data: 0.025 (0.025)
Train: 106 [  50/1251 (  4%)]  Loss:  3.472112 (3.4296)  Time: 1.101s,  930.32/s  (1.120s,  914.08/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [ 100/1251 (  8%)]  Loss:  3.511333 (3.4569)  Time: 1.101s,  930.17/s  (1.112s,  920.56/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [ 150/1251 ( 12%)]  Loss:  3.423551 (3.4485)  Time: 1.103s,  928.16/s  (1.113s,  920.39/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [ 200/1251 ( 16%)]  Loss:  3.431553 (3.4451)  Time: 1.131s,  905.53/s  (1.113s,  920.42/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [ 250/1251 ( 20%)]  Loss:  3.502512 (3.4547)  Time: 1.091s,  938.94/s  (1.111s,  921.35/s)  LR: 3.625e-04  Data: 0.010 (0.012)
Train: 106 [ 300/1251 ( 24%)]  Loss:  3.403478 (3.4474)  Time: 1.103s,  928.05/s  (1.110s,  922.50/s)  LR: 3.625e-04  Data: 0.017 (0.012)
Train: 106 [ 350/1251 ( 28%)]  Loss:  3.365323 (3.4371)  Time: 1.101s,  930.32/s  (1.109s,  923.13/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [ 400/1251 ( 32%)]  Loss:  2.862936 (3.3733)  Time: 1.104s,  927.61/s  (1.108s,  923.80/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [ 450/1251 ( 36%)]  Loss:  3.363901 (3.3724)  Time: 1.099s,  931.43/s  (1.109s,  923.08/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [ 500/1251 ( 40%)]  Loss:  3.622250 (3.3951)  Time: 1.099s,  931.91/s  (1.109s,  923.18/s)  LR: 3.625e-04  Data: 0.013 (0.012)
Train: 106 [ 550/1251 ( 44%)]  Loss:  3.154692 (3.3751)  Time: 1.098s,  932.86/s  (1.110s,  922.71/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [ 600/1251 ( 48%)]  Loss:  3.471739 (3.3825)  Time: 1.098s,  932.45/s  (1.109s,  923.07/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [ 650/1251 ( 52%)]  Loss:  3.309074 (3.3773)  Time: 1.102s,  929.16/s  (1.110s,  922.48/s)  LR: 3.625e-04  Data: 0.009 (0.012)
Train: 106 [ 700/1251 ( 56%)]  Loss:  3.422659 (3.3803)  Time: 1.122s,  912.86/s  (1.111s,  921.98/s)  LR: 3.625e-04  Data: 0.013 (0.012)
Train: 106 [ 750/1251 ( 60%)]  Loss:  3.327729 (3.3770)  Time: 1.102s,  929.20/s  (1.111s,  921.63/s)  LR: 3.625e-04  Data: 0.010 (0.012)
Train: 106 [ 800/1251 ( 64%)]  Loss:  3.167088 (3.3647)  Time: 1.121s,  913.08/s  (1.112s,  921.28/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [ 850/1251 ( 68%)]  Loss:  3.087776 (3.3493)  Time: 1.099s,  931.54/s  (1.112s,  920.69/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [ 900/1251 ( 72%)]  Loss:  3.528141 (3.3587)  Time: 1.119s,  915.20/s  (1.112s,  920.73/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [ 950/1251 ( 76%)]  Loss:  3.347363 (3.3581)  Time: 1.122s,  912.31/s  (1.112s,  920.72/s)  LR: 3.625e-04  Data: 0.010 (0.012)
Train: 106 [1000/1251 ( 80%)]  Loss:  3.382842 (3.3593)  Time: 1.129s,  907.34/s  (1.112s,  920.93/s)  LR: 3.625e-04  Data: 0.013 (0.012)
Train: 106 [1050/1251 ( 84%)]  Loss:  3.321195 (3.3576)  Time: 1.101s,  929.85/s  (1.112s,  921.18/s)  LR: 3.625e-04  Data: 0.013 (0.012)
Train: 106 [1100/1251 ( 88%)]  Loss:  3.450038 (3.3616)  Time: 1.096s,  934.05/s  (1.112s,  921.24/s)  LR: 3.625e-04  Data: 0.013 (0.012)
Train: 106 [1150/1251 ( 92%)]  Loss:  3.196302 (3.3547)  Time: 1.098s,  932.28/s  (1.111s,  921.43/s)  LR: 3.625e-04  Data: 0.012 (0.012)
Train: 106 [1200/1251 ( 96%)]  Loss:  3.231374 (3.3498)  Time: 1.099s,  931.94/s  (1.111s,  921.67/s)  LR: 3.625e-04  Data: 0.011 (0.012)
Train: 106 [1250/1251 (100%)]  Loss:  3.722999 (3.3641)  Time: 1.079s,  948.66/s  (1.111s,  921.71/s)  LR: 3.625e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.263 (3.263)  Loss:  0.5435 (0.5435)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.6306 (1.0382)  Acc@1: 86.3208 (76.3800)  Acc@5: 96.8160 (93.4240)
Test (EMA): [   0/48]  Time: 3.221 (3.221)  Loss:  0.4415 (0.4415)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  0.5670 (0.9132)  Acc@1: 86.3207 (78.6040)  Acc@5: 97.7594 (94.5240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 78.19000003173828)

Train: 107 [   0/1251 (  0%)]  Loss:  3.440585 (3.4406)  Time: 1.109s,  923.66/s  (1.109s,  923.66/s)  LR: 3.602e-04  Data: 0.026 (0.026)
Train: 107 [  50/1251 (  4%)]  Loss:  3.014032 (3.2273)  Time: 1.120s,  914.58/s  (1.108s,  924.23/s)  LR: 3.602e-04  Data: 0.010 (0.012)
Train: 107 [ 100/1251 (  8%)]  Loss:  3.091337 (3.1820)  Time: 1.095s,  935.47/s  (1.115s,  918.29/s)  LR: 3.602e-04  Data: 0.011 (0.012)
Train: 107 [ 150/1251 ( 12%)]  Loss:  3.170384 (3.1791)  Time: 1.099s,  931.74/s  (1.112s,  921.25/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [ 200/1251 ( 16%)]  Loss:  3.583937 (3.2601)  Time: 1.097s,  933.46/s  (1.111s,  921.42/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [ 250/1251 ( 20%)]  Loss:  3.361506 (3.2770)  Time: 1.130s,  906.31/s  (1.111s,  921.98/s)  LR: 3.602e-04  Data: 0.010 (0.012)
Train: 107 [ 300/1251 ( 24%)]  Loss:  3.308510 (3.2815)  Time: 1.178s,  869.47/s  (1.111s,  921.81/s)  LR: 3.602e-04  Data: 0.011 (0.012)
Train: 107 [ 350/1251 ( 28%)]  Loss:  3.428027 (3.2998)  Time: 1.097s,  933.86/s  (1.110s,  922.37/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [ 400/1251 ( 32%)]  Loss:  3.290743 (3.2988)  Time: 1.099s,  931.59/s  (1.111s,  922.01/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [ 450/1251 ( 36%)]  Loss:  3.694783 (3.3384)  Time: 1.097s,  933.30/s  (1.110s,  922.49/s)  LR: 3.602e-04  Data: 0.010 (0.012)
Train: 107 [ 500/1251 ( 40%)]  Loss:  3.193042 (3.3252)  Time: 1.097s,  933.35/s  (1.109s,  923.02/s)  LR: 3.602e-04  Data: 0.013 (0.012)
Train: 107 [ 550/1251 ( 44%)]  Loss:  3.403284 (3.3317)  Time: 1.098s,  932.22/s  (1.110s,  922.76/s)  LR: 3.602e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 107 [ 600/1251 ( 48%)]  Loss:  3.042166 (3.3094)  Time: 1.130s,  906.30/s  (1.110s,  922.19/s)  LR: 3.602e-04  Data: 0.011 (0.012)
Train: 107 [ 650/1251 ( 52%)]  Loss:  3.839844 (3.3473)  Time: 1.121s,  913.51/s  (1.111s,  921.62/s)  LR: 3.602e-04  Data: 0.010 (0.012)
Train: 107 [ 700/1251 ( 56%)]  Loss:  3.386526 (3.3499)  Time: 1.098s,  932.92/s  (1.111s,  921.29/s)  LR: 3.602e-04  Data: 0.013 (0.012)
Train: 107 [ 750/1251 ( 60%)]  Loss:  3.353605 (3.3501)  Time: 1.097s,  933.53/s  (1.111s,  921.82/s)  LR: 3.602e-04  Data: 0.013 (0.012)
Train: 107 [ 800/1251 ( 64%)]  Loss:  3.249151 (3.3442)  Time: 1.098s,  932.55/s  (1.110s,  922.26/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [ 850/1251 ( 68%)]  Loss:  3.230746 (3.3379)  Time: 1.193s,  858.06/s  (1.110s,  922.22/s)  LR: 3.602e-04  Data: 0.010 (0.012)
Train: 107 [ 900/1251 ( 72%)]  Loss:  3.572407 (3.3502)  Time: 1.097s,  933.58/s  (1.110s,  922.38/s)  LR: 3.602e-04  Data: 0.011 (0.012)
Train: 107 [ 950/1251 ( 76%)]  Loss:  3.209402 (3.3432)  Time: 1.098s,  932.61/s  (1.110s,  922.52/s)  LR: 3.602e-04  Data: 0.013 (0.012)
Train: 107 [1000/1251 ( 80%)]  Loss:  3.488352 (3.3501)  Time: 1.099s,  931.47/s  (1.110s,  922.82/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [1050/1251 ( 84%)]  Loss:  3.323449 (3.3489)  Time: 1.095s,  935.48/s  (1.109s,  922.97/s)  LR: 3.602e-04  Data: 0.011 (0.012)
Train: 107 [1100/1251 ( 88%)]  Loss:  3.195309 (3.3422)  Time: 1.106s,  925.72/s  (1.110s,  922.90/s)  LR: 3.602e-04  Data: 0.011 (0.011)
Train: 107 [1150/1251 ( 92%)]  Loss:  3.346882 (3.3424)  Time: 1.116s,  917.71/s  (1.109s,  922.98/s)  LR: 3.602e-04  Data: 0.013 (0.012)
Train: 107 [1200/1251 ( 96%)]  Loss:  3.499393 (3.3487)  Time: 1.100s,  930.82/s  (1.110s,  922.77/s)  LR: 3.602e-04  Data: 0.012 (0.012)
Train: 107 [1250/1251 (100%)]  Loss:  3.485955 (3.3540)  Time: 1.103s,  928.35/s  (1.110s,  922.55/s)  LR: 3.602e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.237 (3.237)  Loss:  0.5005 (0.5005)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6526 (1.0402)  Acc@1: 85.9670 (76.4460)  Acc@5: 96.9340 (93.4340)
Test (EMA): [   0/48]  Time: 3.143 (3.143)  Loss:  0.4412 (0.4412)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5669 (0.9120)  Acc@1: 86.0849 (78.6320)  Acc@5: 97.7594 (94.5440)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 78.25200010986327)

Train: 108 [   0/1251 (  0%)]  Loss:  3.390761 (3.3908)  Time: 1.103s,  928.46/s  (1.103s,  928.46/s)  LR: 3.579e-04  Data: 0.020 (0.020)
Train: 108 [  50/1251 (  4%)]  Loss:  3.492121 (3.4414)  Time: 1.097s,  933.28/s  (1.113s,  919.99/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 100/1251 (  8%)]  Loss:  3.541416 (3.4748)  Time: 1.098s,  932.87/s  (1.107s,  925.44/s)  LR: 3.579e-04  Data: 0.013 (0.012)
Train: 108 [ 150/1251 ( 12%)]  Loss:  3.163468 (3.3969)  Time: 1.095s,  935.39/s  (1.106s,  925.95/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 200/1251 ( 16%)]  Loss:  3.640670 (3.4457)  Time: 1.098s,  932.48/s  (1.105s,  926.69/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 250/1251 ( 20%)]  Loss:  3.389781 (3.4364)  Time: 1.194s,  857.62/s  (1.106s,  925.62/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 300/1251 ( 24%)]  Loss:  3.310469 (3.4184)  Time: 1.099s,  932.08/s  (1.106s,  925.86/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [ 350/1251 ( 28%)]  Loss:  3.346905 (3.4094)  Time: 1.096s,  934.14/s  (1.106s,  925.64/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [ 400/1251 ( 32%)]  Loss:  3.418386 (3.4104)  Time: 1.105s,  926.55/s  (1.106s,  925.62/s)  LR: 3.579e-04  Data: 0.017 (0.012)
Train: 108 [ 450/1251 ( 36%)]  Loss:  3.149632 (3.3844)  Time: 1.097s,  933.08/s  (1.107s,  925.24/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 500/1251 ( 40%)]  Loss:  3.598554 (3.4038)  Time: 1.096s,  934.42/s  (1.107s,  925.18/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [ 550/1251 ( 44%)]  Loss:  3.210485 (3.3877)  Time: 1.095s,  934.88/s  (1.107s,  925.00/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 600/1251 ( 48%)]  Loss:  3.785756 (3.4183)  Time: 1.104s,  927.22/s  (1.107s,  924.88/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 650/1251 ( 52%)]  Loss:  3.505826 (3.4246)  Time: 1.104s,  927.31/s  (1.107s,  925.06/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [ 700/1251 ( 56%)]  Loss:  3.252650 (3.4131)  Time: 1.097s,  933.38/s  (1.107s,  925.19/s)  LR: 3.579e-04  Data: 0.013 (0.012)
Train: 108 [ 750/1251 ( 60%)]  Loss:  3.711957 (3.4318)  Time: 1.109s,  923.02/s  (1.107s,  925.37/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [ 800/1251 ( 64%)]  Loss:  3.622325 (3.4430)  Time: 1.097s,  933.56/s  (1.107s,  925.32/s)  LR: 3.579e-04  Data: 0.013 (0.012)
Train: 108 [ 850/1251 ( 68%)]  Loss:  3.165015 (3.4276)  Time: 1.099s,  931.81/s  (1.107s,  924.94/s)  LR: 3.579e-04  Data: 0.015 (0.012)
Train: 108 [ 900/1251 ( 72%)]  Loss:  3.269400 (3.4192)  Time: 1.098s,  932.93/s  (1.107s,  924.90/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [ 950/1251 ( 76%)]  Loss:  3.364848 (3.4165)  Time: 1.132s,  904.31/s  (1.107s,  925.06/s)  LR: 3.579e-04  Data: 0.010 (0.012)
Train: 108 [1000/1251 ( 80%)]  Loss:  3.228173 (3.4076)  Time: 1.130s,  906.30/s  (1.107s,  924.72/s)  LR: 3.579e-04  Data: 0.013 (0.012)
Train: 108 [1050/1251 ( 84%)]  Loss:  3.259784 (3.4008)  Time: 1.100s,  930.94/s  (1.107s,  924.64/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [1100/1251 ( 88%)]  Loss:  3.530780 (3.4065)  Time: 1.130s,  905.89/s  (1.107s,  924.73/s)  LR: 3.579e-04  Data: 0.012 (0.012)
Train: 108 [1150/1251 ( 92%)]  Loss:  3.488526 (3.4099)  Time: 1.099s,  931.36/s  (1.107s,  924.73/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [1200/1251 ( 96%)]  Loss:  3.628273 (3.4186)  Time: 1.118s,  916.19/s  (1.107s,  924.82/s)  LR: 3.579e-04  Data: 0.011 (0.012)
Train: 108 [1250/1251 (100%)]  Loss:  3.207395 (3.4105)  Time: 1.104s,  927.83/s  (1.107s,  924.68/s)  LR: 3.579e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.178 (3.178)  Loss:  0.5094 (0.5094)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6107 (1.0252)  Acc@1: 85.7311 (76.5000)  Acc@5: 97.4057 (93.4760)
Test (EMA): [   0/48]  Time: 3.204 (3.204)  Loss:  0.4401 (0.4401)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5667 (0.9109)  Acc@1: 86.0849 (78.6720)  Acc@5: 97.9953 (94.5780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 78.29599998046875)

Train: 109 [   0/1251 (  0%)]  Loss:  2.926032 (2.9260)  Time: 1.106s,  925.65/s  (1.106s,  925.65/s)  LR: 3.555e-04  Data: 0.023 (0.023)
Train: 109 [  50/1251 (  4%)]  Loss:  3.362303 (3.1442)  Time: 1.102s,  929.56/s  (1.103s,  928.70/s)  LR: 3.555e-04  Data: 0.011 (0.012)
Train: 109 [ 100/1251 (  8%)]  Loss:  2.935436 (3.0746)  Time: 1.096s,  934.60/s  (1.104s,  927.35/s)  LR: 3.555e-04  Data: 0.011 (0.011)
Train: 109 [ 150/1251 ( 12%)]  Loss:  3.166923 (3.0977)  Time: 1.099s,  931.99/s  (1.103s,  928.60/s)  LR: 3.555e-04  Data: 0.011 (0.011)
Train: 109 [ 200/1251 ( 16%)]  Loss:  3.636642 (3.2055)  Time: 1.098s,  932.32/s  (1.103s,  928.29/s)  LR: 3.555e-04  Data: 0.012 (0.011)
Train: 109 [ 250/1251 ( 20%)]  Loss:  3.372205 (3.2333)  Time: 1.102s,  928.91/s  (1.103s,  928.55/s)  LR: 3.555e-04  Data: 0.012 (0.011)
Train: 109 [ 300/1251 ( 24%)]  Loss:  3.330939 (3.2472)  Time: 1.120s,  914.06/s  (1.104s,  927.24/s)  LR: 3.555e-04  Data: 0.010 (0.011)
Train: 109 [ 350/1251 ( 28%)]  Loss:  3.230108 (3.2451)  Time: 1.129s,  907.12/s  (1.105s,  926.67/s)  LR: 3.555e-04  Data: 0.011 (0.011)
Train: 109 [ 400/1251 ( 32%)]  Loss:  3.298041 (3.2510)  Time: 1.099s,  931.77/s  (1.106s,  926.20/s)  LR: 3.555e-04  Data: 0.012 (0.011)
Train: 109 [ 450/1251 ( 36%)]  Loss:  3.320636 (3.2579)  Time: 1.100s,  930.76/s  (1.106s,  925.73/s)  LR: 3.555e-04  Data: 0.014 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 109 [ 500/1251 ( 40%)]  Loss:  3.353695 (3.2666)  Time: 1.096s,  934.49/s  (1.106s,  926.23/s)  LR: 3.555e-04  Data: 0.011 (0.012)
Train: 109 [ 550/1251 ( 44%)]  Loss:  3.367659 (3.2751)  Time: 1.091s,  938.62/s  (1.106s,  925.75/s)  LR: 3.555e-04  Data: 0.010 (0.012)
Train: 109 [ 600/1251 ( 48%)]  Loss:  3.506718 (3.2929)  Time: 1.110s,  922.16/s  (1.106s,  925.46/s)  LR: 3.555e-04  Data: 0.011 (0.012)
Train: 109 [ 650/1251 ( 52%)]  Loss:  2.899723 (3.2648)  Time: 1.123s,  911.54/s  (1.107s,  925.17/s)  LR: 3.555e-04  Data: 0.012 (0.012)
Train: 109 [ 700/1251 ( 56%)]  Loss:  3.587691 (3.2863)  Time: 1.100s,  931.32/s  (1.107s,  925.23/s)  LR: 3.555e-04  Data: 0.010 (0.012)
Train: 109 [ 750/1251 ( 60%)]  Loss:  3.010286 (3.2691)  Time: 1.096s,  934.49/s  (1.107s,  925.15/s)  LR: 3.555e-04  Data: 0.011 (0.012)
Train: 109 [ 800/1251 ( 64%)]  Loss:  3.249808 (3.2679)  Time: 1.097s,  933.83/s  (1.107s,  925.28/s)  LR: 3.555e-04  Data: 0.013 (0.012)
Train: 109 [ 850/1251 ( 68%)]  Loss:  3.569289 (3.2847)  Time: 1.127s,  908.60/s  (1.107s,  925.43/s)  LR: 3.555e-04  Data: 0.011 (0.012)
Train: 109 [ 900/1251 ( 72%)]  Loss:  3.504791 (3.2963)  Time: 1.097s,  933.73/s  (1.107s,  925.32/s)  LR: 3.555e-04  Data: 0.010 (0.012)
Train: 109 [ 950/1251 ( 76%)]  Loss:  3.180751 (3.2905)  Time: 1.120s,  914.21/s  (1.107s,  924.83/s)  LR: 3.555e-04  Data: 0.010 (0.012)
Train: 109 [1000/1251 ( 80%)]  Loss:  3.160069 (3.2843)  Time: 1.098s,  932.55/s  (1.107s,  924.72/s)  LR: 3.555e-04  Data: 0.013 (0.012)
Train: 109 [1050/1251 ( 84%)]  Loss:  3.373466 (3.2883)  Time: 1.100s,  930.54/s  (1.107s,  924.76/s)  LR: 3.555e-04  Data: 0.011 (0.012)
Train: 109 [1100/1251 ( 88%)]  Loss:  3.730521 (3.3076)  Time: 1.096s,  934.34/s  (1.108s,  924.33/s)  LR: 3.555e-04  Data: 0.010 (0.012)
Train: 109 [1150/1251 ( 92%)]  Loss:  3.186482 (3.3025)  Time: 1.097s,  933.44/s  (1.108s,  924.42/s)  LR: 3.555e-04  Data: 0.014 (0.012)
Train: 109 [1200/1251 ( 96%)]  Loss:  3.429997 (3.3076)  Time: 1.097s,  933.11/s  (1.108s,  924.55/s)  LR: 3.555e-04  Data: 0.012 (0.012)
Train: 109 [1250/1251 (100%)]  Loss:  3.561821 (3.3174)  Time: 1.080s,  948.01/s  (1.107s,  924.67/s)  LR: 3.555e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.264 (3.264)  Loss:  0.5136 (0.5136)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.6464 (1.0206)  Acc@1: 85.7311 (76.5240)  Acc@5: 97.8774 (93.4640)
Test (EMA): [   0/48]  Time: 3.149 (3.149)  Loss:  0.4400 (0.4400)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  0.5666 (0.9095)  Acc@1: 86.0849 (78.7160)  Acc@5: 97.9953 (94.5920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 78.30000010986328)

Train: 110 [   0/1251 (  0%)]  Loss:  3.544643 (3.5446)  Time: 1.115s,  918.76/s  (1.115s,  918.76/s)  LR: 3.532e-04  Data: 0.023 (0.023)
Train: 110 [  50/1251 (  4%)]  Loss:  3.795115 (3.6699)  Time: 1.097s,  933.63/s  (1.109s,  923.11/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 100/1251 (  8%)]  Loss:  3.607122 (3.6490)  Time: 1.098s,  932.87/s  (1.104s,  927.46/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [ 150/1251 ( 12%)]  Loss:  3.253066 (3.5500)  Time: 1.122s,  912.79/s  (1.105s,  926.87/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [ 200/1251 ( 16%)]  Loss:  3.195366 (3.4791)  Time: 1.097s,  933.06/s  (1.107s,  924.89/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 250/1251 ( 20%)]  Loss:  3.249377 (3.4408)  Time: 1.135s,  901.81/s  (1.110s,  922.87/s)  LR: 3.532e-04  Data: 0.014 (0.012)
Train: 110 [ 300/1251 ( 24%)]  Loss:  3.677934 (3.4747)  Time: 1.137s,  900.68/s  (1.109s,  923.17/s)  LR: 3.532e-04  Data: 0.010 (0.012)
Train: 110 [ 350/1251 ( 28%)]  Loss:  3.534527 (3.4821)  Time: 1.100s,  930.56/s  (1.109s,  923.44/s)  LR: 3.532e-04  Data: 0.013 (0.012)
Train: 110 [ 400/1251 ( 32%)]  Loss:  3.157035 (3.4460)  Time: 1.100s,  931.26/s  (1.109s,  923.32/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 450/1251 ( 36%)]  Loss:  3.359422 (3.4374)  Time: 1.104s,  927.51/s  (1.108s,  924.08/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 500/1251 ( 40%)]  Loss:  3.005376 (3.3981)  Time: 1.097s,  933.77/s  (1.108s,  924.18/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [ 550/1251 ( 44%)]  Loss:  3.293987 (3.3894)  Time: 1.102s,  929.10/s  (1.108s,  924.33/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 600/1251 ( 48%)]  Loss:  3.228148 (3.3770)  Time: 1.098s,  932.42/s  (1.108s,  924.26/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 650/1251 ( 52%)]  Loss:  3.214388 (3.3654)  Time: 1.101s,  930.42/s  (1.107s,  924.62/s)  LR: 3.532e-04  Data: 0.010 (0.012)
Train: 110 [ 700/1251 ( 56%)]  Loss:  3.364078 (3.3653)  Time: 1.106s,  925.91/s  (1.108s,  924.36/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [ 750/1251 ( 60%)]  Loss:  3.375084 (3.3659)  Time: 1.097s,  933.48/s  (1.108s,  924.39/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [ 800/1251 ( 64%)]  Loss:  3.273856 (3.3605)  Time: 1.121s,  913.38/s  (1.108s,  924.41/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 850/1251 ( 68%)]  Loss:  3.373554 (3.3612)  Time: 1.100s,  930.96/s  (1.108s,  924.34/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [ 900/1251 ( 72%)]  Loss:  3.371238 (3.3618)  Time: 1.093s,  936.77/s  (1.108s,  924.11/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [ 950/1251 ( 76%)]  Loss:  3.177054 (3.3525)  Time: 1.123s,  911.57/s  (1.108s,  924.08/s)  LR: 3.532e-04  Data: 0.012 (0.012)
Train: 110 [1000/1251 ( 80%)]  Loss:  3.439748 (3.3567)  Time: 1.120s,  914.69/s  (1.109s,  923.37/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [1050/1251 ( 84%)]  Loss:  3.365183 (3.3571)  Time: 1.099s,  931.95/s  (1.109s,  923.33/s)  LR: 3.532e-04  Data: 0.015 (0.012)
Train: 110 [1100/1251 ( 88%)]  Loss:  3.598265 (3.3675)  Time: 1.193s,  858.66/s  (1.109s,  923.49/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [1150/1251 ( 92%)]  Loss:  3.513464 (3.3736)  Time: 1.096s,  934.26/s  (1.109s,  923.55/s)  LR: 3.532e-04  Data: 0.013 (0.012)
Train: 110 [1200/1251 ( 96%)]  Loss:  3.266295 (3.3693)  Time: 1.097s,  933.14/s  (1.109s,  923.63/s)  LR: 3.532e-04  Data: 0.011 (0.012)
Train: 110 [1250/1251 (100%)]  Loss:  3.430459 (3.3717)  Time: 1.082s,  946.13/s  (1.109s,  923.42/s)  LR: 3.532e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.281 (3.281)  Loss:  0.5295 (0.5295)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6398 (1.0230)  Acc@1: 85.0236 (76.8120)  Acc@5: 97.1698 (93.6560)
Test (EMA): [   0/48]  Time: 3.216 (3.216)  Loss:  0.4400 (0.4400)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5666 (0.9080)  Acc@1: 86.2028 (78.7580)  Acc@5: 97.7594 (94.5940)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 78.35999998046876)

Train: 111 [   0/1251 (  0%)]  Loss:  3.212910 (3.2129)  Time: 1.135s,  902.28/s  (1.135s,  902.28/s)  LR: 3.508e-04  Data: 0.026 (0.026)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 111 [  50/1251 (  4%)]  Loss:  2.940627 (3.0768)  Time: 1.100s,  931.27/s  (1.102s,  928.83/s)  LR: 3.508e-04  Data: 0.012 (0.012)
Train: 111 [ 100/1251 (  8%)]  Loss:  2.894354 (3.0160)  Time: 1.130s,  906.18/s  (1.107s,  925.19/s)  LR: 3.508e-04  Data: 0.011 (0.012)
Train: 111 [ 150/1251 ( 12%)]  Loss:  3.327440 (3.0938)  Time: 1.097s,  933.08/s  (1.106s,  925.84/s)  LR: 3.508e-04  Data: 0.011 (0.011)
Train: 111 [ 200/1251 ( 16%)]  Loss:  3.234262 (3.1219)  Time: 1.096s,  933.90/s  (1.106s,  926.21/s)  LR: 3.508e-04  Data: 0.012 (0.011)
Train: 111 [ 250/1251 ( 20%)]  Loss:  3.335392 (3.1575)  Time: 1.104s,  927.89/s  (1.105s,  926.40/s)  LR: 3.508e-04  Data: 0.012 (0.011)
Train: 111 [ 300/1251 ( 24%)]  Loss:  3.178513 (3.1605)  Time: 1.099s,  931.69/s  (1.106s,  926.16/s)  LR: 3.508e-04  Data: 0.011 (0.011)
Train: 111 [ 350/1251 ( 28%)]  Loss:  3.417943 (3.1927)  Time: 1.117s,  916.47/s  (1.108s,  923.82/s)  LR: 3.508e-04  Data: 0.009 (0.011)
Train: 111 [ 400/1251 ( 32%)]  Loss:  3.042484 (3.1760)  Time: 1.094s,  936.00/s  (1.110s,  922.87/s)  LR: 3.508e-04  Data: 0.011 (0.011)
Train: 111 [ 450/1251 ( 36%)]  Loss:  3.375513 (3.1959)  Time: 1.101s,  929.71/s  (1.109s,  923.43/s)  LR: 3.508e-04  Data: 0.012 (0.011)
Train: 111 [ 500/1251 ( 40%)]  Loss:  3.369120 (3.2117)  Time: 1.144s,  895.31/s  (1.109s,  922.99/s)  LR: 3.508e-04  Data: 0.012 (0.011)
Train: 111 [ 550/1251 ( 44%)]  Loss:  3.056326 (3.1987)  Time: 1.096s,  934.66/s  (1.110s,  922.92/s)  LR: 3.508e-04  Data: 0.011 (0.011)
Train: 111 [ 600/1251 ( 48%)]  Loss:  3.345299 (3.2100)  Time: 1.093s,  936.68/s  (1.109s,  923.43/s)  LR: 3.508e-04  Data: 0.012 (0.011)
Train: 111 [ 650/1251 ( 52%)]  Loss:  3.328745 (3.2185)  Time: 1.121s,  913.37/s  (1.109s,  923.07/s)  LR: 3.508e-04  Data: 0.011 (0.011)
Train: 111 [ 700/1251 ( 56%)]  Loss:  3.324588 (3.2256)  Time: 1.103s,  928.51/s  (1.109s,  923.24/s)  LR: 3.508e-04  Data: 0.012 (0.012)
Train: 111 [ 750/1251 ( 60%)]  Loss:  3.477560 (3.2413)  Time: 1.101s,  930.01/s  (1.109s,  923.52/s)  LR: 3.508e-04  Data: 0.011 (0.012)
Train: 111 [ 800/1251 ( 64%)]  Loss:  3.580254 (3.2613)  Time: 1.098s,  932.36/s  (1.108s,  923.78/s)  LR: 3.508e-04  Data: 0.012 (0.012)
Train: 111 [ 850/1251 ( 68%)]  Loss:  3.694465 (3.2853)  Time: 1.199s,  854.26/s  (1.108s,  923.98/s)  LR: 3.508e-04  Data: 0.010 (0.012)
Train: 111 [ 900/1251 ( 72%)]  Loss:  3.726048 (3.3085)  Time: 1.095s,  935.30/s  (1.108s,  924.04/s)  LR: 3.508e-04  Data: 0.012 (0.012)
Train: 111 [ 950/1251 ( 76%)]  Loss:  3.520640 (3.3191)  Time: 1.102s,  929.15/s  (1.108s,  924.28/s)  LR: 3.508e-04  Data: 0.014 (0.012)
Train: 111 [1000/1251 ( 80%)]  Loss:  3.305038 (3.3185)  Time: 1.092s,  937.86/s  (1.108s,  924.37/s)  LR: 3.508e-04  Data: 0.010 (0.012)
Train: 111 [1050/1251 ( 84%)]  Loss:  3.497252 (3.3266)  Time: 1.097s,  933.39/s  (1.108s,  924.20/s)  LR: 3.508e-04  Data: 0.012 (0.012)
Train: 111 [1100/1251 ( 88%)]  Loss:  3.379704 (3.3289)  Time: 1.098s,  932.46/s  (1.108s,  924.18/s)  LR: 3.508e-04  Data: 0.013 (0.012)
Train: 111 [1150/1251 ( 92%)]  Loss:  3.015387 (3.3158)  Time: 1.103s,  928.78/s  (1.108s,  924.34/s)  LR: 3.508e-04  Data: 0.011 (0.012)
Train: 111 [1200/1251 ( 96%)]  Loss:  3.330759 (3.3164)  Time: 1.100s,  930.94/s  (1.108s,  924.38/s)  LR: 3.508e-04  Data: 0.012 (0.012)
Train: 111 [1250/1251 (100%)]  Loss:  3.379111 (3.3188)  Time: 1.080s,  948.44/s  (1.108s,  924.38/s)  LR: 3.508e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.182 (3.182)  Loss:  0.5521 (0.5521)  Acc@1: 89.3555 (89.3555)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6762 (1.0482)  Acc@1: 84.7877 (76.6860)  Acc@5: 97.1698 (93.6820)
Test (EMA): [   0/48]  Time: 3.149 (3.149)  Loss:  0.4396 (0.4396)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.5662 (0.9061)  Acc@1: 86.0849 (78.7660)  Acc@5: 97.7594 (94.6080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 78.44200003173827)

Train: 112 [   0/1251 (  0%)]  Loss:  3.503435 (3.5034)  Time: 1.104s,  927.94/s  (1.104s,  927.94/s)  LR: 3.484e-04  Data: 0.024 (0.024)
Train: 112 [  50/1251 (  4%)]  Loss:  3.645705 (3.5746)  Time: 1.096s,  934.11/s  (1.106s,  925.69/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 100/1251 (  8%)]  Loss:  3.617965 (3.5890)  Time: 1.119s,  915.25/s  (1.109s,  923.72/s)  LR: 3.484e-04  Data: 0.012 (0.012)
Train: 112 [ 150/1251 ( 12%)]  Loss:  3.523050 (3.5725)  Time: 1.097s,  933.25/s  (1.111s,  921.90/s)  LR: 3.484e-04  Data: 0.014 (0.012)
Train: 112 [ 200/1251 ( 16%)]  Loss:  3.287712 (3.5156)  Time: 1.118s,  916.17/s  (1.109s,  923.48/s)  LR: 3.484e-04  Data: 0.013 (0.012)
Train: 112 [ 250/1251 ( 20%)]  Loss:  2.995844 (3.4290)  Time: 1.114s,  919.37/s  (1.109s,  923.41/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 300/1251 ( 24%)]  Loss:  3.173814 (3.3925)  Time: 1.100s,  931.29/s  (1.109s,  923.00/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 350/1251 ( 28%)]  Loss:  3.511657 (3.4074)  Time: 1.096s,  934.49/s  (1.108s,  924.14/s)  LR: 3.484e-04  Data: 0.012 (0.012)
Train: 112 [ 400/1251 ( 32%)]  Loss:  3.430549 (3.4100)  Time: 1.125s,  910.49/s  (1.108s,  924.36/s)  LR: 3.484e-04  Data: 0.012 (0.012)
Train: 112 [ 450/1251 ( 36%)]  Loss:  3.368135 (3.4058)  Time: 1.093s,  936.69/s  (1.111s,  922.08/s)  LR: 3.484e-04  Data: 0.010 (0.012)
Train: 112 [ 500/1251 ( 40%)]  Loss:  3.314544 (3.3975)  Time: 1.099s,  932.16/s  (1.111s,  921.56/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 550/1251 ( 44%)]  Loss:  3.376561 (3.3957)  Time: 1.096s,  934.30/s  (1.111s,  921.83/s)  LR: 3.484e-04  Data: 0.012 (0.012)
Train: 112 [ 600/1251 ( 48%)]  Loss:  3.270552 (3.3861)  Time: 1.097s,  933.51/s  (1.110s,  922.13/s)  LR: 3.484e-04  Data: 0.012 (0.012)
Train: 112 [ 650/1251 ( 52%)]  Loss:  3.585498 (3.4004)  Time: 1.101s,  930.27/s  (1.110s,  922.49/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 700/1251 ( 56%)]  Loss:  3.250881 (3.3904)  Time: 1.130s,  906.54/s  (1.110s,  922.69/s)  LR: 3.484e-04  Data: 0.010 (0.012)
Train: 112 [ 750/1251 ( 60%)]  Loss:  3.341376 (3.3873)  Time: 1.099s,  931.97/s  (1.109s,  923.08/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 800/1251 ( 64%)]  Loss:  3.208745 (3.3768)  Time: 1.141s,  897.42/s  (1.110s,  922.57/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [ 850/1251 ( 68%)]  Loss:  3.346215 (3.3751)  Time: 1.096s,  934.73/s  (1.111s,  922.08/s)  LR: 3.484e-04  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 112 [ 900/1251 ( 72%)]  Loss:  3.101413 (3.3607)  Time: 1.106s,  925.59/s  (1.111s,  921.81/s)  LR: 3.484e-04  Data: 0.011 (0.011)
Train: 112 [ 950/1251 ( 76%)]  Loss:  3.357587 (3.3606)  Time: 1.097s,  933.44/s  (1.111s,  921.69/s)  LR: 3.484e-04  Data: 0.011 (0.011)
Train: 112 [1000/1251 ( 80%)]  Loss:  3.089129 (3.3476)  Time: 1.098s,  932.31/s  (1.111s,  921.97/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [1050/1251 ( 84%)]  Loss:  3.392559 (3.3497)  Time: 1.099s,  932.12/s  (1.111s,  921.91/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [1100/1251 ( 88%)]  Loss:  3.267091 (3.3461)  Time: 1.100s,  930.93/s  (1.110s,  922.20/s)  LR: 3.484e-04  Data: 0.011 (0.012)
Train: 112 [1150/1251 ( 92%)]  Loss:  3.245447 (3.3419)  Time: 1.129s,  906.68/s  (1.111s,  922.02/s)  LR: 3.484e-04  Data: 0.014 (0.012)
Train: 112 [1200/1251 ( 96%)]  Loss:  3.146316 (3.3341)  Time: 1.097s,  933.10/s  (1.110s,  922.18/s)  LR: 3.484e-04  Data: 0.013 (0.012)
Train: 112 [1250/1251 (100%)]  Loss:  3.312288 (3.3332)  Time: 1.080s,  948.04/s  (1.110s,  922.30/s)  LR: 3.484e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.351 (3.351)  Loss:  0.4989 (0.4989)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6252 (1.0016)  Acc@1: 85.2594 (76.6840)  Acc@5: 97.4057 (93.6300)
Test (EMA): [   0/48]  Time: 3.109 (3.109)  Loss:  0.4393 (0.4393)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.401)  Loss:  0.5662 (0.9048)  Acc@1: 86.0849 (78.8100)  Acc@5: 97.6415 (94.6140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 78.49200000488281)

Train: 113 [   0/1251 (  0%)]  Loss:  3.446178 (3.4462)  Time: 1.117s,  916.57/s  (1.117s,  916.57/s)  LR: 3.460e-04  Data: 0.022 (0.022)
Train: 113 [  50/1251 (  4%)]  Loss:  3.243320 (3.3447)  Time: 1.096s,  934.72/s  (1.112s,  920.95/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 100/1251 (  8%)]  Loss:  3.037285 (3.2423)  Time: 1.099s,  932.05/s  (1.111s,  921.33/s)  LR: 3.460e-04  Data: 0.013 (0.012)
Train: 113 [ 150/1251 ( 12%)]  Loss:  3.156756 (3.2209)  Time: 1.097s,  933.07/s  (1.111s,  922.03/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 200/1251 ( 16%)]  Loss:  3.294586 (3.2356)  Time: 1.098s,  932.82/s  (1.111s,  921.40/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 250/1251 ( 20%)]  Loss:  3.332905 (3.2518)  Time: 1.095s,  934.80/s  (1.112s,  920.93/s)  LR: 3.460e-04  Data: 0.013 (0.012)
Train: 113 [ 300/1251 ( 24%)]  Loss:  3.286540 (3.2568)  Time: 1.097s,  933.46/s  (1.111s,  921.51/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 350/1251 ( 28%)]  Loss:  3.371268 (3.2711)  Time: 1.096s,  934.06/s  (1.111s,  921.67/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 400/1251 ( 32%)]  Loss:  3.579620 (3.3054)  Time: 1.120s,  914.06/s  (1.110s,  922.31/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 450/1251 ( 36%)]  Loss:  3.682770 (3.3431)  Time: 1.099s,  932.01/s  (1.111s,  921.84/s)  LR: 3.460e-04  Data: 0.013 (0.012)
Train: 113 [ 500/1251 ( 40%)]  Loss:  3.170902 (3.3275)  Time: 1.099s,  931.92/s  (1.111s,  921.49/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 550/1251 ( 44%)]  Loss:  3.359777 (3.3302)  Time: 1.108s,  924.37/s  (1.111s,  921.87/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 600/1251 ( 48%)]  Loss:  3.451911 (3.3395)  Time: 1.127s,  908.58/s  (1.111s,  921.94/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 650/1251 ( 52%)]  Loss:  3.362917 (3.3412)  Time: 1.096s,  934.44/s  (1.110s,  922.21/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 700/1251 ( 56%)]  Loss:  3.614583 (3.3594)  Time: 1.130s,  906.21/s  (1.110s,  922.31/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 750/1251 ( 60%)]  Loss:  3.423877 (3.3634)  Time: 1.145s,  894.19/s  (1.111s,  921.49/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 800/1251 ( 64%)]  Loss:  3.131703 (3.3498)  Time: 1.097s,  933.86/s  (1.111s,  921.58/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 850/1251 ( 68%)]  Loss:  3.345752 (3.3496)  Time: 1.096s,  934.21/s  (1.111s,  921.83/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [ 900/1251 ( 72%)]  Loss:  3.035539 (3.3331)  Time: 1.145s,  894.41/s  (1.111s,  921.75/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [ 950/1251 ( 76%)]  Loss:  3.378243 (3.3353)  Time: 1.097s,  933.73/s  (1.111s,  921.77/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [1000/1251 ( 80%)]  Loss:  3.283479 (3.3329)  Time: 1.096s,  934.52/s  (1.111s,  921.97/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [1050/1251 ( 84%)]  Loss:  3.268113 (3.3299)  Time: 1.095s,  934.74/s  (1.111s,  921.91/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [1100/1251 ( 88%)]  Loss:  3.269868 (3.3273)  Time: 1.100s,  930.75/s  (1.111s,  921.71/s)  LR: 3.460e-04  Data: 0.012 (0.012)
Train: 113 [1150/1251 ( 92%)]  Loss:  3.157360 (3.3202)  Time: 1.097s,  933.08/s  (1.111s,  921.72/s)  LR: 3.460e-04  Data: 0.014 (0.012)
Train: 113 [1200/1251 ( 96%)]  Loss:  3.061448 (3.3099)  Time: 1.118s,  915.76/s  (1.111s,  921.75/s)  LR: 3.460e-04  Data: 0.011 (0.012)
Train: 113 [1250/1251 (100%)]  Loss:  3.429147 (3.3145)  Time: 1.077s,  950.46/s  (1.111s,  921.71/s)  LR: 3.460e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.244 (3.244)  Loss:  0.5248 (0.5248)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.399)  Loss:  0.6251 (1.0188)  Acc@1: 86.5566 (76.7460)  Acc@5: 97.1698 (93.5680)
Test (EMA): [   0/48]  Time: 3.188 (3.188)  Loss:  0.4390 (0.4390)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5658 (0.9034)  Acc@1: 86.0849 (78.8540)  Acc@5: 97.5236 (94.6280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 78.51799987548829)

Train: 114 [   0/1251 (  0%)]  Loss:  3.636065 (3.6361)  Time: 1.111s,  921.75/s  (1.111s,  921.75/s)  LR: 3.436e-04  Data: 0.027 (0.027)
Train: 114 [  50/1251 (  4%)]  Loss:  3.153645 (3.3949)  Time: 1.100s,  930.89/s  (1.107s,  925.00/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [ 100/1251 (  8%)]  Loss:  3.477456 (3.4224)  Time: 1.102s,  929.25/s  (1.106s,  925.79/s)  LR: 3.436e-04  Data: 0.010 (0.012)
Train: 114 [ 150/1251 ( 12%)]  Loss:  3.419305 (3.4216)  Time: 1.115s,  918.31/s  (1.108s,  924.51/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 200/1251 ( 16%)]  Loss:  3.564006 (3.4501)  Time: 1.096s,  934.02/s  (1.110s,  922.82/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [ 250/1251 ( 20%)]  Loss:  3.304216 (3.4258)  Time: 1.132s,  904.90/s  (1.111s,  921.35/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 300/1251 ( 24%)]  Loss:  3.254905 (3.4014)  Time: 1.095s,  934.78/s  (1.111s,  921.97/s)  LR: 3.436e-04  Data: 0.013 (0.012)
Train: 114 [ 350/1251 ( 28%)]  Loss:  3.318050 (3.3910)  Time: 1.098s,  932.88/s  (1.110s,  922.43/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 400/1251 ( 32%)]  Loss:  3.385899 (3.3904)  Time: 1.097s,  933.26/s  (1.110s,  922.46/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 114 [ 450/1251 ( 36%)]  Loss:  3.207718 (3.3721)  Time: 1.098s,  932.51/s  (1.110s,  922.62/s)  LR: 3.436e-04  Data: 0.013 (0.012)
Train: 114 [ 500/1251 ( 40%)]  Loss:  3.349308 (3.3701)  Time: 1.095s,  935.23/s  (1.110s,  922.51/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [ 550/1251 ( 44%)]  Loss:  3.411052 (3.3735)  Time: 1.129s,  906.77/s  (1.110s,  922.67/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 600/1251 ( 48%)]  Loss:  3.355054 (3.3721)  Time: 1.097s,  933.24/s  (1.110s,  922.57/s)  LR: 3.436e-04  Data: 0.014 (0.012)
Train: 114 [ 650/1251 ( 52%)]  Loss:  3.670542 (3.3934)  Time: 1.157s,  885.06/s  (1.111s,  922.04/s)  LR: 3.436e-04  Data: 0.014 (0.012)
Train: 114 [ 700/1251 ( 56%)]  Loss:  3.292052 (3.3866)  Time: 1.096s,  934.02/s  (1.110s,  922.36/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 750/1251 ( 60%)]  Loss:  3.400944 (3.3875)  Time: 1.100s,  930.61/s  (1.110s,  922.54/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 800/1251 ( 64%)]  Loss:  3.424632 (3.3897)  Time: 1.120s,  914.00/s  (1.110s,  922.59/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [ 850/1251 ( 68%)]  Loss:  3.496426 (3.3956)  Time: 1.098s,  933.00/s  (1.110s,  922.59/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [ 900/1251 ( 72%)]  Loss:  3.376043 (3.3946)  Time: 1.095s,  935.38/s  (1.110s,  922.68/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [ 950/1251 ( 76%)]  Loss:  3.505737 (3.4002)  Time: 1.097s,  933.27/s  (1.110s,  922.87/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [1000/1251 ( 80%)]  Loss:  3.142871 (3.3879)  Time: 1.103s,  928.05/s  (1.109s,  923.19/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [1050/1251 ( 84%)]  Loss:  3.493137 (3.3927)  Time: 1.105s,  926.86/s  (1.110s,  922.80/s)  LR: 3.436e-04  Data: 0.011 (0.012)
Train: 114 [1100/1251 ( 88%)]  Loss:  3.263920 (3.3871)  Time: 1.095s,  935.01/s  (1.109s,  923.06/s)  LR: 3.436e-04  Data: 0.010 (0.012)
Train: 114 [1150/1251 ( 92%)]  Loss:  3.346807 (3.3854)  Time: 1.122s,  912.50/s  (1.109s,  923.02/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [1200/1251 ( 96%)]  Loss:  3.415832 (3.3866)  Time: 1.197s,  855.48/s  (1.109s,  923.02/s)  LR: 3.436e-04  Data: 0.012 (0.012)
Train: 114 [1250/1251 (100%)]  Loss:  3.183692 (3.3788)  Time: 1.087s,  941.81/s  (1.110s,  922.89/s)  LR: 3.436e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.203 (3.203)  Loss:  0.5012 (0.5012)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6254 (0.9989)  Acc@1: 86.5566 (76.8360)  Acc@5: 97.6415 (93.6060)
Test (EMA): [   0/48]  Time: 3.236 (3.236)  Loss:  0.4386 (0.4386)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5667 (0.9021)  Acc@1: 86.2028 (78.8520)  Acc@5: 97.4057 (94.6500)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 78.55399995361329)

Train: 115 [   0/1251 (  0%)]  Loss:  3.381260 (3.3813)  Time: 1.108s,  924.07/s  (1.108s,  924.07/s)  LR: 3.412e-04  Data: 0.025 (0.025)
Train: 115 [  50/1251 (  4%)]  Loss:  3.289686 (3.3355)  Time: 1.100s,  930.97/s  (1.105s,  926.91/s)  LR: 3.412e-04  Data: 0.010 (0.012)
Train: 115 [ 100/1251 (  8%)]  Loss:  3.475501 (3.3821)  Time: 1.159s,  883.59/s  (1.113s,  920.27/s)  LR: 3.412e-04  Data: 0.013 (0.011)
Train: 115 [ 150/1251 ( 12%)]  Loss:  3.222126 (3.3421)  Time: 1.113s,  920.34/s  (1.111s,  921.81/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [ 200/1251 ( 16%)]  Loss:  3.331564 (3.3400)  Time: 1.096s,  934.62/s  (1.110s,  922.87/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 250/1251 ( 20%)]  Loss:  3.453268 (3.3589)  Time: 1.094s,  935.67/s  (1.109s,  923.16/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [ 300/1251 ( 24%)]  Loss:  3.557520 (3.3873)  Time: 1.130s,  905.83/s  (1.111s,  921.57/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [ 350/1251 ( 28%)]  Loss:  3.179501 (3.3613)  Time: 1.101s,  930.04/s  (1.112s,  920.61/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 400/1251 ( 32%)]  Loss:  3.364343 (3.3616)  Time: 1.096s,  934.07/s  (1.113s,  919.89/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 450/1251 ( 36%)]  Loss:  3.242876 (3.3498)  Time: 1.103s,  928.26/s  (1.113s,  920.28/s)  LR: 3.412e-04  Data: 0.010 (0.012)
Train: 115 [ 500/1251 ( 40%)]  Loss:  3.397836 (3.3541)  Time: 1.095s,  935.26/s  (1.112s,  920.49/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [ 550/1251 ( 44%)]  Loss:  3.251909 (3.3456)  Time: 1.098s,  932.86/s  (1.112s,  921.17/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 600/1251 ( 48%)]  Loss:  2.933665 (3.3139)  Time: 1.094s,  935.89/s  (1.112s,  921.12/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [ 650/1251 ( 52%)]  Loss:  3.420179 (3.3215)  Time: 1.139s,  899.02/s  (1.112s,  921.05/s)  LR: 3.412e-04  Data: 0.010 (0.012)
Train: 115 [ 700/1251 ( 56%)]  Loss:  3.531118 (3.3355)  Time: 1.192s,  859.16/s  (1.111s,  921.42/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 750/1251 ( 60%)]  Loss:  3.206976 (3.3275)  Time: 1.098s,  932.47/s  (1.111s,  921.87/s)  LR: 3.412e-04  Data: 0.013 (0.012)
Train: 115 [ 800/1251 ( 64%)]  Loss:  3.380429 (3.3306)  Time: 1.099s,  931.80/s  (1.111s,  921.58/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 850/1251 ( 68%)]  Loss:  3.338510 (3.3310)  Time: 1.098s,  932.75/s  (1.111s,  921.75/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [ 900/1251 ( 72%)]  Loss:  3.289772 (3.3288)  Time: 1.097s,  933.52/s  (1.111s,  921.68/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [ 950/1251 ( 76%)]  Loss:  3.594997 (3.3422)  Time: 1.103s,  928.40/s  (1.111s,  922.04/s)  LR: 3.412e-04  Data: 0.014 (0.012)
Train: 115 [1000/1251 ( 80%)]  Loss:  3.124468 (3.3318)  Time: 1.102s,  929.13/s  (1.111s,  922.01/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [1050/1251 ( 84%)]  Loss:  3.378825 (3.3339)  Time: 1.105s,  926.65/s  (1.110s,  922.19/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [1100/1251 ( 88%)]  Loss:  3.568587 (3.3441)  Time: 1.188s,  861.61/s  (1.110s,  922.30/s)  LR: 3.412e-04  Data: 0.011 (0.012)
Train: 115 [1150/1251 ( 92%)]  Loss:  3.317528 (3.3430)  Time: 1.100s,  930.51/s  (1.110s,  922.48/s)  LR: 3.412e-04  Data: 0.012 (0.012)
Train: 115 [1200/1251 ( 96%)]  Loss:  3.405591 (3.3455)  Time: 1.098s,  932.93/s  (1.110s,  922.50/s)  LR: 3.412e-04  Data: 0.013 (0.012)
Train: 115 [1250/1251 (100%)]  Loss:  3.102238 (3.3362)  Time: 1.102s,  929.11/s  (1.110s,  922.41/s)  LR: 3.412e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.151 (3.151)  Loss:  0.5022 (0.5022)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6231 (1.0008)  Acc@1: 86.4387 (77.0480)  Acc@5: 97.4057 (93.6700)
Test (EMA): [   0/48]  Time: 3.078 (3.078)  Loss:  0.4391 (0.4391)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5681 (0.9008)  Acc@1: 86.2028 (78.9080)  Acc@5: 97.4057 (94.6620)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 78.60399987548828)

Train: 116 [   0/1251 (  0%)]  Loss:  3.478797 (3.4788)  Time: 1.102s,  928.95/s  (1.102s,  928.95/s)  LR: 3.388e-04  Data: 0.020 (0.020)
Train: 116 [  50/1251 (  4%)]  Loss:  3.452332 (3.4656)  Time: 1.101s,  930.28/s  (1.107s,  925.33/s)  LR: 3.388e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 116 [ 100/1251 (  8%)]  Loss:  3.323120 (3.4181)  Time: 1.132s,  904.75/s  (1.109s,  923.67/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 150/1251 ( 12%)]  Loss:  3.323675 (3.3945)  Time: 1.096s,  934.60/s  (1.110s,  922.92/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 200/1251 ( 16%)]  Loss:  3.798522 (3.4753)  Time: 1.098s,  932.36/s  (1.110s,  922.40/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 250/1251 ( 20%)]  Loss:  3.254212 (3.4384)  Time: 1.132s,  904.44/s  (1.110s,  922.35/s)  LR: 3.388e-04  Data: 0.012 (0.012)
Train: 116 [ 300/1251 ( 24%)]  Loss:  3.386342 (3.4310)  Time: 1.099s,  932.13/s  (1.111s,  922.08/s)  LR: 3.388e-04  Data: 0.014 (0.012)
Train: 116 [ 350/1251 ( 28%)]  Loss:  3.303925 (3.4151)  Time: 1.097s,  933.17/s  (1.111s,  921.76/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 400/1251 ( 32%)]  Loss:  3.405273 (3.4140)  Time: 1.131s,  905.00/s  (1.112s,  921.10/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 450/1251 ( 36%)]  Loss:  3.567238 (3.4293)  Time: 1.123s,  912.00/s  (1.114s,  919.58/s)  LR: 3.388e-04  Data: 0.012 (0.012)
Train: 116 [ 500/1251 ( 40%)]  Loss:  3.277574 (3.4155)  Time: 1.096s,  934.48/s  (1.115s,  918.66/s)  LR: 3.388e-04  Data: 0.012 (0.012)
Train: 116 [ 550/1251 ( 44%)]  Loss:  3.097327 (3.3890)  Time: 1.095s,  935.16/s  (1.114s,  919.52/s)  LR: 3.388e-04  Data: 0.010 (0.012)
Train: 116 [ 600/1251 ( 48%)]  Loss:  3.523013 (3.3993)  Time: 1.102s,  929.13/s  (1.113s,  920.11/s)  LR: 3.388e-04  Data: 0.010 (0.012)
Train: 116 [ 650/1251 ( 52%)]  Loss:  3.068262 (3.3757)  Time: 1.117s,  917.03/s  (1.113s,  920.44/s)  LR: 3.388e-04  Data: 0.010 (0.012)
Train: 116 [ 700/1251 ( 56%)]  Loss:  3.138726 (3.3599)  Time: 1.119s,  915.24/s  (1.114s,  919.51/s)  LR: 3.388e-04  Data: 0.012 (0.012)
Train: 116 [ 750/1251 ( 60%)]  Loss:  3.020239 (3.3387)  Time: 1.133s,  903.40/s  (1.113s,  919.65/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 800/1251 ( 64%)]  Loss:  3.481350 (3.3471)  Time: 1.126s,  909.30/s  (1.113s,  919.86/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [ 850/1251 ( 68%)]  Loss:  3.527076 (3.3571)  Time: 1.123s,  911.62/s  (1.113s,  919.94/s)  LR: 3.388e-04  Data: 0.010 (0.012)
Train: 116 [ 900/1251 ( 72%)]  Loss:  3.623586 (3.3711)  Time: 1.123s,  911.79/s  (1.113s,  919.79/s)  LR: 3.388e-04  Data: 0.012 (0.012)
Train: 116 [ 950/1251 ( 76%)]  Loss:  3.172990 (3.3612)  Time: 1.094s,  936.37/s  (1.113s,  919.95/s)  LR: 3.388e-04  Data: 0.013 (0.012)
Train: 116 [1000/1251 ( 80%)]  Loss:  3.361668 (3.3612)  Time: 1.119s,  915.20/s  (1.113s,  920.00/s)  LR: 3.388e-04  Data: 0.010 (0.012)
Train: 116 [1050/1251 ( 84%)]  Loss:  3.279674 (3.3575)  Time: 1.097s,  933.73/s  (1.113s,  919.96/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [1100/1251 ( 88%)]  Loss:  3.497639 (3.3636)  Time: 1.098s,  932.95/s  (1.113s,  920.32/s)  LR: 3.388e-04  Data: 0.013 (0.012)
Train: 116 [1150/1251 ( 92%)]  Loss:  3.186619 (3.3562)  Time: 1.132s,  904.28/s  (1.113s,  920.22/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [1200/1251 ( 96%)]  Loss:  3.251165 (3.3520)  Time: 1.098s,  932.99/s  (1.112s,  920.60/s)  LR: 3.388e-04  Data: 0.011 (0.012)
Train: 116 [1250/1251 (100%)]  Loss:  3.360952 (3.3524)  Time: 1.080s,  947.91/s  (1.113s,  920.35/s)  LR: 3.388e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.261 (3.261)  Loss:  0.4973 (0.4973)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6204 (0.9980)  Acc@1: 86.3207 (76.8740)  Acc@5: 97.2877 (93.7220)
Test (EMA): [   0/48]  Time: 3.126 (3.126)  Loss:  0.4389 (0.4389)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5688 (0.8993)  Acc@1: 86.2028 (78.9620)  Acc@5: 97.4057 (94.6580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 78.63200003173829)

Train: 117 [   0/1251 (  0%)]  Loss:  3.199255 (3.1993)  Time: 1.124s,  911.38/s  (1.124s,  911.38/s)  LR: 3.363e-04  Data: 0.041 (0.041)
Train: 117 [  50/1251 (  4%)]  Loss:  3.591102 (3.3952)  Time: 1.097s,  933.41/s  (1.107s,  925.36/s)  LR: 3.363e-04  Data: 0.013 (0.012)
Train: 117 [ 100/1251 (  8%)]  Loss:  3.276161 (3.3555)  Time: 1.098s,  932.90/s  (1.105s,  926.52/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Train: 117 [ 150/1251 ( 12%)]  Loss:  3.366874 (3.3583)  Time: 1.098s,  932.94/s  (1.106s,  925.78/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Train: 117 [ 200/1251 ( 16%)]  Loss:  3.477639 (3.3822)  Time: 1.097s,  933.58/s  (1.105s,  926.63/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [ 250/1251 ( 20%)]  Loss:  2.989501 (3.3168)  Time: 1.097s,  933.17/s  (1.105s,  926.54/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [ 300/1251 ( 24%)]  Loss:  3.310700 (3.3159)  Time: 1.100s,  930.70/s  (1.107s,  925.37/s)  LR: 3.363e-04  Data: 0.010 (0.012)
Train: 117 [ 350/1251 ( 28%)]  Loss:  3.502114 (3.3392)  Time: 1.119s,  915.26/s  (1.109s,  923.45/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [ 400/1251 ( 32%)]  Loss:  3.258987 (3.3303)  Time: 1.099s,  931.94/s  (1.109s,  923.09/s)  LR: 3.363e-04  Data: 0.014 (0.012)
Train: 117 [ 450/1251 ( 36%)]  Loss:  3.161269 (3.3134)  Time: 1.117s,  916.83/s  (1.110s,  922.59/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Train: 117 [ 500/1251 ( 40%)]  Loss:  3.158437 (3.2993)  Time: 1.101s,  930.32/s  (1.110s,  922.78/s)  LR: 3.363e-04  Data: 0.010 (0.012)
Train: 117 [ 550/1251 ( 44%)]  Loss:  3.506085 (3.3165)  Time: 1.096s,  934.59/s  (1.109s,  923.50/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [ 600/1251 ( 48%)]  Loss:  3.089099 (3.2990)  Time: 1.097s,  933.07/s  (1.109s,  923.57/s)  LR: 3.363e-04  Data: 0.014 (0.012)
Train: 117 [ 650/1251 ( 52%)]  Loss:  3.471117 (3.3113)  Time: 1.097s,  933.10/s  (1.109s,  923.23/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [ 700/1251 ( 56%)]  Loss:  3.294440 (3.3102)  Time: 1.119s,  915.10/s  (1.109s,  923.54/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Train: 117 [ 750/1251 ( 60%)]  Loss:  3.557258 (3.3256)  Time: 1.095s,  934.90/s  (1.109s,  923.26/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Train: 117 [ 800/1251 ( 64%)]  Loss:  3.147562 (3.3152)  Time: 1.099s,  931.49/s  (1.109s,  922.99/s)  LR: 3.363e-04  Data: 0.013 (0.012)
Train: 117 [ 850/1251 ( 68%)]  Loss:  3.396703 (3.3197)  Time: 1.121s,  913.64/s  (1.109s,  923.30/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Train: 117 [ 900/1251 ( 72%)]  Loss:  3.045193 (3.3052)  Time: 1.096s,  934.20/s  (1.110s,  922.58/s)  LR: 3.363e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 117 [ 950/1251 ( 76%)]  Loss:  3.409707 (3.3105)  Time: 1.099s,  931.90/s  (1.110s,  922.88/s)  LR: 3.363e-04  Data: 0.014 (0.012)
Train: 117 [1000/1251 ( 80%)]  Loss:  3.226970 (3.3065)  Time: 1.098s,  932.59/s  (1.110s,  922.54/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [1050/1251 ( 84%)]  Loss:  3.261545 (3.3044)  Time: 1.096s,  934.68/s  (1.110s,  922.88/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [1100/1251 ( 88%)]  Loss:  3.365865 (3.3071)  Time: 1.196s,  856.22/s  (1.109s,  922.98/s)  LR: 3.363e-04  Data: 0.010 (0.012)
Train: 117 [1150/1251 ( 92%)]  Loss:  3.563807 (3.3178)  Time: 1.097s,  933.83/s  (1.109s,  923.01/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [1200/1251 ( 96%)]  Loss:  3.584548 (3.3285)  Time: 1.122s,  912.51/s  (1.109s,  923.00/s)  LR: 3.363e-04  Data: 0.012 (0.012)
Train: 117 [1250/1251 (100%)]  Loss:  3.306461 (3.3276)  Time: 1.080s,  948.38/s  (1.109s,  922.99/s)  LR: 3.363e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.193 (3.193)  Loss:  0.5066 (0.5066)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.399)  Loss:  0.6216 (0.9887)  Acc@1: 87.2642 (77.3000)  Acc@5: 97.5236 (93.7220)
Test (EMA): [   0/48]  Time: 3.245 (3.245)  Loss:  0.4378 (0.4378)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5673 (0.8979)  Acc@1: 85.9670 (78.9700)  Acc@5: 97.5236 (94.6880)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 78.67200003173828)

Train: 118 [   0/1251 (  0%)]  Loss:  3.386484 (3.3865)  Time: 1.105s,  926.96/s  (1.105s,  926.96/s)  LR: 3.339e-04  Data: 0.023 (0.023)
Train: 118 [  50/1251 (  4%)]  Loss:  3.421603 (3.4040)  Time: 1.098s,  932.82/s  (1.108s,  923.90/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 100/1251 (  8%)]  Loss:  3.459256 (3.4224)  Time: 1.096s,  933.90/s  (1.108s,  924.49/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 150/1251 ( 12%)]  Loss:  3.485021 (3.4381)  Time: 1.097s,  933.32/s  (1.107s,  924.75/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 200/1251 ( 16%)]  Loss:  3.547559 (3.4600)  Time: 1.107s,  924.95/s  (1.108s,  924.11/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 250/1251 ( 20%)]  Loss:  3.060847 (3.3935)  Time: 1.097s,  933.30/s  (1.107s,  924.75/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 300/1251 ( 24%)]  Loss:  3.498194 (3.4084)  Time: 1.124s,  911.20/s  (1.107s,  925.08/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 350/1251 ( 28%)]  Loss:  3.366327 (3.4032)  Time: 1.121s,  913.10/s  (1.106s,  925.81/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 400/1251 ( 32%)]  Loss:  2.970339 (3.3551)  Time: 1.095s,  934.97/s  (1.106s,  925.53/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 450/1251 ( 36%)]  Loss:  3.695294 (3.3891)  Time: 1.097s,  933.43/s  (1.107s,  925.40/s)  LR: 3.339e-04  Data: 0.013 (0.012)
Train: 118 [ 500/1251 ( 40%)]  Loss:  3.441590 (3.3939)  Time: 1.096s,  934.67/s  (1.106s,  925.62/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 550/1251 ( 44%)]  Loss:  3.026485 (3.3632)  Time: 1.096s,  933.96/s  (1.108s,  924.47/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 600/1251 ( 48%)]  Loss:  3.634812 (3.3841)  Time: 1.098s,  932.45/s  (1.107s,  924.61/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 650/1251 ( 52%)]  Loss:  3.464550 (3.3899)  Time: 1.097s,  933.47/s  (1.107s,  924.72/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 700/1251 ( 56%)]  Loss:  3.354673 (3.3875)  Time: 1.099s,  931.89/s  (1.107s,  924.78/s)  LR: 3.339e-04  Data: 0.014 (0.012)
Train: 118 [ 750/1251 ( 60%)]  Loss:  3.129517 (3.3714)  Time: 1.093s,  936.67/s  (1.108s,  923.97/s)  LR: 3.339e-04  Data: 0.010 (0.012)
Train: 118 [ 800/1251 ( 64%)]  Loss:  3.315180 (3.3681)  Time: 1.099s,  931.45/s  (1.108s,  923.97/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [ 850/1251 ( 68%)]  Loss:  3.334154 (3.3662)  Time: 1.121s,  913.20/s  (1.108s,  924.03/s)  LR: 3.339e-04  Data: 0.010 (0.012)
Train: 118 [ 900/1251 ( 72%)]  Loss:  3.506065 (3.3736)  Time: 1.100s,  931.18/s  (1.108s,  924.04/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [ 950/1251 ( 76%)]  Loss:  3.330086 (3.3714)  Time: 1.107s,  925.36/s  (1.109s,  923.58/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [1000/1251 ( 80%)]  Loss:  3.277425 (3.3669)  Time: 1.095s,  935.17/s  (1.109s,  923.63/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [1050/1251 ( 84%)]  Loss:  3.371517 (3.3671)  Time: 1.195s,  857.24/s  (1.108s,  923.84/s)  LR: 3.339e-04  Data: 0.011 (0.012)
Train: 118 [1100/1251 ( 88%)]  Loss:  3.428419 (3.3698)  Time: 1.097s,  933.46/s  (1.108s,  923.78/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [1150/1251 ( 92%)]  Loss:  3.177751 (3.3618)  Time: 1.103s,  928.41/s  (1.108s,  923.91/s)  LR: 3.339e-04  Data: 0.012 (0.012)
Train: 118 [1200/1251 ( 96%)]  Loss:  3.602020 (3.3714)  Time: 1.103s,  928.48/s  (1.108s,  924.01/s)  LR: 3.339e-04  Data: 0.010 (0.012)
Train: 118 [1250/1251 (100%)]  Loss:  3.449605 (3.3744)  Time: 1.074s,  953.26/s  (1.108s,  924.15/s)  LR: 3.339e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.198 (3.198)  Loss:  0.5115 (0.5115)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6298 (1.0247)  Acc@1: 86.4387 (76.7960)  Acc@5: 97.8774 (93.7760)
Test (EMA): [   0/48]  Time: 3.245 (3.245)  Loss:  0.4368 (0.4368)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.5664 (0.8963)  Acc@1: 85.9670 (78.9880)  Acc@5: 97.5236 (94.7240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 78.71600003173828)

Train: 119 [   0/1251 (  0%)]  Loss:  3.531276 (3.5313)  Time: 1.101s,  930.09/s  (1.101s,  930.09/s)  LR: 3.314e-04  Data: 0.020 (0.020)
Train: 119 [  50/1251 (  4%)]  Loss:  3.551832 (3.5416)  Time: 1.096s,  934.06/s  (1.108s,  923.79/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [ 100/1251 (  8%)]  Loss:  3.102366 (3.3952)  Time: 1.098s,  932.67/s  (1.108s,  924.30/s)  LR: 3.314e-04  Data: 0.013 (0.012)
Train: 119 [ 150/1251 ( 12%)]  Loss:  3.556094 (3.4354)  Time: 1.118s,  916.03/s  (1.110s,  922.23/s)  LR: 3.314e-04  Data: 0.010 (0.012)
Train: 119 [ 200/1251 ( 16%)]  Loss:  2.954420 (3.3392)  Time: 1.094s,  935.98/s  (1.112s,  921.04/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [ 250/1251 ( 20%)]  Loss:  3.242573 (3.3231)  Time: 1.103s,  928.74/s  (1.110s,  922.42/s)  LR: 3.314e-04  Data: 0.014 (0.012)
Train: 119 [ 300/1251 ( 24%)]  Loss:  3.537219 (3.3537)  Time: 1.098s,  932.63/s  (1.109s,  923.69/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [ 350/1251 ( 28%)]  Loss:  3.531422 (3.3759)  Time: 1.130s,  906.13/s  (1.109s,  923.13/s)  LR: 3.314e-04  Data: 0.010 (0.012)
Train: 119 [ 400/1251 ( 32%)]  Loss:  3.222472 (3.3589)  Time: 1.117s,  916.49/s  (1.110s,  922.41/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [ 450/1251 ( 36%)]  Loss:  3.210727 (3.3440)  Time: 1.099s,  931.82/s  (1.110s,  922.32/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [ 500/1251 ( 40%)]  Loss:  3.403014 (3.3494)  Time: 1.101s,  929.78/s  (1.110s,  922.39/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [ 550/1251 ( 44%)]  Loss:  3.341937 (3.3488)  Time: 1.097s,  933.42/s  (1.110s,  922.77/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [ 600/1251 ( 48%)]  Loss:  3.223720 (3.3392)  Time: 1.101s,  930.02/s  (1.109s,  922.96/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [ 650/1251 ( 52%)]  Loss:  3.411612 (3.3443)  Time: 1.097s,  933.11/s  (1.109s,  923.40/s)  LR: 3.314e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 119 [ 700/1251 ( 56%)]  Loss:  3.103772 (3.3283)  Time: 1.098s,  932.93/s  (1.109s,  923.66/s)  LR: 3.314e-04  Data: 0.014 (0.012)
Train: 119 [ 750/1251 ( 60%)]  Loss:  3.407775 (3.3333)  Time: 1.097s,  933.47/s  (1.108s,  924.04/s)  LR: 3.314e-04  Data: 0.013 (0.012)
Train: 119 [ 800/1251 ( 64%)]  Loss:  3.478511 (3.3418)  Time: 1.106s,  925.48/s  (1.109s,  923.53/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [ 850/1251 ( 68%)]  Loss:  3.523377 (3.3519)  Time: 1.099s,  931.48/s  (1.108s,  923.78/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [ 900/1251 ( 72%)]  Loss:  3.565383 (3.3631)  Time: 1.099s,  931.59/s  (1.108s,  923.99/s)  LR: 3.314e-04  Data: 0.013 (0.012)
Train: 119 [ 950/1251 ( 76%)]  Loss:  3.264974 (3.3582)  Time: 1.099s,  931.43/s  (1.108s,  924.07/s)  LR: 3.314e-04  Data: 0.010 (0.012)
Train: 119 [1000/1251 ( 80%)]  Loss:  3.429659 (3.3616)  Time: 1.192s,  858.76/s  (1.109s,  923.61/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [1050/1251 ( 84%)]  Loss:  3.390957 (3.3630)  Time: 1.098s,  932.33/s  (1.109s,  923.75/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [1100/1251 ( 88%)]  Loss:  3.470419 (3.3676)  Time: 1.095s,  934.87/s  (1.108s,  923.77/s)  LR: 3.314e-04  Data: 0.012 (0.012)
Train: 119 [1150/1251 ( 92%)]  Loss:  3.315471 (3.3655)  Time: 1.100s,  930.91/s  (1.109s,  923.72/s)  LR: 3.314e-04  Data: 0.010 (0.012)
Train: 119 [1200/1251 ( 96%)]  Loss:  3.328885 (3.3640)  Time: 1.119s,  914.90/s  (1.108s,  923.86/s)  LR: 3.314e-04  Data: 0.011 (0.012)
Train: 119 [1250/1251 (100%)]  Loss:  3.470752 (3.3681)  Time: 1.081s,  947.55/s  (1.109s,  923.74/s)  LR: 3.314e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.301 (3.301)  Loss:  0.4749 (0.4749)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.5814 (0.9869)  Acc@1: 86.4387 (76.9560)  Acc@5: 97.6415 (93.8380)
Test (EMA): [   0/48]  Time: 3.142 (3.142)  Loss:  0.4353 (0.4353)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.401)  Loss:  0.5647 (0.8951)  Acc@1: 86.2028 (79.0200)  Acc@5: 97.6415 (94.7300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 78.75799995361328)

Train: 120 [   0/1251 (  0%)]  Loss:  3.209142 (3.2091)  Time: 1.121s,  913.37/s  (1.121s,  913.37/s)  LR: 3.290e-04  Data: 0.029 (0.029)
Train: 120 [  50/1251 (  4%)]  Loss:  3.172021 (3.1906)  Time: 1.094s,  935.68/s  (1.103s,  928.51/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 100/1251 (  8%)]  Loss:  3.059435 (3.1469)  Time: 1.099s,  932.02/s  (1.110s,  922.17/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 150/1251 ( 12%)]  Loss:  3.067427 (3.1270)  Time: 1.099s,  931.56/s  (1.108s,  924.39/s)  LR: 3.290e-04  Data: 0.013 (0.012)
Train: 120 [ 200/1251 ( 16%)]  Loss:  3.324980 (3.1666)  Time: 1.100s,  930.72/s  (1.108s,  924.03/s)  LR: 3.290e-04  Data: 0.013 (0.012)
Train: 120 [ 250/1251 ( 20%)]  Loss:  3.474235 (3.2179)  Time: 1.209s,  846.67/s  (1.108s,  924.04/s)  LR: 3.290e-04  Data: 0.010 (0.012)
Train: 120 [ 300/1251 ( 24%)]  Loss:  2.922267 (3.1756)  Time: 1.118s,  915.83/s  (1.109s,  923.75/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 350/1251 ( 28%)]  Loss:  3.401004 (3.2038)  Time: 1.108s,  924.49/s  (1.108s,  924.17/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 400/1251 ( 32%)]  Loss:  3.490547 (3.2357)  Time: 1.194s,  857.59/s  (1.108s,  924.13/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 450/1251 ( 36%)]  Loss:  2.949860 (3.2071)  Time: 1.096s,  934.10/s  (1.108s,  924.60/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 500/1251 ( 40%)]  Loss:  3.288851 (3.2145)  Time: 1.100s,  930.86/s  (1.107s,  924.76/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 550/1251 ( 44%)]  Loss:  3.322833 (3.2236)  Time: 1.102s,  929.10/s  (1.107s,  924.77/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 600/1251 ( 48%)]  Loss:  3.184115 (3.2205)  Time: 1.161s,  882.31/s  (1.107s,  925.10/s)  LR: 3.290e-04  Data: 0.010 (0.012)
Train: 120 [ 650/1251 ( 52%)]  Loss:  2.951500 (3.2013)  Time: 1.094s,  935.82/s  (1.107s,  925.24/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [ 700/1251 ( 56%)]  Loss:  3.112921 (3.1954)  Time: 1.104s,  927.75/s  (1.106s,  925.60/s)  LR: 3.290e-04  Data: 0.010 (0.012)
Train: 120 [ 750/1251 ( 60%)]  Loss:  3.385468 (3.2073)  Time: 1.097s,  933.30/s  (1.106s,  925.64/s)  LR: 3.290e-04  Data: 0.012 (0.012)
Train: 120 [ 800/1251 ( 64%)]  Loss:  3.280927 (3.2116)  Time: 1.099s,  932.06/s  (1.106s,  925.73/s)  LR: 3.290e-04  Data: 0.013 (0.012)
Train: 120 [ 850/1251 ( 68%)]  Loss:  3.408652 (3.2226)  Time: 1.095s,  935.11/s  (1.106s,  925.76/s)  LR: 3.290e-04  Data: 0.012 (0.012)
Train: 120 [ 900/1251 ( 72%)]  Loss:  3.039410 (3.2129)  Time: 1.120s,  914.46/s  (1.107s,  925.31/s)  LR: 3.290e-04  Data: 0.013 (0.012)
Train: 120 [ 950/1251 ( 76%)]  Loss:  2.891647 (3.1969)  Time: 1.101s,  930.45/s  (1.107s,  924.88/s)  LR: 3.290e-04  Data: 0.012 (0.012)
Train: 120 [1000/1251 ( 80%)]  Loss:  3.406740 (3.2069)  Time: 1.100s,  930.70/s  (1.107s,  924.78/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [1050/1251 ( 84%)]  Loss:  3.489108 (3.2197)  Time: 1.101s,  929.77/s  (1.107s,  924.98/s)  LR: 3.290e-04  Data: 0.011 (0.012)
Train: 120 [1100/1251 ( 88%)]  Loss:  3.584237 (3.2355)  Time: 1.125s,  909.94/s  (1.107s,  925.08/s)  LR: 3.290e-04  Data: 0.013 (0.012)
Train: 120 [1150/1251 ( 92%)]  Loss:  3.441518 (3.2441)  Time: 1.107s,  925.26/s  (1.107s,  924.86/s)  LR: 3.290e-04  Data: 0.012 (0.012)
Train: 120 [1200/1251 ( 96%)]  Loss:  3.036128 (3.2358)  Time: 1.103s,  928.68/s  (1.107s,  924.77/s)  LR: 3.290e-04  Data: 0.012 (0.012)
Train: 120 [1250/1251 (100%)]  Loss:  3.116582 (3.2312)  Time: 1.105s,  926.71/s  (1.107s,  924.69/s)  LR: 3.290e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.353 (3.353)  Loss:  0.5101 (0.5101)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.230 (0.405)  Loss:  0.6409 (1.0130)  Acc@1: 85.0236 (77.0800)  Acc@5: 97.5236 (93.6300)
Test (EMA): [   0/48]  Time: 3.098 (3.098)  Loss:  0.4339 (0.4339)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5642 (0.8939)  Acc@1: 86.2028 (79.0000)  Acc@5: 97.5236 (94.7440)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 78.76600003173829)

Train: 121 [   0/1251 (  0%)]  Loss:  3.413464 (3.4135)  Time: 1.106s,  926.23/s  (1.106s,  926.23/s)  LR: 3.265e-04  Data: 0.024 (0.024)
Train: 121 [  50/1251 (  4%)]  Loss:  3.234360 (3.3239)  Time: 1.096s,  934.07/s  (1.112s,  920.88/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 100/1251 (  8%)]  Loss:  3.801640 (3.4832)  Time: 1.105s,  926.48/s  (1.115s,  918.20/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 150/1251 ( 12%)]  Loss:  2.894711 (3.3360)  Time: 1.100s,  931.22/s  (1.112s,  920.61/s)  LR: 3.265e-04  Data: 0.015 (0.011)
Train: 121 [ 200/1251 ( 16%)]  Loss:  3.258940 (3.3206)  Time: 1.211s,  845.27/s  (1.111s,  921.75/s)  LR: 3.265e-04  Data: 0.010 (0.012)
Train: 121 [ 250/1251 ( 20%)]  Loss:  3.440560 (3.3406)  Time: 1.096s,  934.47/s  (1.109s,  923.01/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [ 300/1251 ( 24%)]  Loss:  3.318685 (3.3375)  Time: 1.098s,  933.03/s  (1.109s,  923.22/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [ 350/1251 ( 28%)]  Loss:  3.262528 (3.3281)  Time: 1.098s,  932.22/s  (1.109s,  923.56/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 400/1251 ( 32%)]  Loss:  3.417257 (3.3380)  Time: 1.108s,  924.29/s  (1.108s,  923.82/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [ 450/1251 ( 36%)]  Loss:  3.364057 (3.3406)  Time: 1.098s,  932.84/s  (1.108s,  924.29/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 500/1251 ( 40%)]  Loss:  3.114201 (3.3200)  Time: 1.100s,  930.86/s  (1.107s,  924.77/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [ 550/1251 ( 44%)]  Loss:  3.288803 (3.3174)  Time: 1.194s,  857.62/s  (1.108s,  924.18/s)  LR: 3.265e-04  Data: 0.015 (0.012)
Train: 121 [ 600/1251 ( 48%)]  Loss:  3.473882 (3.3295)  Time: 1.099s,  931.46/s  (1.108s,  924.32/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 121 [ 650/1251 ( 52%)]  Loss:  3.222281 (3.3218)  Time: 1.094s,  935.74/s  (1.107s,  924.82/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [ 700/1251 ( 56%)]  Loss:  3.409002 (3.3276)  Time: 1.189s,  861.15/s  (1.108s,  924.45/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 750/1251 ( 60%)]  Loss:  3.392586 (3.3317)  Time: 1.098s,  932.84/s  (1.107s,  924.62/s)  LR: 3.265e-04  Data: 0.013 (0.012)
Train: 121 [ 800/1251 ( 64%)]  Loss:  3.075696 (3.3166)  Time: 1.126s,  909.07/s  (1.108s,  924.59/s)  LR: 3.265e-04  Data: 0.010 (0.012)
Train: 121 [ 850/1251 ( 68%)]  Loss:  3.305016 (3.3160)  Time: 1.098s,  932.90/s  (1.107s,  924.65/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 900/1251 ( 72%)]  Loss:  3.265759 (3.3133)  Time: 1.096s,  934.08/s  (1.108s,  924.02/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [ 950/1251 ( 76%)]  Loss:  3.524165 (3.3239)  Time: 1.098s,  932.53/s  (1.108s,  924.15/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [1000/1251 ( 80%)]  Loss:  3.134572 (3.3149)  Time: 1.104s,  927.79/s  (1.108s,  923.92/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [1050/1251 ( 84%)]  Loss:  3.589264 (3.3273)  Time: 1.094s,  936.26/s  (1.108s,  923.85/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [1100/1251 ( 88%)]  Loss:  3.512839 (3.3354)  Time: 1.096s,  934.07/s  (1.108s,  924.06/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [1150/1251 ( 92%)]  Loss:  3.528700 (3.3435)  Time: 1.095s,  935.09/s  (1.108s,  923.80/s)  LR: 3.265e-04  Data: 0.012 (0.012)
Train: 121 [1200/1251 ( 96%)]  Loss:  3.276779 (3.3408)  Time: 1.096s,  934.66/s  (1.108s,  924.09/s)  LR: 3.265e-04  Data: 0.011 (0.012)
Train: 121 [1250/1251 (100%)]  Loss:  3.302178 (3.3393)  Time: 1.078s,  949.94/s  (1.109s,  923.75/s)  LR: 3.265e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.282 (3.282)  Loss:  0.4908 (0.4908)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.401)  Loss:  0.6549 (0.9988)  Acc@1: 86.4387 (77.0860)  Acc@5: 96.8160 (93.8600)
Test (EMA): [   0/48]  Time: 3.189 (3.189)  Loss:  0.4313 (0.4313)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5639 (0.8925)  Acc@1: 86.2028 (79.0460)  Acc@5: 97.5236 (94.7640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 78.81000003173828)

Train: 122 [   0/1251 (  0%)]  Loss:  3.049159 (3.0492)  Time: 1.105s,  926.80/s  (1.105s,  926.80/s)  LR: 3.240e-04  Data: 0.021 (0.021)
Train: 122 [  50/1251 (  4%)]  Loss:  3.137896 (3.0935)  Time: 1.098s,  932.85/s  (1.102s,  929.55/s)  LR: 3.240e-04  Data: 0.013 (0.012)
Train: 122 [ 100/1251 (  8%)]  Loss:  3.328415 (3.1718)  Time: 1.096s,  934.63/s  (1.107s,  924.94/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [ 150/1251 ( 12%)]  Loss:  3.433241 (3.2372)  Time: 1.211s,  845.82/s  (1.106s,  925.57/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [ 200/1251 ( 16%)]  Loss:  3.405474 (3.2708)  Time: 1.091s,  938.91/s  (1.105s,  926.64/s)  LR: 3.240e-04  Data: 0.010 (0.012)
Train: 122 [ 250/1251 ( 20%)]  Loss:  3.217129 (3.2619)  Time: 1.115s,  918.68/s  (1.105s,  926.77/s)  LR: 3.240e-04  Data: 0.012 (0.012)
Train: 122 [ 300/1251 ( 24%)]  Loss:  3.215367 (3.2552)  Time: 1.101s,  929.73/s  (1.105s,  926.79/s)  LR: 3.240e-04  Data: 0.013 (0.012)
Train: 122 [ 350/1251 ( 28%)]  Loss:  3.249452 (3.2545)  Time: 1.097s,  933.77/s  (1.105s,  926.76/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [ 400/1251 ( 32%)]  Loss:  3.498670 (3.2816)  Time: 1.096s,  934.59/s  (1.106s,  925.68/s)  LR: 3.240e-04  Data: 0.012 (0.012)
Train: 122 [ 450/1251 ( 36%)]  Loss:  3.205251 (3.2740)  Time: 1.124s,  911.42/s  (1.106s,  925.58/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [ 500/1251 ( 40%)]  Loss:  3.198175 (3.2671)  Time: 1.097s,  933.66/s  (1.106s,  925.72/s)  LR: 3.240e-04  Data: 0.010 (0.012)
Train: 122 [ 550/1251 ( 44%)]  Loss:  3.190813 (3.2608)  Time: 1.102s,  929.32/s  (1.106s,  925.85/s)  LR: 3.240e-04  Data: 0.016 (0.012)
Train: 122 [ 600/1251 ( 48%)]  Loss:  3.276495 (3.2620)  Time: 1.123s,  911.64/s  (1.106s,  926.04/s)  LR: 3.240e-04  Data: 0.012 (0.012)
Train: 122 [ 650/1251 ( 52%)]  Loss:  3.380322 (3.2704)  Time: 1.098s,  933.00/s  (1.107s,  925.28/s)  LR: 3.240e-04  Data: 0.014 (0.012)
Train: 122 [ 700/1251 ( 56%)]  Loss:  3.662866 (3.2966)  Time: 1.133s,  903.71/s  (1.106s,  925.56/s)  LR: 3.240e-04  Data: 0.012 (0.012)
Train: 122 [ 750/1251 ( 60%)]  Loss:  3.143461 (3.2870)  Time: 1.101s,  930.21/s  (1.106s,  925.58/s)  LR: 3.240e-04  Data: 0.010 (0.012)
Train: 122 [ 800/1251 ( 64%)]  Loss:  3.410094 (3.2943)  Time: 1.102s,  929.22/s  (1.106s,  925.90/s)  LR: 3.240e-04  Data: 0.013 (0.012)
Train: 122 [ 850/1251 ( 68%)]  Loss:  3.542818 (3.3081)  Time: 1.096s,  934.05/s  (1.106s,  926.03/s)  LR: 3.240e-04  Data: 0.012 (0.012)
Train: 122 [ 900/1251 ( 72%)]  Loss:  3.595349 (3.3232)  Time: 1.095s,  935.31/s  (1.106s,  926.12/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [ 950/1251 ( 76%)]  Loss:  3.087089 (3.3114)  Time: 1.099s,  931.83/s  (1.106s,  926.27/s)  LR: 3.240e-04  Data: 0.012 (0.012)
Train: 122 [1000/1251 ( 80%)]  Loss:  3.291570 (3.3104)  Time: 1.131s,  905.22/s  (1.106s,  926.11/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [1050/1251 ( 84%)]  Loss:  3.179825 (3.3045)  Time: 1.124s,  911.16/s  (1.106s,  926.20/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [1100/1251 ( 88%)]  Loss:  3.355664 (3.3067)  Time: 1.101s,  929.92/s  (1.106s,  926.13/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [1150/1251 ( 92%)]  Loss:  3.646202 (3.3209)  Time: 1.103s,  928.42/s  (1.106s,  925.69/s)  LR: 3.240e-04  Data: 0.011 (0.012)
Train: 122 [1200/1251 ( 96%)]  Loss:  3.479755 (3.3272)  Time: 1.098s,  933.00/s  (1.106s,  925.56/s)  LR: 3.240e-04  Data: 0.013 (0.012)
Train: 122 [1250/1251 (100%)]  Loss:  3.738896 (3.3431)  Time: 1.078s,  949.68/s  (1.106s,  925.47/s)  LR: 3.240e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.332 (3.332)  Loss:  0.5488 (0.5488)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6787 (1.0389)  Acc@1: 87.0283 (77.1280)  Acc@5: 97.6415 (93.8460)
Test (EMA): [   0/48]  Time: 3.183 (3.183)  Loss:  0.4292 (0.4292)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.8516 (97.8516)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5645 (0.8913)  Acc@1: 86.3208 (79.0760)  Acc@5: 97.5236 (94.8000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 78.85199995361329)

Train: 123 [   0/1251 (  0%)]  Loss:  3.348549 (3.3485)  Time: 1.103s,  928.61/s  (1.103s,  928.61/s)  LR: 3.216e-04  Data: 0.020 (0.020)
Train: 123 [  50/1251 (  4%)]  Loss:  3.289572 (3.3191)  Time: 1.198s,  854.45/s  (1.108s,  924.16/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 100/1251 (  8%)]  Loss:  3.092388 (3.2435)  Time: 1.105s,  926.93/s  (1.105s,  926.30/s)  LR: 3.216e-04  Data: 0.011 (0.011)
Train: 123 [ 150/1251 ( 12%)]  Loss:  3.375354 (3.2765)  Time: 1.100s,  931.31/s  (1.105s,  926.64/s)  LR: 3.216e-04  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 123 [ 200/1251 ( 16%)]  Loss:  3.439785 (3.3091)  Time: 1.098s,  932.78/s  (1.104s,  927.38/s)  LR: 3.216e-04  Data: 0.014 (0.011)
Train: 123 [ 250/1251 ( 20%)]  Loss:  3.060897 (3.2678)  Time: 1.098s,  933.01/s  (1.104s,  927.58/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 300/1251 ( 24%)]  Loss:  3.662410 (3.3241)  Time: 1.103s,  928.28/s  (1.104s,  927.23/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 350/1251 ( 28%)]  Loss:  3.145777 (3.3018)  Time: 1.121s,  913.65/s  (1.105s,  927.12/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 400/1251 ( 32%)]  Loss:  3.070268 (3.2761)  Time: 1.097s,  933.28/s  (1.104s,  927.13/s)  LR: 3.216e-04  Data: 0.012 (0.012)
Train: 123 [ 450/1251 ( 36%)]  Loss:  3.447268 (3.2932)  Time: 1.097s,  933.72/s  (1.105s,  926.61/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 500/1251 ( 40%)]  Loss:  3.354597 (3.2988)  Time: 1.110s,  922.33/s  (1.105s,  926.29/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 550/1251 ( 44%)]  Loss:  3.432640 (3.3100)  Time: 1.096s,  934.17/s  (1.105s,  926.74/s)  LR: 3.216e-04  Data: 0.012 (0.012)
Train: 123 [ 600/1251 ( 48%)]  Loss:  3.580972 (3.3308)  Time: 1.097s,  933.03/s  (1.105s,  926.51/s)  LR: 3.216e-04  Data: 0.011 (0.012)
Train: 123 [ 650/1251 ( 52%)]  Loss:  3.484836 (3.3418)  Time: 1.120s,  914.66/s  (1.105s,  926.33/s)  LR: 3.216e-04  Data: 0.010 (0.012)
Train: 123 [ 700/1251 ( 56%)]  Loss:  2.904993 (3.3127)  Time: 1.095s,  934.87/s  (1.107s,  925.28/s)  LR: 3.216e-04  Data: 0.011 (0.012)
Train: 123 [ 750/1251 ( 60%)]  Loss:  3.246766 (3.3086)  Time: 1.125s,  910.43/s  (1.107s,  925.29/s)  LR: 3.216e-04  Data: 0.012 (0.012)
Train: 123 [ 800/1251 ( 64%)]  Loss:  3.343844 (3.3106)  Time: 1.095s,  935.48/s  (1.107s,  925.11/s)  LR: 3.216e-04  Data: 0.012 (0.012)
Train: 123 [ 850/1251 ( 68%)]  Loss:  3.112520 (3.2996)  Time: 1.097s,  933.75/s  (1.107s,  925.22/s)  LR: 3.216e-04  Data: 0.011 (0.012)
Train: 123 [ 900/1251 ( 72%)]  Loss:  2.939673 (3.2807)  Time: 1.095s,  935.25/s  (1.107s,  925.40/s)  LR: 3.216e-04  Data: 0.012 (0.012)
Train: 123 [ 950/1251 ( 76%)]  Loss:  3.662337 (3.2998)  Time: 1.096s,  933.94/s  (1.107s,  925.24/s)  LR: 3.216e-04  Data: 0.011 (0.012)
Train: 123 [1000/1251 ( 80%)]  Loss:  3.535669 (3.3110)  Time: 1.098s,  932.71/s  (1.107s,  925.38/s)  LR: 3.216e-04  Data: 0.013 (0.012)
Train: 123 [1050/1251 ( 84%)]  Loss:  3.373743 (3.3139)  Time: 1.096s,  933.90/s  (1.107s,  925.14/s)  LR: 3.216e-04  Data: 0.011 (0.012)
Train: 123 [1100/1251 ( 88%)]  Loss:  3.417073 (3.3183)  Time: 1.104s,  927.62/s  (1.107s,  925.18/s)  LR: 3.216e-04  Data: 0.011 (0.012)
Train: 123 [1150/1251 ( 92%)]  Loss:  3.194937 (3.3132)  Time: 1.097s,  933.32/s  (1.107s,  925.30/s)  LR: 3.216e-04  Data: 0.012 (0.012)
Train: 123 [1200/1251 ( 96%)]  Loss:  3.353064 (3.3148)  Time: 1.096s,  934.28/s  (1.107s,  925.39/s)  LR: 3.216e-04  Data: 0.014 (0.012)
Train: 123 [1250/1251 (100%)]  Loss:  3.715957 (3.3302)  Time: 1.091s,  938.73/s  (1.107s,  925.17/s)  LR: 3.216e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.258 (3.258)  Loss:  0.4627 (0.4627)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6068 (0.9904)  Acc@1: 86.5566 (77.3920)  Acc@5: 96.9340 (93.9320)
Test (EMA): [   0/48]  Time: 3.116 (3.116)  Loss:  0.4271 (0.4271)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5646 (0.8901)  Acc@1: 86.4387 (79.1120)  Acc@5: 97.5236 (94.8080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 78.85400003173828)

Train: 124 [   0/1251 (  0%)]  Loss:  3.085915 (3.0859)  Time: 1.102s,  929.34/s  (1.102s,  929.34/s)  LR: 3.191e-04  Data: 0.019 (0.019)
Train: 124 [  50/1251 (  4%)]  Loss:  3.223206 (3.1546)  Time: 1.098s,  932.77/s  (1.110s,  922.13/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 100/1251 (  8%)]  Loss:  3.173415 (3.1608)  Time: 1.097s,  933.48/s  (1.109s,  923.45/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 150/1251 ( 12%)]  Loss:  3.304521 (3.1968)  Time: 1.096s,  934.25/s  (1.108s,  923.87/s)  LR: 3.191e-04  Data: 0.013 (0.012)
Train: 124 [ 200/1251 ( 16%)]  Loss:  3.545779 (3.2666)  Time: 1.098s,  932.70/s  (1.107s,  925.18/s)  LR: 3.191e-04  Data: 0.010 (0.012)
Train: 124 [ 250/1251 ( 20%)]  Loss:  3.354087 (3.2812)  Time: 1.099s,  931.53/s  (1.106s,  925.63/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 300/1251 ( 24%)]  Loss:  3.214635 (3.2717)  Time: 1.098s,  933.02/s  (1.107s,  925.43/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 350/1251 ( 28%)]  Loss:  3.288931 (3.2738)  Time: 1.096s,  934.53/s  (1.106s,  926.27/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 400/1251 ( 32%)]  Loss:  3.410972 (3.2891)  Time: 1.123s,  911.81/s  (1.105s,  926.40/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 450/1251 ( 36%)]  Loss:  3.621991 (3.3223)  Time: 1.099s,  931.84/s  (1.107s,  924.88/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 500/1251 ( 40%)]  Loss:  3.238510 (3.3147)  Time: 1.103s,  928.15/s  (1.108s,  924.50/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 550/1251 ( 44%)]  Loss:  3.433665 (3.3246)  Time: 1.091s,  938.65/s  (1.107s,  924.79/s)  LR: 3.191e-04  Data: 0.010 (0.012)
Train: 124 [ 600/1251 ( 48%)]  Loss:  3.374298 (3.3285)  Time: 1.097s,  933.13/s  (1.107s,  925.03/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 650/1251 ( 52%)]  Loss:  3.073669 (3.3103)  Time: 1.136s,  901.77/s  (1.107s,  924.84/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 700/1251 ( 56%)]  Loss:  3.194827 (3.3026)  Time: 1.099s,  932.17/s  (1.107s,  924.63/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 750/1251 ( 60%)]  Loss:  3.249090 (3.2992)  Time: 1.094s,  935.64/s  (1.108s,  924.53/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 800/1251 ( 64%)]  Loss:  3.284286 (3.2983)  Time: 1.107s,  924.70/s  (1.107s,  924.65/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 850/1251 ( 68%)]  Loss:  3.156639 (3.2905)  Time: 1.097s,  933.80/s  (1.107s,  924.85/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [ 900/1251 ( 72%)]  Loss:  3.300805 (3.2910)  Time: 1.119s,  915.48/s  (1.107s,  924.68/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Train: 124 [ 950/1251 ( 76%)]  Loss:  3.251451 (3.2890)  Time: 1.096s,  934.38/s  (1.107s,  924.70/s)  LR: 3.191e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 124 [1000/1251 ( 80%)]  Loss:  3.537827 (3.3009)  Time: 1.098s,  932.78/s  (1.107s,  924.64/s)  LR: 3.191e-04  Data: 0.014 (0.012)
Train: 124 [1050/1251 ( 84%)]  Loss:  3.461003 (3.3082)  Time: 1.102s,  929.11/s  (1.107s,  924.87/s)  LR: 3.191e-04  Data: 0.016 (0.012)
Train: 124 [1100/1251 ( 88%)]  Loss:  3.209444 (3.3039)  Time: 1.098s,  932.44/s  (1.107s,  924.84/s)  LR: 3.191e-04  Data: 0.016 (0.012)
Train: 124 [1150/1251 ( 92%)]  Loss:  3.485379 (3.3114)  Time: 1.211s,  845.70/s  (1.107s,  924.95/s)  LR: 3.191e-04  Data: 0.010 (0.012)
Train: 124 [1200/1251 ( 96%)]  Loss:  3.165250 (3.3056)  Time: 1.097s,  933.10/s  (1.108s,  924.58/s)  LR: 3.191e-04  Data: 0.012 (0.012)
Train: 124 [1250/1251 (100%)]  Loss:  3.090340 (3.2973)  Time: 1.084s,  944.80/s  (1.108s,  924.40/s)  LR: 3.191e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.272 (3.272)  Loss:  0.4655 (0.4655)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.6224 (1.0103)  Acc@1: 85.7311 (77.2080)  Acc@5: 97.2877 (93.8620)
Test (EMA): [   0/48]  Time: 3.134 (3.134)  Loss:  0.4254 (0.4254)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5641 (0.8889)  Acc@1: 86.5566 (79.1820)  Acc@5: 97.4057 (94.8300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 78.90799995361328)

Train: 125 [   0/1251 (  0%)]  Loss:  3.334982 (3.3350)  Time: 1.104s,  927.40/s  (1.104s,  927.40/s)  LR: 3.166e-04  Data: 0.020 (0.020)
Train: 125 [  50/1251 (  4%)]  Loss:  3.399822 (3.3674)  Time: 1.098s,  932.75/s  (1.109s,  923.04/s)  LR: 3.166e-04  Data: 0.013 (0.012)
Train: 125 [ 100/1251 (  8%)]  Loss:  3.183967 (3.3063)  Time: 1.121s,  913.76/s  (1.108s,  924.26/s)  LR: 3.166e-04  Data: 0.010 (0.012)
Train: 125 [ 150/1251 ( 12%)]  Loss:  3.320637 (3.3099)  Time: 1.094s,  936.12/s  (1.107s,  924.81/s)  LR: 3.166e-04  Data: 0.011 (0.012)
Train: 125 [ 200/1251 ( 16%)]  Loss:  3.175989 (3.2831)  Time: 1.101s,  930.30/s  (1.107s,  925.36/s)  LR: 3.166e-04  Data: 0.012 (0.012)
Train: 125 [ 250/1251 ( 20%)]  Loss:  3.302288 (3.2863)  Time: 1.105s,  926.31/s  (1.106s,  925.78/s)  LR: 3.166e-04  Data: 0.020 (0.012)
Train: 125 [ 300/1251 ( 24%)]  Loss:  3.330413 (3.2926)  Time: 1.119s,  915.06/s  (1.106s,  926.03/s)  LR: 3.166e-04  Data: 0.010 (0.012)
Train: 125 [ 350/1251 ( 28%)]  Loss:  3.177608 (3.2782)  Time: 1.097s,  933.06/s  (1.106s,  926.12/s)  LR: 3.166e-04  Data: 0.012 (0.012)
Train: 125 [ 400/1251 ( 32%)]  Loss:  3.134368 (3.2622)  Time: 1.092s,  937.99/s  (1.106s,  925.99/s)  LR: 3.166e-04  Data: 0.010 (0.012)
Train: 125 [ 450/1251 ( 36%)]  Loss:  3.379520 (3.2740)  Time: 1.097s,  933.25/s  (1.105s,  926.45/s)  LR: 3.166e-04  Data: 0.015 (0.012)
Train: 125 [ 500/1251 ( 40%)]  Loss:  3.094388 (3.2576)  Time: 1.098s,  932.62/s  (1.105s,  926.60/s)  LR: 3.166e-04  Data: 0.011 (0.012)
Train: 125 [ 550/1251 ( 44%)]  Loss:  3.183450 (3.2515)  Time: 1.096s,  934.71/s  (1.105s,  926.77/s)  LR: 3.166e-04  Data: 0.012 (0.012)
Train: 125 [ 600/1251 ( 48%)]  Loss:  3.792895 (3.2931)  Time: 1.101s,  930.21/s  (1.105s,  926.55/s)  LR: 3.166e-04  Data: 0.012 (0.012)
Train: 125 [ 650/1251 ( 52%)]  Loss:  3.444902 (3.3039)  Time: 1.097s,  933.78/s  (1.105s,  926.56/s)  LR: 3.166e-04  Data: 0.012 (0.012)
Train: 125 [ 700/1251 ( 56%)]  Loss:  3.194375 (3.2966)  Time: 1.095s,  935.03/s  (1.105s,  926.65/s)  LR: 3.166e-04  Data: 0.010 (0.012)
Train: 125 [ 750/1251 ( 60%)]  Loss:  3.421029 (3.3044)  Time: 1.094s,  936.20/s  (1.105s,  926.65/s)  LR: 3.166e-04  Data: 0.011 (0.012)
Train: 125 [ 800/1251 ( 64%)]  Loss:  3.516160 (3.3169)  Time: 1.099s,  931.53/s  (1.105s,  926.74/s)  LR: 3.166e-04  Data: 0.012 (0.012)
Train: 125 [ 850/1251 ( 68%)]  Loss:  3.391948 (3.3210)  Time: 1.101s,  929.97/s  (1.105s,  926.62/s)  LR: 3.166e-04  Data: 0.011 (0.012)
Train: 125 [ 900/1251 ( 72%)]  Loss:  3.526563 (3.3319)  Time: 1.196s,  856.33/s  (1.105s,  926.74/s)  LR: 3.166e-04  Data: 0.010 (0.012)
Train: 125 [ 950/1251 ( 76%)]  Loss:  3.105656 (3.3205)  Time: 1.129s,  907.17/s  (1.105s,  926.77/s)  LR: 3.166e-04  Data: 0.011 (0.012)
Train: 125 [1000/1251 ( 80%)]  Loss:  3.660649 (3.3367)  Time: 1.098s,  932.28/s  (1.105s,  926.50/s)  LR: 3.166e-04  Data: 0.014 (0.012)
Train: 125 [1050/1251 ( 84%)]  Loss:  3.107499 (3.3263)  Time: 1.098s,  932.84/s  (1.105s,  926.41/s)  LR: 3.166e-04  Data: 0.014 (0.012)
Train: 125 [1100/1251 ( 88%)]  Loss:  3.490981 (3.3335)  Time: 1.201s,  852.76/s  (1.105s,  926.34/s)  LR: 3.166e-04  Data: 0.010 (0.012)
Train: 125 [1150/1251 ( 92%)]  Loss:  3.141433 (3.3255)  Time: 1.098s,  932.74/s  (1.105s,  926.29/s)  LR: 3.166e-04  Data: 0.011 (0.012)
Train: 125 [1200/1251 ( 96%)]  Loss:  3.153543 (3.3186)  Time: 1.100s,  931.23/s  (1.106s,  926.20/s)  LR: 3.166e-04  Data: 0.013 (0.012)
Train: 125 [1250/1251 (100%)]  Loss:  3.280613 (3.3171)  Time: 1.080s,  948.21/s  (1.106s,  926.11/s)  LR: 3.166e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.310 (3.310)  Loss:  0.4992 (0.4992)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.398)  Loss:  0.6180 (0.9988)  Acc@1: 86.2028 (77.0840)  Acc@5: 97.1698 (93.8520)
Test (EMA): [   0/48]  Time: 3.241 (3.241)  Loss:  0.4241 (0.4241)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5644 (0.8878)  Acc@1: 86.4387 (79.2180)  Acc@5: 97.5236 (94.8380)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 78.96199995361329)

Train: 126 [   0/1251 (  0%)]  Loss:  3.138536 (3.1385)  Time: 1.129s,  906.73/s  (1.129s,  906.73/s)  LR: 3.141e-04  Data: 0.020 (0.020)
Train: 126 [  50/1251 (  4%)]  Loss:  3.311576 (3.2251)  Time: 1.092s,  937.36/s  (1.112s,  920.97/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 100/1251 (  8%)]  Loss:  3.198334 (3.2161)  Time: 1.098s,  932.37/s  (1.110s,  922.28/s)  LR: 3.141e-04  Data: 0.013 (0.012)
Train: 126 [ 150/1251 ( 12%)]  Loss:  3.413670 (3.2655)  Time: 1.097s,  933.19/s  (1.110s,  922.68/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 200/1251 ( 16%)]  Loss:  3.320408 (3.2765)  Time: 1.094s,  936.32/s  (1.109s,  923.63/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Train: 126 [ 250/1251 ( 20%)]  Loss:  3.183018 (3.2609)  Time: 1.106s,  925.88/s  (1.108s,  923.79/s)  LR: 3.141e-04  Data: 0.010 (0.012)
Train: 126 [ 300/1251 ( 24%)]  Loss:  3.150246 (3.2451)  Time: 1.099s,  931.83/s  (1.108s,  924.41/s)  LR: 3.141e-04  Data: 0.015 (0.012)
Train: 126 [ 350/1251 ( 28%)]  Loss:  3.567577 (3.2854)  Time: 1.097s,  933.78/s  (1.109s,  923.24/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 400/1251 ( 32%)]  Loss:  3.341953 (3.2917)  Time: 1.103s,  928.50/s  (1.108s,  924.15/s)  LR: 3.141e-04  Data: 0.010 (0.012)
Train: 126 [ 450/1251 ( 36%)]  Loss:  3.112253 (3.2738)  Time: 1.098s,  932.81/s  (1.108s,  924.59/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 126 [ 500/1251 ( 40%)]  Loss:  3.454276 (3.2902)  Time: 1.100s,  931.15/s  (1.107s,  925.33/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 550/1251 ( 44%)]  Loss:  3.233343 (3.2854)  Time: 1.096s,  934.27/s  (1.107s,  925.24/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 600/1251 ( 48%)]  Loss:  3.504981 (3.3023)  Time: 1.095s,  934.91/s  (1.107s,  924.61/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 650/1251 ( 52%)]  Loss:  3.558890 (3.3206)  Time: 1.100s,  931.32/s  (1.107s,  924.67/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 700/1251 ( 56%)]  Loss:  3.577437 (3.3378)  Time: 1.129s,  907.06/s  (1.107s,  924.61/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Train: 126 [ 750/1251 ( 60%)]  Loss:  3.209837 (3.3298)  Time: 1.122s,  913.00/s  (1.108s,  924.15/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 800/1251 ( 64%)]  Loss:  3.526829 (3.3414)  Time: 1.094s,  935.82/s  (1.108s,  924.59/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Train: 126 [ 850/1251 ( 68%)]  Loss:  3.391274 (3.3441)  Time: 1.206s,  848.91/s  (1.107s,  924.65/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Train: 126 [ 900/1251 ( 72%)]  Loss:  3.312260 (3.3425)  Time: 1.103s,  928.51/s  (1.108s,  924.53/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [ 950/1251 ( 76%)]  Loss:  2.966918 (3.3237)  Time: 1.099s,  931.81/s  (1.107s,  924.76/s)  LR: 3.141e-04  Data: 0.013 (0.012)
Train: 126 [1000/1251 ( 80%)]  Loss:  3.557544 (3.3348)  Time: 1.106s,  925.83/s  (1.108s,  924.57/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Train: 126 [1050/1251 ( 84%)]  Loss:  2.950658 (3.3174)  Time: 1.104s,  927.44/s  (1.108s,  924.58/s)  LR: 3.141e-04  Data: 0.013 (0.012)
Train: 126 [1100/1251 ( 88%)]  Loss:  3.738324 (3.3357)  Time: 1.100s,  930.93/s  (1.107s,  924.78/s)  LR: 3.141e-04  Data: 0.012 (0.012)
Train: 126 [1150/1251 ( 92%)]  Loss:  3.367665 (3.3370)  Time: 1.097s,  933.12/s  (1.107s,  925.01/s)  LR: 3.141e-04  Data: 0.013 (0.012)
Train: 126 [1200/1251 ( 96%)]  Loss:  3.432005 (3.3408)  Time: 1.096s,  934.50/s  (1.107s,  924.93/s)  LR: 3.141e-04  Data: 0.011 (0.012)
Train: 126 [1250/1251 (100%)]  Loss:  3.221165 (3.3362)  Time: 1.105s,  926.75/s  (1.107s,  924.84/s)  LR: 3.141e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.308 (3.308)  Loss:  0.4878 (0.4878)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5932 (0.9832)  Acc@1: 86.9104 (77.3480)  Acc@5: 97.5236 (93.9520)
Test (EMA): [   0/48]  Time: 3.180 (3.180)  Loss:  0.4232 (0.4232)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5651 (0.8867)  Acc@1: 86.3208 (79.2780)  Acc@5: 97.5236 (94.8840)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 78.96999998046876)

Train: 127 [   0/1251 (  0%)]  Loss:  3.257518 (3.2575)  Time: 1.116s,  917.94/s  (1.116s,  917.94/s)  LR: 3.115e-04  Data: 0.024 (0.024)
Train: 127 [  50/1251 (  4%)]  Loss:  3.408875 (3.3332)  Time: 1.199s,  854.06/s  (1.114s,  919.45/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 100/1251 (  8%)]  Loss:  3.177265 (3.2812)  Time: 1.101s,  929.86/s  (1.110s,  922.61/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 150/1251 ( 12%)]  Loss:  3.121296 (3.2412)  Time: 1.098s,  932.92/s  (1.109s,  923.32/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 200/1251 ( 16%)]  Loss:  3.381773 (3.2693)  Time: 1.096s,  934.57/s  (1.107s,  924.64/s)  LR: 3.115e-04  Data: 0.014 (0.012)
Train: 127 [ 250/1251 ( 20%)]  Loss:  3.537763 (3.3141)  Time: 1.099s,  931.37/s  (1.108s,  924.58/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [ 300/1251 ( 24%)]  Loss:  3.454626 (3.3342)  Time: 1.096s,  934.34/s  (1.109s,  923.70/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 350/1251 ( 28%)]  Loss:  3.352381 (3.3364)  Time: 1.096s,  934.03/s  (1.108s,  924.28/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 400/1251 ( 32%)]  Loss:  3.465557 (3.3508)  Time: 1.096s,  934.39/s  (1.108s,  924.26/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [ 450/1251 ( 36%)]  Loss:  3.251844 (3.3409)  Time: 1.099s,  932.00/s  (1.107s,  924.67/s)  LR: 3.115e-04  Data: 0.013 (0.012)
Train: 127 [ 500/1251 ( 40%)]  Loss:  3.513943 (3.3566)  Time: 1.108s,  924.12/s  (1.107s,  924.78/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [ 550/1251 ( 44%)]  Loss:  3.539575 (3.3719)  Time: 1.120s,  914.69/s  (1.108s,  924.57/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 600/1251 ( 48%)]  Loss:  3.226395 (3.3607)  Time: 1.098s,  933.01/s  (1.108s,  924.46/s)  LR: 3.115e-04  Data: 0.013 (0.012)
Train: 127 [ 650/1251 ( 52%)]  Loss:  3.423488 (3.3652)  Time: 1.095s,  935.24/s  (1.108s,  924.46/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [ 700/1251 ( 56%)]  Loss:  3.478683 (3.3727)  Time: 1.097s,  933.86/s  (1.108s,  924.50/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 750/1251 ( 60%)]  Loss:  3.513515 (3.3815)  Time: 1.096s,  934.19/s  (1.108s,  924.31/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 800/1251 ( 64%)]  Loss:  3.299482 (3.3767)  Time: 1.193s,  858.57/s  (1.107s,  924.63/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 850/1251 ( 68%)]  Loss:  3.485524 (3.3828)  Time: 1.131s,  905.36/s  (1.108s,  924.52/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 900/1251 ( 72%)]  Loss:  3.164846 (3.3713)  Time: 1.094s,  936.27/s  (1.107s,  924.62/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [ 950/1251 ( 76%)]  Loss:  3.182568 (3.3618)  Time: 1.192s,  859.27/s  (1.107s,  924.65/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [1000/1251 ( 80%)]  Loss:  3.612368 (3.3738)  Time: 1.092s,  937.70/s  (1.108s,  924.25/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [1050/1251 ( 84%)]  Loss:  3.280830 (3.3696)  Time: 1.099s,  931.70/s  (1.108s,  923.92/s)  LR: 3.115e-04  Data: 0.014 (0.012)
Train: 127 [1100/1251 ( 88%)]  Loss:  3.350298 (3.3687)  Time: 1.097s,  933.61/s  (1.108s,  924.23/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [1150/1251 ( 92%)]  Loss:  3.331262 (3.3672)  Time: 1.124s,  911.22/s  (1.108s,  924.47/s)  LR: 3.115e-04  Data: 0.011 (0.012)
Train: 127 [1200/1251 ( 96%)]  Loss:  3.312701 (3.3650)  Time: 1.095s,  935.07/s  (1.108s,  924.45/s)  LR: 3.115e-04  Data: 0.012 (0.012)
Train: 127 [1250/1251 (100%)]  Loss:  3.454124 (3.3684)  Time: 1.081s,  947.59/s  (1.108s,  924.43/s)  LR: 3.115e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.259 (3.259)  Loss:  0.5041 (0.5041)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.6157 (0.9976)  Acc@1: 85.4953 (77.3440)  Acc@5: 97.1698 (93.7980)
Test (EMA): [   0/48]  Time: 3.143 (3.143)  Loss:  0.4215 (0.4215)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5641 (0.8856)  Acc@1: 86.2028 (79.3140)  Acc@5: 97.5236 (94.9080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 78.98799998046874)

Train: 128 [   0/1251 (  0%)]  Loss:  3.652028 (3.6520)  Time: 1.107s,  925.20/s  (1.107s,  925.20/s)  LR: 3.090e-04  Data: 0.024 (0.024)
Train: 128 [  50/1251 (  4%)]  Loss:  3.536242 (3.5941)  Time: 1.096s,  934.16/s  (1.104s,  927.49/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 100/1251 (  8%)]  Loss:  3.067871 (3.4187)  Time: 1.098s,  932.38/s  (1.104s,  927.25/s)  LR: 3.090e-04  Data: 0.013 (0.012)
Train: 128 [ 150/1251 ( 12%)]  Loss:  3.323085 (3.3948)  Time: 1.100s,  930.73/s  (1.106s,  926.07/s)  LR: 3.090e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 128 [ 200/1251 ( 16%)]  Loss:  3.078407 (3.3315)  Time: 1.096s,  934.01/s  (1.104s,  927.54/s)  LR: 3.090e-04  Data: 0.012 (0.012)
Train: 128 [ 250/1251 ( 20%)]  Loss:  3.146271 (3.3007)  Time: 1.102s,  929.29/s  (1.105s,  926.49/s)  LR: 3.090e-04  Data: 0.012 (0.012)
Train: 128 [ 300/1251 ( 24%)]  Loss:  3.078624 (3.2689)  Time: 1.101s,  930.46/s  (1.105s,  926.91/s)  LR: 3.090e-04  Data: 0.013 (0.012)
Train: 128 [ 350/1251 ( 28%)]  Loss:  3.464272 (3.2934)  Time: 1.116s,  917.26/s  (1.106s,  926.17/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 400/1251 ( 32%)]  Loss:  3.432261 (3.3088)  Time: 1.098s,  932.90/s  (1.106s,  925.78/s)  LR: 3.090e-04  Data: 0.013 (0.012)
Train: 128 [ 450/1251 ( 36%)]  Loss:  2.806738 (3.2586)  Time: 1.093s,  936.78/s  (1.107s,  925.14/s)  LR: 3.090e-04  Data: 0.009 (0.012)
Train: 128 [ 500/1251 ( 40%)]  Loss:  3.441894 (3.2752)  Time: 1.099s,  931.65/s  (1.106s,  925.56/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 550/1251 ( 44%)]  Loss:  3.234309 (3.2718)  Time: 1.096s,  934.18/s  (1.106s,  925.77/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 600/1251 ( 48%)]  Loss:  3.354666 (3.2782)  Time: 1.101s,  930.47/s  (1.106s,  925.59/s)  LR: 3.090e-04  Data: 0.014 (0.012)
Train: 128 [ 650/1251 ( 52%)]  Loss:  3.371021 (3.2848)  Time: 1.094s,  935.61/s  (1.107s,  925.19/s)  LR: 3.090e-04  Data: 0.012 (0.012)
Train: 128 [ 700/1251 ( 56%)]  Loss:  3.212150 (3.2800)  Time: 1.098s,  932.78/s  (1.107s,  925.05/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 750/1251 ( 60%)]  Loss:  3.503988 (3.2940)  Time: 1.097s,  933.48/s  (1.107s,  925.07/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 800/1251 ( 64%)]  Loss:  3.118339 (3.2837)  Time: 1.098s,  932.79/s  (1.107s,  925.12/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [ 850/1251 ( 68%)]  Loss:  3.198493 (3.2789)  Time: 1.095s,  934.79/s  (1.107s,  925.13/s)  LR: 3.090e-04  Data: 0.013 (0.012)
Train: 128 [ 900/1251 ( 72%)]  Loss:  3.139024 (3.2716)  Time: 1.193s,  858.43/s  (1.107s,  924.81/s)  LR: 3.090e-04  Data: 0.010 (0.012)
Train: 128 [ 950/1251 ( 76%)]  Loss:  3.086150 (3.2623)  Time: 1.197s,  855.34/s  (1.107s,  924.98/s)  LR: 3.090e-04  Data: 0.012 (0.012)
Train: 128 [1000/1251 ( 80%)]  Loss:  3.145121 (3.2567)  Time: 1.103s,  928.78/s  (1.107s,  925.02/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [1050/1251 ( 84%)]  Loss:  3.493101 (3.2675)  Time: 1.094s,  935.72/s  (1.107s,  924.85/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [1100/1251 ( 88%)]  Loss:  3.025848 (3.2570)  Time: 1.095s,  934.82/s  (1.107s,  924.64/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [1150/1251 ( 92%)]  Loss:  3.084419 (3.2498)  Time: 1.138s,  899.82/s  (1.107s,  924.72/s)  LR: 3.090e-04  Data: 0.012 (0.012)
Train: 128 [1200/1251 ( 96%)]  Loss:  3.373142 (3.2547)  Time: 1.116s,  917.91/s  (1.107s,  924.79/s)  LR: 3.090e-04  Data: 0.011 (0.012)
Train: 128 [1250/1251 (100%)]  Loss:  3.335493 (3.2578)  Time: 1.081s,  947.50/s  (1.108s,  924.56/s)  LR: 3.090e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.241 (3.241)  Loss:  0.4892 (0.4892)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6708 (1.0123)  Acc@1: 85.9670 (77.4600)  Acc@5: 97.6415 (93.9740)
Test (EMA): [   0/48]  Time: 3.185 (3.185)  Loss:  0.4201 (0.4201)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5646 (0.8844)  Acc@1: 85.8491 (79.3660)  Acc@5: 97.5236 (94.8860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 78.99999995361328)

Train: 129 [   0/1251 (  0%)]  Loss:  3.342086 (3.3421)  Time: 1.122s,  912.65/s  (1.122s,  912.65/s)  LR: 3.065e-04  Data: 0.019 (0.019)
Train: 129 [  50/1251 (  4%)]  Loss:  3.071673 (3.2069)  Time: 1.096s,  934.64/s  (1.112s,  920.96/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [ 100/1251 (  8%)]  Loss:  3.015116 (3.1430)  Time: 1.097s,  933.32/s  (1.111s,  922.06/s)  LR: 3.065e-04  Data: 0.013 (0.012)
Train: 129 [ 150/1251 ( 12%)]  Loss:  3.622156 (3.2628)  Time: 1.096s,  934.25/s  (1.109s,  923.59/s)  LR: 3.065e-04  Data: 0.014 (0.012)
Train: 129 [ 200/1251 ( 16%)]  Loss:  3.045540 (3.2193)  Time: 1.095s,  935.04/s  (1.109s,  923.19/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [ 250/1251 ( 20%)]  Loss:  3.282704 (3.2299)  Time: 1.095s,  935.26/s  (1.108s,  924.07/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [ 300/1251 ( 24%)]  Loss:  3.283829 (3.2376)  Time: 1.095s,  934.75/s  (1.108s,  924.20/s)  LR: 3.065e-04  Data: 0.012 (0.012)
Train: 129 [ 350/1251 ( 28%)]  Loss:  3.680888 (3.2930)  Time: 1.098s,  932.98/s  (1.108s,  924.31/s)  LR: 3.065e-04  Data: 0.014 (0.012)
Train: 129 [ 400/1251 ( 32%)]  Loss:  3.526109 (3.3189)  Time: 1.119s,  914.87/s  (1.108s,  924.29/s)  LR: 3.065e-04  Data: 0.010 (0.012)
Train: 129 [ 450/1251 ( 36%)]  Loss:  3.499223 (3.3369)  Time: 1.198s,  854.50/s  (1.108s,  924.35/s)  LR: 3.065e-04  Data: 0.012 (0.012)
Train: 129 [ 500/1251 ( 40%)]  Loss:  3.489528 (3.3508)  Time: 1.104s,  927.53/s  (1.107s,  924.66/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [ 550/1251 ( 44%)]  Loss:  3.433182 (3.3577)  Time: 1.098s,  932.28/s  (1.107s,  925.16/s)  LR: 3.065e-04  Data: 0.013 (0.012)
Train: 129 [ 600/1251 ( 48%)]  Loss:  2.820196 (3.3163)  Time: 1.096s,  934.14/s  (1.107s,  925.27/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [ 650/1251 ( 52%)]  Loss:  3.169132 (3.3058)  Time: 1.104s,  927.42/s  (1.107s,  925.26/s)  LR: 3.065e-04  Data: 0.010 (0.012)
Train: 129 [ 700/1251 ( 56%)]  Loss:  2.982080 (3.2842)  Time: 1.098s,  932.24/s  (1.107s,  925.43/s)  LR: 3.065e-04  Data: 0.014 (0.012)
Train: 129 [ 750/1251 ( 60%)]  Loss:  3.195128 (3.2787)  Time: 1.097s,  933.61/s  (1.107s,  925.31/s)  LR: 3.065e-04  Data: 0.012 (0.012)
Train: 129 [ 800/1251 ( 64%)]  Loss:  3.305686 (3.2803)  Time: 1.130s,  906.26/s  (1.107s,  925.31/s)  LR: 3.065e-04  Data: 0.014 (0.012)
Train: 129 [ 850/1251 ( 68%)]  Loss:  3.591802 (3.2976)  Time: 1.092s,  937.34/s  (1.107s,  925.24/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [ 900/1251 ( 72%)]  Loss:  3.227194 (3.2939)  Time: 1.097s,  933.62/s  (1.107s,  925.16/s)  LR: 3.065e-04  Data: 0.012 (0.012)
Train: 129 [ 950/1251 ( 76%)]  Loss:  3.513442 (3.3048)  Time: 1.136s,  901.22/s  (1.107s,  924.73/s)  LR: 3.065e-04  Data: 0.012 (0.012)
Train: 129 [1000/1251 ( 80%)]  Loss:  3.272807 (3.3033)  Time: 1.101s,  930.39/s  (1.108s,  924.60/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [1050/1251 ( 84%)]  Loss:  3.392704 (3.3074)  Time: 1.096s,  934.67/s  (1.107s,  924.74/s)  LR: 3.065e-04  Data: 0.013 (0.012)
Train: 129 [1100/1251 ( 88%)]  Loss:  3.369210 (3.3101)  Time: 1.096s,  934.34/s  (1.107s,  924.89/s)  LR: 3.065e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 129 [1150/1251 ( 92%)]  Loss:  3.500801 (3.3180)  Time: 1.098s,  932.72/s  (1.107s,  925.07/s)  LR: 3.065e-04  Data: 0.014 (0.012)
Train: 129 [1200/1251 ( 96%)]  Loss:  3.362716 (3.3198)  Time: 1.102s,  929.00/s  (1.107s,  925.10/s)  LR: 3.065e-04  Data: 0.011 (0.012)
Train: 129 [1250/1251 (100%)]  Loss:  3.505093 (3.3269)  Time: 1.081s,  946.90/s  (1.107s,  924.85/s)  LR: 3.065e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.198 (3.198)  Loss:  0.4894 (0.4894)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.6335 (0.9910)  Acc@1: 86.4387 (77.5520)  Acc@5: 97.1698 (93.9940)
Test (EMA): [   0/48]  Time: 3.241 (3.241)  Loss:  0.4195 (0.4195)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  0.5636 (0.8833)  Acc@1: 85.8491 (79.3660)  Acc@5: 97.5236 (94.8800)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 79.01999995361328)

Train: 130 [   0/1251 (  0%)]  Loss:  3.148253 (3.1483)  Time: 1.132s,  904.49/s  (1.132s,  904.49/s)  LR: 3.040e-04  Data: 0.025 (0.025)
Train: 130 [  50/1251 (  4%)]  Loss:  3.452993 (3.3006)  Time: 1.099s,  932.17/s  (1.105s,  926.57/s)  LR: 3.040e-04  Data: 0.013 (0.012)
Train: 130 [ 100/1251 (  8%)]  Loss:  3.438443 (3.3466)  Time: 1.120s,  914.65/s  (1.108s,  924.45/s)  LR: 3.040e-04  Data: 0.010 (0.012)
Train: 130 [ 150/1251 ( 12%)]  Loss:  3.306033 (3.3364)  Time: 1.101s,  929.99/s  (1.111s,  922.07/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 200/1251 ( 16%)]  Loss:  3.236054 (3.3164)  Time: 1.098s,  932.51/s  (1.109s,  923.52/s)  LR: 3.040e-04  Data: 0.012 (0.012)
Train: 130 [ 250/1251 ( 20%)]  Loss:  3.416844 (3.3331)  Time: 1.102s,  928.95/s  (1.108s,  923.85/s)  LR: 3.040e-04  Data: 0.012 (0.012)
Train: 130 [ 300/1251 ( 24%)]  Loss:  3.055993 (3.2935)  Time: 1.093s,  937.20/s  (1.108s,  924.24/s)  LR: 3.040e-04  Data: 0.010 (0.012)
Train: 130 [ 350/1251 ( 28%)]  Loss:  2.903247 (3.2447)  Time: 1.098s,  932.24/s  (1.107s,  924.80/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 400/1251 ( 32%)]  Loss:  3.110612 (3.2298)  Time: 1.098s,  932.91/s  (1.106s,  925.54/s)  LR: 3.040e-04  Data: 0.012 (0.012)
Train: 130 [ 450/1251 ( 36%)]  Loss:  3.542740 (3.2611)  Time: 1.099s,  931.67/s  (1.106s,  925.64/s)  LR: 3.040e-04  Data: 0.013 (0.012)
Train: 130 [ 500/1251 ( 40%)]  Loss:  3.373892 (3.2714)  Time: 1.095s,  934.95/s  (1.106s,  925.94/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 550/1251 ( 44%)]  Loss:  3.092586 (3.2565)  Time: 1.099s,  931.88/s  (1.106s,  925.90/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 600/1251 ( 48%)]  Loss:  3.384764 (3.2663)  Time: 1.098s,  932.77/s  (1.106s,  926.12/s)  LR: 3.040e-04  Data: 0.012 (0.012)
Train: 130 [ 650/1251 ( 52%)]  Loss:  3.359210 (3.2730)  Time: 1.130s,  905.93/s  (1.107s,  925.42/s)  LR: 3.040e-04  Data: 0.013 (0.012)
Train: 130 [ 700/1251 ( 56%)]  Loss:  2.933116 (3.2503)  Time: 1.095s,  935.14/s  (1.107s,  924.75/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 750/1251 ( 60%)]  Loss:  3.338609 (3.2558)  Time: 1.098s,  932.71/s  (1.107s,  924.71/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 800/1251 ( 64%)]  Loss:  3.423417 (3.2657)  Time: 1.102s,  929.35/s  (1.108s,  924.59/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 850/1251 ( 68%)]  Loss:  3.395146 (3.2729)  Time: 1.121s,  913.87/s  (1.107s,  924.67/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 900/1251 ( 72%)]  Loss:  3.610991 (3.2907)  Time: 1.095s,  934.97/s  (1.108s,  924.55/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [ 950/1251 ( 76%)]  Loss:  3.437506 (3.2980)  Time: 1.093s,  937.14/s  (1.108s,  924.24/s)  LR: 3.040e-04  Data: 0.010 (0.012)
Train: 130 [1000/1251 ( 80%)]  Loss:  3.216239 (3.2941)  Time: 1.101s,  929.82/s  (1.108s,  924.32/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [1050/1251 ( 84%)]  Loss:  3.337875 (3.2961)  Time: 1.101s,  929.82/s  (1.108s,  924.48/s)  LR: 3.040e-04  Data: 0.012 (0.012)
Train: 130 [1100/1251 ( 88%)]  Loss:  3.344992 (3.2982)  Time: 1.099s,  932.00/s  (1.108s,  924.53/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [1150/1251 ( 92%)]  Loss:  3.499066 (3.3066)  Time: 1.093s,  936.89/s  (1.107s,  924.65/s)  LR: 3.040e-04  Data: 0.010 (0.012)
Train: 130 [1200/1251 ( 96%)]  Loss:  3.270592 (3.3052)  Time: 1.097s,  933.61/s  (1.108s,  924.42/s)  LR: 3.040e-04  Data: 0.011 (0.012)
Train: 130 [1250/1251 (100%)]  Loss:  3.336990 (3.3064)  Time: 1.103s,  928.02/s  (1.108s,  924.07/s)  LR: 3.040e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.273 (3.273)  Loss:  0.4928 (0.4928)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6232 (1.0029)  Acc@1: 85.8491 (77.2700)  Acc@5: 96.9340 (93.9140)
Test (EMA): [   0/48]  Time: 3.250 (3.250)  Loss:  0.4180 (0.4180)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5630 (0.8822)  Acc@1: 85.8491 (79.4420)  Acc@5: 97.4057 (94.8820)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 79.04599995361328)

Train: 131 [   0/1251 (  0%)]  Loss:  3.299901 (3.2999)  Time: 1.132s,  904.41/s  (1.132s,  904.41/s)  LR: 3.014e-04  Data: 0.023 (0.023)
Train: 131 [  50/1251 (  4%)]  Loss:  3.263257 (3.2816)  Time: 1.119s,  915.35/s  (1.122s,  912.34/s)  LR: 3.014e-04  Data: 0.010 (0.012)
Train: 131 [ 100/1251 (  8%)]  Loss:  3.714587 (3.4259)  Time: 1.096s,  934.23/s  (1.116s,  917.17/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 150/1251 ( 12%)]  Loss:  3.098129 (3.3440)  Time: 1.096s,  934.48/s  (1.112s,  920.52/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 200/1251 ( 16%)]  Loss:  3.312834 (3.3377)  Time: 1.129s,  907.14/s  (1.111s,  921.45/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [ 250/1251 ( 20%)]  Loss:  3.133444 (3.3037)  Time: 1.096s,  934.57/s  (1.114s,  919.24/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [ 300/1251 ( 24%)]  Loss:  3.282312 (3.3006)  Time: 1.100s,  930.63/s  (1.113s,  919.93/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [ 350/1251 ( 28%)]  Loss:  3.376132 (3.3101)  Time: 1.098s,  932.71/s  (1.112s,  920.64/s)  LR: 3.014e-04  Data: 0.014 (0.012)
Train: 131 [ 400/1251 ( 32%)]  Loss:  3.363209 (3.3160)  Time: 1.107s,  924.96/s  (1.112s,  920.74/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 450/1251 ( 36%)]  Loss:  3.394538 (3.3238)  Time: 1.095s,  934.75/s  (1.112s,  920.53/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [ 500/1251 ( 40%)]  Loss:  3.273201 (3.3192)  Time: 1.099s,  931.94/s  (1.111s,  921.45/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 550/1251 ( 44%)]  Loss:  3.399029 (3.3259)  Time: 1.102s,  929.12/s  (1.110s,  922.24/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 600/1251 ( 48%)]  Loss:  3.260036 (3.3208)  Time: 1.118s,  915.88/s  (1.110s,  922.59/s)  LR: 3.014e-04  Data: 0.013 (0.012)
Train: 131 [ 650/1251 ( 52%)]  Loss:  3.385919 (3.3255)  Time: 1.100s,  931.11/s  (1.110s,  922.54/s)  LR: 3.014e-04  Data: 0.010 (0.012)
Train: 131 [ 700/1251 ( 56%)]  Loss:  3.276309 (3.3222)  Time: 1.094s,  936.16/s  (1.110s,  922.71/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 131 [ 750/1251 ( 60%)]  Loss:  3.359697 (3.3245)  Time: 1.099s,  931.94/s  (1.110s,  922.88/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 800/1251 ( 64%)]  Loss:  3.531804 (3.3367)  Time: 1.100s,  931.33/s  (1.109s,  923.10/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 850/1251 ( 68%)]  Loss:  3.081567 (3.3226)  Time: 1.092s,  937.64/s  (1.109s,  923.35/s)  LR: 3.014e-04  Data: 0.010 (0.012)
Train: 131 [ 900/1251 ( 72%)]  Loss:  3.096629 (3.3107)  Time: 1.101s,  929.79/s  (1.109s,  923.57/s)  LR: 3.014e-04  Data: 0.012 (0.012)
Train: 131 [ 950/1251 ( 76%)]  Loss:  3.282564 (3.3093)  Time: 1.196s,  856.34/s  (1.109s,  923.68/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [1000/1251 ( 80%)]  Loss:  3.362480 (3.3118)  Time: 1.134s,  903.11/s  (1.109s,  923.65/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [1050/1251 ( 84%)]  Loss:  3.567420 (3.3234)  Time: 1.099s,  931.70/s  (1.109s,  923.57/s)  LR: 3.014e-04  Data: 0.010 (0.012)
Train: 131 [1100/1251 ( 88%)]  Loss:  2.501201 (3.2877)  Time: 1.095s,  935.07/s  (1.108s,  923.82/s)  LR: 3.014e-04  Data: 0.013 (0.012)
Train: 131 [1150/1251 ( 92%)]  Loss:  3.401550 (3.2924)  Time: 1.099s,  932.18/s  (1.109s,  923.55/s)  LR: 3.014e-04  Data: 0.011 (0.012)
Train: 131 [1200/1251 ( 96%)]  Loss:  3.347848 (3.2946)  Time: 1.098s,  932.44/s  (1.109s,  923.63/s)  LR: 3.014e-04  Data: 0.010 (0.012)
Train: 131 [1250/1251 (100%)]  Loss:  3.140420 (3.2887)  Time: 1.080s,  948.49/s  (1.109s,  923.44/s)  LR: 3.014e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.242 (3.242)  Loss:  0.4489 (0.4489)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6252 (0.9757)  Acc@1: 85.8491 (77.5800)  Acc@5: 97.1698 (94.0780)
Test (EMA): [   0/48]  Time: 3.043 (3.043)  Loss:  0.4165 (0.4165)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5614 (0.8811)  Acc@1: 85.8491 (79.4700)  Acc@5: 97.5236 (94.9260)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 79.07600000488281)

Train: 132 [   0/1251 (  0%)]  Loss:  3.477807 (3.4778)  Time: 1.111s,  922.03/s  (1.111s,  922.03/s)  LR: 2.989e-04  Data: 0.022 (0.022)
Train: 132 [  50/1251 (  4%)]  Loss:  3.262454 (3.3701)  Time: 1.100s,  931.09/s  (1.110s,  922.64/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 100/1251 (  8%)]  Loss:  3.209709 (3.3167)  Time: 1.100s,  931.31/s  (1.106s,  925.58/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 150/1251 ( 12%)]  Loss:  3.295595 (3.3114)  Time: 1.096s,  934.37/s  (1.106s,  925.89/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 200/1251 ( 16%)]  Loss:  2.932792 (3.2357)  Time: 1.103s,  928.75/s  (1.106s,  925.76/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 250/1251 ( 20%)]  Loss:  3.224389 (3.2338)  Time: 1.100s,  931.03/s  (1.106s,  925.70/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [ 300/1251 ( 24%)]  Loss:  3.446823 (3.2642)  Time: 1.109s,  923.12/s  (1.106s,  925.83/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [ 350/1251 ( 28%)]  Loss:  3.270369 (3.2650)  Time: 1.097s,  933.85/s  (1.107s,  925.26/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 400/1251 ( 32%)]  Loss:  3.135196 (3.2506)  Time: 1.122s,  912.55/s  (1.107s,  925.26/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 132 [ 450/1251 ( 36%)]  Loss:  3.178774 (3.2434)  Time: 1.114s,  918.90/s  (1.107s,  924.96/s)  LR: 2.989e-04  Data: 0.013 (0.012)
Train: 132 [ 500/1251 ( 40%)]  Loss:  3.162679 (3.2361)  Time: 1.127s,  908.91/s  (1.107s,  925.13/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 550/1251 ( 44%)]  Loss:  3.555313 (3.2627)  Time: 1.119s,  914.70/s  (1.108s,  924.30/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 132 [ 600/1251 ( 48%)]  Loss:  3.629263 (3.2909)  Time: 1.097s,  933.73/s  (1.109s,  923.55/s)  LR: 2.989e-04  Data: 0.013 (0.012)
Train: 132 [ 650/1251 ( 52%)]  Loss:  3.215129 (3.2854)  Time: 1.099s,  931.42/s  (1.108s,  923.88/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 700/1251 ( 56%)]  Loss:  3.419360 (3.2944)  Time: 1.098s,  932.77/s  (1.109s,  923.67/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [ 750/1251 ( 60%)]  Loss:  3.446177 (3.3039)  Time: 1.131s,  905.63/s  (1.108s,  924.00/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [ 800/1251 ( 64%)]  Loss:  3.018695 (3.2871)  Time: 1.100s,  931.00/s  (1.108s,  924.10/s)  LR: 2.989e-04  Data: 0.010 (0.012)
Train: 132 [ 850/1251 ( 68%)]  Loss:  3.113137 (3.2774)  Time: 1.098s,  933.02/s  (1.108s,  924.11/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [ 900/1251 ( 72%)]  Loss:  3.095559 (3.2679)  Time: 1.180s,  867.79/s  (1.108s,  924.16/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [ 950/1251 ( 76%)]  Loss:  3.310141 (3.2700)  Time: 1.098s,  932.94/s  (1.108s,  924.28/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 132 [1000/1251 ( 80%)]  Loss:  3.457047 (3.2789)  Time: 1.102s,  929.56/s  (1.108s,  924.34/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [1050/1251 ( 84%)]  Loss:  3.518694 (3.2898)  Time: 1.099s,  931.89/s  (1.108s,  924.40/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [1100/1251 ( 88%)]  Loss:  3.305431 (3.2905)  Time: 1.106s,  926.03/s  (1.107s,  924.64/s)  LR: 2.989e-04  Data: 0.012 (0.012)
Train: 132 [1150/1251 ( 92%)]  Loss:  3.239102 (3.2883)  Time: 1.132s,  904.91/s  (1.108s,  924.37/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [1200/1251 ( 96%)]  Loss:  3.259928 (3.2872)  Time: 1.105s,  926.56/s  (1.108s,  924.53/s)  LR: 2.989e-04  Data: 0.011 (0.012)
Train: 132 [1250/1251 (100%)]  Loss:  3.268709 (3.2865)  Time: 1.086s,  942.77/s  (1.108s,  924.34/s)  LR: 2.989e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.220 (3.220)  Loss:  0.4898 (0.4898)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.6299 (0.9964)  Acc@1: 85.8491 (77.2480)  Acc@5: 97.5236 (93.9860)
Test (EMA): [   0/48]  Time: 3.165 (3.165)  Loss:  0.4154 (0.4154)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.5607 (0.8797)  Acc@1: 85.6132 (79.4940)  Acc@5: 97.5236 (94.9300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 79.11200005615234)

Train: 133 [   0/1251 (  0%)]  Loss:  3.410176 (3.4102)  Time: 1.133s,  903.40/s  (1.133s,  903.40/s)  LR: 2.963e-04  Data: 0.026 (0.026)
Train: 133 [  50/1251 (  4%)]  Loss:  3.154776 (3.2825)  Time: 1.098s,  932.79/s  (1.118s,  915.57/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [ 100/1251 (  8%)]  Loss:  3.273597 (3.2795)  Time: 1.096s,  934.35/s  (1.113s,  919.66/s)  LR: 2.963e-04  Data: 0.010 (0.012)
Train: 133 [ 150/1251 ( 12%)]  Loss:  3.324310 (3.2907)  Time: 1.134s,  903.17/s  (1.110s,  922.11/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [ 200/1251 ( 16%)]  Loss:  3.246146 (3.2818)  Time: 1.096s,  934.01/s  (1.111s,  921.56/s)  LR: 2.963e-04  Data: 0.013 (0.012)
Train: 133 [ 250/1251 ( 20%)]  Loss:  3.567252 (3.3294)  Time: 1.098s,  932.78/s  (1.110s,  922.18/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [ 300/1251 ( 24%)]  Loss:  3.312629 (3.3270)  Time: 1.103s,  928.18/s  (1.110s,  922.22/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [ 350/1251 ( 28%)]  Loss:  3.494564 (3.3479)  Time: 1.096s,  934.66/s  (1.112s,  920.83/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [ 400/1251 ( 32%)]  Loss:  3.569282 (3.3725)  Time: 1.098s,  932.31/s  (1.111s,  921.97/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [ 450/1251 ( 36%)]  Loss:  3.307275 (3.3660)  Time: 1.100s,  930.79/s  (1.110s,  922.16/s)  LR: 2.963e-04  Data: 0.013 (0.012)
Train: 133 [ 500/1251 ( 40%)]  Loss:  3.404958 (3.3695)  Time: 1.097s,  933.85/s  (1.110s,  922.40/s)  LR: 2.963e-04  Data: 0.010 (0.012)
Train: 133 [ 550/1251 ( 44%)]  Loss:  3.390445 (3.3713)  Time: 1.101s,  930.39/s  (1.111s,  921.94/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [ 600/1251 ( 48%)]  Loss:  2.906388 (3.3355)  Time: 1.096s,  934.18/s  (1.111s,  922.08/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [ 650/1251 ( 52%)]  Loss:  3.556852 (3.3513)  Time: 1.102s,  928.95/s  (1.110s,  922.19/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [ 700/1251 ( 56%)]  Loss:  3.081397 (3.3333)  Time: 1.097s,  933.80/s  (1.110s,  922.52/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [ 750/1251 ( 60%)]  Loss:  3.488928 (3.3431)  Time: 1.100s,  931.13/s  (1.110s,  922.63/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [ 800/1251 ( 64%)]  Loss:  3.088018 (3.3281)  Time: 1.100s,  930.71/s  (1.110s,  922.80/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [ 850/1251 ( 68%)]  Loss:  3.228723 (3.3225)  Time: 1.111s,  921.35/s  (1.110s,  922.92/s)  LR: 2.963e-04  Data: 0.009 (0.012)
Train: 133 [ 900/1251 ( 72%)]  Loss:  3.286962 (3.3207)  Time: 1.099s,  931.72/s  (1.110s,  922.59/s)  LR: 2.963e-04  Data: 0.010 (0.012)
Train: 133 [ 950/1251 ( 76%)]  Loss:  3.288104 (3.3190)  Time: 1.095s,  935.01/s  (1.110s,  922.34/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [1000/1251 ( 80%)]  Loss:  3.532179 (3.3292)  Time: 1.111s,  921.93/s  (1.110s,  922.45/s)  LR: 2.963e-04  Data: 0.010 (0.012)
Train: 133 [1050/1251 ( 84%)]  Loss:  3.325904 (3.3290)  Time: 1.103s,  928.66/s  (1.110s,  922.78/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [1100/1251 ( 88%)]  Loss:  3.590934 (3.3404)  Time: 1.097s,  933.48/s  (1.110s,  922.91/s)  LR: 2.963e-04  Data: 0.011 (0.012)
Train: 133 [1150/1251 ( 92%)]  Loss:  3.243369 (3.3364)  Time: 1.095s,  934.94/s  (1.109s,  923.06/s)  LR: 2.963e-04  Data: 0.012 (0.012)
Train: 133 [1200/1251 ( 96%)]  Loss:  3.450261 (3.3409)  Time: 1.098s,  932.58/s  (1.109s,  923.21/s)  LR: 2.963e-04  Data: 0.013 (0.012)
Train: 133 [1250/1251 (100%)]  Loss:  3.199772 (3.3355)  Time: 1.080s,  948.01/s  (1.109s,  923.37/s)  LR: 2.963e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.171 (3.171)  Loss:  0.4837 (0.4837)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.6559 (0.9732)  Acc@1: 86.0849 (77.7240)  Acc@5: 97.1698 (94.0380)
Test (EMA): [   0/48]  Time: 3.236 (3.236)  Loss:  0.4144 (0.4144)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.5592 (0.8784)  Acc@1: 85.6132 (79.5140)  Acc@5: 97.5236 (94.9220)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 79.18200010742187)

Train: 134 [   0/1251 (  0%)]  Loss:  3.315297 (3.3153)  Time: 1.103s,  927.98/s  (1.103s,  927.98/s)  LR: 2.938e-04  Data: 0.020 (0.020)
Train: 134 [  50/1251 (  4%)]  Loss:  3.310899 (3.3131)  Time: 1.100s,  930.88/s  (1.106s,  925.50/s)  LR: 2.938e-04  Data: 0.013 (0.012)
Train: 134 [ 100/1251 (  8%)]  Loss:  3.157543 (3.2612)  Time: 1.097s,  933.68/s  (1.107s,  924.89/s)  LR: 2.938e-04  Data: 0.011 (0.012)
Train: 134 [ 150/1251 ( 12%)]  Loss:  3.203312 (3.2468)  Time: 1.098s,  932.33/s  (1.107s,  925.40/s)  LR: 2.938e-04  Data: 0.011 (0.012)
Train: 134 [ 200/1251 ( 16%)]  Loss:  3.056961 (3.2088)  Time: 1.098s,  932.78/s  (1.108s,  924.18/s)  LR: 2.938e-04  Data: 0.011 (0.011)
Train: 134 [ 250/1251 ( 20%)]  Loss:  3.369340 (3.2356)  Time: 1.097s,  933.81/s  (1.109s,  923.41/s)  LR: 2.938e-04  Data: 0.010 (0.012)
Train: 134 [ 300/1251 ( 24%)]  Loss:  3.572477 (3.2837)  Time: 1.095s,  935.48/s  (1.110s,  922.49/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 350/1251 ( 28%)]  Loss:  3.495112 (3.3101)  Time: 1.100s,  931.32/s  (1.110s,  922.66/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 400/1251 ( 32%)]  Loss:  3.263405 (3.3049)  Time: 1.095s,  934.81/s  (1.109s,  923.25/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 450/1251 ( 36%)]  Loss:  3.178043 (3.2922)  Time: 1.105s,  927.00/s  (1.108s,  923.99/s)  LR: 2.938e-04  Data: 0.010 (0.012)
Train: 134 [ 500/1251 ( 40%)]  Loss:  3.422566 (3.3041)  Time: 1.126s,  909.52/s  (1.108s,  923.93/s)  LR: 2.938e-04  Data: 0.013 (0.012)
Train: 134 [ 550/1251 ( 44%)]  Loss:  3.237262 (3.2985)  Time: 1.098s,  932.80/s  (1.108s,  924.38/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 600/1251 ( 48%)]  Loss:  3.616135 (3.3230)  Time: 1.097s,  933.82/s  (1.108s,  924.20/s)  LR: 2.938e-04  Data: 0.011 (0.012)
Train: 134 [ 650/1251 ( 52%)]  Loss:  3.444383 (3.3316)  Time: 1.130s,  906.42/s  (1.109s,  923.69/s)  LR: 2.938e-04  Data: 0.011 (0.012)
Train: 134 [ 700/1251 ( 56%)]  Loss:  3.675880 (3.3546)  Time: 1.099s,  932.11/s  (1.108s,  923.84/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 750/1251 ( 60%)]  Loss:  3.365273 (3.3552)  Time: 1.098s,  932.38/s  (1.108s,  924.32/s)  LR: 2.938e-04  Data: 0.014 (0.012)
Train: 134 [ 800/1251 ( 64%)]  Loss:  3.404647 (3.3581)  Time: 1.102s,  929.43/s  (1.108s,  924.27/s)  LR: 2.938e-04  Data: 0.011 (0.012)
Train: 134 [ 850/1251 ( 68%)]  Loss:  3.542103 (3.3684)  Time: 1.106s,  926.02/s  (1.108s,  924.11/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 900/1251 ( 72%)]  Loss:  2.932865 (3.3454)  Time: 1.099s,  931.92/s  (1.108s,  924.22/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [ 950/1251 ( 76%)]  Loss:  2.917658 (3.3241)  Time: 1.097s,  933.35/s  (1.108s,  923.91/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [1000/1251 ( 80%)]  Loss:  3.193868 (3.3179)  Time: 1.094s,  936.13/s  (1.109s,  923.57/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [1050/1251 ( 84%)]  Loss:  3.111933 (3.3085)  Time: 1.098s,  932.79/s  (1.109s,  923.70/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [1100/1251 ( 88%)]  Loss:  3.461436 (3.3151)  Time: 1.098s,  932.44/s  (1.108s,  923.95/s)  LR: 2.938e-04  Data: 0.012 (0.012)
Train: 134 [1150/1251 ( 92%)]  Loss:  3.097464 (3.3061)  Time: 1.092s,  937.49/s  (1.108s,  923.96/s)  LR: 2.938e-04  Data: 0.010 (0.012)
Train: 134 [1200/1251 ( 96%)]  Loss:  2.911748 (3.2903)  Time: 1.097s,  933.53/s  (1.108s,  924.24/s)  LR: 2.938e-04  Data: 0.011 (0.012)
Train: 134 [1250/1251 (100%)]  Loss:  3.254594 (3.2889)  Time: 1.082s,  946.64/s  (1.108s,  924.20/s)  LR: 2.938e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.241 (3.241)  Loss:  0.4936 (0.4936)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6317 (0.9721)  Acc@1: 85.1415 (77.7980)  Acc@5: 96.9340 (94.1080)
Test (EMA): [   0/48]  Time: 3.153 (3.153)  Loss:  0.4139 (0.4139)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5585 (0.8772)  Acc@1: 85.7311 (79.5040)  Acc@5: 97.7594 (94.9360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 79.21800018554687)

Train: 135 [   0/1251 (  0%)]  Loss:  3.168619 (3.1686)  Time: 1.107s,  925.29/s  (1.107s,  925.29/s)  LR: 2.912e-04  Data: 0.025 (0.025)
Train: 135 [  50/1251 (  4%)]  Loss:  3.269381 (3.2190)  Time: 1.104s,  927.88/s  (1.103s,  928.38/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [ 100/1251 (  8%)]  Loss:  3.357101 (3.2650)  Time: 1.096s,  934.24/s  (1.107s,  924.93/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [ 150/1251 ( 12%)]  Loss:  3.067639 (3.2157)  Time: 1.097s,  933.79/s  (1.105s,  926.46/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [ 200/1251 ( 16%)]  Loss:  3.319744 (3.2365)  Time: 1.096s,  934.62/s  (1.106s,  926.25/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [ 250/1251 ( 20%)]  Loss:  3.632850 (3.3026)  Time: 1.093s,  936.46/s  (1.106s,  926.05/s)  LR: 2.912e-04  Data: 0.010 (0.012)
Train: 135 [ 300/1251 ( 24%)]  Loss:  3.368977 (3.3120)  Time: 1.098s,  932.82/s  (1.106s,  926.14/s)  LR: 2.912e-04  Data: 0.013 (0.012)
Train: 135 [ 350/1251 ( 28%)]  Loss:  3.411195 (3.3244)  Time: 1.094s,  935.80/s  (1.105s,  926.44/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [ 400/1251 ( 32%)]  Loss:  3.179413 (3.3083)  Time: 1.096s,  934.40/s  (1.105s,  926.86/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [ 450/1251 ( 36%)]  Loss:  2.910968 (3.2686)  Time: 1.102s,  929.45/s  (1.105s,  926.33/s)  LR: 2.912e-04  Data: 0.014 (0.012)
Train: 135 [ 500/1251 ( 40%)]  Loss:  3.386795 (3.2793)  Time: 1.103s,  928.43/s  (1.106s,  925.97/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [ 550/1251 ( 44%)]  Loss:  3.267852 (3.2784)  Time: 1.114s,  919.21/s  (1.106s,  925.83/s)  LR: 2.912e-04  Data: 0.010 (0.012)
Train: 135 [ 600/1251 ( 48%)]  Loss:  3.243859 (3.2757)  Time: 1.097s,  933.60/s  (1.106s,  925.61/s)  LR: 2.912e-04  Data: 0.013 (0.012)
Train: 135 [ 650/1251 ( 52%)]  Loss:  3.185292 (3.2693)  Time: 1.193s,  858.31/s  (1.106s,  925.58/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [ 700/1251 ( 56%)]  Loss:  2.975151 (3.2497)  Time: 1.133s,  903.99/s  (1.107s,  924.99/s)  LR: 2.912e-04  Data: 0.014 (0.012)
Train: 135 [ 750/1251 ( 60%)]  Loss:  3.156329 (3.2438)  Time: 1.138s,  900.04/s  (1.107s,  924.68/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [ 800/1251 ( 64%)]  Loss:  3.109342 (3.2359)  Time: 1.099s,  931.47/s  (1.107s,  924.62/s)  LR: 2.912e-04  Data: 0.014 (0.012)
Train: 135 [ 850/1251 ( 68%)]  Loss:  3.321390 (3.2407)  Time: 1.101s,  929.83/s  (1.107s,  924.86/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [ 900/1251 ( 72%)]  Loss:  3.096915 (3.2331)  Time: 1.118s,  916.00/s  (1.107s,  924.99/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [ 950/1251 ( 76%)]  Loss:  3.524024 (3.2476)  Time: 1.095s,  934.78/s  (1.107s,  925.21/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [1000/1251 ( 80%)]  Loss:  3.353408 (3.2527)  Time: 1.103s,  928.23/s  (1.107s,  925.28/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [1050/1251 ( 84%)]  Loss:  3.460206 (3.2621)  Time: 1.123s,  911.48/s  (1.107s,  925.39/s)  LR: 2.912e-04  Data: 0.012 (0.012)
Train: 135 [1100/1251 ( 88%)]  Loss:  3.341988 (3.2656)  Time: 1.096s,  934.24/s  (1.107s,  924.91/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [1150/1251 ( 92%)]  Loss:  3.527422 (3.2765)  Time: 1.104s,  927.49/s  (1.107s,  925.10/s)  LR: 2.912e-04  Data: 0.010 (0.012)
Train: 135 [1200/1251 ( 96%)]  Loss:  3.307959 (3.2778)  Time: 1.096s,  934.24/s  (1.107s,  925.16/s)  LR: 2.912e-04  Data: 0.011 (0.012)
Train: 135 [1250/1251 (100%)]  Loss:  3.225752 (3.2758)  Time: 1.095s,  935.35/s  (1.107s,  925.20/s)  LR: 2.912e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.292 (3.292)  Loss:  0.4834 (0.4834)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6122 (0.9635)  Acc@1: 85.8491 (77.5920)  Acc@5: 96.6981 (93.9900)
Test (EMA): [   0/48]  Time: 3.099 (3.099)  Loss:  0.4141 (0.4141)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.400)  Loss:  0.5575 (0.8762)  Acc@1: 85.8491 (79.4760)  Acc@5: 97.5236 (94.9660)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 79.27800013427735)

Train: 136 [   0/1251 (  0%)]  Loss:  3.242906 (3.2429)  Time: 1.112s,  920.63/s  (1.112s,  920.63/s)  LR: 2.887e-04  Data: 0.022 (0.022)
Train: 136 [  50/1251 (  4%)]  Loss:  3.268930 (3.2559)  Time: 1.111s,  921.30/s  (1.109s,  923.01/s)  LR: 2.887e-04  Data: 0.017 (0.012)
Train: 136 [ 100/1251 (  8%)]  Loss:  2.882444 (3.1314)  Time: 1.099s,  931.50/s  (1.109s,  922.99/s)  LR: 2.887e-04  Data: 0.011 (0.012)
Train: 136 [ 150/1251 ( 12%)]  Loss:  3.290900 (3.1713)  Time: 1.098s,  932.60/s  (1.110s,  922.13/s)  LR: 2.887e-04  Data: 0.011 (0.012)
Train: 136 [ 200/1251 ( 16%)]  Loss:  3.440619 (3.2252)  Time: 1.099s,  932.17/s  (1.112s,  920.98/s)  LR: 2.887e-04  Data: 0.011 (0.012)
Train: 136 [ 250/1251 ( 20%)]  Loss:  3.350880 (3.2461)  Time: 1.192s,  858.83/s  (1.112s,  920.72/s)  LR: 2.887e-04  Data: 0.010 (0.011)
Train: 136 [ 300/1251 ( 24%)]  Loss:  3.334596 (3.2588)  Time: 1.100s,  931.07/s  (1.113s,  920.14/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 136 [ 350/1251 ( 28%)]  Loss:  3.332528 (3.2680)  Time: 1.097s,  933.60/s  (1.111s,  921.43/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Train: 136 [ 400/1251 ( 32%)]  Loss:  3.325182 (3.2743)  Time: 1.109s,  923.65/s  (1.111s,  921.29/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Train: 136 [ 450/1251 ( 36%)]  Loss:  3.346637 (3.2816)  Time: 1.096s,  934.62/s  (1.112s,  921.14/s)  LR: 2.887e-04  Data: 0.011 (0.012)
Train: 136 [ 500/1251 ( 40%)]  Loss:  3.461442 (3.2979)  Time: 1.092s,  938.00/s  (1.112s,  920.66/s)  LR: 2.887e-04  Data: 0.010 (0.011)
Train: 136 [ 550/1251 ( 44%)]  Loss:  3.509783 (3.3156)  Time: 1.100s,  930.51/s  (1.112s,  921.16/s)  LR: 2.887e-04  Data: 0.011 (0.011)
Train: 136 [ 600/1251 ( 48%)]  Loss:  3.443821 (3.3254)  Time: 1.100s,  931.22/s  (1.112s,  921.23/s)  LR: 2.887e-04  Data: 0.011 (0.011)
Train: 136 [ 650/1251 ( 52%)]  Loss:  3.162124 (3.3138)  Time: 1.132s,  904.60/s  (1.111s,  921.36/s)  LR: 2.887e-04  Data: 0.011 (0.011)
Train: 136 [ 700/1251 ( 56%)]  Loss:  3.473504 (3.3244)  Time: 1.097s,  933.28/s  (1.111s,  921.75/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Train: 136 [ 750/1251 ( 60%)]  Loss:  3.165328 (3.3145)  Time: 1.099s,  931.98/s  (1.111s,  922.09/s)  LR: 2.887e-04  Data: 0.011 (0.011)
Train: 136 [ 800/1251 ( 64%)]  Loss:  3.297028 (3.3135)  Time: 1.097s,  933.61/s  (1.110s,  922.45/s)  LR: 2.887e-04  Data: 0.013 (0.011)
Train: 136 [ 850/1251 ( 68%)]  Loss:  3.065286 (3.2997)  Time: 1.096s,  934.66/s  (1.110s,  922.83/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Train: 136 [ 900/1251 ( 72%)]  Loss:  3.174856 (3.2931)  Time: 1.098s,  932.26/s  (1.109s,  923.04/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Train: 136 [ 950/1251 ( 76%)]  Loss:  3.104955 (3.2837)  Time: 1.097s,  933.03/s  (1.109s,  923.19/s)  LR: 2.887e-04  Data: 0.012 (0.011)
Train: 136 [1000/1251 ( 80%)]  Loss:  3.515964 (3.2947)  Time: 1.099s,  931.62/s  (1.109s,  923.51/s)  LR: 2.887e-04  Data: 0.013 (0.011)
Train: 136 [1050/1251 ( 84%)]  Loss:  3.528751 (3.3054)  Time: 1.096s,  934.35/s  (1.109s,  923.56/s)  LR: 2.887e-04  Data: 0.012 (0.012)
Train: 136 [1100/1251 ( 88%)]  Loss:  3.158735 (3.2990)  Time: 1.122s,  912.70/s  (1.109s,  923.24/s)  LR: 2.887e-04  Data: 0.013 (0.012)
Train: 136 [1150/1251 ( 92%)]  Loss:  3.421609 (3.3041)  Time: 1.096s,  934.07/s  (1.109s,  923.40/s)  LR: 2.887e-04  Data: 0.012 (0.012)
Train: 136 [1200/1251 ( 96%)]  Loss:  3.209726 (3.3003)  Time: 1.096s,  934.27/s  (1.109s,  923.45/s)  LR: 2.887e-04  Data: 0.012 (0.012)
Train: 136 [1250/1251 (100%)]  Loss:  3.512468 (3.3085)  Time: 1.079s,  949.20/s  (1.109s,  923.52/s)  LR: 2.887e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.227 (3.227)  Loss:  0.4796 (0.4796)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.405)  Loss:  0.5918 (0.9869)  Acc@1: 86.3208 (77.5140)  Acc@5: 97.4057 (94.0300)
Test (EMA): [   0/48]  Time: 3.058 (3.058)  Loss:  0.4145 (0.4145)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.0469 (98.0469)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5577 (0.8751)  Acc@1: 85.9670 (79.4980)  Acc@5: 97.5236 (94.9760)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 79.31400008300781)

Train: 137 [   0/1251 (  0%)]  Loss:  3.214929 (3.2149)  Time: 1.104s,  927.72/s  (1.104s,  927.72/s)  LR: 2.861e-04  Data: 0.022 (0.022)
Train: 137 [  50/1251 (  4%)]  Loss:  3.331886 (3.2734)  Time: 1.097s,  933.61/s  (1.103s,  928.65/s)  LR: 2.861e-04  Data: 0.013 (0.012)
Train: 137 [ 100/1251 (  8%)]  Loss:  3.162995 (3.2366)  Time: 1.096s,  934.08/s  (1.105s,  926.28/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [ 150/1251 ( 12%)]  Loss:  3.376850 (3.2717)  Time: 1.097s,  933.37/s  (1.107s,  925.32/s)  LR: 2.861e-04  Data: 0.012 (0.012)
Train: 137 [ 200/1251 ( 16%)]  Loss:  3.076197 (3.2326)  Time: 1.102s,  929.62/s  (1.106s,  925.63/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [ 250/1251 ( 20%)]  Loss:  3.410025 (3.2621)  Time: 1.104s,  927.54/s  (1.106s,  925.58/s)  LR: 2.861e-04  Data: 0.013 (0.012)
Train: 137 [ 300/1251 ( 24%)]  Loss:  3.278359 (3.2645)  Time: 1.127s,  908.23/s  (1.106s,  926.00/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [ 350/1251 ( 28%)]  Loss:  3.339493 (3.2738)  Time: 1.095s,  935.51/s  (1.107s,  925.40/s)  LR: 2.861e-04  Data: 0.012 (0.012)
Train: 137 [ 400/1251 ( 32%)]  Loss:  3.350808 (3.2824)  Time: 1.098s,  932.90/s  (1.106s,  925.65/s)  LR: 2.861e-04  Data: 0.013 (0.012)
Train: 137 [ 450/1251 ( 36%)]  Loss:  3.150275 (3.2692)  Time: 1.099s,  931.34/s  (1.106s,  925.60/s)  LR: 2.861e-04  Data: 0.014 (0.012)
Train: 137 [ 500/1251 ( 40%)]  Loss:  3.456996 (3.2863)  Time: 1.102s,  929.25/s  (1.106s,  925.94/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [ 550/1251 ( 44%)]  Loss:  3.277032 (3.2855)  Time: 1.096s,  934.63/s  (1.106s,  925.94/s)  LR: 2.861e-04  Data: 0.012 (0.012)
Train: 137 [ 600/1251 ( 48%)]  Loss:  3.231210 (3.2813)  Time: 1.099s,  931.89/s  (1.106s,  926.21/s)  LR: 2.861e-04  Data: 0.012 (0.012)
Train: 137 [ 650/1251 ( 52%)]  Loss:  3.402785 (3.2900)  Time: 1.102s,  929.39/s  (1.106s,  926.01/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [ 700/1251 ( 56%)]  Loss:  3.703042 (3.3175)  Time: 1.097s,  933.43/s  (1.106s,  925.80/s)  LR: 2.861e-04  Data: 0.013 (0.012)
Train: 137 [ 750/1251 ( 60%)]  Loss:  3.152171 (3.3072)  Time: 1.103s,  928.13/s  (1.106s,  925.90/s)  LR: 2.861e-04  Data: 0.012 (0.012)
Train: 137 [ 800/1251 ( 64%)]  Loss:  3.305402 (3.3071)  Time: 1.099s,  931.38/s  (1.106s,  925.87/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [ 850/1251 ( 68%)]  Loss:  3.407477 (3.3127)  Time: 1.122s,  912.98/s  (1.106s,  925.92/s)  LR: 2.861e-04  Data: 0.012 (0.012)
Train: 137 [ 900/1251 ( 72%)]  Loss:  3.127116 (3.3029)  Time: 1.102s,  929.49/s  (1.106s,  925.51/s)  LR: 2.861e-04  Data: 0.015 (0.012)
Train: 137 [ 950/1251 ( 76%)]  Loss:  3.155782 (3.2955)  Time: 1.101s,  930.08/s  (1.106s,  925.74/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [1000/1251 ( 80%)]  Loss:  3.207364 (3.2913)  Time: 1.095s,  934.78/s  (1.106s,  925.78/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [1050/1251 ( 84%)]  Loss:  3.349390 (3.2940)  Time: 1.104s,  927.77/s  (1.106s,  925.89/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [1100/1251 ( 88%)]  Loss:  3.261361 (3.2926)  Time: 1.093s,  936.56/s  (1.106s,  925.75/s)  LR: 2.861e-04  Data: 0.010 (0.012)
Train: 137 [1150/1251 ( 92%)]  Loss:  3.228513 (3.2899)  Time: 1.104s,  927.14/s  (1.106s,  925.68/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 137 [1200/1251 ( 96%)]  Loss:  3.345275 (3.2921)  Time: 1.098s,  932.82/s  (1.106s,  925.56/s)  LR: 2.861e-04  Data: 0.011 (0.012)
Train: 137 [1250/1251 (100%)]  Loss:  3.415583 (3.2969)  Time: 1.080s,  948.58/s  (1.107s,  925.42/s)  LR: 2.861e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.363 (3.363)  Loss:  0.5005 (0.5005)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.6007 (0.9926)  Acc@1: 85.6132 (77.5540)  Acc@5: 97.4057 (94.0940)
Test (EMA): [   0/48]  Time: 3.190 (3.190)  Loss:  0.4143 (0.4143)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.5577 (0.8742)  Acc@1: 85.8491 (79.5740)  Acc@5: 97.5236 (94.9700)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 79.36600005859376)

Train: 138 [   0/1251 (  0%)]  Loss:  3.183842 (3.1838)  Time: 1.102s,  929.52/s  (1.102s,  929.52/s)  LR: 2.835e-04  Data: 0.020 (0.020)
Train: 138 [  50/1251 (  4%)]  Loss:  3.246992 (3.2154)  Time: 1.100s,  931.29/s  (1.113s,  920.35/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 100/1251 (  8%)]  Loss:  3.213522 (3.2148)  Time: 1.133s,  903.76/s  (1.110s,  922.66/s)  LR: 2.835e-04  Data: 0.011 (0.012)
Train: 138 [ 150/1251 ( 12%)]  Loss:  3.414406 (3.2647)  Time: 1.097s,  933.09/s  (1.110s,  922.76/s)  LR: 2.835e-04  Data: 0.011 (0.012)
Train: 138 [ 200/1251 ( 16%)]  Loss:  3.370225 (3.2858)  Time: 1.093s,  936.69/s  (1.109s,  923.13/s)  LR: 2.835e-04  Data: 0.010 (0.012)
Train: 138 [ 250/1251 ( 20%)]  Loss:  3.354991 (3.2973)  Time: 1.097s,  933.58/s  (1.108s,  924.49/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 300/1251 ( 24%)]  Loss:  3.147116 (3.2759)  Time: 1.096s,  933.94/s  (1.108s,  924.56/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 350/1251 ( 28%)]  Loss:  3.273625 (3.2756)  Time: 1.119s,  915.01/s  (1.108s,  924.59/s)  LR: 2.835e-04  Data: 0.010 (0.012)
Train: 138 [ 400/1251 ( 32%)]  Loss:  3.512919 (3.3020)  Time: 1.104s,  927.83/s  (1.108s,  923.96/s)  LR: 2.835e-04  Data: 0.017 (0.012)
Train: 138 [ 450/1251 ( 36%)]  Loss:  3.521364 (3.3239)  Time: 1.099s,  932.13/s  (1.107s,  924.79/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 500/1251 ( 40%)]  Loss:  2.947603 (3.2897)  Time: 1.097s,  933.35/s  (1.108s,  924.32/s)  LR: 2.835e-04  Data: 0.010 (0.012)
Train: 138 [ 550/1251 ( 44%)]  Loss:  2.836153 (3.2519)  Time: 1.100s,  931.32/s  (1.108s,  924.23/s)  LR: 2.835e-04  Data: 0.011 (0.012)
Train: 138 [ 600/1251 ( 48%)]  Loss:  3.213613 (3.2490)  Time: 1.116s,  917.77/s  (1.109s,  923.76/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 650/1251 ( 52%)]  Loss:  3.484165 (3.2658)  Time: 1.122s,  912.63/s  (1.110s,  922.87/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 700/1251 ( 56%)]  Loss:  3.444265 (3.2777)  Time: 1.095s,  935.30/s  (1.109s,  923.04/s)  LR: 2.835e-04  Data: 0.010 (0.012)
Train: 138 [ 750/1251 ( 60%)]  Loss:  3.375003 (3.2837)  Time: 1.159s,  883.75/s  (1.110s,  922.91/s)  LR: 2.835e-04  Data: 0.011 (0.012)
Train: 138 [ 800/1251 ( 64%)]  Loss:  3.492846 (3.2960)  Time: 1.095s,  934.91/s  (1.110s,  922.81/s)  LR: 2.835e-04  Data: 0.013 (0.012)
Train: 138 [ 850/1251 ( 68%)]  Loss:  3.062809 (3.2831)  Time: 1.097s,  933.14/s  (1.110s,  922.76/s)  LR: 2.835e-04  Data: 0.014 (0.012)
Train: 138 [ 900/1251 ( 72%)]  Loss:  3.317551 (3.2849)  Time: 1.234s,  829.59/s  (1.109s,  923.14/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [ 950/1251 ( 76%)]  Loss:  2.963173 (3.2688)  Time: 1.098s,  932.55/s  (1.109s,  923.20/s)  LR: 2.835e-04  Data: 0.014 (0.012)
Train: 138 [1000/1251 ( 80%)]  Loss:  3.295496 (3.2701)  Time: 1.099s,  931.39/s  (1.109s,  923.36/s)  LR: 2.835e-04  Data: 0.011 (0.012)
Train: 138 [1050/1251 ( 84%)]  Loss:  3.524876 (3.2817)  Time: 1.098s,  932.47/s  (1.109s,  923.26/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [1100/1251 ( 88%)]  Loss:  3.325289 (3.2836)  Time: 1.093s,  936.72/s  (1.109s,  923.58/s)  LR: 2.835e-04  Data: 0.012 (0.012)
Train: 138 [1150/1251 ( 92%)]  Loss:  3.419760 (3.2892)  Time: 1.097s,  933.04/s  (1.109s,  923.69/s)  LR: 2.835e-04  Data: 0.013 (0.012)
Train: 138 [1200/1251 ( 96%)]  Loss:  3.529263 (3.2988)  Time: 1.098s,  932.48/s  (1.109s,  923.57/s)  LR: 2.835e-04  Data: 0.014 (0.012)
Train: 138 [1250/1251 (100%)]  Loss:  3.245327 (3.2968)  Time: 1.080s,  947.95/s  (1.109s,  923.58/s)  LR: 2.835e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.309 (3.309)  Loss:  0.4988 (0.4988)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6061 (0.9811)  Acc@1: 86.2028 (77.7180)  Acc@5: 97.2877 (94.0000)
Test (EMA): [   0/48]  Time: 3.186 (3.186)  Loss:  0.4146 (0.4146)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5577 (0.8734)  Acc@1: 85.9670 (79.5860)  Acc@5: 97.5236 (95.0100)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 79.36600005859376)

Train: 139 [   0/1251 (  0%)]  Loss:  3.410928 (3.4109)  Time: 1.128s,  908.03/s  (1.128s,  908.03/s)  LR: 2.809e-04  Data: 0.019 (0.019)
Train: 139 [  50/1251 (  4%)]  Loss:  3.156079 (3.2835)  Time: 1.100s,  931.05/s  (1.117s,  917.13/s)  LR: 2.809e-04  Data: 0.010 (0.011)
Train: 139 [ 100/1251 (  8%)]  Loss:  3.197327 (3.2548)  Time: 1.097s,  933.75/s  (1.112s,  920.79/s)  LR: 2.809e-04  Data: 0.012 (0.011)
Train: 139 [ 150/1251 ( 12%)]  Loss:  3.091314 (3.2139)  Time: 1.129s,  907.31/s  (1.110s,  922.29/s)  LR: 2.809e-04  Data: 0.012 (0.011)
Train: 139 [ 200/1251 ( 16%)]  Loss:  3.302074 (3.2315)  Time: 1.093s,  936.68/s  (1.114s,  919.28/s)  LR: 2.809e-04  Data: 0.010 (0.011)
Train: 139 [ 250/1251 ( 20%)]  Loss:  3.409482 (3.2612)  Time: 1.096s,  934.29/s  (1.113s,  919.66/s)  LR: 2.809e-04  Data: 0.012 (0.011)
Train: 139 [ 300/1251 ( 24%)]  Loss:  3.392913 (3.2800)  Time: 1.096s,  934.38/s  (1.111s,  921.55/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [ 350/1251 ( 28%)]  Loss:  3.405303 (3.2957)  Time: 1.099s,  932.11/s  (1.110s,  922.13/s)  LR: 2.809e-04  Data: 0.013 (0.012)
Train: 139 [ 400/1251 ( 32%)]  Loss:  3.434805 (3.3111)  Time: 1.096s,  934.26/s  (1.111s,  922.06/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [ 450/1251 ( 36%)]  Loss:  3.159778 (3.2960)  Time: 1.104s,  927.93/s  (1.110s,  922.11/s)  LR: 2.809e-04  Data: 0.011 (0.011)
Train: 139 [ 500/1251 ( 40%)]  Loss:  3.365043 (3.3023)  Time: 1.095s,  935.45/s  (1.110s,  922.29/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [ 550/1251 ( 44%)]  Loss:  3.159879 (3.2904)  Time: 1.098s,  932.88/s  (1.110s,  922.17/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [ 600/1251 ( 48%)]  Loss:  3.116511 (3.2770)  Time: 1.098s,  932.32/s  (1.110s,  922.28/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [ 650/1251 ( 52%)]  Loss:  3.317273 (3.2799)  Time: 1.096s,  934.39/s  (1.110s,  922.87/s)  LR: 2.809e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 139 [ 700/1251 ( 56%)]  Loss:  3.235528 (3.2769)  Time: 1.096s,  934.63/s  (1.109s,  923.29/s)  LR: 2.809e-04  Data: 0.014 (0.012)
Train: 139 [ 750/1251 ( 60%)]  Loss:  3.308078 (3.2789)  Time: 1.100s,  930.89/s  (1.109s,  923.68/s)  LR: 2.809e-04  Data: 0.011 (0.012)
Train: 139 [ 800/1251 ( 64%)]  Loss:  2.814562 (3.2516)  Time: 1.120s,  914.10/s  (1.109s,  923.49/s)  LR: 2.809e-04  Data: 0.010 (0.012)
Train: 139 [ 850/1251 ( 68%)]  Loss:  3.311134 (3.2549)  Time: 1.218s,  840.68/s  (1.109s,  923.75/s)  LR: 2.809e-04  Data: 0.010 (0.012)
Train: 139 [ 900/1251 ( 72%)]  Loss:  3.360482 (3.2604)  Time: 1.126s,  909.16/s  (1.109s,  923.69/s)  LR: 2.809e-04  Data: 0.010 (0.012)
Train: 139 [ 950/1251 ( 76%)]  Loss:  3.183765 (3.2566)  Time: 1.097s,  933.62/s  (1.108s,  923.98/s)  LR: 2.809e-04  Data: 0.013 (0.012)
Train: 139 [1000/1251 ( 80%)]  Loss:  3.371476 (3.2621)  Time: 1.104s,  927.23/s  (1.108s,  923.98/s)  LR: 2.809e-04  Data: 0.018 (0.012)
Train: 139 [1050/1251 ( 84%)]  Loss:  3.438169 (3.2701)  Time: 1.113s,  920.34/s  (1.108s,  924.24/s)  LR: 2.809e-04  Data: 0.013 (0.012)
Train: 139 [1100/1251 ( 88%)]  Loss:  3.435175 (3.2773)  Time: 1.092s,  938.15/s  (1.108s,  924.11/s)  LR: 2.809e-04  Data: 0.010 (0.012)
Train: 139 [1150/1251 ( 92%)]  Loss:  3.236566 (3.2756)  Time: 1.096s,  934.01/s  (1.108s,  924.38/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [1200/1251 ( 96%)]  Loss:  3.144181 (3.2703)  Time: 1.095s,  935.20/s  (1.108s,  924.59/s)  LR: 2.809e-04  Data: 0.012 (0.012)
Train: 139 [1250/1251 (100%)]  Loss:  3.276309 (3.2705)  Time: 1.083s,  945.89/s  (1.108s,  924.38/s)  LR: 2.809e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.313 (3.313)  Loss:  0.4959 (0.4959)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.6058 (0.9750)  Acc@1: 85.4953 (77.7280)  Acc@5: 97.9953 (94.1400)
Test (EMA): [   0/48]  Time: 3.052 (3.052)  Loss:  0.4145 (0.4145)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  0.5572 (0.8722)  Acc@1: 85.7311 (79.5720)  Acc@5: 97.5236 (95.0040)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 79.44200005859375)

Train: 140 [   0/1251 (  0%)]  Loss:  3.361860 (3.3619)  Time: 1.119s,  915.28/s  (1.119s,  915.28/s)  LR: 2.784e-04  Data: 0.023 (0.023)
Train: 140 [  50/1251 (  4%)]  Loss:  3.190692 (3.2763)  Time: 1.131s,  905.39/s  (1.108s,  924.44/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 100/1251 (  8%)]  Loss:  3.089142 (3.2139)  Time: 1.100s,  931.21/s  (1.114s,  919.09/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 150/1251 ( 12%)]  Loss:  3.408263 (3.2625)  Time: 1.105s,  926.47/s  (1.111s,  921.85/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 200/1251 ( 16%)]  Loss:  3.183203 (3.2466)  Time: 1.096s,  933.97/s  (1.110s,  922.49/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 250/1251 ( 20%)]  Loss:  3.430247 (3.2772)  Time: 1.096s,  934.39/s  (1.110s,  922.76/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 300/1251 ( 24%)]  Loss:  3.316062 (3.2828)  Time: 1.098s,  932.23/s  (1.110s,  922.51/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 350/1251 ( 28%)]  Loss:  3.236624 (3.2770)  Time: 1.121s,  913.41/s  (1.110s,  922.17/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 400/1251 ( 32%)]  Loss:  3.177408 (3.2659)  Time: 1.098s,  932.68/s  (1.111s,  922.01/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 450/1251 ( 36%)]  Loss:  2.999248 (3.2393)  Time: 1.131s,  905.68/s  (1.111s,  921.76/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 500/1251 ( 40%)]  Loss:  3.257936 (3.2410)  Time: 1.098s,  932.42/s  (1.112s,  920.65/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 550/1251 ( 44%)]  Loss:  3.025068 (3.2230)  Time: 1.096s,  934.63/s  (1.113s,  920.43/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 600/1251 ( 48%)]  Loss:  3.231341 (3.2236)  Time: 1.131s,  905.69/s  (1.112s,  920.65/s)  LR: 2.784e-04  Data: 0.013 (0.012)
Train: 140 [ 650/1251 ( 52%)]  Loss:  3.587398 (3.2496)  Time: 1.130s,  905.85/s  (1.114s,  919.32/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 700/1251 ( 56%)]  Loss:  3.585921 (3.2720)  Time: 1.134s,  902.66/s  (1.114s,  919.59/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 750/1251 ( 60%)]  Loss:  3.261562 (3.2714)  Time: 1.099s,  931.80/s  (1.114s,  919.53/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 800/1251 ( 64%)]  Loss:  3.243839 (3.2698)  Time: 1.211s,  845.80/s  (1.113s,  920.17/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [ 850/1251 ( 68%)]  Loss:  3.404248 (3.2772)  Time: 1.100s,  931.21/s  (1.112s,  920.53/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [ 900/1251 ( 72%)]  Loss:  3.241106 (3.2753)  Time: 1.103s,  928.62/s  (1.112s,  920.86/s)  LR: 2.784e-04  Data: 0.013 (0.012)
Train: 140 [ 950/1251 ( 76%)]  Loss:  3.080700 (3.2656)  Time: 1.095s,  934.78/s  (1.112s,  920.67/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [1000/1251 ( 80%)]  Loss:  3.289222 (3.2667)  Time: 1.100s,  930.75/s  (1.112s,  921.06/s)  LR: 2.784e-04  Data: 0.015 (0.012)
Train: 140 [1050/1251 ( 84%)]  Loss:  3.250486 (3.2660)  Time: 1.104s,  927.52/s  (1.111s,  921.28/s)  LR: 2.784e-04  Data: 0.013 (0.012)
Train: 140 [1100/1251 ( 88%)]  Loss:  3.401376 (3.2719)  Time: 1.100s,  930.71/s  (1.111s,  921.58/s)  LR: 2.784e-04  Data: 0.012 (0.012)
Train: 140 [1150/1251 ( 92%)]  Loss:  3.592868 (3.2852)  Time: 1.107s,  925.10/s  (1.111s,  921.76/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [1200/1251 ( 96%)]  Loss:  3.582538 (3.2971)  Time: 1.098s,  932.83/s  (1.111s,  921.84/s)  LR: 2.784e-04  Data: 0.011 (0.012)
Train: 140 [1250/1251 (100%)]  Loss:  3.214894 (3.2940)  Time: 1.080s,  948.49/s  (1.111s,  922.07/s)  LR: 2.784e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.375 (3.375)  Loss:  0.4606 (0.4606)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6020 (0.9760)  Acc@1: 86.2028 (77.6680)  Acc@5: 97.1698 (94.1320)
Test (EMA): [   0/48]  Time: 3.109 (3.109)  Loss:  0.4145 (0.4145)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.1445 (98.1445)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5581 (0.8713)  Acc@1: 85.7311 (79.5740)  Acc@5: 97.5236 (95.0000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 79.47000005859375)

Train: 141 [   0/1251 (  0%)]  Loss:  3.184219 (3.1842)  Time: 1.154s,  887.73/s  (1.154s,  887.73/s)  LR: 2.758e-04  Data: 0.025 (0.025)
Train: 141 [  50/1251 (  4%)]  Loss:  3.102520 (3.1434)  Time: 1.097s,  933.50/s  (1.112s,  920.62/s)  LR: 2.758e-04  Data: 0.012 (0.012)
Train: 141 [ 100/1251 (  8%)]  Loss:  3.157824 (3.1482)  Time: 1.096s,  933.98/s  (1.106s,  925.89/s)  LR: 2.758e-04  Data: 0.010 (0.012)
Train: 141 [ 150/1251 ( 12%)]  Loss:  3.003042 (3.1119)  Time: 1.095s,  935.23/s  (1.110s,  922.72/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 141 [ 200/1251 ( 16%)]  Loss:  3.248744 (3.1393)  Time: 1.096s,  934.30/s  (1.110s,  922.17/s)  LR: 2.758e-04  Data: 0.013 (0.012)
Train: 141 [ 250/1251 ( 20%)]  Loss:  3.083558 (3.1300)  Time: 1.097s,  933.05/s  (1.111s,  921.41/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 300/1251 ( 24%)]  Loss:  3.149637 (3.1328)  Time: 1.097s,  933.72/s  (1.110s,  922.67/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 350/1251 ( 28%)]  Loss:  3.332727 (3.1578)  Time: 1.094s,  936.14/s  (1.109s,  923.18/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 400/1251 ( 32%)]  Loss:  3.086496 (3.1499)  Time: 1.102s,  929.43/s  (1.109s,  923.59/s)  LR: 2.758e-04  Data: 0.010 (0.012)
Train: 141 [ 450/1251 ( 36%)]  Loss:  3.191640 (3.1540)  Time: 1.098s,  932.99/s  (1.109s,  923.57/s)  LR: 2.758e-04  Data: 0.012 (0.012)
Train: 141 [ 500/1251 ( 40%)]  Loss:  3.356658 (3.1725)  Time: 1.095s,  934.89/s  (1.108s,  924.17/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 550/1251 ( 44%)]  Loss:  3.520778 (3.2015)  Time: 1.097s,  933.54/s  (1.108s,  923.78/s)  LR: 2.758e-04  Data: 0.012 (0.012)
Train: 141 [ 600/1251 ( 48%)]  Loss:  3.021430 (3.1876)  Time: 1.123s,  912.12/s  (1.109s,  923.34/s)  LR: 2.758e-04  Data: 0.010 (0.012)
Train: 141 [ 650/1251 ( 52%)]  Loss:  3.078681 (3.1799)  Time: 1.097s,  933.57/s  (1.109s,  923.30/s)  LR: 2.758e-04  Data: 0.013 (0.012)
Train: 141 [ 700/1251 ( 56%)]  Loss:  3.397017 (3.1943)  Time: 1.096s,  934.39/s  (1.109s,  923.60/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 750/1251 ( 60%)]  Loss:  3.393439 (3.2068)  Time: 1.210s,  846.55/s  (1.109s,  923.46/s)  LR: 2.758e-04  Data: 0.010 (0.012)
Train: 141 [ 800/1251 ( 64%)]  Loss:  3.395086 (3.2179)  Time: 1.099s,  931.35/s  (1.109s,  923.39/s)  LR: 2.758e-04  Data: 0.012 (0.012)
Train: 141 [ 850/1251 ( 68%)]  Loss:  3.346608 (3.2250)  Time: 1.113s,  919.77/s  (1.109s,  923.18/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 900/1251 ( 72%)]  Loss:  3.450402 (3.2369)  Time: 1.135s,  902.39/s  (1.109s,  923.13/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [ 950/1251 ( 76%)]  Loss:  3.320819 (3.2411)  Time: 1.095s,  935.26/s  (1.109s,  923.39/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [1000/1251 ( 80%)]  Loss:  3.311778 (3.2444)  Time: 1.098s,  932.31/s  (1.109s,  923.62/s)  LR: 2.758e-04  Data: 0.012 (0.012)
Train: 141 [1050/1251 ( 84%)]  Loss:  2.959981 (3.2315)  Time: 1.119s,  915.14/s  (1.108s,  923.81/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [1100/1251 ( 88%)]  Loss:  3.132373 (3.2272)  Time: 1.104s,  927.85/s  (1.109s,  923.58/s)  LR: 2.758e-04  Data: 0.010 (0.012)
Train: 141 [1150/1251 ( 92%)]  Loss:  3.573506 (3.2416)  Time: 1.105s,  926.61/s  (1.109s,  923.60/s)  LR: 2.758e-04  Data: 0.011 (0.012)
Train: 141 [1200/1251 ( 96%)]  Loss:  3.108561 (3.2363)  Time: 1.107s,  925.29/s  (1.108s,  923.80/s)  LR: 2.758e-04  Data: 0.012 (0.012)
Train: 141 [1250/1251 (100%)]  Loss:  3.545238 (3.2482)  Time: 1.080s,  948.07/s  (1.109s,  923.71/s)  LR: 2.758e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.223 (3.223)  Loss:  0.5393 (0.5393)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.6681 (1.0089)  Acc@1: 85.2594 (77.6640)  Acc@5: 97.0519 (94.1100)
Test (EMA): [   0/48]  Time: 3.247 (3.247)  Loss:  0.4145 (0.4145)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.1445 (98.1445)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5589 (0.8702)  Acc@1: 85.8491 (79.5840)  Acc@5: 97.5236 (95.0340)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 79.47599992919922)

Train: 142 [   0/1251 (  0%)]  Loss:  3.064248 (3.0642)  Time: 1.109s,  923.74/s  (1.109s,  923.74/s)  LR: 2.732e-04  Data: 0.021 (0.021)
Train: 142 [  50/1251 (  4%)]  Loss:  3.447795 (3.2560)  Time: 1.111s,  921.83/s  (1.108s,  923.88/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [ 100/1251 (  8%)]  Loss:  3.243602 (3.2519)  Time: 1.098s,  932.78/s  (1.109s,  923.05/s)  LR: 2.732e-04  Data: 0.012 (0.012)
Train: 142 [ 150/1251 ( 12%)]  Loss:  3.263798 (3.2549)  Time: 1.101s,  930.43/s  (1.106s,  926.21/s)  LR: 2.732e-04  Data: 0.012 (0.012)
Train: 142 [ 200/1251 ( 16%)]  Loss:  3.456863 (3.2953)  Time: 1.095s,  935.12/s  (1.107s,  924.73/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [ 250/1251 ( 20%)]  Loss:  3.077137 (3.2589)  Time: 1.099s,  931.93/s  (1.107s,  924.69/s)  LR: 2.732e-04  Data: 0.011 (0.011)
Train: 142 [ 300/1251 ( 24%)]  Loss:  3.513212 (3.2952)  Time: 1.101s,  930.06/s  (1.108s,  924.29/s)  LR: 2.732e-04  Data: 0.012 (0.012)
Train: 142 [ 350/1251 ( 28%)]  Loss:  3.423031 (3.3112)  Time: 1.098s,  932.46/s  (1.109s,  923.00/s)  LR: 2.732e-04  Data: 0.013 (0.012)
Train: 142 [ 400/1251 ( 32%)]  Loss:  3.333573 (3.3137)  Time: 1.096s,  934.59/s  (1.109s,  923.01/s)  LR: 2.732e-04  Data: 0.014 (0.012)
Train: 142 [ 450/1251 ( 36%)]  Loss:  3.365201 (3.3188)  Time: 1.098s,  932.98/s  (1.109s,  923.19/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [ 500/1251 ( 40%)]  Loss:  3.276260 (3.3150)  Time: 1.109s,  923.51/s  (1.109s,  923.11/s)  LR: 2.732e-04  Data: 0.012 (0.012)
Train: 142 [ 550/1251 ( 44%)]  Loss:  3.504712 (3.3308)  Time: 1.097s,  933.59/s  (1.109s,  923.30/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [ 600/1251 ( 48%)]  Loss:  3.466190 (3.3412)  Time: 1.109s,  923.09/s  (1.108s,  923.80/s)  LR: 2.732e-04  Data: 0.010 (0.012)
Train: 142 [ 650/1251 ( 52%)]  Loss:  3.301115 (3.3383)  Time: 1.130s,  905.96/s  (1.109s,  923.67/s)  LR: 2.732e-04  Data: 0.010 (0.012)
Train: 142 [ 700/1251 ( 56%)]  Loss:  3.513759 (3.3500)  Time: 1.210s,  846.53/s  (1.109s,  923.37/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [ 750/1251 ( 60%)]  Loss:  3.379733 (3.3519)  Time: 1.120s,  914.02/s  (1.109s,  923.18/s)  LR: 2.732e-04  Data: 0.010 (0.012)
Train: 142 [ 800/1251 ( 64%)]  Loss:  3.531539 (3.3625)  Time: 1.096s,  934.72/s  (1.110s,  922.92/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [ 850/1251 ( 68%)]  Loss:  3.057026 (3.3455)  Time: 1.105s,  926.84/s  (1.109s,  923.06/s)  LR: 2.732e-04  Data: 0.018 (0.012)
Train: 142 [ 900/1251 ( 72%)]  Loss:  3.121238 (3.3337)  Time: 1.108s,  924.24/s  (1.109s,  923.48/s)  LR: 2.732e-04  Data: 0.014 (0.012)
Train: 142 [ 950/1251 ( 76%)]  Loss:  2.973763 (3.3157)  Time: 1.097s,  933.66/s  (1.109s,  923.62/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 142 [1000/1251 ( 80%)]  Loss:  3.042693 (3.3027)  Time: 1.099s,  931.73/s  (1.108s,  923.91/s)  LR: 2.732e-04  Data: 0.012 (0.012)
Train: 142 [1050/1251 ( 84%)]  Loss:  3.013959 (3.2896)  Time: 1.098s,  932.64/s  (1.108s,  923.86/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [1100/1251 ( 88%)]  Loss:  3.355922 (3.2925)  Time: 1.096s,  934.52/s  (1.108s,  923.80/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [1150/1251 ( 92%)]  Loss:  3.329178 (3.2940)  Time: 1.105s,  926.85/s  (1.108s,  923.84/s)  LR: 2.732e-04  Data: 0.012 (0.012)
Train: 142 [1200/1251 ( 96%)]  Loss:  3.264399 (3.2928)  Time: 1.096s,  934.68/s  (1.109s,  923.60/s)  LR: 2.732e-04  Data: 0.011 (0.012)
Train: 142 [1250/1251 (100%)]  Loss:  3.352978 (3.2951)  Time: 1.087s,  942.41/s  (1.108s,  923.82/s)  LR: 2.732e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.237 (3.237)  Loss:  0.4670 (0.4670)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.5901 (0.9600)  Acc@1: 86.4387 (77.9300)  Acc@5: 97.0519 (94.0620)
Test (EMA): [   0/48]  Time: 3.210 (3.210)  Loss:  0.4139 (0.4139)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5597 (0.8691)  Acc@1: 86.3208 (79.6120)  Acc@5: 97.5236 (95.0640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 79.4939999560547)

Train: 143 [   0/1251 (  0%)]  Loss:  3.302987 (3.3030)  Time: 1.107s,  924.76/s  (1.107s,  924.76/s)  LR: 2.706e-04  Data: 0.024 (0.024)
Train: 143 [  50/1251 (  4%)]  Loss:  3.463121 (3.3831)  Time: 1.102s,  929.58/s  (1.112s,  920.63/s)  LR: 2.706e-04  Data: 0.011 (0.012)
Train: 143 [ 100/1251 (  8%)]  Loss:  2.860072 (3.2087)  Time: 1.098s,  932.23/s  (1.112s,  921.02/s)  LR: 2.706e-04  Data: 0.012 (0.012)
Train: 143 [ 150/1251 ( 12%)]  Loss:  3.123114 (3.1873)  Time: 1.109s,  923.01/s  (1.115s,  918.70/s)  LR: 2.706e-04  Data: 0.010 (0.012)
Train: 143 [ 200/1251 ( 16%)]  Loss:  3.234654 (3.1968)  Time: 1.098s,  933.00/s  (1.114s,  918.96/s)  LR: 2.706e-04  Data: 0.011 (0.011)
Train: 143 [ 250/1251 ( 20%)]  Loss:  3.233104 (3.2028)  Time: 1.193s,  858.38/s  (1.113s,  919.65/s)  LR: 2.706e-04  Data: 0.011 (0.011)
Train: 143 [ 300/1251 ( 24%)]  Loss:  3.409658 (3.2324)  Time: 1.100s,  931.23/s  (1.112s,  920.48/s)  LR: 2.706e-04  Data: 0.014 (0.011)
Train: 143 [ 350/1251 ( 28%)]  Loss:  3.328660 (3.2444)  Time: 1.096s,  934.61/s  (1.111s,  921.34/s)  LR: 2.706e-04  Data: 0.011 (0.011)
Train: 143 [ 400/1251 ( 32%)]  Loss:  3.286229 (3.2491)  Time: 1.096s,  934.70/s  (1.110s,  922.18/s)  LR: 2.706e-04  Data: 0.012 (0.011)
Train: 143 [ 450/1251 ( 36%)]  Loss:  3.314039 (3.2556)  Time: 1.096s,  934.19/s  (1.111s,  921.74/s)  LR: 2.706e-04  Data: 0.012 (0.011)
Train: 143 [ 500/1251 ( 40%)]  Loss:  3.580548 (3.2851)  Time: 1.099s,  932.09/s  (1.111s,  922.07/s)  LR: 2.706e-04  Data: 0.014 (0.011)
Train: 143 [ 550/1251 ( 44%)]  Loss:  3.532334 (3.3057)  Time: 1.116s,  917.45/s  (1.110s,  922.34/s)  LR: 2.706e-04  Data: 0.011 (0.011)
Train: 143 [ 600/1251 ( 48%)]  Loss:  3.054080 (3.2864)  Time: 1.096s,  934.31/s  (1.110s,  922.25/s)  LR: 2.706e-04  Data: 0.012 (0.011)
Train: 143 [ 650/1251 ( 52%)]  Loss:  3.111600 (3.2739)  Time: 1.122s,  912.35/s  (1.110s,  922.70/s)  LR: 2.706e-04  Data: 0.012 (0.011)
Train: 143 [ 700/1251 ( 56%)]  Loss:  3.265817 (3.2733)  Time: 1.099s,  931.87/s  (1.110s,  922.86/s)  LR: 2.706e-04  Data: 0.013 (0.012)
Train: 143 [ 750/1251 ( 60%)]  Loss:  3.269409 (3.2731)  Time: 1.096s,  934.10/s  (1.109s,  923.30/s)  LR: 2.706e-04  Data: 0.011 (0.012)
Train: 143 [ 800/1251 ( 64%)]  Loss:  3.388420 (3.2799)  Time: 1.194s,  857.81/s  (1.109s,  923.19/s)  LR: 2.706e-04  Data: 0.011 (0.012)
Train: 143 [ 850/1251 ( 68%)]  Loss:  3.422498 (3.2878)  Time: 1.097s,  933.81/s  (1.109s,  923.63/s)  LR: 2.706e-04  Data: 0.012 (0.012)
Train: 143 [ 900/1251 ( 72%)]  Loss:  3.208476 (3.2836)  Time: 1.099s,  931.71/s  (1.109s,  923.73/s)  LR: 2.706e-04  Data: 0.012 (0.012)
Train: 143 [ 950/1251 ( 76%)]  Loss:  3.221111 (3.2805)  Time: 1.097s,  933.85/s  (1.109s,  923.30/s)  LR: 2.706e-04  Data: 0.010 (0.012)
Train: 143 [1000/1251 ( 80%)]  Loss:  3.046365 (3.2693)  Time: 1.089s,  940.28/s  (1.109s,  923.53/s)  LR: 2.706e-04  Data: 0.010 (0.012)
Train: 143 [1050/1251 ( 84%)]  Loss:  3.332045 (3.2722)  Time: 1.096s,  934.14/s  (1.109s,  923.76/s)  LR: 2.706e-04  Data: 0.012 (0.012)
Train: 143 [1100/1251 ( 88%)]  Loss:  3.062129 (3.2631)  Time: 1.097s,  933.16/s  (1.109s,  923.75/s)  LR: 2.706e-04  Data: 0.012 (0.012)
Train: 143 [1150/1251 ( 92%)]  Loss:  3.547011 (3.2749)  Time: 1.095s,  935.04/s  (1.108s,  923.86/s)  LR: 2.706e-04  Data: 0.011 (0.012)
Train: 143 [1200/1251 ( 96%)]  Loss:  3.568958 (3.2867)  Time: 1.094s,  935.97/s  (1.108s,  924.03/s)  LR: 2.706e-04  Data: 0.011 (0.012)
Train: 143 [1250/1251 (100%)]  Loss:  3.295835 (3.2870)  Time: 1.080s,  948.31/s  (1.109s,  923.74/s)  LR: 2.706e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.249 (3.249)  Loss:  0.4370 (0.4370)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6136 (0.9575)  Acc@1: 85.7311 (77.8840)  Acc@5: 97.4057 (94.2160)
Test (EMA): [   0/48]  Time: 3.133 (3.133)  Loss:  0.4131 (0.4131)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.2422 (98.2422)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5604 (0.8683)  Acc@1: 86.2028 (79.6640)  Acc@5: 97.4057 (95.0960)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 79.49799998046875)

Train: 144 [   0/1251 (  0%)]  Loss:  3.358691 (3.3587)  Time: 1.104s,  927.61/s  (1.104s,  927.61/s)  LR: 2.680e-04  Data: 0.023 (0.023)
Train: 144 [  50/1251 (  4%)]  Loss:  3.029386 (3.1940)  Time: 1.193s,  858.33/s  (1.102s,  928.99/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [ 100/1251 (  8%)]  Loss:  3.071188 (3.1531)  Time: 1.101s,  930.23/s  (1.108s,  923.85/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 150/1251 ( 12%)]  Loss:  3.160376 (3.1549)  Time: 1.130s,  906.41/s  (1.107s,  925.14/s)  LR: 2.680e-04  Data: 0.010 (0.012)
Train: 144 [ 200/1251 ( 16%)]  Loss:  3.246130 (3.1732)  Time: 1.097s,  933.28/s  (1.107s,  924.69/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [ 250/1251 ( 20%)]  Loss:  2.984432 (3.1417)  Time: 1.136s,  901.06/s  (1.108s,  924.18/s)  LR: 2.680e-04  Data: 0.010 (0.012)
Train: 144 [ 300/1251 ( 24%)]  Loss:  3.245048 (3.1565)  Time: 1.096s,  934.09/s  (1.109s,  923.45/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 350/1251 ( 28%)]  Loss:  3.375453 (3.1838)  Time: 1.097s,  933.63/s  (1.109s,  923.46/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 400/1251 ( 32%)]  Loss:  3.313524 (3.1982)  Time: 1.098s,  932.81/s  (1.108s,  924.07/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [ 450/1251 ( 36%)]  Loss:  2.715722 (3.1500)  Time: 1.102s,  929.47/s  (1.108s,  924.07/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [ 500/1251 ( 40%)]  Loss:  3.236096 (3.1578)  Time: 1.102s,  929.18/s  (1.108s,  924.22/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 550/1251 ( 44%)]  Loss:  3.395259 (3.1776)  Time: 1.101s,  930.01/s  (1.108s,  924.16/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [ 600/1251 ( 48%)]  Loss:  3.335616 (3.1898)  Time: 1.105s,  927.08/s  (1.108s,  924.29/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 650/1251 ( 52%)]  Loss:  3.094889 (3.1830)  Time: 1.096s,  933.90/s  (1.108s,  924.29/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [ 700/1251 ( 56%)]  Loss:  3.537087 (3.2066)  Time: 1.097s,  933.04/s  (1.108s,  924.52/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 144 [ 750/1251 ( 60%)]  Loss:  3.292247 (3.2119)  Time: 1.093s,  936.79/s  (1.108s,  924.51/s)  LR: 2.680e-04  Data: 0.010 (0.012)
Train: 144 [ 800/1251 ( 64%)]  Loss:  3.323917 (3.2185)  Time: 1.131s,  905.75/s  (1.108s,  924.27/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 850/1251 ( 68%)]  Loss:  3.451539 (3.2315)  Time: 1.103s,  928.78/s  (1.108s,  924.30/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 900/1251 ( 72%)]  Loss:  3.447003 (3.2428)  Time: 1.097s,  933.49/s  (1.108s,  924.39/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [ 950/1251 ( 76%)]  Loss:  3.330604 (3.2472)  Time: 1.125s,  910.42/s  (1.109s,  923.69/s)  LR: 2.680e-04  Data: 0.010 (0.012)
Train: 144 [1000/1251 ( 80%)]  Loss:  3.051504 (3.2379)  Time: 1.134s,  902.77/s  (1.109s,  923.72/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [1050/1251 ( 84%)]  Loss:  3.406983 (3.2456)  Time: 1.094s,  935.98/s  (1.109s,  923.36/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [1100/1251 ( 88%)]  Loss:  3.469594 (3.2553)  Time: 1.104s,  927.80/s  (1.109s,  923.43/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [1150/1251 ( 92%)]  Loss:  3.448804 (3.2634)  Time: 1.103s,  928.39/s  (1.109s,  923.62/s)  LR: 2.680e-04  Data: 0.012 (0.012)
Train: 144 [1200/1251 ( 96%)]  Loss:  3.032737 (3.2542)  Time: 1.101s,  929.73/s  (1.109s,  923.31/s)  LR: 2.680e-04  Data: 0.011 (0.012)
Train: 144 [1250/1251 (100%)]  Loss:  3.159545 (3.2505)  Time: 1.084s,  944.40/s  (1.109s,  923.33/s)  LR: 2.680e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.298 (3.298)  Loss:  0.4817 (0.4817)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6196 (0.9970)  Acc@1: 86.4387 (77.6740)  Acc@5: 97.4057 (94.2000)
Test (EMA): [   0/48]  Time: 3.108 (3.108)  Loss:  0.4120 (0.4120)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5602 (0.8672)  Acc@1: 85.9670 (79.7040)  Acc@5: 97.5236 (95.0860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 79.50399987792969)

Train: 145 [   0/1251 (  0%)]  Loss:  3.172990 (3.1730)  Time: 1.101s,  930.40/s  (1.101s,  930.40/s)  LR: 2.655e-04  Data: 0.020 (0.020)
Train: 145 [  50/1251 (  4%)]  Loss:  3.374394 (3.2737)  Time: 1.096s,  934.03/s  (1.111s,  922.02/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [ 100/1251 (  8%)]  Loss:  2.935177 (3.1609)  Time: 1.112s,  920.97/s  (1.111s,  921.98/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 150/1251 ( 12%)]  Loss:  3.298837 (3.1953)  Time: 1.096s,  933.97/s  (1.109s,  923.00/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [ 200/1251 ( 16%)]  Loss:  3.195694 (3.1954)  Time: 1.122s,  912.33/s  (1.108s,  924.54/s)  LR: 2.655e-04  Data: 0.013 (0.012)
Train: 145 [ 250/1251 ( 20%)]  Loss:  3.205058 (3.1970)  Time: 1.095s,  935.00/s  (1.107s,  924.87/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 300/1251 ( 24%)]  Loss:  2.919841 (3.1574)  Time: 1.131s,  905.00/s  (1.108s,  924.48/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 350/1251 ( 28%)]  Loss:  3.442604 (3.1931)  Time: 1.106s,  925.94/s  (1.109s,  923.58/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 400/1251 ( 32%)]  Loss:  3.499111 (3.2271)  Time: 1.116s,  917.64/s  (1.108s,  923.90/s)  LR: 2.655e-04  Data: 0.010 (0.012)
Train: 145 [ 450/1251 ( 36%)]  Loss:  3.340462 (3.2384)  Time: 1.097s,  933.72/s  (1.108s,  924.21/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 500/1251 ( 40%)]  Loss:  3.121711 (3.2278)  Time: 1.126s,  909.65/s  (1.109s,  923.66/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [ 550/1251 ( 44%)]  Loss:  3.294431 (3.2334)  Time: 1.122s,  912.49/s  (1.108s,  923.93/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [ 600/1251 ( 48%)]  Loss:  3.185843 (3.2297)  Time: 1.098s,  932.92/s  (1.109s,  923.08/s)  LR: 2.655e-04  Data: 0.010 (0.012)
Train: 145 [ 650/1251 ( 52%)]  Loss:  3.122130 (3.2220)  Time: 1.189s,  861.45/s  (1.110s,  922.71/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 700/1251 ( 56%)]  Loss:  3.430671 (3.2359)  Time: 1.097s,  933.05/s  (1.110s,  922.85/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 750/1251 ( 60%)]  Loss:  3.054671 (3.2246)  Time: 1.101s,  930.47/s  (1.109s,  923.30/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [ 800/1251 ( 64%)]  Loss:  3.572308 (3.2451)  Time: 1.098s,  932.51/s  (1.109s,  923.64/s)  LR: 2.655e-04  Data: 0.010 (0.012)
Train: 145 [ 850/1251 ( 68%)]  Loss:  2.924908 (3.2273)  Time: 1.096s,  933.95/s  (1.108s,  923.80/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 900/1251 ( 72%)]  Loss:  3.330302 (3.2327)  Time: 1.097s,  933.87/s  (1.108s,  923.88/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [ 950/1251 ( 76%)]  Loss:  3.238834 (3.2330)  Time: 1.099s,  931.45/s  (1.108s,  923.86/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [1000/1251 ( 80%)]  Loss:  3.502575 (3.2458)  Time: 1.097s,  933.48/s  (1.109s,  923.68/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [1050/1251 ( 84%)]  Loss:  3.211448 (3.2443)  Time: 1.096s,  934.60/s  (1.108s,  923.90/s)  LR: 2.655e-04  Data: 0.012 (0.012)
Train: 145 [1100/1251 ( 88%)]  Loss:  3.264528 (3.2452)  Time: 1.124s,  910.76/s  (1.108s,  924.08/s)  LR: 2.655e-04  Data: 0.011 (0.012)
Train: 145 [1150/1251 ( 92%)]  Loss:  3.104502 (3.2393)  Time: 1.098s,  932.80/s  (1.108s,  924.21/s)  LR: 2.655e-04  Data: 0.013 (0.012)
Train: 145 [1200/1251 ( 96%)]  Loss:  3.115027 (3.2343)  Time: 1.098s,  932.91/s  (1.108s,  924.35/s)  LR: 2.655e-04  Data: 0.010 (0.012)
Train: 145 [1250/1251 (100%)]  Loss:  3.474731 (3.2436)  Time: 1.078s,  949.61/s  (1.108s,  924.17/s)  LR: 2.655e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.217 (3.217)  Loss:  0.4743 (0.4743)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.401)  Loss:  0.5588 (0.9639)  Acc@1: 87.3821 (77.8620)  Acc@5: 97.4057 (94.2220)
Test (EMA): [   0/48]  Time: 3.253 (3.253)  Loss:  0.4112 (0.4112)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5597 (0.8662)  Acc@1: 86.2028 (79.6920)  Acc@5: 97.5236 (95.0900)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 79.51399995605469)

Train: 146 [   0/1251 (  0%)]  Loss:  3.018407 (3.0184)  Time: 1.123s,  911.61/s  (1.123s,  911.61/s)  LR: 2.629e-04  Data: 0.025 (0.025)
Train: 146 [  50/1251 (  4%)]  Loss:  3.395735 (3.2071)  Time: 1.126s,  909.53/s  (1.108s,  923.85/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [ 100/1251 (  8%)]  Loss:  3.362807 (3.2590)  Time: 1.122s,  913.01/s  (1.111s,  921.86/s)  LR: 2.629e-04  Data: 0.010 (0.012)
Train: 146 [ 150/1251 ( 12%)]  Loss:  3.328974 (3.2765)  Time: 1.099s,  931.48/s  (1.108s,  924.28/s)  LR: 2.629e-04  Data: 0.010 (0.012)
Train: 146 [ 200/1251 ( 16%)]  Loss:  2.927505 (3.2067)  Time: 1.096s,  934.04/s  (1.107s,  925.34/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [ 250/1251 ( 20%)]  Loss:  3.197266 (3.2051)  Time: 1.101s,  929.80/s  (1.106s,  925.77/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [ 300/1251 ( 24%)]  Loss:  3.031655 (3.1803)  Time: 1.095s,  934.89/s  (1.107s,  924.86/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 146 [ 350/1251 ( 28%)]  Loss:  3.249300 (3.1890)  Time: 1.128s,  908.03/s  (1.107s,  925.24/s)  LR: 2.629e-04  Data: 0.013 (0.012)
Train: 146 [ 400/1251 ( 32%)]  Loss:  2.928992 (3.1601)  Time: 1.132s,  904.90/s  (1.107s,  925.37/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [ 450/1251 ( 36%)]  Loss:  3.293693 (3.1734)  Time: 1.098s,  932.66/s  (1.107s,  924.94/s)  LR: 2.629e-04  Data: 0.013 (0.012)
Train: 146 [ 500/1251 ( 40%)]  Loss:  3.055564 (3.1627)  Time: 1.128s,  907.43/s  (1.107s,  924.87/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [ 550/1251 ( 44%)]  Loss:  2.912971 (3.1419)  Time: 1.099s,  931.47/s  (1.108s,  924.39/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [ 600/1251 ( 48%)]  Loss:  3.029668 (3.1333)  Time: 1.097s,  933.15/s  (1.107s,  924.65/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [ 650/1251 ( 52%)]  Loss:  3.010260 (3.1245)  Time: 1.098s,  932.48/s  (1.108s,  924.42/s)  LR: 2.629e-04  Data: 0.014 (0.012)
Train: 146 [ 700/1251 ( 56%)]  Loss:  3.228849 (3.1314)  Time: 1.110s,  922.56/s  (1.107s,  924.63/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [ 750/1251 ( 60%)]  Loss:  3.246062 (3.1386)  Time: 1.130s,  906.43/s  (1.108s,  924.18/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [ 800/1251 ( 64%)]  Loss:  3.577786 (3.1644)  Time: 1.098s,  932.20/s  (1.108s,  924.19/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [ 850/1251 ( 68%)]  Loss:  2.954854 (3.1528)  Time: 1.098s,  932.48/s  (1.108s,  924.44/s)  LR: 2.629e-04  Data: 0.013 (0.012)
Train: 146 [ 900/1251 ( 72%)]  Loss:  3.588963 (3.1758)  Time: 1.093s,  936.49/s  (1.108s,  924.52/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [ 950/1251 ( 76%)]  Loss:  3.268386 (3.1804)  Time: 1.105s,  926.67/s  (1.108s,  924.31/s)  LR: 2.629e-04  Data: 0.010 (0.012)
Train: 146 [1000/1251 ( 80%)]  Loss:  3.251262 (3.1838)  Time: 1.098s,  932.35/s  (1.108s,  924.46/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [1050/1251 ( 84%)]  Loss:  3.206039 (3.1848)  Time: 1.113s,  920.26/s  (1.108s,  924.49/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [1100/1251 ( 88%)]  Loss:  3.527381 (3.1997)  Time: 1.110s,  922.19/s  (1.108s,  924.48/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [1150/1251 ( 92%)]  Loss:  3.502057 (3.2123)  Time: 1.098s,  932.93/s  (1.108s,  924.34/s)  LR: 2.629e-04  Data: 0.011 (0.012)
Train: 146 [1200/1251 ( 96%)]  Loss:  3.389452 (3.2194)  Time: 1.097s,  933.21/s  (1.108s,  924.32/s)  LR: 2.629e-04  Data: 0.012 (0.012)
Train: 146 [1250/1251 (100%)]  Loss:  3.138522 (3.2162)  Time: 1.080s,  948.13/s  (1.108s,  924.44/s)  LR: 2.629e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.249 (3.249)  Loss:  0.5143 (0.5143)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6485 (0.9747)  Acc@1: 85.9670 (77.8820)  Acc@5: 97.0519 (94.1900)
Test (EMA): [   0/48]  Time: 3.185 (3.185)  Loss:  0.4114 (0.4114)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.2422 (98.2422)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5588 (0.8652)  Acc@1: 86.3208 (79.7120)  Acc@5: 97.5236 (95.0940)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 79.57199987792968)

Train: 147 [   0/1251 (  0%)]  Loss:  3.276573 (3.2766)  Time: 1.107s,  924.86/s  (1.107s,  924.86/s)  LR: 2.603e-04  Data: 0.025 (0.025)
Train: 147 [  50/1251 (  4%)]  Loss:  3.258693 (3.2676)  Time: 1.100s,  930.98/s  (1.109s,  923.73/s)  LR: 2.603e-04  Data: 0.010 (0.012)
Train: 147 [ 100/1251 (  8%)]  Loss:  3.267496 (3.2676)  Time: 1.095s,  935.51/s  (1.108s,  924.39/s)  LR: 2.603e-04  Data: 0.011 (0.011)
Train: 147 [ 150/1251 ( 12%)]  Loss:  2.686144 (3.1222)  Time: 1.099s,  931.98/s  (1.108s,  924.46/s)  LR: 2.603e-04  Data: 0.012 (0.011)
Train: 147 [ 200/1251 ( 16%)]  Loss:  3.591200 (3.2160)  Time: 1.123s,  911.59/s  (1.108s,  924.52/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [ 250/1251 ( 20%)]  Loss:  2.836718 (3.1528)  Time: 1.097s,  933.46/s  (1.108s,  924.32/s)  LR: 2.603e-04  Data: 0.010 (0.012)
Train: 147 [ 300/1251 ( 24%)]  Loss:  2.937010 (3.1220)  Time: 1.129s,  906.79/s  (1.108s,  924.40/s)  LR: 2.603e-04  Data: 0.010 (0.011)
Train: 147 [ 350/1251 ( 28%)]  Loss:  3.320858 (3.1468)  Time: 1.099s,  931.47/s  (1.109s,  923.73/s)  LR: 2.603e-04  Data: 0.012 (0.011)
Train: 147 [ 400/1251 ( 32%)]  Loss:  2.873346 (3.1164)  Time: 1.134s,  902.78/s  (1.109s,  923.26/s)  LR: 2.603e-04  Data: 0.011 (0.011)
Train: 147 [ 450/1251 ( 36%)]  Loss:  3.554093 (3.1602)  Time: 1.095s,  934.91/s  (1.111s,  921.86/s)  LR: 2.603e-04  Data: 0.010 (0.011)
Train: 147 [ 500/1251 ( 40%)]  Loss:  3.049581 (3.1502)  Time: 1.095s,  935.23/s  (1.110s,  922.31/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [ 550/1251 ( 44%)]  Loss:  3.147092 (3.1499)  Time: 1.120s,  914.17/s  (1.110s,  922.74/s)  LR: 2.603e-04  Data: 0.011 (0.012)
Train: 147 [ 600/1251 ( 48%)]  Loss:  2.940486 (3.1338)  Time: 1.097s,  933.81/s  (1.110s,  922.87/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [ 650/1251 ( 52%)]  Loss:  3.514851 (3.1610)  Time: 1.095s,  934.83/s  (1.109s,  923.08/s)  LR: 2.603e-04  Data: 0.011 (0.012)
Train: 147 [ 700/1251 ( 56%)]  Loss:  3.268130 (3.1682)  Time: 1.091s,  938.25/s  (1.109s,  923.34/s)  LR: 2.603e-04  Data: 0.009 (0.012)
Train: 147 [ 750/1251 ( 60%)]  Loss:  3.163264 (3.1678)  Time: 1.099s,  932.17/s  (1.109s,  923.49/s)  LR: 2.603e-04  Data: 0.014 (0.012)
Train: 147 [ 800/1251 ( 64%)]  Loss:  3.243655 (3.1723)  Time: 1.099s,  931.65/s  (1.109s,  923.63/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [ 850/1251 ( 68%)]  Loss:  3.099948 (3.1683)  Time: 1.095s,  935.13/s  (1.108s,  923.85/s)  LR: 2.603e-04  Data: 0.010 (0.012)
Train: 147 [ 900/1251 ( 72%)]  Loss:  3.233103 (3.1717)  Time: 1.095s,  934.97/s  (1.108s,  924.14/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [ 950/1251 ( 76%)]  Loss:  3.230023 (3.1746)  Time: 1.100s,  931.28/s  (1.108s,  924.16/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [1000/1251 ( 80%)]  Loss:  3.210228 (3.1763)  Time: 1.100s,  930.96/s  (1.109s,  923.74/s)  LR: 2.603e-04  Data: 0.014 (0.012)
Train: 147 [1050/1251 ( 84%)]  Loss:  3.022862 (3.1693)  Time: 1.094s,  935.79/s  (1.108s,  923.79/s)  LR: 2.603e-04  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 147 [1100/1251 ( 88%)]  Loss:  3.053150 (3.1643)  Time: 1.096s,  934.52/s  (1.108s,  923.95/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [1150/1251 ( 92%)]  Loss:  3.521173 (3.1792)  Time: 1.098s,  932.41/s  (1.108s,  923.98/s)  LR: 2.603e-04  Data: 0.013 (0.012)
Train: 147 [1200/1251 ( 96%)]  Loss:  3.224205 (3.1810)  Time: 1.144s,  895.29/s  (1.108s,  924.21/s)  LR: 2.603e-04  Data: 0.012 (0.012)
Train: 147 [1250/1251 (100%)]  Loss:  3.142168 (3.1795)  Time: 1.088s,  941.51/s  (1.109s,  923.68/s)  LR: 2.603e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.396 (3.396)  Loss:  0.4634 (0.4634)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6091 (0.9580)  Acc@1: 86.3208 (78.0980)  Acc@5: 97.7594 (94.2700)
Test (EMA): [   0/48]  Time: 3.111 (3.111)  Loss:  0.4109 (0.4109)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.2422 (98.2422)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.5583 (0.8645)  Acc@1: 86.4387 (79.7620)  Acc@5: 97.4057 (95.1140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 79.5739998779297)

Train: 148 [   0/1251 (  0%)]  Loss:  3.323309 (3.3233)  Time: 1.100s,  930.69/s  (1.100s,  930.69/s)  LR: 2.577e-04  Data: 0.020 (0.020)
Train: 148 [  50/1251 (  4%)]  Loss:  3.330138 (3.3267)  Time: 1.096s,  934.38/s  (1.104s,  927.35/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [ 100/1251 (  8%)]  Loss:  3.245113 (3.2995)  Time: 1.103s,  928.07/s  (1.106s,  925.87/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [ 150/1251 ( 12%)]  Loss:  3.356664 (3.3138)  Time: 1.096s,  933.95/s  (1.106s,  925.73/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 200/1251 ( 16%)]  Loss:  3.046701 (3.2604)  Time: 1.102s,  929.48/s  (1.105s,  926.70/s)  LR: 2.577e-04  Data: 0.013 (0.012)
Train: 148 [ 250/1251 ( 20%)]  Loss:  2.972723 (3.2124)  Time: 1.135s,  902.35/s  (1.107s,  925.41/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 300/1251 ( 24%)]  Loss:  3.478950 (3.2505)  Time: 1.140s,  898.36/s  (1.107s,  925.04/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [ 350/1251 ( 28%)]  Loss:  3.216527 (3.2463)  Time: 1.102s,  929.51/s  (1.108s,  923.93/s)  LR: 2.577e-04  Data: 0.013 (0.012)
Train: 148 [ 400/1251 ( 32%)]  Loss:  3.404328 (3.2638)  Time: 1.097s,  933.61/s  (1.109s,  923.04/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 450/1251 ( 36%)]  Loss:  3.595840 (3.2970)  Time: 1.098s,  932.30/s  (1.109s,  923.08/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 500/1251 ( 40%)]  Loss:  2.997715 (3.2698)  Time: 1.098s,  932.88/s  (1.109s,  923.74/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [ 550/1251 ( 44%)]  Loss:  3.122158 (3.2575)  Time: 1.095s,  935.40/s  (1.108s,  924.19/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 600/1251 ( 48%)]  Loss:  2.950911 (3.2339)  Time: 1.119s,  915.35/s  (1.108s,  924.26/s)  LR: 2.577e-04  Data: 0.013 (0.012)
Train: 148 [ 650/1251 ( 52%)]  Loss:  3.271134 (3.2366)  Time: 1.106s,  926.22/s  (1.108s,  923.83/s)  LR: 2.577e-04  Data: 0.009 (0.012)
Train: 148 [ 700/1251 ( 56%)]  Loss:  3.414490 (3.2484)  Time: 1.121s,  913.82/s  (1.108s,  924.12/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [ 750/1251 ( 60%)]  Loss:  3.322290 (3.2531)  Time: 1.094s,  935.67/s  (1.108s,  923.87/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 800/1251 ( 64%)]  Loss:  2.931978 (3.2342)  Time: 1.102s,  929.03/s  (1.108s,  924.06/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [ 850/1251 ( 68%)]  Loss:  3.213978 (3.2331)  Time: 1.099s,  932.05/s  (1.108s,  924.40/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [ 900/1251 ( 72%)]  Loss:  3.199983 (3.2313)  Time: 1.093s,  936.59/s  (1.108s,  924.54/s)  LR: 2.577e-04  Data: 0.013 (0.012)
Train: 148 [ 950/1251 ( 76%)]  Loss:  3.324358 (3.2360)  Time: 1.099s,  931.93/s  (1.107s,  924.65/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [1000/1251 ( 80%)]  Loss:  3.169971 (3.2328)  Time: 1.132s,  904.25/s  (1.108s,  924.24/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [1050/1251 ( 84%)]  Loss:  3.482249 (3.2442)  Time: 1.097s,  933.72/s  (1.108s,  924.30/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [1100/1251 ( 88%)]  Loss:  3.153811 (3.2402)  Time: 1.097s,  933.12/s  (1.108s,  924.32/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [1150/1251 ( 92%)]  Loss:  3.379244 (3.2460)  Time: 1.106s,  926.23/s  (1.108s,  924.26/s)  LR: 2.577e-04  Data: 0.012 (0.012)
Train: 148 [1200/1251 ( 96%)]  Loss:  3.090850 (3.2398)  Time: 1.104s,  927.15/s  (1.108s,  924.14/s)  LR: 2.577e-04  Data: 0.011 (0.012)
Train: 148 [1250/1251 (100%)]  Loss:  3.098891 (3.2344)  Time: 1.080s,  948.57/s  (1.108s,  924.21/s)  LR: 2.577e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.275 (3.275)  Loss:  0.4534 (0.4534)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6206 (0.9521)  Acc@1: 86.4387 (78.1560)  Acc@5: 97.0519 (94.1780)
Test (EMA): [   0/48]  Time: 3.155 (3.155)  Loss:  0.4095 (0.4095)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5577 (0.8637)  Acc@1: 86.6745 (79.7700)  Acc@5: 97.4057 (95.1300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 79.57399992919922)

Train: 149 [   0/1251 (  0%)]  Loss:  3.222731 (3.2227)  Time: 1.104s,  927.70/s  (1.104s,  927.70/s)  LR: 2.551e-04  Data: 0.020 (0.020)
Train: 149 [  50/1251 (  4%)]  Loss:  2.853750 (3.0382)  Time: 1.097s,  933.25/s  (1.108s,  923.91/s)  LR: 2.551e-04  Data: 0.013 (0.012)
Train: 149 [ 100/1251 (  8%)]  Loss:  3.457267 (3.1779)  Time: 1.100s,  931.28/s  (1.110s,  922.26/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [ 150/1251 ( 12%)]  Loss:  2.810562 (3.0861)  Time: 1.095s,  935.02/s  (1.109s,  923.24/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [ 200/1251 ( 16%)]  Loss:  3.292239 (3.1273)  Time: 1.101s,  930.45/s  (1.108s,  924.10/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [ 250/1251 ( 20%)]  Loss:  3.365349 (3.1670)  Time: 1.097s,  933.05/s  (1.108s,  924.46/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [ 300/1251 ( 24%)]  Loss:  3.435764 (3.2054)  Time: 1.144s,  894.86/s  (1.109s,  923.72/s)  LR: 2.551e-04  Data: 0.010 (0.012)
Train: 149 [ 350/1251 ( 28%)]  Loss:  3.490409 (3.2410)  Time: 1.104s,  927.33/s  (1.108s,  924.38/s)  LR: 2.551e-04  Data: 0.010 (0.012)
Train: 149 [ 400/1251 ( 32%)]  Loss:  3.553372 (3.2757)  Time: 1.106s,  925.54/s  (1.107s,  924.73/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [ 450/1251 ( 36%)]  Loss:  3.443912 (3.2925)  Time: 1.130s,  905.92/s  (1.107s,  925.00/s)  LR: 2.551e-04  Data: 0.013 (0.012)
Train: 149 [ 500/1251 ( 40%)]  Loss:  3.200879 (3.2842)  Time: 1.097s,  933.26/s  (1.108s,  924.30/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [ 550/1251 ( 44%)]  Loss:  3.143431 (3.2725)  Time: 1.099s,  931.99/s  (1.108s,  924.30/s)  LR: 2.551e-04  Data: 0.016 (0.012)
Train: 149 [ 600/1251 ( 48%)]  Loss:  3.314351 (3.2757)  Time: 1.103s,  928.17/s  (1.108s,  924.23/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 149 [ 650/1251 ( 52%)]  Loss:  3.459595 (3.2888)  Time: 1.099s,  931.83/s  (1.108s,  924.33/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [ 700/1251 ( 56%)]  Loss:  3.188169 (3.2821)  Time: 1.099s,  931.34/s  (1.108s,  924.12/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [ 750/1251 ( 60%)]  Loss:  3.065866 (3.2686)  Time: 1.122s,  912.95/s  (1.108s,  924.07/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [ 800/1251 ( 64%)]  Loss:  3.352779 (3.2736)  Time: 1.194s,  857.29/s  (1.108s,  924.19/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [ 850/1251 ( 68%)]  Loss:  3.322759 (3.2763)  Time: 1.098s,  932.66/s  (1.108s,  924.29/s)  LR: 2.551e-04  Data: 0.013 (0.012)
Train: 149 [ 900/1251 ( 72%)]  Loss:  3.403128 (3.2830)  Time: 1.099s,  931.95/s  (1.108s,  924.57/s)  LR: 2.551e-04  Data: 0.010 (0.012)
Train: 149 [ 950/1251 ( 76%)]  Loss:  3.288678 (3.2832)  Time: 1.192s,  859.03/s  (1.108s,  924.37/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [1000/1251 ( 80%)]  Loss:  2.762305 (3.2584)  Time: 1.099s,  931.48/s  (1.107s,  924.66/s)  LR: 2.551e-04  Data: 0.017 (0.012)
Train: 149 [1050/1251 ( 84%)]  Loss:  3.330032 (3.2617)  Time: 1.098s,  932.45/s  (1.107s,  924.85/s)  LR: 2.551e-04  Data: 0.014 (0.012)
Train: 149 [1100/1251 ( 88%)]  Loss:  3.432739 (3.2691)  Time: 1.120s,  914.31/s  (1.107s,  924.85/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [1150/1251 ( 92%)]  Loss:  3.228169 (3.2674)  Time: 1.101s,  930.17/s  (1.108s,  924.55/s)  LR: 2.551e-04  Data: 0.011 (0.012)
Train: 149 [1200/1251 ( 96%)]  Loss:  3.438795 (3.2743)  Time: 1.098s,  932.57/s  (1.108s,  924.40/s)  LR: 2.551e-04  Data: 0.012 (0.012)
Train: 149 [1250/1251 (100%)]  Loss:  3.203452 (3.2716)  Time: 1.084s,  944.66/s  (1.108s,  924.43/s)  LR: 2.551e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.215 (3.215)  Loss:  0.4545 (0.4545)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.398)  Loss:  0.6061 (0.9575)  Acc@1: 86.0849 (77.9620)  Acc@5: 96.8160 (94.2720)
Test (EMA): [   0/48]  Time: 3.175 (3.175)  Loss:  0.4078 (0.4078)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5568 (0.8628)  Acc@1: 86.6745 (79.7840)  Acc@5: 97.4057 (95.1460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 79.58399992919922)

Train: 150 [   0/1251 (  0%)]  Loss:  3.194312 (3.1943)  Time: 1.104s,  927.67/s  (1.104s,  927.67/s)  LR: 2.525e-04  Data: 0.020 (0.020)
Train: 150 [  50/1251 (  4%)]  Loss:  3.269921 (3.2321)  Time: 1.189s,  861.34/s  (1.110s,  922.52/s)  LR: 2.525e-04  Data: 0.011 (0.012)
Train: 150 [ 100/1251 (  8%)]  Loss:  3.358472 (3.2742)  Time: 1.101s,  930.32/s  (1.111s,  921.45/s)  LR: 2.525e-04  Data: 0.011 (0.012)
Train: 150 [ 150/1251 ( 12%)]  Loss:  3.154453 (3.2443)  Time: 1.135s,  902.19/s  (1.112s,  920.53/s)  LR: 2.525e-04  Data: 0.012 (0.011)
Train: 150 [ 200/1251 ( 16%)]  Loss:  3.233898 (3.2422)  Time: 1.097s,  933.07/s  (1.112s,  920.57/s)  LR: 2.525e-04  Data: 0.012 (0.011)
Train: 150 [ 250/1251 ( 20%)]  Loss:  3.312006 (3.2538)  Time: 1.098s,  932.99/s  (1.112s,  921.16/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 300/1251 ( 24%)]  Loss:  3.238222 (3.2516)  Time: 1.097s,  933.17/s  (1.110s,  922.83/s)  LR: 2.525e-04  Data: 0.012 (0.011)
Train: 150 [ 350/1251 ( 28%)]  Loss:  3.227968 (3.2487)  Time: 1.097s,  933.13/s  (1.109s,  923.37/s)  LR: 2.525e-04  Data: 0.013 (0.011)
Train: 150 [ 400/1251 ( 32%)]  Loss:  3.177654 (3.2408)  Time: 1.096s,  933.99/s  (1.110s,  922.88/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 450/1251 ( 36%)]  Loss:  3.242857 (3.2410)  Time: 1.098s,  932.88/s  (1.110s,  922.63/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 500/1251 ( 40%)]  Loss:  3.186317 (3.2360)  Time: 1.105s,  926.68/s  (1.109s,  923.33/s)  LR: 2.525e-04  Data: 0.012 (0.011)
Train: 150 [ 550/1251 ( 44%)]  Loss:  3.283635 (3.2400)  Time: 1.100s,  931.13/s  (1.109s,  923.26/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 600/1251 ( 48%)]  Loss:  3.305491 (3.2450)  Time: 1.094s,  935.80/s  (1.109s,  923.30/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 650/1251 ( 52%)]  Loss:  3.355636 (3.2529)  Time: 1.108s,  924.13/s  (1.108s,  923.83/s)  LR: 2.525e-04  Data: 0.010 (0.011)
Train: 150 [ 700/1251 ( 56%)]  Loss:  2.716945 (3.2172)  Time: 1.099s,  931.57/s  (1.109s,  923.42/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 750/1251 ( 60%)]  Loss:  3.347414 (3.2253)  Time: 1.194s,  857.75/s  (1.109s,  923.57/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 800/1251 ( 64%)]  Loss:  3.168272 (3.2220)  Time: 1.097s,  933.23/s  (1.109s,  923.68/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 850/1251 ( 68%)]  Loss:  3.413180 (3.2326)  Time: 1.099s,  932.01/s  (1.108s,  923.91/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 900/1251 ( 72%)]  Loss:  3.367523 (3.2397)  Time: 1.097s,  933.62/s  (1.109s,  923.56/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [ 950/1251 ( 76%)]  Loss:  3.407451 (3.2481)  Time: 1.097s,  933.12/s  (1.108s,  923.98/s)  LR: 2.525e-04  Data: 0.012 (0.011)
Train: 150 [1000/1251 ( 80%)]  Loss:  3.162528 (3.2440)  Time: 1.123s,  911.76/s  (1.109s,  923.54/s)  LR: 2.525e-04  Data: 0.010 (0.011)
Train: 150 [1050/1251 ( 84%)]  Loss:  3.162186 (3.2403)  Time: 1.100s,  930.67/s  (1.109s,  923.76/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [1100/1251 ( 88%)]  Loss:  3.212800 (3.2391)  Time: 1.096s,  934.64/s  (1.108s,  923.83/s)  LR: 2.525e-04  Data: 0.011 (0.011)
Train: 150 [1150/1251 ( 92%)]  Loss:  2.993496 (3.2289)  Time: 1.101s,  930.46/s  (1.108s,  923.80/s)  LR: 2.525e-04  Data: 0.014 (0.011)
Train: 150 [1200/1251 ( 96%)]  Loss:  3.269555 (3.2305)  Time: 1.098s,  932.87/s  (1.108s,  923.95/s)  LR: 2.525e-04  Data: 0.012 (0.011)
Train: 150 [1250/1251 (100%)]  Loss:  3.315536 (3.2338)  Time: 1.081s,  947.54/s  (1.108s,  923.95/s)  LR: 2.525e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.299 (3.299)  Loss:  0.4619 (0.4619)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.5761 (0.9458)  Acc@1: 85.9670 (78.2460)  Acc@5: 97.7594 (94.3400)
Test (EMA): [   0/48]  Time: 3.148 (3.148)  Loss:  0.4075 (0.4075)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5564 (0.8618)  Acc@1: 87.1462 (79.8000)  Acc@5: 97.4057 (95.1500)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 79.58599998046876)

Train: 151 [   0/1251 (  0%)]  Loss:  3.057139 (3.0571)  Time: 1.101s,  929.65/s  (1.101s,  929.65/s)  LR: 2.499e-04  Data: 0.020 (0.020)
Train: 151 [  50/1251 (  4%)]  Loss:  3.326596 (3.1919)  Time: 1.101s,  929.67/s  (1.102s,  928.85/s)  LR: 2.499e-04  Data: 0.011 (0.012)
Train: 151 [ 100/1251 (  8%)]  Loss:  3.250902 (3.2115)  Time: 1.098s,  932.76/s  (1.105s,  926.32/s)  LR: 2.499e-04  Data: 0.012 (0.011)
Train: 151 [ 150/1251 ( 12%)]  Loss:  3.207739 (3.2106)  Time: 1.097s,  933.22/s  (1.107s,  925.31/s)  LR: 2.499e-04  Data: 0.013 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 151 [ 200/1251 ( 16%)]  Loss:  3.448185 (3.2581)  Time: 1.122s,  912.33/s  (1.107s,  924.81/s)  LR: 2.499e-04  Data: 0.012 (0.011)
Train: 151 [ 250/1251 ( 20%)]  Loss:  3.051625 (3.2237)  Time: 1.097s,  933.45/s  (1.109s,  923.73/s)  LR: 2.499e-04  Data: 0.013 (0.012)
Train: 151 [ 300/1251 ( 24%)]  Loss:  3.139797 (3.2117)  Time: 1.098s,  932.91/s  (1.109s,  923.08/s)  LR: 2.499e-04  Data: 0.013 (0.011)
Train: 151 [ 350/1251 ( 28%)]  Loss:  3.281332 (3.2204)  Time: 1.097s,  933.66/s  (1.110s,  922.48/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 400/1251 ( 32%)]  Loss:  3.439761 (3.2448)  Time: 1.096s,  934.03/s  (1.110s,  922.57/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 450/1251 ( 36%)]  Loss:  3.091626 (3.2295)  Time: 1.135s,  902.58/s  (1.110s,  922.70/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 500/1251 ( 40%)]  Loss:  3.143280 (3.2216)  Time: 1.100s,  931.14/s  (1.110s,  922.38/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 550/1251 ( 44%)]  Loss:  3.370403 (3.2340)  Time: 1.193s,  858.36/s  (1.110s,  922.58/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 600/1251 ( 48%)]  Loss:  3.201200 (3.2315)  Time: 1.094s,  935.80/s  (1.109s,  923.15/s)  LR: 2.499e-04  Data: 0.011 (0.012)
Train: 151 [ 650/1251 ( 52%)]  Loss:  3.422038 (3.2451)  Time: 1.105s,  926.98/s  (1.109s,  923.48/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 700/1251 ( 56%)]  Loss:  3.115361 (3.2365)  Time: 1.102s,  929.34/s  (1.109s,  923.22/s)  LR: 2.499e-04  Data: 0.012 (0.011)
Train: 151 [ 750/1251 ( 60%)]  Loss:  3.188294 (3.2335)  Time: 1.122s,  912.89/s  (1.109s,  923.27/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 800/1251 ( 64%)]  Loss:  2.881436 (3.2127)  Time: 1.099s,  932.09/s  (1.109s,  923.47/s)  LR: 2.499e-04  Data: 0.011 (0.011)
Train: 151 [ 850/1251 ( 68%)]  Loss:  3.318516 (3.2186)  Time: 1.097s,  933.48/s  (1.109s,  923.45/s)  LR: 2.499e-04  Data: 0.012 (0.011)
Train: 151 [ 900/1251 ( 72%)]  Loss:  3.387350 (3.2275)  Time: 1.097s,  933.16/s  (1.108s,  923.82/s)  LR: 2.499e-04  Data: 0.012 (0.012)
Train: 151 [ 950/1251 ( 76%)]  Loss:  3.274932 (3.2299)  Time: 1.096s,  934.05/s  (1.109s,  923.32/s)  LR: 2.499e-04  Data: 0.011 (0.012)
Train: 151 [1000/1251 ( 80%)]  Loss:  3.352632 (3.2357)  Time: 1.096s,  934.72/s  (1.109s,  923.59/s)  LR: 2.499e-04  Data: 0.012 (0.012)
Train: 151 [1050/1251 ( 84%)]  Loss:  3.279668 (3.2377)  Time: 1.098s,  932.91/s  (1.109s,  923.36/s)  LR: 2.499e-04  Data: 0.012 (0.011)
Train: 151 [1100/1251 ( 88%)]  Loss:  3.560392 (3.2517)  Time: 1.122s,  913.05/s  (1.109s,  923.36/s)  LR: 2.499e-04  Data: 0.010 (0.011)
Train: 151 [1150/1251 ( 92%)]  Loss:  2.887161 (3.2366)  Time: 1.095s,  934.74/s  (1.109s,  923.15/s)  LR: 2.499e-04  Data: 0.013 (0.011)
Train: 151 [1200/1251 ( 96%)]  Loss:  3.474095 (3.2461)  Time: 1.097s,  933.70/s  (1.109s,  923.31/s)  LR: 2.499e-04  Data: 0.012 (0.011)
Train: 151 [1250/1251 (100%)]  Loss:  3.179848 (3.2435)  Time: 1.081s,  947.65/s  (1.109s,  923.36/s)  LR: 2.499e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.338 (3.338)  Loss:  0.4441 (0.4441)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.412)  Loss:  0.6260 (0.9456)  Acc@1: 86.4387 (78.2320)  Acc@5: 97.1698 (94.3560)
Test (EMA): [   0/48]  Time: 3.116 (3.116)  Loss:  0.4064 (0.4064)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.5554 (0.8607)  Acc@1: 87.1462 (79.8600)  Acc@5: 97.4057 (95.1580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 79.61200013427734)

Train: 152 [   0/1251 (  0%)]  Loss:  3.432077 (3.4321)  Time: 1.103s,  928.11/s  (1.103s,  928.11/s)  LR: 2.473e-04  Data: 0.021 (0.021)
Train: 152 [  50/1251 (  4%)]  Loss:  3.249200 (3.3406)  Time: 1.095s,  934.79/s  (1.107s,  924.65/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 100/1251 (  8%)]  Loss:  3.374736 (3.3520)  Time: 1.103s,  928.59/s  (1.106s,  925.66/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [ 150/1251 ( 12%)]  Loss:  3.169803 (3.3065)  Time: 1.103s,  928.78/s  (1.110s,  922.25/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [ 200/1251 ( 16%)]  Loss:  3.307018 (3.3066)  Time: 1.098s,  932.58/s  (1.110s,  922.37/s)  LR: 2.473e-04  Data: 0.015 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 152 [ 250/1251 ( 20%)]  Loss:  3.243800 (3.2961)  Time: 1.097s,  933.20/s  (1.112s,  921.11/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 300/1251 ( 24%)]  Loss:  3.129178 (3.2723)  Time: 1.099s,  931.69/s  (1.110s,  922.40/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 350/1251 ( 28%)]  Loss:  3.131315 (3.2546)  Time: 1.097s,  933.26/s  (1.110s,  922.22/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 400/1251 ( 32%)]  Loss:  2.987824 (3.2250)  Time: 1.101s,  930.23/s  (1.110s,  922.50/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [ 450/1251 ( 36%)]  Loss:  3.294920 (3.2320)  Time: 1.097s,  933.12/s  (1.110s,  922.67/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 500/1251 ( 40%)]  Loss:  3.199318 (3.2290)  Time: 1.135s,  902.40/s  (1.110s,  922.88/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 550/1251 ( 44%)]  Loss:  3.217748 (3.2281)  Time: 1.134s,  903.25/s  (1.110s,  922.45/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 600/1251 ( 48%)]  Loss:  2.972872 (3.2084)  Time: 1.131s,  905.59/s  (1.110s,  922.22/s)  LR: 2.473e-04  Data: 0.013 (0.012)
Train: 152 [ 650/1251 ( 52%)]  Loss:  3.246675 (3.2112)  Time: 1.132s,  904.90/s  (1.111s,  921.78/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 700/1251 ( 56%)]  Loss:  3.238606 (3.2130)  Time: 1.104s,  927.46/s  (1.111s,  922.10/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 750/1251 ( 60%)]  Loss:  3.171090 (3.2104)  Time: 1.098s,  932.66/s  (1.110s,  922.37/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [ 800/1251 ( 64%)]  Loss:  3.418377 (3.2226)  Time: 1.097s,  933.38/s  (1.110s,  922.59/s)  LR: 2.473e-04  Data: 0.013 (0.012)
Train: 152 [ 850/1251 ( 68%)]  Loss:  3.169262 (3.2197)  Time: 1.097s,  933.83/s  (1.109s,  923.11/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 900/1251 ( 72%)]  Loss:  3.352753 (3.2267)  Time: 1.104s,  927.43/s  (1.109s,  923.18/s)  LR: 2.473e-04  Data: 0.011 (0.012)
Train: 152 [ 950/1251 ( 76%)]  Loss:  3.263788 (3.2285)  Time: 1.098s,  932.78/s  (1.109s,  923.52/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [1000/1251 ( 80%)]  Loss:  3.092191 (3.2220)  Time: 1.114s,  919.02/s  (1.109s,  923.76/s)  LR: 2.473e-04  Data: 0.010 (0.012)
Train: 152 [1050/1251 ( 84%)]  Loss:  3.204290 (3.2212)  Time: 1.095s,  935.33/s  (1.108s,  923.84/s)  LR: 2.473e-04  Data: 0.009 (0.012)
Train: 152 [1100/1251 ( 88%)]  Loss:  2.955700 (3.2097)  Time: 1.100s,  931.11/s  (1.108s,  924.00/s)  LR: 2.473e-04  Data: 0.010 (0.012)
Train: 152 [1150/1251 ( 92%)]  Loss:  3.268430 (3.2121)  Time: 1.100s,  930.80/s  (1.108s,  924.13/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [1200/1251 ( 96%)]  Loss:  3.037470 (3.2051)  Time: 1.095s,  935.18/s  (1.108s,  924.31/s)  LR: 2.473e-04  Data: 0.012 (0.012)
Train: 152 [1250/1251 (100%)]  Loss:  3.390737 (3.2123)  Time: 1.080s,  948.14/s  (1.108s,  924.15/s)  LR: 2.473e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.396 (3.396)  Loss:  0.4548 (0.4548)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6199 (0.9662)  Acc@1: 87.1462 (78.0740)  Acc@5: 97.5236 (94.3420)
Test (EMA): [   0/48]  Time: 3.121 (3.121)  Loss:  0.4055 (0.4055)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5559 (0.8600)  Acc@1: 87.1462 (79.9460)  Acc@5: 97.5236 (95.1760)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 79.6640000830078)

Train: 153 [   0/1251 (  0%)]  Loss:  3.222956 (3.2230)  Time: 1.104s,  927.55/s  (1.104s,  927.55/s)  LR: 2.447e-04  Data: 0.021 (0.021)
Train: 153 [  50/1251 (  4%)]  Loss:  3.417553 (3.3203)  Time: 1.103s,  928.25/s  (1.106s,  926.10/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [ 100/1251 (  8%)]  Loss:  3.463282 (3.3679)  Time: 1.097s,  933.68/s  (1.106s,  925.68/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [ 150/1251 ( 12%)]  Loss:  2.928707 (3.2581)  Time: 1.129s,  906.92/s  (1.108s,  924.28/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [ 200/1251 ( 16%)]  Loss:  3.154097 (3.2373)  Time: 1.099s,  931.42/s  (1.112s,  921.25/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [ 250/1251 ( 20%)]  Loss:  3.248237 (3.2391)  Time: 1.098s,  932.52/s  (1.110s,  922.42/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [ 300/1251 ( 24%)]  Loss:  3.222803 (3.2368)  Time: 1.190s,  860.46/s  (1.110s,  922.57/s)  LR: 2.447e-04  Data: 0.013 (0.012)
Train: 153 [ 350/1251 ( 28%)]  Loss:  3.288237 (3.2432)  Time: 1.100s,  930.87/s  (1.111s,  921.90/s)  LR: 2.447e-04  Data: 0.013 (0.012)
Train: 153 [ 400/1251 ( 32%)]  Loss:  2.857860 (3.2004)  Time: 1.131s,  905.36/s  (1.110s,  922.35/s)  LR: 2.447e-04  Data: 0.011 (0.012)
Train: 153 [ 450/1251 ( 36%)]  Loss:  3.340314 (3.2144)  Time: 1.121s,  913.61/s  (1.110s,  922.47/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [ 500/1251 ( 40%)]  Loss:  3.108603 (3.2048)  Time: 1.097s,  933.38/s  (1.110s,  922.37/s)  LR: 2.447e-04  Data: 0.011 (0.012)
Train: 153 [ 550/1251 ( 44%)]  Loss:  3.472014 (3.2271)  Time: 1.120s,  914.11/s  (1.110s,  922.56/s)  LR: 2.447e-04  Data: 0.011 (0.012)
Train: 153 [ 600/1251 ( 48%)]  Loss:  3.430651 (3.2427)  Time: 1.099s,  931.48/s  (1.110s,  922.90/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [ 650/1251 ( 52%)]  Loss:  3.243953 (3.2428)  Time: 1.095s,  934.78/s  (1.109s,  923.08/s)  LR: 2.447e-04  Data: 0.011 (0.012)
Train: 153 [ 700/1251 ( 56%)]  Loss:  3.367275 (3.2511)  Time: 1.099s,  931.71/s  (1.109s,  923.45/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [ 750/1251 ( 60%)]  Loss:  3.524263 (3.2682)  Time: 1.097s,  933.41/s  (1.109s,  923.64/s)  LR: 2.447e-04  Data: 0.013 (0.012)
Train: 153 [ 800/1251 ( 64%)]  Loss:  3.505159 (3.2821)  Time: 1.130s,  905.90/s  (1.108s,  923.89/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [ 850/1251 ( 68%)]  Loss:  3.305951 (3.2834)  Time: 1.221s,  838.93/s  (1.109s,  923.56/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [ 900/1251 ( 72%)]  Loss:  3.151613 (3.2765)  Time: 1.100s,  930.61/s  (1.108s,  923.95/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [ 950/1251 ( 76%)]  Loss:  3.435469 (3.2844)  Time: 1.099s,  931.41/s  (1.108s,  923.94/s)  LR: 2.447e-04  Data: 0.011 (0.012)
Train: 153 [1000/1251 ( 80%)]  Loss:  3.340228 (3.2871)  Time: 1.096s,  934.20/s  (1.108s,  924.01/s)  LR: 2.447e-04  Data: 0.010 (0.012)
Train: 153 [1050/1251 ( 84%)]  Loss:  3.025031 (3.2752)  Time: 1.095s,  934.76/s  (1.108s,  924.31/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [1100/1251 ( 88%)]  Loss:  3.146619 (3.2696)  Time: 1.133s,  904.11/s  (1.108s,  924.12/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [1150/1251 ( 92%)]  Loss:  3.331107 (3.2722)  Time: 1.095s,  935.24/s  (1.108s,  924.30/s)  LR: 2.447e-04  Data: 0.012 (0.012)
Train: 153 [1200/1251 ( 96%)]  Loss:  3.564277 (3.2839)  Time: 1.117s,  916.93/s  (1.108s,  924.10/s)  LR: 2.447e-04  Data: 0.009 (0.012)
Train: 153 [1250/1251 (100%)]  Loss:  3.352887 (3.2865)  Time: 1.079s,  948.85/s  (1.108s,  923.92/s)  LR: 2.447e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.207 (3.207)  Loss:  0.4974 (0.4974)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.6220 (0.9735)  Acc@1: 86.5566 (78.1760)  Acc@5: 97.6415 (94.3820)
Test (EMA): [   0/48]  Time: 3.251 (3.251)  Loss:  0.4045 (0.4045)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  0.5557 (0.8593)  Acc@1: 87.2641 (79.9600)  Acc@5: 97.5236 (95.1940)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 79.69199995361328)

Train: 154 [   0/1251 (  0%)]  Loss:  3.268808 (3.2688)  Time: 1.114s,  919.14/s  (1.114s,  919.14/s)  LR: 2.421e-04  Data: 0.022 (0.022)
Train: 154 [  50/1251 (  4%)]  Loss:  3.130955 (3.1999)  Time: 1.096s,  934.59/s  (1.105s,  926.81/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [ 100/1251 (  8%)]  Loss:  3.294759 (3.2315)  Time: 1.097s,  933.12/s  (1.104s,  927.46/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 150/1251 ( 12%)]  Loss:  3.092019 (3.1966)  Time: 1.122s,  912.90/s  (1.106s,  926.18/s)  LR: 2.421e-04  Data: 0.010 (0.012)
Train: 154 [ 200/1251 ( 16%)]  Loss:  3.210691 (3.1994)  Time: 1.099s,  931.89/s  (1.107s,  924.92/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [ 250/1251 ( 20%)]  Loss:  3.287279 (3.2141)  Time: 1.099s,  932.11/s  (1.107s,  924.97/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [ 300/1251 ( 24%)]  Loss:  3.068973 (3.1934)  Time: 1.098s,  933.03/s  (1.107s,  925.10/s)  LR: 2.421e-04  Data: 0.010 (0.012)
Train: 154 [ 350/1251 ( 28%)]  Loss:  3.095578 (3.1811)  Time: 1.133s,  903.96/s  (1.107s,  925.14/s)  LR: 2.421e-04  Data: 0.013 (0.012)
Train: 154 [ 400/1251 ( 32%)]  Loss:  3.058835 (3.1675)  Time: 1.098s,  932.90/s  (1.109s,  923.21/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [ 450/1251 ( 36%)]  Loss:  3.675069 (3.2183)  Time: 1.098s,  932.69/s  (1.110s,  922.87/s)  LR: 2.421e-04  Data: 0.013 (0.012)
Train: 154 [ 500/1251 ( 40%)]  Loss:  3.169251 (3.2138)  Time: 1.116s,  917.20/s  (1.109s,  923.15/s)  LR: 2.421e-04  Data: 0.010 (0.012)
Train: 154 [ 550/1251 ( 44%)]  Loss:  2.995181 (3.1956)  Time: 1.100s,  930.92/s  (1.109s,  923.64/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 600/1251 ( 48%)]  Loss:  3.032611 (3.1831)  Time: 1.095s,  935.37/s  (1.110s,  922.91/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 650/1251 ( 52%)]  Loss:  3.472238 (3.2037)  Time: 1.099s,  931.73/s  (1.109s,  923.15/s)  LR: 2.421e-04  Data: 0.013 (0.012)
Train: 154 [ 700/1251 ( 56%)]  Loss:  3.236381 (3.2059)  Time: 1.096s,  934.64/s  (1.109s,  923.28/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 750/1251 ( 60%)]  Loss:  3.449234 (3.2211)  Time: 1.133s,  903.87/s  (1.109s,  923.27/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 800/1251 ( 64%)]  Loss:  3.152995 (3.2171)  Time: 1.103s,  928.52/s  (1.109s,  923.18/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 850/1251 ( 68%)]  Loss:  3.258403 (3.2194)  Time: 1.102s,  929.12/s  (1.109s,  923.60/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [ 900/1251 ( 72%)]  Loss:  2.882174 (3.2017)  Time: 1.099s,  932.07/s  (1.109s,  923.57/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [ 950/1251 ( 76%)]  Loss:  3.340030 (3.2086)  Time: 1.104s,  927.65/s  (1.109s,  923.55/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [1000/1251 ( 80%)]  Loss:  3.318292 (3.2138)  Time: 1.097s,  933.16/s  (1.109s,  923.54/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [1050/1251 ( 84%)]  Loss:  3.417699 (3.2231)  Time: 1.097s,  933.76/s  (1.109s,  923.63/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [1100/1251 ( 88%)]  Loss:  3.360079 (3.2290)  Time: 1.095s,  935.00/s  (1.109s,  923.72/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [1150/1251 ( 92%)]  Loss:  3.218037 (3.2286)  Time: 1.098s,  932.53/s  (1.108s,  923.83/s)  LR: 2.421e-04  Data: 0.012 (0.012)
Train: 154 [1200/1251 ( 96%)]  Loss:  3.385845 (3.2349)  Time: 1.124s,  910.79/s  (1.108s,  923.89/s)  LR: 2.421e-04  Data: 0.011 (0.012)
Train: 154 [1250/1251 (100%)]  Loss:  3.140400 (3.2312)  Time: 1.082s,  946.58/s  (1.109s,  923.35/s)  LR: 2.421e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.229 (3.229)  Loss:  0.4875 (0.4875)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6531 (0.9696)  Acc@1: 85.6132 (78.1320)  Acc@5: 96.9340 (94.3380)
Test (EMA): [   0/48]  Time: 3.246 (3.246)  Loss:  0.4044 (0.4044)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  0.5547 (0.8586)  Acc@1: 87.0283 (79.9700)  Acc@5: 97.5236 (95.2140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 79.70399998046875)

Train: 155 [   0/1251 (  0%)]  Loss:  3.106603 (3.1066)  Time: 1.101s,  930.39/s  (1.101s,  930.39/s)  LR: 2.395e-04  Data: 0.020 (0.020)
Train: 155 [  50/1251 (  4%)]  Loss:  3.150827 (3.1287)  Time: 1.128s,  907.81/s  (1.115s,  918.13/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [ 100/1251 (  8%)]  Loss:  3.205854 (3.1544)  Time: 1.123s,  912.16/s  (1.113s,  919.80/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [ 150/1251 ( 12%)]  Loss:  3.232195 (3.1739)  Time: 1.111s,  921.60/s  (1.109s,  923.41/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 200/1251 ( 16%)]  Loss:  3.278654 (3.1948)  Time: 1.099s,  931.91/s  (1.109s,  923.24/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [ 250/1251 ( 20%)]  Loss:  3.471258 (3.2409)  Time: 1.095s,  934.75/s  (1.109s,  923.59/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 300/1251 ( 24%)]  Loss:  3.343118 (3.2555)  Time: 1.146s,  893.82/s  (1.109s,  923.76/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 350/1251 ( 28%)]  Loss:  3.047316 (3.2295)  Time: 1.098s,  932.33/s  (1.110s,  922.56/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [ 400/1251 ( 32%)]  Loss:  2.737056 (3.1748)  Time: 1.100s,  930.66/s  (1.109s,  923.26/s)  LR: 2.395e-04  Data: 0.010 (0.012)
Train: 155 [ 450/1251 ( 36%)]  Loss:  3.230369 (3.1803)  Time: 1.100s,  930.70/s  (1.109s,  923.76/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 500/1251 ( 40%)]  Loss:  3.416061 (3.2018)  Time: 1.096s,  934.67/s  (1.108s,  923.87/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 550/1251 ( 44%)]  Loss:  3.012131 (3.1860)  Time: 1.120s,  914.31/s  (1.109s,  923.28/s)  LR: 2.395e-04  Data: 0.010 (0.012)
Train: 155 [ 600/1251 ( 48%)]  Loss:  3.156874 (3.1837)  Time: 1.100s,  930.84/s  (1.109s,  923.08/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 155 [ 650/1251 ( 52%)]  Loss:  3.341038 (3.1950)  Time: 1.096s,  934.17/s  (1.110s,  922.85/s)  LR: 2.395e-04  Data: 0.013 (0.012)
Train: 155 [ 700/1251 ( 56%)]  Loss:  3.120800 (3.1900)  Time: 1.100s,  930.49/s  (1.110s,  922.77/s)  LR: 2.395e-04  Data: 0.010 (0.012)
Train: 155 [ 750/1251 ( 60%)]  Loss:  2.989636 (3.1775)  Time: 1.096s,  934.39/s  (1.110s,  922.26/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [ 800/1251 ( 64%)]  Loss:  3.321290 (3.1859)  Time: 1.097s,  933.36/s  (1.110s,  922.57/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [ 850/1251 ( 68%)]  Loss:  2.971082 (3.1740)  Time: 1.102s,  928.97/s  (1.110s,  922.24/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 900/1251 ( 72%)]  Loss:  2.899056 (3.1595)  Time: 1.099s,  931.95/s  (1.110s,  922.37/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [ 950/1251 ( 76%)]  Loss:  3.099208 (3.1565)  Time: 1.097s,  933.66/s  (1.110s,  922.64/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [1000/1251 ( 80%)]  Loss:  3.396685 (3.1680)  Time: 1.103s,  928.62/s  (1.110s,  922.85/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [1050/1251 ( 84%)]  Loss:  3.506949 (3.1834)  Time: 1.099s,  932.12/s  (1.110s,  922.79/s)  LR: 2.395e-04  Data: 0.013 (0.012)
Train: 155 [1100/1251 ( 88%)]  Loss:  3.090442 (3.1793)  Time: 1.091s,  938.36/s  (1.110s,  922.55/s)  LR: 2.395e-04  Data: 0.010 (0.012)
Train: 155 [1150/1251 ( 92%)]  Loss:  3.243353 (3.1820)  Time: 1.096s,  933.90/s  (1.110s,  922.90/s)  LR: 2.395e-04  Data: 0.012 (0.012)
Train: 155 [1200/1251 ( 96%)]  Loss:  3.348031 (3.1886)  Time: 1.104s,  927.88/s  (1.109s,  922.99/s)  LR: 2.395e-04  Data: 0.011 (0.012)
Train: 155 [1250/1251 (100%)]  Loss:  3.767081 (3.2109)  Time: 1.088s,  941.22/s  (1.109s,  923.18/s)  LR: 2.395e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.265 (3.265)  Loss:  0.4960 (0.4960)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6463 (0.9692)  Acc@1: 86.0849 (78.1140)  Acc@5: 96.6981 (94.3500)
Test (EMA): [   0/48]  Time: 3.144 (3.144)  Loss:  0.4042 (0.4042)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5542 (0.8577)  Acc@1: 86.7924 (79.9780)  Acc@5: 97.5236 (95.2180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 79.71200000488281)

Train: 156 [   0/1251 (  0%)]  Loss:  3.164210 (3.1642)  Time: 1.105s,  926.95/s  (1.105s,  926.95/s)  LR: 2.370e-04  Data: 0.022 (0.022)
Train: 156 [  50/1251 (  4%)]  Loss:  3.216273 (3.1902)  Time: 1.111s,  922.06/s  (1.114s,  918.85/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 100/1251 (  8%)]  Loss:  3.281877 (3.2208)  Time: 1.096s,  934.40/s  (1.111s,  921.64/s)  LR: 2.370e-04  Data: 0.014 (0.012)
Train: 156 [ 150/1251 ( 12%)]  Loss:  3.192362 (3.2137)  Time: 1.095s,  935.06/s  (1.112s,  920.86/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 200/1251 ( 16%)]  Loss:  3.394920 (3.2499)  Time: 1.098s,  932.50/s  (1.110s,  922.29/s)  LR: 2.370e-04  Data: 0.013 (0.012)
Train: 156 [ 250/1251 ( 20%)]  Loss:  3.377701 (3.2712)  Time: 1.101s,  930.41/s  (1.110s,  922.58/s)  LR: 2.370e-04  Data: 0.013 (0.012)
Train: 156 [ 300/1251 ( 24%)]  Loss:  3.268372 (3.2708)  Time: 1.119s,  915.02/s  (1.111s,  921.81/s)  LR: 2.370e-04  Data: 0.012 (0.012)
Train: 156 [ 350/1251 ( 28%)]  Loss:  3.041366 (3.2421)  Time: 1.095s,  935.04/s  (1.110s,  922.69/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 400/1251 ( 32%)]  Loss:  2.873276 (3.2012)  Time: 1.102s,  929.02/s  (1.109s,  923.60/s)  LR: 2.370e-04  Data: 0.012 (0.012)
Train: 156 [ 450/1251 ( 36%)]  Loss:  3.101601 (3.1912)  Time: 1.102s,  929.17/s  (1.108s,  924.16/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 500/1251 ( 40%)]  Loss:  3.244599 (3.1961)  Time: 1.090s,  939.85/s  (1.109s,  923.75/s)  LR: 2.370e-04  Data: 0.009 (0.012)
Train: 156 [ 550/1251 ( 44%)]  Loss:  3.193213 (3.1958)  Time: 1.103s,  928.76/s  (1.108s,  924.17/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 600/1251 ( 48%)]  Loss:  3.142496 (3.1917)  Time: 1.098s,  932.86/s  (1.108s,  924.04/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 650/1251 ( 52%)]  Loss:  3.326877 (3.2014)  Time: 1.098s,  932.90/s  (1.108s,  924.52/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 700/1251 ( 56%)]  Loss:  3.106160 (3.1950)  Time: 1.193s,  858.53/s  (1.108s,  924.52/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 750/1251 ( 60%)]  Loss:  3.191082 (3.1948)  Time: 1.132s,  904.69/s  (1.107s,  924.89/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 800/1251 ( 64%)]  Loss:  2.939915 (3.1798)  Time: 1.102s,  928.80/s  (1.107s,  924.64/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 850/1251 ( 68%)]  Loss:  3.111751 (3.1760)  Time: 1.098s,  932.64/s  (1.107s,  924.77/s)  LR: 2.370e-04  Data: 0.013 (0.012)
Train: 156 [ 900/1251 ( 72%)]  Loss:  3.215889 (3.1781)  Time: 1.135s,  902.09/s  (1.108s,  924.04/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [ 950/1251 ( 76%)]  Loss:  2.968669 (3.1676)  Time: 1.094s,  936.25/s  (1.108s,  924.18/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [1000/1251 ( 80%)]  Loss:  3.221119 (3.1702)  Time: 1.100s,  930.92/s  (1.108s,  924.36/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [1050/1251 ( 84%)]  Loss:  3.206908 (3.1718)  Time: 1.097s,  933.05/s  (1.108s,  924.39/s)  LR: 2.370e-04  Data: 0.014 (0.012)
Train: 156 [1100/1251 ( 88%)]  Loss:  3.062246 (3.1671)  Time: 1.101s,  930.28/s  (1.107s,  924.62/s)  LR: 2.370e-04  Data: 0.012 (0.012)
Train: 156 [1150/1251 ( 92%)]  Loss:  3.195186 (3.1683)  Time: 1.095s,  935.54/s  (1.108s,  924.60/s)  LR: 2.370e-04  Data: 0.012 (0.012)
Train: 156 [1200/1251 ( 96%)]  Loss:  2.727176 (3.1506)  Time: 1.097s,  933.74/s  (1.108s,  924.46/s)  LR: 2.370e-04  Data: 0.011 (0.012)
Train: 156 [1250/1251 (100%)]  Loss:  2.933746 (3.1423)  Time: 1.102s,  929.01/s  (1.108s,  924.24/s)  LR: 2.370e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.268 (3.268)  Loss:  0.5126 (0.5126)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6593 (0.9680)  Acc@1: 85.8491 (78.4100)  Acc@5: 97.4057 (94.4140)
Test (EMA): [   0/48]  Time: 3.105 (3.105)  Loss:  0.4053 (0.4053)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5530 (0.8569)  Acc@1: 87.0283 (79.9580)  Acc@5: 97.6415 (95.2240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 79.76200005615235)

Train: 157 [   0/1251 (  0%)]  Loss:  3.285699 (3.2857)  Time: 1.099s,  931.38/s  (1.099s,  931.38/s)  LR: 2.344e-04  Data: 0.020 (0.020)
Train: 157 [  50/1251 (  4%)]  Loss:  3.246858 (3.2663)  Time: 1.106s,  925.94/s  (1.113s,  919.75/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [ 100/1251 (  8%)]  Loss:  3.312689 (3.2817)  Time: 1.121s,  913.31/s  (1.110s,  922.41/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 150/1251 ( 12%)]  Loss:  3.063255 (3.2271)  Time: 1.105s,  926.57/s  (1.108s,  924.18/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 157 [ 200/1251 ( 16%)]  Loss:  3.210241 (3.2237)  Time: 1.096s,  934.69/s  (1.108s,  924.34/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [ 250/1251 ( 20%)]  Loss:  2.815349 (3.1557)  Time: 1.094s,  936.18/s  (1.107s,  924.71/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 300/1251 ( 24%)]  Loss:  3.426333 (3.1943)  Time: 1.097s,  933.62/s  (1.107s,  924.95/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 350/1251 ( 28%)]  Loss:  3.045177 (3.1757)  Time: 1.096s,  934.19/s  (1.108s,  924.45/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 400/1251 ( 32%)]  Loss:  3.213081 (3.1799)  Time: 1.096s,  933.90/s  (1.107s,  924.72/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [ 450/1251 ( 36%)]  Loss:  3.394378 (3.2013)  Time: 1.098s,  932.30/s  (1.108s,  924.25/s)  LR: 2.344e-04  Data: 0.015 (0.012)
Train: 157 [ 500/1251 ( 40%)]  Loss:  3.612105 (3.2387)  Time: 1.099s,  931.89/s  (1.108s,  924.57/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 550/1251 ( 44%)]  Loss:  3.506761 (3.2610)  Time: 1.094s,  935.95/s  (1.108s,  924.46/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 600/1251 ( 48%)]  Loss:  3.368894 (3.2693)  Time: 1.097s,  933.47/s  (1.107s,  924.82/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [ 650/1251 ( 52%)]  Loss:  3.062529 (3.2545)  Time: 1.098s,  932.54/s  (1.107s,  924.67/s)  LR: 2.344e-04  Data: 0.013 (0.012)
Train: 157 [ 700/1251 ( 56%)]  Loss:  3.100409 (3.2443)  Time: 1.118s,  916.29/s  (1.107s,  924.84/s)  LR: 2.344e-04  Data: 0.009 (0.012)
Train: 157 [ 750/1251 ( 60%)]  Loss:  3.272201 (3.2460)  Time: 1.099s,  931.43/s  (1.108s,  924.30/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [ 800/1251 ( 64%)]  Loss:  3.277105 (3.2478)  Time: 1.097s,  933.31/s  (1.108s,  924.52/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [ 850/1251 ( 68%)]  Loss:  3.134113 (3.2415)  Time: 1.097s,  933.36/s  (1.108s,  924.41/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [ 900/1251 ( 72%)]  Loss:  3.317517 (3.2455)  Time: 1.101s,  930.19/s  (1.108s,  924.29/s)  LR: 2.344e-04  Data: 0.010 (0.012)
Train: 157 [ 950/1251 ( 76%)]  Loss:  3.329709 (3.2497)  Time: 1.105s,  926.74/s  (1.108s,  924.46/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [1000/1251 ( 80%)]  Loss:  3.044310 (3.2399)  Time: 1.121s,  913.48/s  (1.108s,  924.43/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [1050/1251 ( 84%)]  Loss:  3.140616 (3.2354)  Time: 1.097s,  933.79/s  (1.107s,  924.61/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [1100/1251 ( 88%)]  Loss:  3.460246 (3.2452)  Time: 1.092s,  937.96/s  (1.108s,  924.47/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [1150/1251 ( 92%)]  Loss:  3.346779 (3.2494)  Time: 1.095s,  935.12/s  (1.108s,  924.55/s)  LR: 2.344e-04  Data: 0.011 (0.012)
Train: 157 [1200/1251 ( 96%)]  Loss:  2.895453 (3.2353)  Time: 1.098s,  932.61/s  (1.108s,  924.38/s)  LR: 2.344e-04  Data: 0.012 (0.012)
Train: 157 [1250/1251 (100%)]  Loss:  3.174730 (3.2329)  Time: 1.103s,  928.63/s  (1.108s,  924.20/s)  LR: 2.344e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.186 (3.186)  Loss:  0.5044 (0.5044)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.400)  Loss:  0.6400 (0.9673)  Acc@1: 86.2028 (78.5240)  Acc@5: 97.2877 (94.5160)
Test (EMA): [   0/48]  Time: 3.186 (3.186)  Loss:  0.4057 (0.4057)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.412)  Loss:  0.5531 (0.8561)  Acc@1: 87.1462 (79.9740)  Acc@5: 97.6415 (95.2480)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 79.77000002929688)

Train: 158 [   0/1251 (  0%)]  Loss:  3.283900 (3.2839)  Time: 1.130s,  906.17/s  (1.130s,  906.17/s)  LR: 2.318e-04  Data: 0.021 (0.021)
Train: 158 [  50/1251 (  4%)]  Loss:  3.132202 (3.2081)  Time: 1.226s,  834.96/s  (1.112s,  920.64/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 100/1251 (  8%)]  Loss:  2.975541 (3.1305)  Time: 1.098s,  932.75/s  (1.107s,  925.16/s)  LR: 2.318e-04  Data: 0.011 (0.011)
Train: 158 [ 150/1251 ( 12%)]  Loss:  3.306395 (3.1745)  Time: 1.096s,  933.95/s  (1.106s,  925.89/s)  LR: 2.318e-04  Data: 0.011 (0.011)
Train: 158 [ 200/1251 ( 16%)]  Loss:  3.467308 (3.2331)  Time: 1.126s,  909.45/s  (1.107s,  925.12/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 250/1251 ( 20%)]  Loss:  3.268292 (3.2389)  Time: 1.099s,  931.44/s  (1.108s,  924.27/s)  LR: 2.318e-04  Data: 0.012 (0.012)
Train: 158 [ 300/1251 ( 24%)]  Loss:  3.497238 (3.2758)  Time: 1.098s,  932.40/s  (1.108s,  924.54/s)  LR: 2.318e-04  Data: 0.012 (0.012)
Train: 158 [ 350/1251 ( 28%)]  Loss:  3.096675 (3.2534)  Time: 1.100s,  931.06/s  (1.108s,  924.47/s)  LR: 2.318e-04  Data: 0.012 (0.012)
Train: 158 [ 400/1251 ( 32%)]  Loss:  3.373546 (3.2668)  Time: 1.095s,  935.44/s  (1.110s,  922.68/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 450/1251 ( 36%)]  Loss:  3.263025 (3.2664)  Time: 1.128s,  908.12/s  (1.110s,  922.77/s)  LR: 2.318e-04  Data: 0.010 (0.012)
Train: 158 [ 500/1251 ( 40%)]  Loss:  3.325254 (3.2718)  Time: 1.098s,  932.41/s  (1.110s,  922.67/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 550/1251 ( 44%)]  Loss:  3.184191 (3.2645)  Time: 1.205s,  849.79/s  (1.109s,  923.03/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 600/1251 ( 48%)]  Loss:  3.276356 (3.2654)  Time: 1.103s,  928.51/s  (1.110s,  922.85/s)  LR: 2.318e-04  Data: 0.010 (0.012)
Train: 158 [ 650/1251 ( 52%)]  Loss:  3.467653 (3.2798)  Time: 1.099s,  932.12/s  (1.109s,  923.35/s)  LR: 2.318e-04  Data: 0.013 (0.012)
Train: 158 [ 700/1251 ( 56%)]  Loss:  3.022243 (3.2627)  Time: 1.100s,  930.63/s  (1.109s,  923.61/s)  LR: 2.318e-04  Data: 0.012 (0.012)
Train: 158 [ 750/1251 ( 60%)]  Loss:  3.415286 (3.2722)  Time: 1.097s,  933.32/s  (1.109s,  923.68/s)  LR: 2.318e-04  Data: 0.013 (0.012)
Train: 158 [ 800/1251 ( 64%)]  Loss:  3.158649 (3.2655)  Time: 1.097s,  933.75/s  (1.108s,  923.90/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 850/1251 ( 68%)]  Loss:  3.369910 (3.2713)  Time: 1.108s,  924.03/s  (1.108s,  923.87/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [ 900/1251 ( 72%)]  Loss:  2.931004 (3.2534)  Time: 1.096s,  934.57/s  (1.108s,  924.12/s)  LR: 2.318e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 158 [ 950/1251 ( 76%)]  Loss:  3.107189 (3.2461)  Time: 1.131s,  905.57/s  (1.109s,  923.52/s)  LR: 2.318e-04  Data: 0.014 (0.012)
Train: 158 [1000/1251 ( 80%)]  Loss:  3.195896 (3.2437)  Time: 1.131s,  905.09/s  (1.109s,  923.23/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [1050/1251 ( 84%)]  Loss:  3.110278 (3.2376)  Time: 1.103s,  928.52/s  (1.109s,  923.25/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [1100/1251 ( 88%)]  Loss:  3.218985 (3.2368)  Time: 1.106s,  926.27/s  (1.109s,  923.46/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [1150/1251 ( 92%)]  Loss:  3.123946 (3.2321)  Time: 1.104s,  927.54/s  (1.109s,  923.60/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [1200/1251 ( 96%)]  Loss:  3.395650 (3.2387)  Time: 1.099s,  932.06/s  (1.108s,  923.84/s)  LR: 2.318e-04  Data: 0.011 (0.012)
Train: 158 [1250/1251 (100%)]  Loss:  3.252425 (3.2392)  Time: 1.081s,  947.40/s  (1.108s,  923.81/s)  LR: 2.318e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.275 (3.275)  Loss:  0.4793 (0.4793)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6217 (0.9684)  Acc@1: 86.2028 (78.0860)  Acc@5: 97.2877 (94.3460)
Test (EMA): [   0/48]  Time: 3.351 (3.351)  Loss:  0.4059 (0.4059)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5534 (0.8551)  Acc@1: 87.2642 (79.9920)  Acc@5: 97.6415 (95.2840)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 79.78400002929688)

Train: 159 [   0/1251 (  0%)]  Loss:  3.387288 (3.3873)  Time: 1.103s,  928.01/s  (1.103s,  928.01/s)  LR: 2.292e-04  Data: 0.024 (0.024)
Train: 159 [  50/1251 (  4%)]  Loss:  3.343161 (3.3652)  Time: 1.107s,  924.73/s  (1.100s,  930.91/s)  LR: 2.292e-04  Data: 0.013 (0.012)
Train: 159 [ 100/1251 (  8%)]  Loss:  3.241701 (3.3241)  Time: 1.098s,  932.29/s  (1.105s,  926.38/s)  LR: 2.292e-04  Data: 0.013 (0.012)
Train: 159 [ 150/1251 ( 12%)]  Loss:  3.322424 (3.3236)  Time: 1.100s,  931.28/s  (1.107s,  924.86/s)  LR: 2.292e-04  Data: 0.011 (0.012)
Train: 159 [ 200/1251 ( 16%)]  Loss:  3.100857 (3.2791)  Time: 1.096s,  934.18/s  (1.107s,  924.98/s)  LR: 2.292e-04  Data: 0.012 (0.011)
Train: 159 [ 250/1251 ( 20%)]  Loss:  3.009983 (3.2342)  Time: 1.097s,  933.10/s  (1.107s,  924.86/s)  LR: 2.292e-04  Data: 0.013 (0.011)
Train: 159 [ 300/1251 ( 24%)]  Loss:  3.250468 (3.2366)  Time: 1.102s,  929.19/s  (1.106s,  925.83/s)  LR: 2.292e-04  Data: 0.011 (0.011)
Train: 159 [ 350/1251 ( 28%)]  Loss:  3.213943 (3.2337)  Time: 1.100s,  930.60/s  (1.106s,  925.79/s)  LR: 2.292e-04  Data: 0.010 (0.011)
Train: 159 [ 400/1251 ( 32%)]  Loss:  2.975636 (3.2051)  Time: 1.098s,  932.29/s  (1.106s,  925.65/s)  LR: 2.292e-04  Data: 0.012 (0.011)
Train: 159 [ 450/1251 ( 36%)]  Loss:  2.912004 (3.1757)  Time: 1.124s,  911.40/s  (1.107s,  925.09/s)  LR: 2.292e-04  Data: 0.013 (0.011)
Train: 159 [ 500/1251 ( 40%)]  Loss:  2.976491 (3.1576)  Time: 1.209s,  847.18/s  (1.107s,  924.73/s)  LR: 2.292e-04  Data: 0.013 (0.012)
Train: 159 [ 550/1251 ( 44%)]  Loss:  3.077266 (3.1509)  Time: 1.096s,  934.44/s  (1.108s,  923.99/s)  LR: 2.292e-04  Data: 0.012 (0.012)
Train: 159 [ 600/1251 ( 48%)]  Loss:  3.276688 (3.1606)  Time: 1.101s,  930.13/s  (1.108s,  924.51/s)  LR: 2.292e-04  Data: 0.012 (0.012)
Train: 159 [ 650/1251 ( 52%)]  Loss:  3.367743 (3.1754)  Time: 1.124s,  911.42/s  (1.108s,  924.03/s)  LR: 2.292e-04  Data: 0.011 (0.012)
Train: 159 [ 700/1251 ( 56%)]  Loss:  3.048792 (3.1670)  Time: 1.096s,  933.95/s  (1.108s,  923.97/s)  LR: 2.292e-04  Data: 0.012 (0.012)
Train: 159 [ 750/1251 ( 60%)]  Loss:  3.022138 (3.1579)  Time: 1.121s,  913.40/s  (1.109s,  923.54/s)  LR: 2.292e-04  Data: 0.010 (0.012)
Train: 159 [ 800/1251 ( 64%)]  Loss:  3.177176 (3.1590)  Time: 1.122s,  912.79/s  (1.109s,  922.96/s)  LR: 2.292e-04  Data: 0.011 (0.012)
Train: 159 [ 850/1251 ( 68%)]  Loss:  2.926069 (3.1461)  Time: 1.097s,  933.11/s  (1.110s,  922.77/s)  LR: 2.292e-04  Data: 0.012 (0.012)
Train: 159 [ 900/1251 ( 72%)]  Loss:  3.150574 (3.1463)  Time: 1.130s,  906.23/s  (1.110s,  922.31/s)  LR: 2.292e-04  Data: 0.013 (0.011)
Train: 159 [ 950/1251 ( 76%)]  Loss:  3.042900 (3.1412)  Time: 1.100s,  931.31/s  (1.110s,  922.30/s)  LR: 2.292e-04  Data: 0.010 (0.011)
Train: 159 [1000/1251 ( 80%)]  Loss:  3.243839 (3.1461)  Time: 1.101s,  929.82/s  (1.111s,  921.83/s)  LR: 2.292e-04  Data: 0.016 (0.011)
Train: 159 [1050/1251 ( 84%)]  Loss:  3.313668 (3.1537)  Time: 1.098s,  932.72/s  (1.111s,  922.06/s)  LR: 2.292e-04  Data: 0.012 (0.011)
Train: 159 [1100/1251 ( 88%)]  Loss:  3.157188 (3.1538)  Time: 1.094s,  936.09/s  (1.110s,  922.16/s)  LR: 2.292e-04  Data: 0.009 (0.011)
Train: 159 [1150/1251 ( 92%)]  Loss:  3.219066 (3.1565)  Time: 1.095s,  934.99/s  (1.110s,  922.58/s)  LR: 2.292e-04  Data: 0.011 (0.011)
Train: 159 [1200/1251 ( 96%)]  Loss:  3.288335 (3.1618)  Time: 1.101s,  930.44/s  (1.110s,  922.78/s)  LR: 2.292e-04  Data: 0.012 (0.011)
Train: 159 [1250/1251 (100%)]  Loss:  3.124236 (3.1604)  Time: 1.081s,  947.34/s  (1.110s,  922.73/s)  LR: 2.292e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.237 (3.237)  Loss:  0.4400 (0.4400)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.407)  Loss:  0.6066 (0.9249)  Acc@1: 85.6132 (78.6620)  Acc@5: 97.0519 (94.4720)
Test (EMA): [   0/48]  Time: 3.110 (3.110)  Loss:  0.4055 (0.4055)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5532 (0.8542)  Acc@1: 87.5000 (80.0320)  Acc@5: 97.6415 (95.2980)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 79.80000010498047)

Train: 160 [   0/1251 (  0%)]  Loss:  3.372703 (3.3727)  Time: 1.147s,  892.70/s  (1.147s,  892.70/s)  LR: 2.266e-04  Data: 0.022 (0.022)
Train: 160 [  50/1251 (  4%)]  Loss:  3.416065 (3.3944)  Time: 1.100s,  930.79/s  (1.113s,  920.29/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [ 100/1251 (  8%)]  Loss:  3.176885 (3.3219)  Time: 1.099s,  931.62/s  (1.111s,  921.50/s)  LR: 2.266e-04  Data: 0.011 (0.011)
Train: 160 [ 150/1251 ( 12%)]  Loss:  3.485923 (3.3629)  Time: 1.129s,  907.29/s  (1.111s,  921.70/s)  LR: 2.266e-04  Data: 0.012 (0.011)
Train: 160 [ 200/1251 ( 16%)]  Loss:  3.333239 (3.3570)  Time: 1.104s,  927.94/s  (1.110s,  922.52/s)  LR: 2.266e-04  Data: 0.011 (0.011)
Train: 160 [ 250/1251 ( 20%)]  Loss:  3.216529 (3.3336)  Time: 1.095s,  935.28/s  (1.110s,  922.33/s)  LR: 2.266e-04  Data: 0.010 (0.011)
Train: 160 [ 300/1251 ( 24%)]  Loss:  3.418847 (3.3457)  Time: 1.099s,  931.94/s  (1.110s,  922.28/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 350/1251 ( 28%)]  Loss:  3.365440 (3.3482)  Time: 1.097s,  933.20/s  (1.109s,  923.34/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 400/1251 ( 32%)]  Loss:  3.040040 (3.3140)  Time: 1.097s,  933.74/s  (1.109s,  923.42/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [ 450/1251 ( 36%)]  Loss:  3.121476 (3.2947)  Time: 1.211s,  845.42/s  (1.109s,  923.73/s)  LR: 2.266e-04  Data: 0.014 (0.012)
Train: 160 [ 500/1251 ( 40%)]  Loss:  3.203527 (3.2864)  Time: 1.120s,  914.28/s  (1.109s,  923.08/s)  LR: 2.266e-04  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 160 [ 550/1251 ( 44%)]  Loss:  3.007048 (3.2631)  Time: 1.102s,  929.34/s  (1.110s,  922.62/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 600/1251 ( 48%)]  Loss:  3.115450 (3.2518)  Time: 1.179s,  868.22/s  (1.110s,  922.36/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [ 650/1251 ( 52%)]  Loss:  3.412776 (3.2633)  Time: 1.093s,  936.52/s  (1.110s,  922.72/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 700/1251 ( 56%)]  Loss:  2.816207 (3.2335)  Time: 1.195s,  856.58/s  (1.111s,  921.95/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 750/1251 ( 60%)]  Loss:  3.213300 (3.2322)  Time: 1.107s,  925.36/s  (1.110s,  922.28/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 800/1251 ( 64%)]  Loss:  3.020127 (3.2197)  Time: 1.123s,  912.02/s  (1.110s,  922.63/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 850/1251 ( 68%)]  Loss:  3.418569 (3.2308)  Time: 1.097s,  933.83/s  (1.110s,  922.80/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [ 900/1251 ( 72%)]  Loss:  3.296572 (3.2342)  Time: 1.096s,  934.69/s  (1.110s,  922.84/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [ 950/1251 ( 76%)]  Loss:  3.377961 (3.2414)  Time: 1.097s,  933.77/s  (1.110s,  922.85/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [1000/1251 ( 80%)]  Loss:  3.236309 (3.2412)  Time: 1.096s,  934.01/s  (1.109s,  923.17/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [1050/1251 ( 84%)]  Loss:  3.340532 (3.2457)  Time: 1.093s,  936.55/s  (1.109s,  923.20/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [1100/1251 ( 88%)]  Loss:  3.405519 (3.2527)  Time: 1.106s,  925.85/s  (1.109s,  923.43/s)  LR: 2.266e-04  Data: 0.011 (0.012)
Train: 160 [1150/1251 ( 92%)]  Loss:  2.916792 (3.2387)  Time: 1.099s,  931.36/s  (1.109s,  923.38/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [1200/1251 ( 96%)]  Loss:  3.405125 (3.2453)  Time: 1.097s,  933.26/s  (1.109s,  923.03/s)  LR: 2.266e-04  Data: 0.012 (0.012)
Train: 160 [1250/1251 (100%)]  Loss:  2.974108 (3.2349)  Time: 1.102s,  929.11/s  (1.110s,  922.52/s)  LR: 2.266e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.261 (3.261)  Loss:  0.4793 (0.4793)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6171 (0.9397)  Acc@1: 85.1415 (78.3340)  Acc@5: 96.9340 (94.4140)
Test (EMA): [   0/48]  Time: 3.133 (3.133)  Loss:  0.4051 (0.4051)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.3398 (98.3398)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5533 (0.8536)  Acc@1: 87.2642 (80.0300)  Acc@5: 97.6415 (95.2720)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 79.85999997558594)

Train: 161 [   0/1251 (  0%)]  Loss:  3.388118 (3.3881)  Time: 1.112s,  920.75/s  (1.112s,  920.75/s)  LR: 2.241e-04  Data: 0.027 (0.027)
Train: 161 [  50/1251 (  4%)]  Loss:  3.165400 (3.2768)  Time: 1.097s,  933.34/s  (1.114s,  919.06/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 100/1251 (  8%)]  Loss:  3.343314 (3.2989)  Time: 1.098s,  932.44/s  (1.109s,  923.51/s)  LR: 2.241e-04  Data: 0.013 (0.012)
Train: 161 [ 150/1251 ( 12%)]  Loss:  3.056394 (3.2383)  Time: 1.098s,  932.89/s  (1.109s,  923.73/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Train: 161 [ 200/1251 ( 16%)]  Loss:  3.250646 (3.2408)  Time: 1.117s,  916.44/s  (1.110s,  922.12/s)  LR: 2.241e-04  Data: 0.010 (0.012)
Train: 161 [ 250/1251 ( 20%)]  Loss:  3.394065 (3.2663)  Time: 1.159s,  883.31/s  (1.112s,  921.01/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 300/1251 ( 24%)]  Loss:  3.098954 (3.2424)  Time: 1.123s,  912.23/s  (1.112s,  920.65/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 350/1251 ( 28%)]  Loss:  3.069204 (3.2208)  Time: 1.103s,  928.51/s  (1.111s,  921.32/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 400/1251 ( 32%)]  Loss:  2.983173 (3.1944)  Time: 1.102s,  929.01/s  (1.111s,  921.91/s)  LR: 2.241e-04  Data: 0.010 (0.012)
Train: 161 [ 450/1251 ( 36%)]  Loss:  3.158472 (3.1908)  Time: 1.119s,  915.15/s  (1.112s,  920.99/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Train: 161 [ 500/1251 ( 40%)]  Loss:  3.311807 (3.2018)  Time: 1.099s,  931.76/s  (1.113s,  919.94/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 550/1251 ( 44%)]  Loss:  3.051674 (3.1893)  Time: 1.178s,  869.07/s  (1.113s,  920.40/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 161 [ 600/1251 ( 48%)]  Loss:  3.440259 (3.2086)  Time: 1.114s,  918.88/s  (1.112s,  920.93/s)  LR: 2.241e-04  Data: 0.010 (0.012)
Train: 161 [ 650/1251 ( 52%)]  Loss:  3.062140 (3.1981)  Time: 1.098s,  932.79/s  (1.112s,  921.24/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 700/1251 ( 56%)]  Loss:  3.301464 (3.2050)  Time: 1.095s,  934.79/s  (1.111s,  921.68/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Train: 161 [ 750/1251 ( 60%)]  Loss:  3.225650 (3.2063)  Time: 1.101s,  930.47/s  (1.111s,  921.85/s)  LR: 2.241e-04  Data: 0.016 (0.012)
Train: 161 [ 800/1251 ( 64%)]  Loss:  3.309772 (3.2124)  Time: 1.112s,  920.83/s  (1.111s,  922.07/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 850/1251 ( 68%)]  Loss:  3.075195 (3.2048)  Time: 1.096s,  934.08/s  (1.111s,  921.90/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [ 900/1251 ( 72%)]  Loss:  2.882236 (3.1878)  Time: 1.097s,  933.26/s  (1.111s,  921.80/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Train: 161 [ 950/1251 ( 76%)]  Loss:  3.360873 (3.1964)  Time: 1.097s,  933.85/s  (1.111s,  921.87/s)  LR: 2.241e-04  Data: 0.013 (0.012)
Train: 161 [1000/1251 ( 80%)]  Loss:  3.407416 (3.2065)  Time: 1.097s,  933.82/s  (1.111s,  922.07/s)  LR: 2.241e-04  Data: 0.011 (0.012)
Train: 161 [1050/1251 ( 84%)]  Loss:  3.361466 (3.2135)  Time: 1.095s,  935.46/s  (1.110s,  922.32/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Train: 161 [1100/1251 ( 88%)]  Loss:  3.346026 (3.2193)  Time: 1.093s,  936.58/s  (1.110s,  922.31/s)  LR: 2.241e-04  Data: 0.010 (0.012)
Train: 161 [1150/1251 ( 92%)]  Loss:  3.049018 (3.2122)  Time: 1.096s,  934.37/s  (1.110s,  922.25/s)  LR: 2.241e-04  Data: 0.012 (0.012)
Train: 161 [1200/1251 ( 96%)]  Loss:  3.267379 (3.2144)  Time: 1.120s,  914.65/s  (1.110s,  922.13/s)  LR: 2.241e-04  Data: 0.010 (0.012)
Train: 161 [1250/1251 (100%)]  Loss:  3.346423 (3.2195)  Time: 1.085s,  944.07/s  (1.111s,  922.01/s)  LR: 2.241e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.266 (3.266)  Loss:  0.4416 (0.4416)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.398)  Loss:  0.6116 (0.9279)  Acc@1: 85.6132 (78.4680)  Acc@5: 97.5236 (94.4980)
Test (EMA): [   0/48]  Time: 3.223 (3.223)  Loss:  0.4051 (0.4051)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5526 (0.8529)  Acc@1: 87.5000 (80.0680)  Acc@5: 97.5236 (95.2520)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 79.94599997558593)

Train: 162 [   0/1251 (  0%)]  Loss:  2.899067 (2.8991)  Time: 1.102s,  928.97/s  (1.102s,  928.97/s)  LR: 2.215e-04  Data: 0.021 (0.021)
Train: 162 [  50/1251 (  4%)]  Loss:  3.121912 (3.0105)  Time: 1.124s,  911.19/s  (1.106s,  925.55/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [ 100/1251 (  8%)]  Loss:  3.191777 (3.0709)  Time: 1.101s,  929.68/s  (1.106s,  925.76/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 150/1251 ( 12%)]  Loss:  3.089401 (3.0755)  Time: 1.099s,  931.97/s  (1.105s,  926.55/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 200/1251 ( 16%)]  Loss:  3.242059 (3.1088)  Time: 1.101s,  930.31/s  (1.105s,  926.54/s)  LR: 2.215e-04  Data: 0.010 (0.012)
Train: 162 [ 250/1251 ( 20%)]  Loss:  3.289715 (3.1390)  Time: 1.113s,  919.72/s  (1.105s,  927.00/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [ 300/1251 ( 24%)]  Loss:  3.061880 (3.1280)  Time: 1.095s,  934.79/s  (1.105s,  926.42/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 350/1251 ( 28%)]  Loss:  3.138706 (3.1293)  Time: 1.103s,  928.21/s  (1.107s,  924.94/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 400/1251 ( 32%)]  Loss:  3.521915 (3.1729)  Time: 1.097s,  933.36/s  (1.107s,  924.71/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [ 450/1251 ( 36%)]  Loss:  2.898721 (3.1455)  Time: 1.096s,  934.26/s  (1.107s,  925.05/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [ 500/1251 ( 40%)]  Loss:  3.363131 (3.1653)  Time: 1.092s,  937.57/s  (1.107s,  924.81/s)  LR: 2.215e-04  Data: 0.009 (0.012)
Train: 162 [ 550/1251 ( 44%)]  Loss:  3.046291 (3.1554)  Time: 1.091s,  938.79/s  (1.107s,  925.24/s)  LR: 2.215e-04  Data: 0.010 (0.012)
Train: 162 [ 600/1251 ( 48%)]  Loss:  3.221171 (3.1604)  Time: 1.099s,  931.48/s  (1.107s,  925.09/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [ 650/1251 ( 52%)]  Loss:  3.202637 (3.1635)  Time: 1.101s,  929.94/s  (1.107s,  925.26/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 700/1251 ( 56%)]  Loss:  2.955191 (3.1496)  Time: 1.106s,  926.21/s  (1.107s,  925.37/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 750/1251 ( 60%)]  Loss:  2.937082 (3.1363)  Time: 1.098s,  932.59/s  (1.107s,  925.10/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 800/1251 ( 64%)]  Loss:  3.186743 (3.1393)  Time: 1.101s,  929.82/s  (1.107s,  925.33/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [ 850/1251 ( 68%)]  Loss:  3.088912 (3.1365)  Time: 1.136s,  901.45/s  (1.107s,  925.24/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 900/1251 ( 72%)]  Loss:  3.032912 (3.1310)  Time: 1.096s,  933.92/s  (1.106s,  925.48/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [ 950/1251 ( 76%)]  Loss:  3.086245 (3.1288)  Time: 1.096s,  933.99/s  (1.107s,  924.92/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [1000/1251 ( 80%)]  Loss:  3.355757 (3.1396)  Time: 1.097s,  933.52/s  (1.107s,  924.86/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [1050/1251 ( 84%)]  Loss:  3.082245 (3.1370)  Time: 1.098s,  932.82/s  (1.107s,  924.66/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [1100/1251 ( 88%)]  Loss:  3.033655 (3.1325)  Time: 1.099s,  931.77/s  (1.108s,  924.53/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [1150/1251 ( 92%)]  Loss:  3.277198 (3.1385)  Time: 1.098s,  932.74/s  (1.108s,  924.52/s)  LR: 2.215e-04  Data: 0.012 (0.012)
Train: 162 [1200/1251 ( 96%)]  Loss:  3.005162 (3.1332)  Time: 1.097s,  933.44/s  (1.108s,  924.35/s)  LR: 2.215e-04  Data: 0.011 (0.012)
Train: 162 [1250/1251 (100%)]  Loss:  3.373607 (3.1424)  Time: 1.112s,  921.14/s  (1.108s,  924.11/s)  LR: 2.215e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.241 (3.241)  Loss:  0.4706 (0.4706)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6284 (0.9656)  Acc@1: 87.0283 (78.4260)  Acc@5: 97.5236 (94.3500)
Test (EMA): [   0/48]  Time: 3.216 (3.216)  Loss:  0.4045 (0.4045)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5522 (0.8520)  Acc@1: 87.2642 (80.0580)  Acc@5: 97.5236 (95.2700)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 79.95800005371093)

Train: 163 [   0/1251 (  0%)]  Loss:  2.982853 (2.9829)  Time: 1.103s,  928.39/s  (1.103s,  928.39/s)  LR: 2.189e-04  Data: 0.023 (0.023)
Train: 163 [  50/1251 (  4%)]  Loss:  3.334301 (3.1586)  Time: 1.094s,  936.04/s  (1.105s,  926.94/s)  LR: 2.189e-04  Data: 0.010 (0.012)
Train: 163 [ 100/1251 (  8%)]  Loss:  3.297707 (3.2050)  Time: 1.094s,  935.99/s  (1.106s,  926.03/s)  LR: 2.189e-04  Data: 0.010 (0.012)
Train: 163 [ 150/1251 ( 12%)]  Loss:  3.254722 (3.2174)  Time: 1.095s,  935.11/s  (1.109s,  923.40/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 200/1251 ( 16%)]  Loss:  3.069600 (3.1878)  Time: 1.101s,  930.00/s  (1.107s,  925.35/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 250/1251 ( 20%)]  Loss:  3.280478 (3.2033)  Time: 1.098s,  932.53/s  (1.111s,  921.90/s)  LR: 2.189e-04  Data: 0.017 (0.012)
Train: 163 [ 300/1251 ( 24%)]  Loss:  3.249362 (3.2099)  Time: 1.121s,  913.50/s  (1.110s,  922.77/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 350/1251 ( 28%)]  Loss:  2.900714 (3.1712)  Time: 1.103s,  928.55/s  (1.110s,  922.41/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [ 400/1251 ( 32%)]  Loss:  3.160417 (3.1700)  Time: 1.097s,  933.75/s  (1.109s,  923.31/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 450/1251 ( 36%)]  Loss:  3.397579 (3.1928)  Time: 1.121s,  913.34/s  (1.110s,  922.54/s)  LR: 2.189e-04  Data: 0.010 (0.012)
Train: 163 [ 500/1251 ( 40%)]  Loss:  3.024439 (3.1775)  Time: 1.130s,  906.50/s  (1.110s,  922.34/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [ 550/1251 ( 44%)]  Loss:  3.359915 (3.1927)  Time: 1.113s,  920.00/s  (1.110s,  922.40/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 600/1251 ( 48%)]  Loss:  2.860307 (3.1671)  Time: 1.094s,  935.97/s  (1.110s,  922.76/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [ 650/1251 ( 52%)]  Loss:  3.295293 (3.1763)  Time: 1.137s,  900.63/s  (1.109s,  923.10/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [ 700/1251 ( 56%)]  Loss:  3.309445 (3.1851)  Time: 1.096s,  934.05/s  (1.109s,  923.29/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [ 750/1251 ( 60%)]  Loss:  3.146905 (3.1828)  Time: 1.119s,  915.28/s  (1.109s,  923.66/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 800/1251 ( 64%)]  Loss:  3.214525 (3.1846)  Time: 1.100s,  930.99/s  (1.109s,  923.34/s)  LR: 2.189e-04  Data: 0.014 (0.012)
Train: 163 [ 850/1251 ( 68%)]  Loss:  2.962428 (3.1723)  Time: 1.119s,  915.37/s  (1.109s,  923.57/s)  LR: 2.189e-04  Data: 0.010 (0.012)
Train: 163 [ 900/1251 ( 72%)]  Loss:  3.160643 (3.1717)  Time: 1.109s,  923.51/s  (1.109s,  923.19/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [ 950/1251 ( 76%)]  Loss:  3.522896 (3.1892)  Time: 1.100s,  930.75/s  (1.109s,  923.28/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [1000/1251 ( 80%)]  Loss:  3.416525 (3.2001)  Time: 1.107s,  924.89/s  (1.109s,  923.22/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [1050/1251 ( 84%)]  Loss:  3.019489 (3.1918)  Time: 1.102s,  928.92/s  (1.109s,  923.52/s)  LR: 2.189e-04  Data: 0.013 (0.012)
Train: 163 [1100/1251 ( 88%)]  Loss:  3.296671 (3.1964)  Time: 1.094s,  936.14/s  (1.109s,  923.47/s)  LR: 2.189e-04  Data: 0.011 (0.012)
Train: 163 [1150/1251 ( 92%)]  Loss:  3.412896 (3.2054)  Time: 1.092s,  937.71/s  (1.109s,  923.47/s)  LR: 2.189e-04  Data: 0.010 (0.012)
Train: 163 [1200/1251 ( 96%)]  Loss:  3.230418 (3.2064)  Time: 1.102s,  929.25/s  (1.109s,  923.75/s)  LR: 2.189e-04  Data: 0.012 (0.012)
Train: 163 [1250/1251 (100%)]  Loss:  3.122465 (3.2032)  Time: 1.083s,  945.90/s  (1.108s,  923.83/s)  LR: 2.189e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.198 (3.198)  Loss:  0.4748 (0.4748)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.412)  Loss:  0.5933 (0.9535)  Acc@1: 86.5566 (78.5800)  Acc@5: 97.5236 (94.5080)
Test (EMA): [   0/48]  Time: 3.155 (3.155)  Loss:  0.4051 (0.4051)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.230 (0.412)  Loss:  0.5518 (0.8514)  Acc@1: 87.5000 (80.0920)  Acc@5: 97.5236 (95.2460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 79.95999989746093)

Train: 164 [   0/1251 (  0%)]  Loss:  3.042541 (3.0425)  Time: 1.109s,  923.62/s  (1.109s,  923.62/s)  LR: 2.163e-04  Data: 0.027 (0.027)
Train: 164 [  50/1251 (  4%)]  Loss:  3.277874 (3.1602)  Time: 1.101s,  930.26/s  (1.105s,  926.95/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 100/1251 (  8%)]  Loss:  3.101856 (3.1408)  Time: 1.099s,  932.08/s  (1.106s,  925.96/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 150/1251 ( 12%)]  Loss:  3.324958 (3.1868)  Time: 1.096s,  934.18/s  (1.105s,  926.38/s)  LR: 2.163e-04  Data: 0.013 (0.012)
Train: 164 [ 200/1251 ( 16%)]  Loss:  3.150951 (3.1796)  Time: 1.134s,  902.70/s  (1.106s,  925.57/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 250/1251 ( 20%)]  Loss:  2.947820 (3.1410)  Time: 1.095s,  935.23/s  (1.106s,  926.28/s)  LR: 2.163e-04  Data: 0.013 (0.012)
Train: 164 [ 300/1251 ( 24%)]  Loss:  3.255697 (3.1574)  Time: 1.098s,  932.53/s  (1.106s,  925.70/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [ 350/1251 ( 28%)]  Loss:  3.164690 (3.1583)  Time: 1.096s,  934.53/s  (1.106s,  926.17/s)  LR: 2.163e-04  Data: 0.013 (0.012)
Train: 164 [ 400/1251 ( 32%)]  Loss:  3.481273 (3.1942)  Time: 1.096s,  933.96/s  (1.105s,  926.38/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 450/1251 ( 36%)]  Loss:  3.020420 (3.1768)  Time: 1.126s,  909.60/s  (1.105s,  926.80/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [ 500/1251 ( 40%)]  Loss:  3.200071 (3.1789)  Time: 1.097s,  933.63/s  (1.105s,  926.37/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [ 550/1251 ( 44%)]  Loss:  3.085073 (3.1711)  Time: 1.094s,  936.19/s  (1.105s,  926.48/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [ 600/1251 ( 48%)]  Loss:  3.128525 (3.1678)  Time: 1.097s,  933.06/s  (1.105s,  926.68/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 650/1251 ( 52%)]  Loss:  3.072602 (3.1610)  Time: 1.098s,  932.75/s  (1.106s,  926.17/s)  LR: 2.163e-04  Data: 0.013 (0.012)
Train: 164 [ 700/1251 ( 56%)]  Loss:  3.431058 (3.1790)  Time: 1.099s,  931.39/s  (1.105s,  926.28/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [ 750/1251 ( 60%)]  Loss:  3.608264 (3.2059)  Time: 1.121s,  913.35/s  (1.106s,  926.11/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 800/1251 ( 64%)]  Loss:  3.212809 (3.2063)  Time: 1.121s,  913.62/s  (1.107s,  925.06/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [ 850/1251 ( 68%)]  Loss:  3.254761 (3.2090)  Time: 1.194s,  857.73/s  (1.108s,  924.44/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 164 [ 900/1251 ( 72%)]  Loss:  2.995804 (3.1977)  Time: 1.127s,  908.76/s  (1.109s,  923.67/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [ 950/1251 ( 76%)]  Loss:  3.140019 (3.1949)  Time: 1.095s,  935.47/s  (1.109s,  923.52/s)  LR: 2.163e-04  Data: 0.010 (0.012)
Train: 164 [1000/1251 ( 80%)]  Loss:  3.039102 (3.1874)  Time: 1.119s,  914.97/s  (1.109s,  923.07/s)  LR: 2.163e-04  Data: 0.010 (0.012)
Train: 164 [1050/1251 ( 84%)]  Loss:  3.143009 (3.1854)  Time: 1.098s,  932.93/s  (1.110s,  922.88/s)  LR: 2.163e-04  Data: 0.012 (0.012)
Train: 164 [1100/1251 ( 88%)]  Loss:  3.190670 (3.1856)  Time: 1.101s,  930.45/s  (1.110s,  922.85/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [1150/1251 ( 92%)]  Loss:  3.350748 (3.1925)  Time: 1.095s,  935.58/s  (1.109s,  922.99/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [1200/1251 ( 96%)]  Loss:  3.425852 (3.2019)  Time: 1.132s,  904.79/s  (1.109s,  923.09/s)  LR: 2.163e-04  Data: 0.011 (0.012)
Train: 164 [1250/1251 (100%)]  Loss:  3.268756 (3.2044)  Time: 1.080s,  947.92/s  (1.109s,  923.00/s)  LR: 2.163e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.234 (3.234)  Loss:  0.4588 (0.4588)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.230 (0.410)  Loss:  0.5803 (0.9298)  Acc@1: 87.0283 (78.6820)  Acc@5: 97.4057 (94.5300)
Test (EMA): [   0/48]  Time: 3.143 (3.143)  Loss:  0.4052 (0.4052)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5521 (0.8508)  Acc@1: 87.5000 (80.0800)  Acc@5: 97.5236 (95.2360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 79.96999992431641)

Train: 165 [   0/1251 (  0%)]  Loss:  3.273494 (3.2735)  Time: 1.103s,  928.01/s  (1.103s,  928.01/s)  LR: 2.138e-04  Data: 0.020 (0.020)
Train: 165 [  50/1251 (  4%)]  Loss:  3.103079 (3.1883)  Time: 1.098s,  932.71/s  (1.107s,  924.71/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 100/1251 (  8%)]  Loss:  3.103611 (3.1601)  Time: 1.098s,  932.63/s  (1.104s,  927.50/s)  LR: 2.138e-04  Data: 0.010 (0.012)
Train: 165 [ 150/1251 ( 12%)]  Loss:  3.173822 (3.1635)  Time: 1.096s,  934.39/s  (1.108s,  924.12/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [ 200/1251 ( 16%)]  Loss:  3.200483 (3.1709)  Time: 1.098s,  932.77/s  (1.106s,  925.79/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 250/1251 ( 20%)]  Loss:  3.127067 (3.1636)  Time: 1.100s,  930.77/s  (1.107s,  925.19/s)  LR: 2.138e-04  Data: 0.015 (0.012)
Train: 165 [ 300/1251 ( 24%)]  Loss:  2.920909 (3.1289)  Time: 1.098s,  932.92/s  (1.107s,  924.85/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [ 350/1251 ( 28%)]  Loss:  3.425597 (3.1660)  Time: 1.096s,  934.56/s  (1.107s,  925.26/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 400/1251 ( 32%)]  Loss:  3.044148 (3.1525)  Time: 1.099s,  931.34/s  (1.107s,  925.30/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 450/1251 ( 36%)]  Loss:  3.220354 (3.1593)  Time: 1.097s,  933.82/s  (1.107s,  924.90/s)  LR: 2.138e-04  Data: 0.013 (0.012)
Train: 165 [ 500/1251 ( 40%)]  Loss:  3.062029 (3.1504)  Time: 1.096s,  934.01/s  (1.107s,  924.81/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [ 550/1251 ( 44%)]  Loss:  3.234278 (3.1574)  Time: 1.095s,  935.56/s  (1.108s,  924.06/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [ 600/1251 ( 48%)]  Loss:  2.995263 (3.1449)  Time: 1.102s,  929.58/s  (1.108s,  924.02/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 650/1251 ( 52%)]  Loss:  2.805988 (3.1207)  Time: 1.120s,  914.55/s  (1.108s,  924.04/s)  LR: 2.138e-04  Data: 0.010 (0.012)
Train: 165 [ 700/1251 ( 56%)]  Loss:  3.061802 (3.1168)  Time: 1.099s,  932.05/s  (1.109s,  923.47/s)  LR: 2.138e-04  Data: 0.010 (0.012)
Train: 165 [ 750/1251 ( 60%)]  Loss:  3.395530 (3.1342)  Time: 1.097s,  933.83/s  (1.108s,  923.80/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 800/1251 ( 64%)]  Loss:  3.125413 (3.1337)  Time: 1.098s,  932.40/s  (1.108s,  923.78/s)  LR: 2.138e-04  Data: 0.012 (0.012)
Train: 165 [ 850/1251 ( 68%)]  Loss:  3.234767 (3.1393)  Time: 1.096s,  933.98/s  (1.109s,  923.63/s)  LR: 2.138e-04  Data: 0.013 (0.012)
Train: 165 [ 900/1251 ( 72%)]  Loss:  3.464978 (3.1565)  Time: 1.119s,  914.74/s  (1.109s,  923.60/s)  LR: 2.138e-04  Data: 0.010 (0.012)
Train: 165 [ 950/1251 ( 76%)]  Loss:  3.251073 (3.1612)  Time: 1.133s,  903.62/s  (1.110s,  922.47/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [1000/1251 ( 80%)]  Loss:  3.345441 (3.1700)  Time: 1.096s,  933.93/s  (1.110s,  922.51/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 165 [1050/1251 ( 84%)]  Loss:  3.100648 (3.1668)  Time: 1.127s,  908.56/s  (1.110s,  922.55/s)  LR: 2.138e-04  Data: 0.010 (0.012)
Train: 165 [1100/1251 ( 88%)]  Loss:  2.951598 (3.1575)  Time: 1.097s,  933.44/s  (1.110s,  922.34/s)  LR: 2.138e-04  Data: 0.010 (0.012)
Train: 165 [1150/1251 ( 92%)]  Loss:  3.275560 (3.1624)  Time: 1.102s,  929.35/s  (1.110s,  922.41/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [1200/1251 ( 96%)]  Loss:  3.344535 (3.1697)  Time: 1.097s,  933.22/s  (1.110s,  922.40/s)  LR: 2.138e-04  Data: 0.011 (0.012)
Train: 165 [1250/1251 (100%)]  Loss:  3.274156 (3.1737)  Time: 1.105s,  926.92/s  (1.110s,  922.30/s)  LR: 2.138e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.317 (3.317)  Loss:  0.4422 (0.4422)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.400)  Loss:  0.6102 (0.9341)  Acc@1: 85.1415 (78.4460)  Acc@5: 97.1698 (94.4300)
Test (EMA): [   0/48]  Time: 3.320 (3.320)  Loss:  0.4051 (0.4051)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5524 (0.8499)  Acc@1: 87.6179 (80.1220)  Acc@5: 97.5236 (95.2780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 79.97399997558594)

Train: 166 [   0/1251 (  0%)]  Loss:  3.050742 (3.0507)  Time: 1.102s,  928.80/s  (1.102s,  928.80/s)  LR: 2.112e-04  Data: 0.019 (0.019)
Train: 166 [  50/1251 (  4%)]  Loss:  3.244555 (3.1476)  Time: 1.097s,  933.74/s  (1.102s,  929.09/s)  LR: 2.112e-04  Data: 0.011 (0.011)
Train: 166 [ 100/1251 (  8%)]  Loss:  3.184687 (3.1600)  Time: 1.098s,  932.20/s  (1.111s,  921.29/s)  LR: 2.112e-04  Data: 0.011 (0.011)
Train: 166 [ 150/1251 ( 12%)]  Loss:  3.231797 (3.1779)  Time: 1.129s,  907.15/s  (1.113s,  919.86/s)  LR: 2.112e-04  Data: 0.014 (0.011)
Train: 166 [ 200/1251 ( 16%)]  Loss:  3.234595 (3.1893)  Time: 1.193s,  858.04/s  (1.114s,  918.99/s)  LR: 2.112e-04  Data: 0.010 (0.011)
Train: 166 [ 250/1251 ( 20%)]  Loss:  3.414483 (3.2268)  Time: 1.098s,  932.57/s  (1.113s,  919.70/s)  LR: 2.112e-04  Data: 0.013 (0.011)
Train: 166 [ 300/1251 ( 24%)]  Loss:  2.806979 (3.1668)  Time: 1.097s,  933.85/s  (1.113s,  919.98/s)  LR: 2.112e-04  Data: 0.012 (0.011)
Train: 166 [ 350/1251 ( 28%)]  Loss:  3.143992 (3.1640)  Time: 1.099s,  931.91/s  (1.112s,  920.55/s)  LR: 2.112e-04  Data: 0.012 (0.011)
Train: 166 [ 400/1251 ( 32%)]  Loss:  3.300429 (3.1791)  Time: 1.117s,  916.91/s  (1.112s,  920.86/s)  LR: 2.112e-04  Data: 0.011 (0.011)
Train: 166 [ 450/1251 ( 36%)]  Loss:  3.005517 (3.1618)  Time: 1.131s,  905.16/s  (1.112s,  921.17/s)  LR: 2.112e-04  Data: 0.011 (0.011)
Train: 166 [ 500/1251 ( 40%)]  Loss:  3.138159 (3.1596)  Time: 1.099s,  931.43/s  (1.111s,  921.70/s)  LR: 2.112e-04  Data: 0.011 (0.011)
Train: 166 [ 550/1251 ( 44%)]  Loss:  2.938782 (3.1412)  Time: 1.096s,  934.21/s  (1.111s,  921.87/s)  LR: 2.112e-04  Data: 0.012 (0.011)
Train: 166 [ 600/1251 ( 48%)]  Loss:  3.154217 (3.1422)  Time: 1.098s,  932.60/s  (1.110s,  922.19/s)  LR: 2.112e-04  Data: 0.011 (0.012)
Train: 166 [ 650/1251 ( 52%)]  Loss:  2.858325 (3.1219)  Time: 1.099s,  932.08/s  (1.110s,  922.55/s)  LR: 2.112e-04  Data: 0.012 (0.012)
Train: 166 [ 700/1251 ( 56%)]  Loss:  3.181381 (3.1259)  Time: 1.101s,  929.99/s  (1.109s,  922.94/s)  LR: 2.112e-04  Data: 0.012 (0.012)
Train: 166 [ 750/1251 ( 60%)]  Loss:  3.223274 (3.1320)  Time: 1.130s,  906.16/s  (1.110s,  922.76/s)  LR: 2.112e-04  Data: 0.011 (0.012)
Train: 166 [ 800/1251 ( 64%)]  Loss:  3.075704 (3.1287)  Time: 1.134s,  903.28/s  (1.110s,  922.49/s)  LR: 2.112e-04  Data: 0.010 (0.012)
Train: 166 [ 850/1251 ( 68%)]  Loss:  3.255701 (3.1357)  Time: 1.098s,  932.79/s  (1.111s,  921.90/s)  LR: 2.112e-04  Data: 0.011 (0.012)
Train: 166 [ 900/1251 ( 72%)]  Loss:  3.068698 (3.1322)  Time: 1.096s,  934.18/s  (1.110s,  922.25/s)  LR: 2.112e-04  Data: 0.012 (0.012)
Train: 166 [ 950/1251 ( 76%)]  Loss:  3.118888 (3.1315)  Time: 1.121s,  913.74/s  (1.110s,  922.45/s)  LR: 2.112e-04  Data: 0.012 (0.012)
Train: 166 [1000/1251 ( 80%)]  Loss:  3.076152 (3.1289)  Time: 1.095s,  934.77/s  (1.110s,  922.45/s)  LR: 2.112e-04  Data: 0.012 (0.012)
Train: 166 [1050/1251 ( 84%)]  Loss:  2.990281 (3.1226)  Time: 1.100s,  931.28/s  (1.110s,  922.49/s)  LR: 2.112e-04  Data: 0.013 (0.011)
Train: 166 [1100/1251 ( 88%)]  Loss:  3.481812 (3.1382)  Time: 1.099s,  931.87/s  (1.110s,  922.74/s)  LR: 2.112e-04  Data: 0.012 (0.012)
Train: 166 [1150/1251 ( 92%)]  Loss:  3.222109 (3.1417)  Time: 1.098s,  932.26/s  (1.109s,  923.15/s)  LR: 2.112e-04  Data: 0.011 (0.012)
Train: 166 [1200/1251 ( 96%)]  Loss:  3.350853 (3.1501)  Time: 1.112s,  921.27/s  (1.109s,  923.18/s)  LR: 2.112e-04  Data: 0.013 (0.012)
Train: 166 [1250/1251 (100%)]  Loss:  3.364894 (3.1583)  Time: 1.087s,  941.65/s  (1.109s,  923.43/s)  LR: 2.112e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.210 (3.210)  Loss:  0.4601 (0.4601)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.6085 (0.9423)  Acc@1: 86.2028 (78.5640)  Acc@5: 97.2877 (94.5380)
Test (EMA): [   0/48]  Time: 3.227 (3.227)  Loss:  0.4050 (0.4050)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5515 (0.8493)  Acc@1: 87.6179 (80.1700)  Acc@5: 97.5236 (95.2920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 79.97799995117188)

Train: 167 [   0/1251 (  0%)]  Loss:  3.391901 (3.3919)  Time: 1.108s,  924.07/s  (1.108s,  924.07/s)  LR: 2.087e-04  Data: 0.025 (0.025)
Train: 167 [  50/1251 (  4%)]  Loss:  3.296829 (3.3444)  Time: 1.221s,  838.79/s  (1.112s,  921.25/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 100/1251 (  8%)]  Loss:  3.053686 (3.2475)  Time: 1.097s,  933.30/s  (1.108s,  924.14/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 150/1251 ( 12%)]  Loss:  3.073877 (3.2041)  Time: 1.096s,  933.97/s  (1.115s,  918.79/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 200/1251 ( 16%)]  Loss:  3.148054 (3.1929)  Time: 1.096s,  934.52/s  (1.110s,  922.13/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 250/1251 ( 20%)]  Loss:  3.081676 (3.1743)  Time: 1.096s,  933.99/s  (1.109s,  922.98/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 300/1251 ( 24%)]  Loss:  3.286968 (3.1904)  Time: 1.100s,  931.16/s  (1.109s,  923.50/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 350/1251 ( 28%)]  Loss:  2.895383 (3.1535)  Time: 1.099s,  931.65/s  (1.108s,  923.86/s)  LR: 2.087e-04  Data: 0.010 (0.012)
Train: 167 [ 400/1251 ( 32%)]  Loss:  3.360036 (3.1765)  Time: 1.097s,  933.79/s  (1.110s,  922.59/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 450/1251 ( 36%)]  Loss:  3.244674 (3.1833)  Time: 1.096s,  934.03/s  (1.109s,  922.95/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 500/1251 ( 40%)]  Loss:  3.251893 (3.1895)  Time: 1.144s,  895.38/s  (1.109s,  923.42/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 550/1251 ( 44%)]  Loss:  3.255592 (3.1950)  Time: 1.095s,  934.78/s  (1.109s,  923.76/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 600/1251 ( 48%)]  Loss:  3.304433 (3.2035)  Time: 1.097s,  933.32/s  (1.110s,  922.86/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 650/1251 ( 52%)]  Loss:  3.438989 (3.2203)  Time: 1.120s,  914.15/s  (1.109s,  923.26/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 700/1251 ( 56%)]  Loss:  2.976238 (3.2040)  Time: 1.095s,  935.13/s  (1.109s,  923.42/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 750/1251 ( 60%)]  Loss:  3.186423 (3.2029)  Time: 1.098s,  932.85/s  (1.109s,  923.66/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 800/1251 ( 64%)]  Loss:  3.183240 (3.2018)  Time: 1.095s,  935.03/s  (1.109s,  923.68/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 850/1251 ( 68%)]  Loss:  3.003460 (3.1907)  Time: 1.097s,  933.34/s  (1.108s,  924.02/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [ 900/1251 ( 72%)]  Loss:  3.476974 (3.2058)  Time: 1.120s,  913.90/s  (1.108s,  924.01/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [ 950/1251 ( 76%)]  Loss:  3.310322 (3.2110)  Time: 1.095s,  934.80/s  (1.108s,  923.89/s)  LR: 2.087e-04  Data: 0.010 (0.012)
Train: 167 [1000/1251 ( 80%)]  Loss:  3.113649 (3.2064)  Time: 1.095s,  934.91/s  (1.108s,  923.95/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [1050/1251 ( 84%)]  Loss:  3.245701 (3.2082)  Time: 1.131s,  905.18/s  (1.109s,  923.61/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [1100/1251 ( 88%)]  Loss:  3.254988 (3.2102)  Time: 1.177s,  870.11/s  (1.109s,  923.54/s)  LR: 2.087e-04  Data: 0.012 (0.012)
Train: 167 [1150/1251 ( 92%)]  Loss:  3.166074 (3.2084)  Time: 1.097s,  933.63/s  (1.109s,  923.73/s)  LR: 2.087e-04  Data: 0.010 (0.012)
Train: 167 [1200/1251 ( 96%)]  Loss:  3.122828 (3.2050)  Time: 1.096s,  934.03/s  (1.109s,  923.58/s)  LR: 2.087e-04  Data: 0.011 (0.012)
Train: 167 [1250/1251 (100%)]  Loss:  3.225972 (3.2058)  Time: 1.083s,  945.78/s  (1.109s,  923.52/s)  LR: 2.087e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.195 (3.195)  Loss:  0.5001 (0.5001)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6410 (0.9611)  Acc@1: 85.9670 (78.7420)  Acc@5: 97.1698 (94.5720)
Test (EMA): [   0/48]  Time: 3.093 (3.093)  Loss:  0.4051 (0.4051)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5508 (0.8484)  Acc@1: 87.5000 (80.1820)  Acc@5: 97.5236 (95.3040)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 79.99200002685546)

Train: 168 [   0/1251 (  0%)]  Loss:  3.122402 (3.1224)  Time: 1.128s,  907.99/s  (1.128s,  907.99/s)  LR: 2.061e-04  Data: 0.021 (0.021)
Train: 168 [  50/1251 (  4%)]  Loss:  2.796633 (2.9595)  Time: 1.095s,  934.88/s  (1.112s,  921.18/s)  LR: 2.061e-04  Data: 0.012 (0.012)
Train: 168 [ 100/1251 (  8%)]  Loss:  3.106347 (3.0085)  Time: 1.104s,  927.42/s  (1.111s,  922.10/s)  LR: 2.061e-04  Data: 0.012 (0.012)
Train: 168 [ 150/1251 ( 12%)]  Loss:  3.143194 (3.0421)  Time: 1.099s,  931.70/s  (1.109s,  923.28/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 200/1251 ( 16%)]  Loss:  3.203208 (3.0744)  Time: 1.135s,  902.21/s  (1.108s,  923.86/s)  LR: 2.061e-04  Data: 0.011 (0.012)
Train: 168 [ 250/1251 ( 20%)]  Loss:  2.953094 (3.0541)  Time: 1.095s,  935.05/s  (1.110s,  922.93/s)  LR: 2.061e-04  Data: 0.013 (0.011)
Train: 168 [ 300/1251 ( 24%)]  Loss:  3.326192 (3.0930)  Time: 1.099s,  931.84/s  (1.111s,  921.95/s)  LR: 2.061e-04  Data: 0.013 (0.011)
Train: 168 [ 350/1251 ( 28%)]  Loss:  3.318587 (3.1212)  Time: 1.096s,  933.90/s  (1.112s,  921.25/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [ 400/1251 ( 32%)]  Loss:  2.865798 (3.0928)  Time: 1.107s,  925.39/s  (1.112s,  920.69/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 450/1251 ( 36%)]  Loss:  3.130757 (3.0966)  Time: 1.094s,  935.70/s  (1.112s,  921.00/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 500/1251 ( 40%)]  Loss:  3.368347 (3.1213)  Time: 1.237s,  827.62/s  (1.112s,  920.84/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [ 550/1251 ( 44%)]  Loss:  2.906472 (3.1034)  Time: 1.128s,  908.06/s  (1.112s,  921.21/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 600/1251 ( 48%)]  Loss:  3.182320 (3.1095)  Time: 1.181s,  866.73/s  (1.111s,  921.88/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 650/1251 ( 52%)]  Loss:  3.386249 (3.1293)  Time: 1.101s,  929.71/s  (1.111s,  922.01/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 700/1251 ( 56%)]  Loss:  2.974156 (3.1189)  Time: 1.130s,  906.45/s  (1.110s,  922.13/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 750/1251 ( 60%)]  Loss:  3.249489 (3.1271)  Time: 1.096s,  934.47/s  (1.111s,  921.54/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 800/1251 ( 64%)]  Loss:  3.138392 (3.1277)  Time: 1.097s,  933.55/s  (1.111s,  921.38/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [ 850/1251 ( 68%)]  Loss:  3.560233 (3.1518)  Time: 1.102s,  929.20/s  (1.111s,  921.48/s)  LR: 2.061e-04  Data: 0.011 (0.011)
Train: 168 [ 900/1251 ( 72%)]  Loss:  3.036165 (3.1457)  Time: 1.120s,  914.37/s  (1.112s,  921.06/s)  LR: 2.061e-04  Data: 0.010 (0.011)
Train: 168 [ 950/1251 ( 76%)]  Loss:  3.219149 (3.1494)  Time: 1.101s,  929.93/s  (1.111s,  921.33/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [1000/1251 ( 80%)]  Loss:  2.973248 (3.1410)  Time: 1.121s,  913.41/s  (1.111s,  921.40/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [1050/1251 ( 84%)]  Loss:  3.228746 (3.1450)  Time: 1.096s,  934.02/s  (1.111s,  921.30/s)  LR: 2.061e-04  Data: 0.013 (0.011)
Train: 168 [1100/1251 ( 88%)]  Loss:  3.137822 (3.1447)  Time: 1.123s,  911.54/s  (1.112s,  921.04/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [1150/1251 ( 92%)]  Loss:  3.332254 (3.1525)  Time: 1.120s,  914.43/s  (1.112s,  921.11/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [1200/1251 ( 96%)]  Loss:  3.285469 (3.1578)  Time: 1.153s,  888.44/s  (1.112s,  920.91/s)  LR: 2.061e-04  Data: 0.012 (0.011)
Train: 168 [1250/1251 (100%)]  Loss:  3.398580 (3.1671)  Time: 1.077s,  950.42/s  (1.113s,  920.32/s)  LR: 2.061e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.262 (3.262)  Loss:  0.4583 (0.4583)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5938 (0.9325)  Acc@1: 86.6745 (78.7260)  Acc@5: 97.7594 (94.4280)
Test (EMA): [   0/48]  Time: 3.100 (3.100)  Loss:  0.4053 (0.4053)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5502 (0.8478)  Acc@1: 87.5000 (80.2060)  Acc@5: 97.5236 (95.3200)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 80.03000002685548)

Train: 169 [   0/1251 (  0%)]  Loss:  3.204937 (3.2049)  Time: 1.102s,  929.62/s  (1.102s,  929.62/s)  LR: 2.036e-04  Data: 0.019 (0.019)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 169 [  50/1251 (  4%)]  Loss:  3.229481 (3.2172)  Time: 1.095s,  935.15/s  (1.111s,  921.58/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [ 100/1251 (  8%)]  Loss:  2.846409 (3.0936)  Time: 1.134s,  902.96/s  (1.108s,  924.49/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [ 150/1251 ( 12%)]  Loss:  2.741479 (3.0056)  Time: 1.098s,  932.79/s  (1.108s,  924.41/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [ 200/1251 ( 16%)]  Loss:  3.332467 (3.0710)  Time: 1.098s,  932.19/s  (1.108s,  924.18/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [ 250/1251 ( 20%)]  Loss:  3.169698 (3.0874)  Time: 1.097s,  933.66/s  (1.108s,  924.29/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [ 300/1251 ( 24%)]  Loss:  2.830011 (3.0506)  Time: 1.100s,  931.18/s  (1.107s,  924.69/s)  LR: 2.036e-04  Data: 0.017 (0.012)
Train: 169 [ 350/1251 ( 28%)]  Loss:  2.876985 (3.0289)  Time: 1.135s,  902.35/s  (1.107s,  924.86/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [ 400/1251 ( 32%)]  Loss:  3.187580 (3.0466)  Time: 1.097s,  933.86/s  (1.108s,  924.45/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [ 450/1251 ( 36%)]  Loss:  3.378308 (3.0797)  Time: 1.191s,  859.79/s  (1.107s,  924.82/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [ 500/1251 ( 40%)]  Loss:  3.129410 (3.0843)  Time: 1.097s,  933.56/s  (1.108s,  924.56/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [ 550/1251 ( 44%)]  Loss:  3.233629 (3.0967)  Time: 1.099s,  932.01/s  (1.108s,  924.20/s)  LR: 2.036e-04  Data: 0.014 (0.012)
Train: 169 [ 600/1251 ( 48%)]  Loss:  3.209434 (3.1054)  Time: 1.125s,  909.91/s  (1.108s,  924.32/s)  LR: 2.036e-04  Data: 0.014 (0.012)
Train: 169 [ 650/1251 ( 52%)]  Loss:  3.367486 (3.1241)  Time: 1.098s,  932.45/s  (1.107s,  924.71/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [ 700/1251 ( 56%)]  Loss:  3.130983 (3.1246)  Time: 1.101s,  930.16/s  (1.107s,  924.71/s)  LR: 2.036e-04  Data: 0.013 (0.012)
Train: 169 [ 750/1251 ( 60%)]  Loss:  3.028644 (3.1186)  Time: 1.105s,  926.47/s  (1.107s,  924.86/s)  LR: 2.036e-04  Data: 0.013 (0.012)
Train: 169 [ 800/1251 ( 64%)]  Loss:  3.359060 (3.1327)  Time: 1.097s,  933.06/s  (1.107s,  924.73/s)  LR: 2.036e-04  Data: 0.010 (0.012)
Train: 169 [ 850/1251 ( 68%)]  Loss:  3.302346 (3.1421)  Time: 1.121s,  913.20/s  (1.107s,  924.69/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [ 900/1251 ( 72%)]  Loss:  3.142779 (3.1422)  Time: 1.097s,  933.69/s  (1.107s,  924.63/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [ 950/1251 ( 76%)]  Loss:  3.244123 (3.1473)  Time: 1.146s,  893.34/s  (1.108s,  923.84/s)  LR: 2.036e-04  Data: 0.013 (0.012)
Train: 169 [1000/1251 ( 80%)]  Loss:  3.382867 (3.1585)  Time: 1.096s,  934.32/s  (1.108s,  923.82/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [1050/1251 ( 84%)]  Loss:  3.322615 (3.1659)  Time: 1.127s,  908.26/s  (1.108s,  923.94/s)  LR: 2.036e-04  Data: 0.010 (0.012)
Train: 169 [1100/1251 ( 88%)]  Loss:  3.075516 (3.1620)  Time: 1.101s,  930.30/s  (1.108s,  924.15/s)  LR: 2.036e-04  Data: 0.012 (0.012)
Train: 169 [1150/1251 ( 92%)]  Loss:  3.207795 (3.1639)  Time: 1.100s,  930.65/s  (1.108s,  924.09/s)  LR: 2.036e-04  Data: 0.017 (0.012)
Train: 169 [1200/1251 ( 96%)]  Loss:  3.138813 (3.1629)  Time: 1.119s,  915.18/s  (1.108s,  924.20/s)  LR: 2.036e-04  Data: 0.011 (0.012)
Train: 169 [1250/1251 (100%)]  Loss:  3.409072 (3.1724)  Time: 1.089s,  940.64/s  (1.108s,  923.98/s)  LR: 2.036e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.347 (3.347)  Loss:  0.4790 (0.4790)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.5876 (0.9421)  Acc@1: 86.3208 (78.6820)  Acc@5: 97.1698 (94.5380)
Test (EMA): [   0/48]  Time: 3.204 (3.204)  Loss:  0.4048 (0.4048)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5497 (0.8471)  Acc@1: 87.6179 (80.2240)  Acc@5: 97.5236 (95.3060)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 80.032)

Train: 170 [   0/1251 (  0%)]  Loss:  2.989356 (2.9894)  Time: 1.106s,  925.84/s  (1.106s,  925.84/s)  LR: 2.010e-04  Data: 0.026 (0.026)
Train: 170 [  50/1251 (  4%)]  Loss:  3.295090 (3.1422)  Time: 1.132s,  904.94/s  (1.106s,  926.27/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 100/1251 (  8%)]  Loss:  3.040226 (3.1082)  Time: 1.099s,  931.94/s  (1.108s,  923.82/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 150/1251 ( 12%)]  Loss:  3.325825 (3.1626)  Time: 1.102s,  929.18/s  (1.107s,  925.33/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 200/1251 ( 16%)]  Loss:  3.303858 (3.1909)  Time: 1.100s,  930.94/s  (1.106s,  925.83/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 250/1251 ( 20%)]  Loss:  2.993376 (3.1580)  Time: 1.098s,  932.92/s  (1.107s,  925.20/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 300/1251 ( 24%)]  Loss:  2.776453 (3.1035)  Time: 1.097s,  933.63/s  (1.108s,  924.47/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 350/1251 ( 28%)]  Loss:  3.388054 (3.1390)  Time: 1.106s,  925.80/s  (1.108s,  924.09/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 400/1251 ( 32%)]  Loss:  3.298847 (3.1568)  Time: 1.106s,  925.78/s  (1.107s,  924.71/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 450/1251 ( 36%)]  Loss:  3.284652 (3.1696)  Time: 1.099s,  931.98/s  (1.107s,  925.08/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 500/1251 ( 40%)]  Loss:  3.215226 (3.1737)  Time: 1.095s,  935.16/s  (1.106s,  925.78/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 550/1251 ( 44%)]  Loss:  3.065511 (3.1647)  Time: 1.097s,  933.30/s  (1.106s,  925.61/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 600/1251 ( 48%)]  Loss:  3.205476 (3.1678)  Time: 1.111s,  921.47/s  (1.106s,  925.60/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 650/1251 ( 52%)]  Loss:  3.130391 (3.1652)  Time: 1.100s,  930.97/s  (1.107s,  924.99/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 700/1251 ( 56%)]  Loss:  3.416770 (3.1819)  Time: 1.099s,  931.35/s  (1.107s,  925.17/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [ 750/1251 ( 60%)]  Loss:  3.027958 (3.1723)  Time: 1.099s,  931.68/s  (1.107s,  925.41/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 170 [ 800/1251 ( 64%)]  Loss:  3.003387 (3.1624)  Time: 1.099s,  932.05/s  (1.106s,  925.63/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 850/1251 ( 68%)]  Loss:  3.290242 (3.1695)  Time: 1.125s,  910.39/s  (1.107s,  925.28/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 900/1251 ( 72%)]  Loss:  2.949061 (3.1579)  Time: 1.129s,  907.25/s  (1.108s,  924.32/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [ 950/1251 ( 76%)]  Loss:  2.972831 (3.1486)  Time: 1.097s,  933.22/s  (1.109s,  923.53/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [1000/1251 ( 80%)]  Loss:  3.233824 (3.1527)  Time: 1.099s,  931.55/s  (1.109s,  923.66/s)  LR: 2.010e-04  Data: 0.013 (0.012)
Train: 170 [1050/1251 ( 84%)]  Loss:  3.239063 (3.1566)  Time: 1.097s,  933.31/s  (1.108s,  923.81/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [1100/1251 ( 88%)]  Loss:  3.294002 (3.1626)  Time: 1.096s,  934.14/s  (1.108s,  923.98/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [1150/1251 ( 92%)]  Loss:  2.891058 (3.1513)  Time: 1.098s,  932.68/s  (1.108s,  924.10/s)  LR: 2.010e-04  Data: 0.011 (0.012)
Train: 170 [1200/1251 ( 96%)]  Loss:  3.065381 (3.1478)  Time: 1.096s,  934.57/s  (1.108s,  924.02/s)  LR: 2.010e-04  Data: 0.012 (0.012)
Train: 170 [1250/1251 (100%)]  Loss:  3.332272 (3.1549)  Time: 1.082s,  946.03/s  (1.109s,  923.67/s)  LR: 2.010e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.269 (3.269)  Loss:  0.4289 (0.4289)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.5886 (0.9285)  Acc@1: 86.0849 (78.6300)  Acc@5: 97.4057 (94.4380)
Test (EMA): [   0/48]  Time: 3.208 (3.208)  Loss:  0.4050 (0.4050)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.230 (0.408)  Loss:  0.5496 (0.8468)  Acc@1: 87.8538 (80.2540)  Acc@5: 97.5236 (95.3080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 80.05800002685547)

Train: 171 [   0/1251 (  0%)]  Loss:  3.222779 (3.2228)  Time: 1.113s,  919.74/s  (1.113s,  919.74/s)  LR: 1.985e-04  Data: 0.028 (0.028)
Train: 171 [  50/1251 (  4%)]  Loss:  3.044231 (3.1335)  Time: 1.127s,  908.68/s  (1.107s,  925.16/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [ 100/1251 (  8%)]  Loss:  3.142025 (3.1363)  Time: 1.102s,  929.52/s  (1.106s,  925.50/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [ 150/1251 ( 12%)]  Loss:  3.150224 (3.1398)  Time: 1.097s,  933.43/s  (1.107s,  924.99/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [ 200/1251 ( 16%)]  Loss:  3.370981 (3.1860)  Time: 1.096s,  934.50/s  (1.107s,  925.42/s)  LR: 1.985e-04  Data: 0.012 (0.012)
Train: 171 [ 250/1251 ( 20%)]  Loss:  3.340666 (3.2118)  Time: 1.096s,  934.36/s  (1.106s,  925.48/s)  LR: 1.985e-04  Data: 0.010 (0.012)
Train: 171 [ 300/1251 ( 24%)]  Loss:  3.178422 (3.2070)  Time: 1.134s,  903.22/s  (1.107s,  925.30/s)  LR: 1.985e-04  Data: 0.010 (0.012)
Train: 171 [ 350/1251 ( 28%)]  Loss:  2.992344 (3.1802)  Time: 1.132s,  904.52/s  (1.108s,  924.37/s)  LR: 1.985e-04  Data: 0.011 (0.011)
Train: 171 [ 400/1251 ( 32%)]  Loss:  3.260815 (3.1892)  Time: 1.097s,  933.87/s  (1.108s,  924.59/s)  LR: 1.985e-04  Data: 0.012 (0.011)
Train: 171 [ 450/1251 ( 36%)]  Loss:  3.246832 (3.1949)  Time: 1.096s,  934.43/s  (1.108s,  923.90/s)  LR: 1.985e-04  Data: 0.011 (0.011)
Train: 171 [ 500/1251 ( 40%)]  Loss:  2.878730 (3.1662)  Time: 1.134s,  902.87/s  (1.108s,  923.98/s)  LR: 1.985e-04  Data: 0.011 (0.011)
Train: 171 [ 550/1251 ( 44%)]  Loss:  3.118975 (3.1623)  Time: 1.097s,  933.26/s  (1.108s,  924.24/s)  LR: 1.985e-04  Data: 0.011 (0.011)
Train: 171 [ 600/1251 ( 48%)]  Loss:  3.618092 (3.1973)  Time: 1.097s,  933.52/s  (1.108s,  924.19/s)  LR: 1.985e-04  Data: 0.012 (0.011)
Train: 171 [ 650/1251 ( 52%)]  Loss:  3.141096 (3.1933)  Time: 1.120s,  914.67/s  (1.108s,  924.11/s)  LR: 1.985e-04  Data: 0.011 (0.011)
Train: 171 [ 700/1251 ( 56%)]  Loss:  3.185886 (3.1928)  Time: 1.097s,  933.85/s  (1.109s,  923.71/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [ 750/1251 ( 60%)]  Loss:  2.894261 (3.1741)  Time: 1.122s,  912.31/s  (1.109s,  923.60/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [ 800/1251 ( 64%)]  Loss:  2.709181 (3.1468)  Time: 1.101s,  929.80/s  (1.109s,  923.72/s)  LR: 1.985e-04  Data: 0.012 (0.012)
Train: 171 [ 850/1251 ( 68%)]  Loss:  2.972159 (3.1371)  Time: 1.096s,  934.18/s  (1.109s,  923.39/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [ 900/1251 ( 72%)]  Loss:  3.229070 (3.1419)  Time: 1.091s,  938.44/s  (1.109s,  923.19/s)  LR: 1.985e-04  Data: 0.010 (0.012)
Train: 171 [ 950/1251 ( 76%)]  Loss:  3.171547 (3.1434)  Time: 1.097s,  933.33/s  (1.109s,  923.41/s)  LR: 1.985e-04  Data: 0.012 (0.012)
Train: 171 [1000/1251 ( 80%)]  Loss:  3.179657 (3.1451)  Time: 1.245s,  822.24/s  (1.110s,  922.92/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [1050/1251 ( 84%)]  Loss:  3.363723 (3.1551)  Time: 1.104s,  927.41/s  (1.110s,  922.15/s)  LR: 1.985e-04  Data: 0.010 (0.012)
Train: 171 [1100/1251 ( 88%)]  Loss:  2.928751 (3.1452)  Time: 1.117s,  916.99/s  (1.110s,  922.33/s)  LR: 1.985e-04  Data: 0.010 (0.012)
Train: 171 [1150/1251 ( 92%)]  Loss:  3.089844 (3.1429)  Time: 1.098s,  932.45/s  (1.110s,  922.37/s)  LR: 1.985e-04  Data: 0.014 (0.012)
Train: 171 [1200/1251 ( 96%)]  Loss:  3.300846 (3.1492)  Time: 1.098s,  932.97/s  (1.110s,  922.38/s)  LR: 1.985e-04  Data: 0.011 (0.012)
Train: 171 [1250/1251 (100%)]  Loss:  3.223623 (3.1521)  Time: 1.085s,  943.58/s  (1.110s,  922.46/s)  LR: 1.985e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.218 (3.218)  Loss:  0.4789 (0.4789)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.6022 (0.9361)  Acc@1: 86.2028 (78.6920)  Acc@5: 97.4057 (94.5400)
Test (EMA): [   0/48]  Time: 3.140 (3.140)  Loss:  0.4054 (0.4054)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5504 (0.8466)  Acc@1: 87.7358 (80.2780)  Acc@5: 97.4057 (95.3400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 80.068)

Train: 172 [   0/1251 (  0%)]  Loss:  3.192999 (3.1930)  Time: 1.103s,  928.29/s  (1.103s,  928.29/s)  LR: 1.960e-04  Data: 0.020 (0.020)
Train: 172 [  50/1251 (  4%)]  Loss:  3.487912 (3.3405)  Time: 1.100s,  930.93/s  (1.106s,  926.00/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [ 100/1251 (  8%)]  Loss:  3.018390 (3.2331)  Time: 1.100s,  931.27/s  (1.108s,  924.16/s)  LR: 1.960e-04  Data: 0.012 (0.011)
Train: 172 [ 150/1251 ( 12%)]  Loss:  3.153946 (3.2133)  Time: 1.133s,  903.64/s  (1.107s,  924.62/s)  LR: 1.960e-04  Data: 0.011 (0.011)
Train: 172 [ 200/1251 ( 16%)]  Loss:  3.278325 (3.2263)  Time: 1.101s,  929.81/s  (1.109s,  923.20/s)  LR: 1.960e-04  Data: 0.012 (0.011)
Train: 172 [ 250/1251 ( 20%)]  Loss:  3.452775 (3.2641)  Time: 1.121s,  913.29/s  (1.109s,  923.27/s)  LR: 1.960e-04  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 172 [ 300/1251 ( 24%)]  Loss:  3.323630 (3.2726)  Time: 1.092s,  937.61/s  (1.110s,  922.88/s)  LR: 1.960e-04  Data: 0.010 (0.011)
Train: 172 [ 350/1251 ( 28%)]  Loss:  3.166346 (3.2593)  Time: 1.098s,  932.62/s  (1.109s,  922.96/s)  LR: 1.960e-04  Data: 0.013 (0.012)
Train: 172 [ 400/1251 ( 32%)]  Loss:  3.086495 (3.2401)  Time: 1.102s,  929.50/s  (1.108s,  923.95/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [ 450/1251 ( 36%)]  Loss:  3.040336 (3.2201)  Time: 1.130s,  906.09/s  (1.109s,  923.45/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [ 500/1251 ( 40%)]  Loss:  3.137500 (3.2126)  Time: 1.103s,  928.70/s  (1.108s,  923.81/s)  LR: 1.960e-04  Data: 0.012 (0.012)
Train: 172 [ 550/1251 ( 44%)]  Loss:  2.665665 (3.1670)  Time: 1.096s,  934.20/s  (1.109s,  923.63/s)  LR: 1.960e-04  Data: 0.012 (0.012)
Train: 172 [ 600/1251 ( 48%)]  Loss:  3.291134 (3.1766)  Time: 1.118s,  915.90/s  (1.108s,  923.80/s)  LR: 1.960e-04  Data: 0.012 (0.012)
Train: 172 [ 650/1251 ( 52%)]  Loss:  2.830348 (3.1518)  Time: 1.106s,  926.03/s  (1.109s,  923.50/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [ 700/1251 ( 56%)]  Loss:  3.065086 (3.1461)  Time: 1.098s,  932.85/s  (1.109s,  923.58/s)  LR: 1.960e-04  Data: 0.013 (0.012)
Train: 172 [ 750/1251 ( 60%)]  Loss:  3.184942 (3.1485)  Time: 1.121s,  913.85/s  (1.109s,  923.58/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [ 800/1251 ( 64%)]  Loss:  3.285850 (3.1566)  Time: 1.096s,  934.12/s  (1.109s,  923.58/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [ 850/1251 ( 68%)]  Loss:  3.295221 (3.1643)  Time: 1.100s,  930.98/s  (1.109s,  923.75/s)  LR: 1.960e-04  Data: 0.012 (0.012)
Train: 172 [ 900/1251 ( 72%)]  Loss:  3.168024 (3.1645)  Time: 1.101s,  929.79/s  (1.108s,  924.04/s)  LR: 1.960e-04  Data: 0.013 (0.012)
Train: 172 [ 950/1251 ( 76%)]  Loss:  3.417388 (3.1771)  Time: 1.210s,  846.03/s  (1.108s,  924.12/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [1000/1251 ( 80%)]  Loss:  3.010921 (3.1692)  Time: 1.098s,  933.00/s  (1.108s,  924.09/s)  LR: 1.960e-04  Data: 0.012 (0.012)
Train: 172 [1050/1251 ( 84%)]  Loss:  3.328291 (3.1764)  Time: 1.106s,  925.66/s  (1.108s,  924.30/s)  LR: 1.960e-04  Data: 0.012 (0.012)
Train: 172 [1100/1251 ( 88%)]  Loss:  2.971574 (3.1675)  Time: 1.101s,  929.79/s  (1.108s,  924.45/s)  LR: 1.960e-04  Data: 0.010 (0.012)
Train: 172 [1150/1251 ( 92%)]  Loss:  3.242376 (3.1706)  Time: 1.174s,  872.12/s  (1.108s,  924.60/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [1200/1251 ( 96%)]  Loss:  3.210596 (3.1722)  Time: 1.129s,  907.36/s  (1.107s,  924.65/s)  LR: 1.960e-04  Data: 0.011 (0.012)
Train: 172 [1250/1251 (100%)]  Loss:  3.076021 (3.1685)  Time: 1.088s,  940.78/s  (1.107s,  924.67/s)  LR: 1.960e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.219 (3.219)  Loss:  0.4609 (0.4609)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5819 (0.9375)  Acc@1: 87.0283 (78.9220)  Acc@5: 97.2877 (94.5880)
Test (EMA): [   0/48]  Time: 3.105 (3.105)  Loss:  0.4052 (0.4052)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5497 (0.8461)  Acc@1: 87.7358 (80.2960)  Acc@5: 97.5236 (95.3280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 80.08)

Train: 173 [   0/1251 (  0%)]  Loss:  3.017343 (3.0173)  Time: 1.114s,  919.24/s  (1.114s,  919.24/s)  LR: 1.935e-04  Data: 0.028 (0.028)
Train: 173 [  50/1251 (  4%)]  Loss:  3.302103 (3.1597)  Time: 1.102s,  929.35/s  (1.111s,  921.99/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 100/1251 (  8%)]  Loss:  3.284000 (3.2011)  Time: 1.142s,  896.92/s  (1.108s,  924.37/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 173 [ 150/1251 ( 12%)]  Loss:  2.828079 (3.1079)  Time: 1.094s,  936.30/s  (1.108s,  924.19/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 200/1251 ( 16%)]  Loss:  2.790040 (3.0443)  Time: 1.096s,  934.17/s  (1.109s,  923.73/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 250/1251 ( 20%)]  Loss:  2.710980 (2.9888)  Time: 1.096s,  933.98/s  (1.108s,  924.15/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 300/1251 ( 24%)]  Loss:  2.999272 (2.9903)  Time: 1.127s,  908.26/s  (1.110s,  922.73/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 173 [ 350/1251 ( 28%)]  Loss:  2.966003 (2.9872)  Time: 1.095s,  935.55/s  (1.110s,  922.92/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 400/1251 ( 32%)]  Loss:  3.459469 (3.0397)  Time: 1.105s,  926.67/s  (1.109s,  923.36/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 450/1251 ( 36%)]  Loss:  2.883153 (3.0240)  Time: 1.095s,  935.40/s  (1.108s,  924.07/s)  LR: 1.935e-04  Data: 0.013 (0.012)
Train: 173 [ 500/1251 ( 40%)]  Loss:  3.272912 (3.0467)  Time: 1.097s,  933.52/s  (1.108s,  924.17/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 550/1251 ( 44%)]  Loss:  3.249073 (3.0635)  Time: 1.099s,  931.88/s  (1.108s,  924.38/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 600/1251 ( 48%)]  Loss:  2.865116 (3.0483)  Time: 1.099s,  932.10/s  (1.108s,  924.16/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 650/1251 ( 52%)]  Loss:  3.003076 (3.0450)  Time: 1.097s,  933.26/s  (1.108s,  924.25/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 700/1251 ( 56%)]  Loss:  3.178460 (3.0539)  Time: 1.117s,  916.67/s  (1.108s,  924.32/s)  LR: 1.935e-04  Data: 0.010 (0.012)
Train: 173 [ 750/1251 ( 60%)]  Loss:  2.950312 (3.0475)  Time: 1.096s,  934.46/s  (1.109s,  923.51/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 800/1251 ( 64%)]  Loss:  3.294159 (3.0620)  Time: 1.119s,  914.95/s  (1.109s,  923.62/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 850/1251 ( 68%)]  Loss:  3.047425 (3.0612)  Time: 1.099s,  931.72/s  (1.109s,  923.31/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [ 900/1251 ( 72%)]  Loss:  3.137881 (3.0652)  Time: 1.097s,  933.46/s  (1.109s,  923.45/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [ 950/1251 ( 76%)]  Loss:  3.288903 (3.0764)  Time: 1.099s,  931.78/s  (1.109s,  922.95/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [1000/1251 ( 80%)]  Loss:  3.183349 (3.0815)  Time: 1.096s,  934.40/s  (1.109s,  923.34/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [1050/1251 ( 84%)]  Loss:  3.270082 (3.0901)  Time: 1.102s,  929.16/s  (1.109s,  923.39/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 173 [1100/1251 ( 88%)]  Loss:  3.147146 (3.0925)  Time: 1.119s,  914.83/s  (1.109s,  923.06/s)  LR: 1.935e-04  Data: 0.012 (0.012)
Train: 173 [1150/1251 ( 92%)]  Loss:  2.753181 (3.0784)  Time: 1.102s,  929.51/s  (1.109s,  922.99/s)  LR: 1.935e-04  Data: 0.011 (0.012)
Train: 173 [1200/1251 ( 96%)]  Loss:  2.924064 (3.0722)  Time: 1.096s,  934.32/s  (1.109s,  923.10/s)  LR: 1.935e-04  Data: 0.014 (0.012)
Train: 173 [1250/1251 (100%)]  Loss:  2.792774 (3.0615)  Time: 1.105s,  926.66/s  (1.110s,  922.88/s)  LR: 1.935e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.252 (3.252)  Loss:  0.4600 (0.4600)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6051 (0.9281)  Acc@1: 85.7311 (78.7360)  Acc@5: 97.5236 (94.6000)
Test (EMA): [   0/48]  Time: 3.278 (3.278)  Loss:  0.4049 (0.4049)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5483 (0.8455)  Acc@1: 87.6179 (80.3000)  Acc@5: 97.5236 (95.3460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 80.092)

Train: 174 [   0/1251 (  0%)]  Loss:  3.062358 (3.0624)  Time: 1.113s,  920.02/s  (1.113s,  920.02/s)  LR: 1.909e-04  Data: 0.022 (0.022)
Train: 174 [  50/1251 (  4%)]  Loss:  3.035340 (3.0488)  Time: 1.101s,  929.70/s  (1.107s,  925.42/s)  LR: 1.909e-04  Data: 0.010 (0.012)
Train: 174 [ 100/1251 (  8%)]  Loss:  3.253190 (3.1170)  Time: 1.097s,  933.34/s  (1.109s,  923.23/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 150/1251 ( 12%)]  Loss:  2.993541 (3.0861)  Time: 1.119s,  914.97/s  (1.107s,  925.02/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 200/1251 ( 16%)]  Loss:  3.071433 (3.0832)  Time: 1.095s,  935.17/s  (1.107s,  924.79/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 250/1251 ( 20%)]  Loss:  3.285195 (3.1168)  Time: 1.098s,  932.87/s  (1.108s,  923.94/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 300/1251 ( 24%)]  Loss:  3.060656 (3.1088)  Time: 1.135s,  901.84/s  (1.109s,  923.36/s)  LR: 1.909e-04  Data: 0.011 (0.011)
Train: 174 [ 350/1251 ( 28%)]  Loss:  3.421058 (3.1478)  Time: 1.097s,  933.55/s  (1.109s,  923.58/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 400/1251 ( 32%)]  Loss:  3.059446 (3.1380)  Time: 1.097s,  933.17/s  (1.108s,  924.44/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 450/1251 ( 36%)]  Loss:  3.244918 (3.1487)  Time: 1.101s,  929.73/s  (1.107s,  924.81/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 500/1251 ( 40%)]  Loss:  3.095468 (3.1439)  Time: 1.098s,  932.80/s  (1.108s,  924.33/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 550/1251 ( 44%)]  Loss:  3.314122 (3.1581)  Time: 1.102s,  929.57/s  (1.108s,  924.53/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 600/1251 ( 48%)]  Loss:  2.926706 (3.1403)  Time: 1.090s,  939.27/s  (1.107s,  924.69/s)  LR: 1.909e-04  Data: 0.010 (0.012)
Train: 174 [ 650/1251 ( 52%)]  Loss:  3.228958 (3.1466)  Time: 1.098s,  932.75/s  (1.107s,  924.70/s)  LR: 1.909e-04  Data: 0.013 (0.012)
Train: 174 [ 700/1251 ( 56%)]  Loss:  3.169402 (3.1481)  Time: 1.100s,  931.18/s  (1.107s,  924.72/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 750/1251 ( 60%)]  Loss:  3.003799 (3.1391)  Time: 1.102s,  929.31/s  (1.108s,  924.56/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 800/1251 ( 64%)]  Loss:  3.213167 (3.1435)  Time: 1.094s,  935.61/s  (1.108s,  924.50/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 850/1251 ( 68%)]  Loss:  3.170589 (3.1450)  Time: 1.098s,  932.28/s  (1.107s,  924.75/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [ 900/1251 ( 72%)]  Loss:  2.985261 (3.1366)  Time: 1.098s,  932.20/s  (1.107s,  924.68/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [ 950/1251 ( 76%)]  Loss:  3.208576 (3.1402)  Time: 1.096s,  934.57/s  (1.107s,  924.83/s)  LR: 1.909e-04  Data: 0.013 (0.012)
Train: 174 [1000/1251 ( 80%)]  Loss:  3.036957 (3.1352)  Time: 1.116s,  917.61/s  (1.107s,  924.82/s)  LR: 1.909e-04  Data: 0.010 (0.012)
Train: 174 [1050/1251 ( 84%)]  Loss:  3.352635 (3.1451)  Time: 1.098s,  932.51/s  (1.107s,  924.84/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [1100/1251 ( 88%)]  Loss:  3.019782 (3.1397)  Time: 1.097s,  933.62/s  (1.107s,  925.12/s)  LR: 1.909e-04  Data: 0.012 (0.012)
Train: 174 [1150/1251 ( 92%)]  Loss:  3.210980 (3.1426)  Time: 1.132s,  904.82/s  (1.107s,  925.23/s)  LR: 1.909e-04  Data: 0.011 (0.012)
Train: 174 [1200/1251 ( 96%)]  Loss:  3.046503 (3.1388)  Time: 1.120s,  914.15/s  (1.107s,  924.97/s)  LR: 1.909e-04  Data: 0.010 (0.012)
Train: 174 [1250/1251 (100%)]  Loss:  3.204545 (3.1413)  Time: 1.102s,  928.90/s  (1.107s,  924.64/s)  LR: 1.909e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.264 (3.264)  Loss:  0.4236 (0.4236)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5698 (0.9200)  Acc@1: 86.7924 (78.9320)  Acc@5: 97.7594 (94.6260)
Test (EMA): [   0/48]  Time: 3.265 (3.265)  Loss:  0.4044 (0.4044)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5469 (0.8451)  Acc@1: 87.6179 (80.3080)  Acc@5: 97.5236 (95.3240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 80.121999921875)

Train: 175 [   0/1251 (  0%)]  Loss:  3.360982 (3.3610)  Time: 1.111s,  922.08/s  (1.111s,  922.08/s)  LR: 1.884e-04  Data: 0.026 (0.026)
Train: 175 [  50/1251 (  4%)]  Loss:  3.272666 (3.3168)  Time: 1.092s,  938.09/s  (1.105s,  926.49/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 100/1251 (  8%)]  Loss:  3.188771 (3.2741)  Time: 1.095s,  935.30/s  (1.108s,  924.05/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 150/1251 ( 12%)]  Loss:  3.082648 (3.2263)  Time: 1.127s,  908.67/s  (1.107s,  925.03/s)  LR: 1.884e-04  Data: 0.010 (0.012)
Train: 175 [ 200/1251 ( 16%)]  Loss:  2.967097 (3.1744)  Time: 1.095s,  935.42/s  (1.106s,  925.54/s)  LR: 1.884e-04  Data: 0.010 (0.012)
Train: 175 [ 250/1251 ( 20%)]  Loss:  2.889868 (3.1270)  Time: 1.102s,  929.42/s  (1.106s,  925.53/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 300/1251 ( 24%)]  Loss:  3.069746 (3.1188)  Time: 1.099s,  932.00/s  (1.107s,  925.05/s)  LR: 1.884e-04  Data: 0.013 (0.012)
Train: 175 [ 350/1251 ( 28%)]  Loss:  3.039257 (3.1089)  Time: 1.101s,  930.20/s  (1.108s,  923.91/s)  LR: 1.884e-04  Data: 0.010 (0.012)
Train: 175 [ 400/1251 ( 32%)]  Loss:  2.912746 (3.0871)  Time: 1.095s,  935.02/s  (1.108s,  923.86/s)  LR: 1.884e-04  Data: 0.011 (0.012)
Train: 175 [ 450/1251 ( 36%)]  Loss:  3.181827 (3.0966)  Time: 1.098s,  932.80/s  (1.108s,  924.20/s)  LR: 1.884e-04  Data: 0.014 (0.012)
Train: 175 [ 500/1251 ( 40%)]  Loss:  3.143916 (3.1009)  Time: 1.101s,  930.23/s  (1.108s,  924.60/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 175 [ 550/1251 ( 44%)]  Loss:  3.197953 (3.1090)  Time: 1.066s,  960.48/s  (1.107s,  924.92/s)  LR: 1.884e-04  Data: 0.010 (0.012)
Train: 175 [ 600/1251 ( 48%)]  Loss:  2.969182 (3.0982)  Time: 1.096s,  934.51/s  (1.107s,  925.28/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 650/1251 ( 52%)]  Loss:  3.259999 (3.1098)  Time: 1.103s,  928.62/s  (1.107s,  925.04/s)  LR: 1.884e-04  Data: 0.013 (0.012)
Train: 175 [ 700/1251 ( 56%)]  Loss:  3.319717 (3.1238)  Time: 1.103s,  928.66/s  (1.107s,  924.69/s)  LR: 1.884e-04  Data: 0.011 (0.012)
Train: 175 [ 750/1251 ( 60%)]  Loss:  3.083379 (3.1212)  Time: 1.097s,  933.72/s  (1.107s,  924.74/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 800/1251 ( 64%)]  Loss:  3.191897 (3.1254)  Time: 1.099s,  931.42/s  (1.107s,  925.20/s)  LR: 1.884e-04  Data: 0.011 (0.012)
Train: 175 [ 850/1251 ( 68%)]  Loss:  3.016746 (3.1194)  Time: 1.098s,  932.59/s  (1.107s,  925.42/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 900/1251 ( 72%)]  Loss:  2.736026 (3.0992)  Time: 1.097s,  933.46/s  (1.107s,  925.34/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [ 950/1251 ( 76%)]  Loss:  3.230321 (3.1057)  Time: 1.103s,  928.67/s  (1.107s,  925.39/s)  LR: 1.884e-04  Data: 0.011 (0.012)
Train: 175 [1000/1251 ( 80%)]  Loss:  3.017132 (3.1015)  Time: 1.096s,  934.37/s  (1.107s,  925.07/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [1050/1251 ( 84%)]  Loss:  3.012682 (3.0975)  Time: 1.097s,  933.85/s  (1.107s,  925.12/s)  LR: 1.884e-04  Data: 0.012 (0.012)
Train: 175 [1100/1251 ( 88%)]  Loss:  2.981026 (3.0924)  Time: 1.132s,  904.32/s  (1.107s,  925.26/s)  LR: 1.884e-04  Data: 0.011 (0.012)
Train: 175 [1150/1251 ( 92%)]  Loss:  3.078531 (3.0918)  Time: 1.093s,  936.61/s  (1.107s,  924.78/s)  LR: 1.884e-04  Data: 0.010 (0.012)
Train: 175 [1200/1251 ( 96%)]  Loss:  3.073784 (3.0911)  Time: 1.096s,  933.91/s  (1.107s,  924.91/s)  LR: 1.884e-04  Data: 0.013 (0.012)
Train: 175 [1250/1251 (100%)]  Loss:  3.139581 (3.0930)  Time: 1.079s,  948.67/s  (1.107s,  924.86/s)  LR: 1.884e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.324 (3.324)  Loss:  0.4871 (0.4871)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.6180 (0.9219)  Acc@1: 86.7924 (79.1380)  Acc@5: 97.0519 (94.6540)
Test (EMA): [   0/48]  Time: 3.178 (3.178)  Loss:  0.4041 (0.4041)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5455 (0.8443)  Acc@1: 87.5000 (80.3580)  Acc@5: 97.5236 (95.3360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 80.169999921875)

Train: 176 [   0/1251 (  0%)]  Loss:  3.016737 (3.0167)  Time: 1.103s,  928.45/s  (1.103s,  928.45/s)  LR: 1.859e-04  Data: 0.022 (0.022)
Train: 176 [  50/1251 (  4%)]  Loss:  2.980908 (2.9988)  Time: 1.102s,  929.23/s  (1.103s,  928.33/s)  LR: 1.859e-04  Data: 0.010 (0.012)
Train: 176 [ 100/1251 (  8%)]  Loss:  3.199775 (3.0658)  Time: 1.105s,  926.57/s  (1.105s,  926.87/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 150/1251 ( 12%)]  Loss:  3.023717 (3.0553)  Time: 1.095s,  934.84/s  (1.106s,  925.56/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [ 200/1251 ( 16%)]  Loss:  3.162089 (3.0766)  Time: 1.099s,  931.44/s  (1.106s,  926.12/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 250/1251 ( 20%)]  Loss:  3.158363 (3.0903)  Time: 1.096s,  934.68/s  (1.106s,  926.10/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 300/1251 ( 24%)]  Loss:  3.245996 (3.1125)  Time: 1.095s,  935.04/s  (1.107s,  925.21/s)  LR: 1.859e-04  Data: 0.013 (0.012)
Train: 176 [ 350/1251 ( 28%)]  Loss:  3.174661 (3.1203)  Time: 1.103s,  928.74/s  (1.106s,  925.55/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [ 400/1251 ( 32%)]  Loss:  2.986740 (3.1054)  Time: 1.126s,  909.08/s  (1.106s,  925.87/s)  LR: 1.859e-04  Data: 0.016 (0.012)
Train: 176 [ 450/1251 ( 36%)]  Loss:  3.244594 (3.1194)  Time: 1.146s,  893.73/s  (1.107s,  924.86/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 500/1251 ( 40%)]  Loss:  3.107214 (3.1183)  Time: 1.106s,  925.58/s  (1.107s,  924.64/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [ 550/1251 ( 44%)]  Loss:  2.896891 (3.0998)  Time: 1.101s,  930.43/s  (1.107s,  924.79/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [ 600/1251 ( 48%)]  Loss:  2.740650 (3.0722)  Time: 1.096s,  934.41/s  (1.107s,  924.74/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [ 650/1251 ( 52%)]  Loss:  3.207679 (3.0819)  Time: 1.106s,  926.18/s  (1.107s,  924.92/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 700/1251 ( 56%)]  Loss:  3.167949 (3.0876)  Time: 1.097s,  933.23/s  (1.107s,  925.29/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 750/1251 ( 60%)]  Loss:  3.225743 (3.0962)  Time: 1.131s,  905.12/s  (1.107s,  925.38/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 800/1251 ( 64%)]  Loss:  3.293668 (3.1078)  Time: 1.096s,  934.07/s  (1.107s,  925.15/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 850/1251 ( 68%)]  Loss:  3.267001 (3.1167)  Time: 1.102s,  929.43/s  (1.107s,  925.24/s)  LR: 1.859e-04  Data: 0.010 (0.012)
Train: 176 [ 900/1251 ( 72%)]  Loss:  3.103036 (3.1160)  Time: 1.098s,  932.68/s  (1.107s,  925.41/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [ 950/1251 ( 76%)]  Loss:  3.159183 (3.1181)  Time: 1.133s,  903.77/s  (1.107s,  925.33/s)  LR: 1.859e-04  Data: 0.014 (0.012)
Train: 176 [1000/1251 ( 80%)]  Loss:  3.020401 (3.1135)  Time: 1.099s,  932.02/s  (1.107s,  925.13/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [1050/1251 ( 84%)]  Loss:  3.186558 (3.1168)  Time: 1.103s,  928.45/s  (1.107s,  925.34/s)  LR: 1.859e-04  Data: 0.013 (0.012)
Train: 176 [1100/1251 ( 88%)]  Loss:  3.256985 (3.1229)  Time: 1.098s,  932.62/s  (1.106s,  925.49/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [1150/1251 ( 92%)]  Loss:  2.977656 (3.1168)  Time: 1.179s,  868.56/s  (1.107s,  924.94/s)  LR: 1.859e-04  Data: 0.011 (0.012)
Train: 176 [1200/1251 ( 96%)]  Loss:  3.156524 (3.1184)  Time: 1.096s,  934.24/s  (1.107s,  924.97/s)  LR: 1.859e-04  Data: 0.012 (0.012)
Train: 176 [1250/1251 (100%)]  Loss:  3.271478 (3.1243)  Time: 1.079s,  948.71/s  (1.107s,  924.62/s)  LR: 1.859e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.194 (3.194)  Loss:  0.4195 (0.4195)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5659 (0.9081)  Acc@1: 87.1462 (79.1360)  Acc@5: 97.4057 (94.6700)
Test (EMA): [   0/48]  Time: 3.109 (3.109)  Loss:  0.4043 (0.4043)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.405)  Loss:  0.5444 (0.8437)  Acc@1: 87.6179 (80.3800)  Acc@5: 97.5236 (95.3280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 80.182)

Train: 177 [   0/1251 (  0%)]  Loss:  3.414717 (3.4147)  Time: 1.103s,  928.44/s  (1.103s,  928.44/s)  LR: 1.834e-04  Data: 0.020 (0.020)
Train: 177 [  50/1251 (  4%)]  Loss:  3.120294 (3.2675)  Time: 1.094s,  936.31/s  (1.102s,  929.05/s)  LR: 1.834e-04  Data: 0.010 (0.012)
Train: 177 [ 100/1251 (  8%)]  Loss:  3.116356 (3.2171)  Time: 1.096s,  934.10/s  (1.107s,  925.10/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 150/1251 ( 12%)]  Loss:  2.802822 (3.1135)  Time: 1.102s,  929.29/s  (1.106s,  925.86/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 200/1251 ( 16%)]  Loss:  3.072612 (3.1054)  Time: 1.096s,  934.56/s  (1.106s,  925.64/s)  LR: 1.834e-04  Data: 0.013 (0.012)
Train: 177 [ 250/1251 ( 20%)]  Loss:  3.324671 (3.1419)  Time: 1.096s,  933.94/s  (1.107s,  924.97/s)  LR: 1.834e-04  Data: 0.011 (0.012)
Train: 177 [ 300/1251 ( 24%)]  Loss:  2.805984 (3.0939)  Time: 1.097s,  933.21/s  (1.107s,  925.05/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 350/1251 ( 28%)]  Loss:  2.914205 (3.0715)  Time: 1.118s,  915.86/s  (1.107s,  924.62/s)  LR: 1.834e-04  Data: 0.010 (0.011)
Train: 177 [ 400/1251 ( 32%)]  Loss:  3.213324 (3.0872)  Time: 1.118s,  916.26/s  (1.109s,  923.65/s)  LR: 1.834e-04  Data: 0.010 (0.011)
Train: 177 [ 450/1251 ( 36%)]  Loss:  3.177281 (3.0962)  Time: 1.097s,  933.84/s  (1.109s,  923.54/s)  LR: 1.834e-04  Data: 0.011 (0.011)
Train: 177 [ 500/1251 ( 40%)]  Loss:  3.266634 (3.1117)  Time: 1.098s,  932.74/s  (1.109s,  923.62/s)  LR: 1.834e-04  Data: 0.013 (0.012)
Train: 177 [ 550/1251 ( 44%)]  Loss:  3.166092 (3.1162)  Time: 1.093s,  936.52/s  (1.108s,  923.88/s)  LR: 1.834e-04  Data: 0.010 (0.012)
Train: 177 [ 600/1251 ( 48%)]  Loss:  3.030898 (3.1097)  Time: 1.099s,  932.14/s  (1.108s,  924.16/s)  LR: 1.834e-04  Data: 0.011 (0.012)
Train: 177 [ 650/1251 ( 52%)]  Loss:  2.973091 (3.0999)  Time: 1.094s,  935.70/s  (1.108s,  924.19/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 700/1251 ( 56%)]  Loss:  3.193294 (3.1062)  Time: 1.096s,  933.91/s  (1.107s,  924.73/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 750/1251 ( 60%)]  Loss:  3.033505 (3.1016)  Time: 1.100s,  930.92/s  (1.107s,  924.86/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 800/1251 ( 64%)]  Loss:  3.145615 (3.1042)  Time: 1.098s,  932.86/s  (1.107s,  924.96/s)  LR: 1.834e-04  Data: 0.011 (0.012)
Train: 177 [ 850/1251 ( 68%)]  Loss:  3.095438 (3.1037)  Time: 1.129s,  907.32/s  (1.107s,  925.07/s)  LR: 1.834e-04  Data: 0.011 (0.012)
Train: 177 [ 900/1251 ( 72%)]  Loss:  3.124515 (3.1048)  Time: 1.096s,  934.12/s  (1.107s,  925.26/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [ 950/1251 ( 76%)]  Loss:  3.470661 (3.1231)  Time: 1.098s,  932.59/s  (1.107s,  925.20/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [1000/1251 ( 80%)]  Loss:  2.985810 (3.1166)  Time: 1.097s,  933.49/s  (1.107s,  924.96/s)  LR: 1.834e-04  Data: 0.011 (0.012)
Train: 177 [1050/1251 ( 84%)]  Loss:  2.780079 (3.1013)  Time: 1.096s,  934.21/s  (1.107s,  925.22/s)  LR: 1.834e-04  Data: 0.013 (0.012)
Train: 177 [1100/1251 ( 88%)]  Loss:  3.157241 (3.1037)  Time: 1.107s,  924.72/s  (1.107s,  925.07/s)  LR: 1.834e-04  Data: 0.013 (0.012)
Train: 177 [1150/1251 ( 92%)]  Loss:  3.044881 (3.1013)  Time: 1.099s,  932.05/s  (1.107s,  925.14/s)  LR: 1.834e-04  Data: 0.012 (0.012)
Train: 177 [1200/1251 ( 96%)]  Loss:  3.143124 (3.1029)  Time: 1.096s,  934.04/s  (1.107s,  924.93/s)  LR: 1.834e-04  Data: 0.011 (0.012)
Train: 177 [1250/1251 (100%)]  Loss:  3.222753 (3.1075)  Time: 1.085s,  943.35/s  (1.107s,  924.85/s)  LR: 1.834e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.275 (3.275)  Loss:  0.4579 (0.4579)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.397)  Loss:  0.5648 (0.9217)  Acc@1: 87.1462 (79.0180)  Acc@5: 97.1698 (94.6480)
Test (EMA): [   0/48]  Time: 3.135 (3.135)  Loss:  0.4043 (0.4043)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5430 (0.8432)  Acc@1: 87.7358 (80.4080)  Acc@5: 97.6415 (95.3460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 80.20600012939452)

Train: 178 [   0/1251 (  0%)]  Loss:  3.469438 (3.4694)  Time: 1.128s,  907.42/s  (1.128s,  907.42/s)  LR: 1.810e-04  Data: 0.021 (0.021)
Train: 178 [  50/1251 (  4%)]  Loss:  3.022810 (3.2461)  Time: 1.097s,  933.74/s  (1.108s,  923.99/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 100/1251 (  8%)]  Loss:  3.130732 (3.2077)  Time: 1.096s,  934.69/s  (1.106s,  925.45/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 150/1251 ( 12%)]  Loss:  3.050888 (3.1685)  Time: 1.098s,  932.91/s  (1.107s,  925.37/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 200/1251 ( 16%)]  Loss:  3.113733 (3.1575)  Time: 1.097s,  933.63/s  (1.106s,  926.20/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 250/1251 ( 20%)]  Loss:  2.860908 (3.1081)  Time: 1.099s,  931.91/s  (1.107s,  925.34/s)  LR: 1.810e-04  Data: 0.013 (0.012)
Train: 178 [ 300/1251 ( 24%)]  Loss:  3.243566 (3.1274)  Time: 1.098s,  932.69/s  (1.107s,  924.73/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 350/1251 ( 28%)]  Loss:  3.070735 (3.1204)  Time: 1.099s,  931.67/s  (1.107s,  924.66/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 400/1251 ( 32%)]  Loss:  3.361616 (3.1472)  Time: 1.118s,  915.76/s  (1.107s,  925.23/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 450/1251 ( 36%)]  Loss:  3.114295 (3.1439)  Time: 1.124s,  910.93/s  (1.107s,  925.28/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 500/1251 ( 40%)]  Loss:  3.110096 (3.1408)  Time: 1.096s,  934.66/s  (1.107s,  924.94/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 550/1251 ( 44%)]  Loss:  3.388205 (3.1614)  Time: 1.098s,  932.31/s  (1.107s,  925.13/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 600/1251 ( 48%)]  Loss:  2.958635 (3.1458)  Time: 1.097s,  933.31/s  (1.107s,  925.11/s)  LR: 1.810e-04  Data: 0.014 (0.012)
Train: 178 [ 650/1251 ( 52%)]  Loss:  3.130212 (3.1447)  Time: 1.119s,  915.04/s  (1.107s,  925.10/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 700/1251 ( 56%)]  Loss:  3.169718 (3.1464)  Time: 1.097s,  933.84/s  (1.107s,  925.11/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 750/1251 ( 60%)]  Loss:  2.799477 (3.1247)  Time: 1.133s,  903.77/s  (1.108s,  924.33/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [ 800/1251 ( 64%)]  Loss:  3.251829 (3.1322)  Time: 1.103s,  928.36/s  (1.108s,  923.82/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 850/1251 ( 68%)]  Loss:  3.320315 (3.1426)  Time: 1.102s,  929.19/s  (1.109s,  923.56/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 178 [ 900/1251 ( 72%)]  Loss:  3.049670 (3.1377)  Time: 1.095s,  934.95/s  (1.108s,  923.83/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [ 950/1251 ( 76%)]  Loss:  3.377161 (3.1497)  Time: 1.099s,  931.81/s  (1.108s,  924.04/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [1000/1251 ( 80%)]  Loss:  3.323360 (3.1580)  Time: 1.102s,  929.54/s  (1.108s,  923.89/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [1050/1251 ( 84%)]  Loss:  3.340808 (3.1663)  Time: 1.098s,  932.93/s  (1.109s,  923.74/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [1100/1251 ( 88%)]  Loss:  3.144302 (3.1653)  Time: 1.120s,  914.51/s  (1.109s,  923.70/s)  LR: 1.810e-04  Data: 0.012 (0.012)
Train: 178 [1150/1251 ( 92%)]  Loss:  3.060522 (3.1610)  Time: 1.108s,  924.50/s  (1.108s,  923.84/s)  LR: 1.810e-04  Data: 0.011 (0.012)
Train: 178 [1200/1251 ( 96%)]  Loss:  3.246100 (3.1644)  Time: 1.099s,  932.16/s  (1.108s,  924.10/s)  LR: 1.810e-04  Data: 0.014 (0.012)
Train: 178 [1250/1251 (100%)]  Loss:  3.430833 (3.1746)  Time: 1.079s,  949.26/s  (1.108s,  923.87/s)  LR: 1.810e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.280 (3.280)  Loss:  0.4851 (0.4851)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.6213 (0.9433)  Acc@1: 86.3208 (78.9380)  Acc@5: 97.6415 (94.6160)
Test (EMA): [   0/48]  Time: 3.148 (3.148)  Loss:  0.4043 (0.4043)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.412)  Loss:  0.5422 (0.8428)  Acc@1: 87.6179 (80.4440)  Acc@5: 97.6415 (95.3580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 80.223999921875)

Train: 179 [   0/1251 (  0%)]  Loss:  3.031169 (3.0312)  Time: 1.105s,  926.60/s  (1.105s,  926.60/s)  LR: 1.785e-04  Data: 0.024 (0.024)
Train: 179 [  50/1251 (  4%)]  Loss:  3.148217 (3.0897)  Time: 1.101s,  929.80/s  (1.100s,  931.22/s)  LR: 1.785e-04  Data: 0.010 (0.012)
Train: 179 [ 100/1251 (  8%)]  Loss:  3.156507 (3.1120)  Time: 1.095s,  935.06/s  (1.102s,  929.00/s)  LR: 1.785e-04  Data: 0.012 (0.012)
Train: 179 [ 150/1251 ( 12%)]  Loss:  3.131236 (3.1168)  Time: 1.097s,  933.14/s  (1.104s,  927.65/s)  LR: 1.785e-04  Data: 0.012 (0.012)
Train: 179 [ 200/1251 ( 16%)]  Loss:  2.997872 (3.0930)  Time: 1.104s,  927.39/s  (1.105s,  926.67/s)  LR: 1.785e-04  Data: 0.011 (0.012)
Train: 179 [ 250/1251 ( 20%)]  Loss:  3.102912 (3.0947)  Time: 1.121s,  913.36/s  (1.108s,  924.37/s)  LR: 1.785e-04  Data: 0.011 (0.012)
Train: 179 [ 300/1251 ( 24%)]  Loss:  3.139926 (3.1011)  Time: 1.119s,  914.83/s  (1.111s,  921.67/s)  LR: 1.785e-04  Data: 0.010 (0.012)
Train: 179 [ 350/1251 ( 28%)]  Loss:  2.956756 (3.0831)  Time: 1.103s,  928.58/s  (1.112s,  920.52/s)  LR: 1.785e-04  Data: 0.011 (0.012)
Train: 179 [ 400/1251 ( 32%)]  Loss:  3.416214 (3.1201)  Time: 1.119s,  915.47/s  (1.113s,  920.36/s)  LR: 1.785e-04  Data: 0.010 (0.011)
Train: 179 [ 450/1251 ( 36%)]  Loss:  3.180494 (3.1261)  Time: 1.098s,  932.98/s  (1.112s,  920.89/s)  LR: 1.785e-04  Data: 0.011 (0.011)
Train: 179 [ 500/1251 ( 40%)]  Loss:  2.967659 (3.1117)  Time: 1.095s,  935.13/s  (1.112s,  921.23/s)  LR: 1.785e-04  Data: 0.012 (0.011)
Train: 179 [ 550/1251 ( 44%)]  Loss:  3.126035 (3.1129)  Time: 1.097s,  933.58/s  (1.111s,  921.75/s)  LR: 1.785e-04  Data: 0.012 (0.011)
Train: 179 [ 600/1251 ( 48%)]  Loss:  3.299210 (3.1272)  Time: 1.098s,  932.67/s  (1.110s,  922.28/s)  LR: 1.785e-04  Data: 0.012 (0.011)
Train: 179 [ 650/1251 ( 52%)]  Loss:  3.118216 (3.1266)  Time: 1.091s,  938.70/s  (1.110s,  922.39/s)  LR: 1.785e-04  Data: 0.011 (0.011)
Train: 179 [ 700/1251 ( 56%)]  Loss:  3.078434 (3.1234)  Time: 1.097s,  933.77/s  (1.110s,  922.68/s)  LR: 1.785e-04  Data: 0.011 (0.011)
Train: 179 [ 750/1251 ( 60%)]  Loss:  3.282449 (3.1333)  Time: 1.187s,  862.79/s  (1.109s,  923.02/s)  LR: 1.785e-04  Data: 0.012 (0.011)
Train: 179 [ 800/1251 ( 64%)]  Loss:  3.069871 (3.1296)  Time: 1.096s,  934.10/s  (1.110s,  922.74/s)  LR: 1.785e-04  Data: 0.013 (0.012)
Train: 179 [ 850/1251 ( 68%)]  Loss:  3.083665 (3.1270)  Time: 1.097s,  933.75/s  (1.110s,  922.79/s)  LR: 1.785e-04  Data: 0.011 (0.012)
Train: 179 [ 900/1251 ( 72%)]  Loss:  2.983698 (3.1195)  Time: 1.094s,  936.04/s  (1.109s,  923.05/s)  LR: 1.785e-04  Data: 0.011 (0.012)
Train: 179 [ 950/1251 ( 76%)]  Loss:  3.404805 (3.1338)  Time: 1.099s,  931.41/s  (1.109s,  923.04/s)  LR: 1.785e-04  Data: 0.013 (0.012)
Train: 179 [1000/1251 ( 80%)]  Loss:  3.023149 (3.1285)  Time: 1.101s,  929.82/s  (1.110s,  922.92/s)  LR: 1.785e-04  Data: 0.012 (0.012)
Train: 179 [1050/1251 ( 84%)]  Loss:  3.210048 (3.1322)  Time: 1.099s,  931.55/s  (1.109s,  923.04/s)  LR: 1.785e-04  Data: 0.016 (0.012)
Train: 179 [1100/1251 ( 88%)]  Loss:  3.147388 (3.1329)  Time: 1.176s,  870.69/s  (1.109s,  923.26/s)  LR: 1.785e-04  Data: 0.012 (0.012)
Train: 179 [1150/1251 ( 92%)]  Loss:  2.927101 (3.1243)  Time: 1.138s,  899.90/s  (1.109s,  923.46/s)  LR: 1.785e-04  Data: 0.012 (0.012)
Train: 179 [1200/1251 ( 96%)]  Loss:  3.319172 (3.1321)  Time: 1.098s,  932.88/s  (1.109s,  923.55/s)  LR: 1.785e-04  Data: 0.011 (0.012)
Train: 179 [1250/1251 (100%)]  Loss:  2.939791 (3.1247)  Time: 1.101s,  929.97/s  (1.110s,  922.94/s)  LR: 1.785e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.321 (3.321)  Loss:  0.4341 (0.4341)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.5891 (0.9297)  Acc@1: 86.7925 (78.9500)  Acc@5: 97.7594 (94.6480)
Test (EMA): [   0/48]  Time: 3.023 (3.023)  Loss:  0.4043 (0.4043)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5423 (0.8422)  Acc@1: 87.7358 (80.4940)  Acc@5: 97.7594 (95.3600)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 80.25400002441407)

Train: 180 [   0/1251 (  0%)]  Loss:  3.216437 (3.2164)  Time: 1.108s,  924.06/s  (1.108s,  924.06/s)  LR: 1.760e-04  Data: 0.024 (0.024)
Train: 180 [  50/1251 (  4%)]  Loss:  2.619905 (2.9182)  Time: 1.097s,  933.74/s  (1.107s,  924.64/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 100/1251 (  8%)]  Loss:  2.924736 (2.9204)  Time: 1.101s,  930.09/s  (1.107s,  925.42/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [ 150/1251 ( 12%)]  Loss:  2.947641 (2.9272)  Time: 1.101s,  930.27/s  (1.107s,  924.82/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [ 200/1251 ( 16%)]  Loss:  3.115817 (2.9649)  Time: 1.100s,  931.14/s  (1.107s,  925.16/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [ 250/1251 ( 20%)]  Loss:  3.186997 (3.0019)  Time: 1.133s,  903.87/s  (1.107s,  924.66/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 300/1251 ( 24%)]  Loss:  3.503158 (3.0735)  Time: 1.104s,  927.64/s  (1.107s,  924.77/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [ 350/1251 ( 28%)]  Loss:  3.360476 (3.1094)  Time: 1.096s,  934.14/s  (1.107s,  924.90/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 180 [ 400/1251 ( 32%)]  Loss:  3.389889 (3.1406)  Time: 1.120s,  914.49/s  (1.107s,  924.71/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [ 450/1251 ( 36%)]  Loss:  2.991951 (3.1257)  Time: 1.096s,  934.66/s  (1.107s,  924.85/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 500/1251 ( 40%)]  Loss:  3.248099 (3.1368)  Time: 1.102s,  929.28/s  (1.107s,  924.74/s)  LR: 1.760e-04  Data: 0.013 (0.012)
Train: 180 [ 550/1251 ( 44%)]  Loss:  3.308373 (3.1511)  Time: 1.096s,  934.09/s  (1.107s,  924.95/s)  LR: 1.760e-04  Data: 0.010 (0.012)
Train: 180 [ 600/1251 ( 48%)]  Loss:  3.163620 (3.1521)  Time: 1.187s,  862.89/s  (1.107s,  924.91/s)  LR: 1.760e-04  Data: 0.010 (0.012)
Train: 180 [ 650/1251 ( 52%)]  Loss:  3.241171 (3.1584)  Time: 1.098s,  932.90/s  (1.107s,  925.34/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 700/1251 ( 56%)]  Loss:  3.165402 (3.1589)  Time: 1.225s,  836.01/s  (1.107s,  925.17/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 750/1251 ( 60%)]  Loss:  3.019449 (3.1502)  Time: 1.096s,  933.91/s  (1.107s,  925.25/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [ 800/1251 ( 64%)]  Loss:  3.145776 (3.1499)  Time: 1.097s,  933.27/s  (1.107s,  925.05/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 850/1251 ( 68%)]  Loss:  2.862859 (3.1340)  Time: 1.121s,  913.51/s  (1.107s,  924.62/s)  LR: 1.760e-04  Data: 0.010 (0.012)
Train: 180 [ 900/1251 ( 72%)]  Loss:  2.879417 (3.1206)  Time: 1.097s,  933.65/s  (1.107s,  924.74/s)  LR: 1.760e-04  Data: 0.012 (0.012)
Train: 180 [ 950/1251 ( 76%)]  Loss:  3.297726 (3.1294)  Time: 1.096s,  934.67/s  (1.107s,  924.82/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [1000/1251 ( 80%)]  Loss:  3.213174 (3.1334)  Time: 1.099s,  932.17/s  (1.107s,  925.00/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [1050/1251 ( 84%)]  Loss:  2.988960 (3.1269)  Time: 1.092s,  937.39/s  (1.107s,  925.01/s)  LR: 1.760e-04  Data: 0.010 (0.012)
Train: 180 [1100/1251 ( 88%)]  Loss:  2.948406 (3.1191)  Time: 1.189s,  861.30/s  (1.107s,  925.13/s)  LR: 1.760e-04  Data: 0.013 (0.012)
Train: 180 [1150/1251 ( 92%)]  Loss:  3.251594 (3.1246)  Time: 1.095s,  935.43/s  (1.107s,  925.18/s)  LR: 1.760e-04  Data: 0.013 (0.012)
Train: 180 [1200/1251 ( 96%)]  Loss:  3.219790 (3.1284)  Time: 1.098s,  933.00/s  (1.107s,  924.97/s)  LR: 1.760e-04  Data: 0.011 (0.012)
Train: 180 [1250/1251 (100%)]  Loss:  2.914251 (3.1202)  Time: 1.087s,  941.96/s  (1.108s,  924.39/s)  LR: 1.760e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.194 (3.194)  Loss:  0.4429 (0.4429)  Acc@1: 92.4805 (92.4805)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.6018 (0.9147)  Acc@1: 85.9670 (79.0920)  Acc@5: 97.1698 (94.7220)
Test (EMA): [   0/48]  Time: 3.105 (3.105)  Loss:  0.4037 (0.4037)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  0.5423 (0.8416)  Acc@1: 87.6179 (80.5020)  Acc@5: 97.7594 (95.3680)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 80.27799997314453)

Train: 181 [   0/1251 (  0%)]  Loss:  2.847370 (2.8474)  Time: 1.105s,  926.35/s  (1.105s,  926.35/s)  LR: 1.736e-04  Data: 0.021 (0.021)
Train: 181 [  50/1251 (  4%)]  Loss:  3.261718 (3.0545)  Time: 1.092s,  938.04/s  (1.112s,  920.67/s)  LR: 1.736e-04  Data: 0.010 (0.012)
Train: 181 [ 100/1251 (  8%)]  Loss:  2.916085 (3.0084)  Time: 1.098s,  932.36/s  (1.111s,  921.69/s)  LR: 1.736e-04  Data: 0.010 (0.012)
Train: 181 [ 150/1251 ( 12%)]  Loss:  3.072649 (3.0245)  Time: 1.101s,  929.72/s  (1.109s,  923.20/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 200/1251 ( 16%)]  Loss:  3.047138 (3.0290)  Time: 1.098s,  932.95/s  (1.109s,  923.39/s)  LR: 1.736e-04  Data: 0.014 (0.012)
Train: 181 [ 250/1251 ( 20%)]  Loss:  2.961337 (3.0177)  Time: 1.096s,  934.32/s  (1.109s,  923.76/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 300/1251 ( 24%)]  Loss:  3.134265 (3.0344)  Time: 1.096s,  934.33/s  (1.108s,  924.16/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 350/1251 ( 28%)]  Loss:  3.122776 (3.0454)  Time: 1.099s,  931.83/s  (1.110s,  922.37/s)  LR: 1.736e-04  Data: 0.013 (0.012)
Train: 181 [ 400/1251 ( 32%)]  Loss:  3.353779 (3.0797)  Time: 1.131s,  905.25/s  (1.110s,  922.26/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 450/1251 ( 36%)]  Loss:  3.110594 (3.0828)  Time: 1.099s,  932.05/s  (1.110s,  922.82/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [ 500/1251 ( 40%)]  Loss:  3.172235 (3.0909)  Time: 1.098s,  932.64/s  (1.109s,  923.44/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [ 550/1251 ( 44%)]  Loss:  3.042420 (3.0869)  Time: 1.195s,  857.13/s  (1.110s,  922.83/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 600/1251 ( 48%)]  Loss:  3.338502 (3.1062)  Time: 1.120s,  914.47/s  (1.109s,  923.02/s)  LR: 1.736e-04  Data: 0.010 (0.012)
Train: 181 [ 650/1251 ( 52%)]  Loss:  3.087963 (3.1049)  Time: 1.098s,  932.75/s  (1.109s,  923.37/s)  LR: 1.736e-04  Data: 0.014 (0.012)
Train: 181 [ 700/1251 ( 56%)]  Loss:  3.115429 (3.1056)  Time: 1.100s,  930.63/s  (1.109s,  923.40/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 750/1251 ( 60%)]  Loss:  2.976983 (3.0976)  Time: 1.101s,  930.19/s  (1.109s,  922.94/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 800/1251 ( 64%)]  Loss:  3.153796 (3.1009)  Time: 1.098s,  932.80/s  (1.109s,  923.27/s)  LR: 1.736e-04  Data: 0.013 (0.012)
Train: 181 [ 850/1251 ( 68%)]  Loss:  3.074131 (3.0994)  Time: 1.097s,  933.05/s  (1.109s,  923.37/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [ 900/1251 ( 72%)]  Loss:  3.126482 (3.1008)  Time: 1.096s,  934.42/s  (1.109s,  923.43/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Train: 181 [ 950/1251 ( 76%)]  Loss:  3.345452 (3.1131)  Time: 1.095s,  934.78/s  (1.109s,  923.77/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [1000/1251 ( 80%)]  Loss:  3.055224 (3.1103)  Time: 1.120s,  914.25/s  (1.109s,  923.56/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [1050/1251 ( 84%)]  Loss:  3.054464 (3.1078)  Time: 1.183s,  865.57/s  (1.109s,  923.22/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [1100/1251 ( 88%)]  Loss:  3.264551 (3.1146)  Time: 1.103s,  928.39/s  (1.109s,  923.39/s)  LR: 1.736e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 181 [1150/1251 ( 92%)]  Loss:  3.240924 (3.1198)  Time: 1.096s,  934.20/s  (1.109s,  923.71/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [1200/1251 ( 96%)]  Loss:  3.089882 (3.1186)  Time: 1.100s,  930.56/s  (1.109s,  923.63/s)  LR: 1.736e-04  Data: 0.012 (0.012)
Train: 181 [1250/1251 (100%)]  Loss:  3.047650 (3.1159)  Time: 1.104s,  927.29/s  (1.109s,  923.53/s)  LR: 1.736e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.338 (3.338)  Loss:  0.4521 (0.4521)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.230 (0.401)  Loss:  0.5738 (0.9288)  Acc@1: 87.5000 (79.0240)  Acc@5: 97.7594 (94.7580)
Test (EMA): [   0/48]  Time: 3.314 (3.314)  Loss:  0.4038 (0.4038)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5427 (0.8410)  Acc@1: 87.7358 (80.4900)  Acc@5: 97.7594 (95.3820)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 80.29599997314453)

Train: 182 [   0/1251 (  0%)]  Loss:  3.234469 (3.2345)  Time: 1.103s,  928.44/s  (1.103s,  928.44/s)  LR: 1.711e-04  Data: 0.021 (0.021)
Train: 182 [  50/1251 (  4%)]  Loss:  3.241232 (3.2379)  Time: 1.095s,  934.85/s  (1.110s,  922.94/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [ 100/1251 (  8%)]  Loss:  2.967701 (3.1478)  Time: 1.130s,  906.55/s  (1.111s,  921.72/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 150/1251 ( 12%)]  Loss:  3.068131 (3.1279)  Time: 1.190s,  860.34/s  (1.115s,  918.39/s)  LR: 1.711e-04  Data: 0.013 (0.012)
Train: 182 [ 200/1251 ( 16%)]  Loss:  3.267154 (3.1557)  Time: 1.100s,  930.56/s  (1.113s,  920.12/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 250/1251 ( 20%)]  Loss:  3.370491 (3.1915)  Time: 1.120s,  914.39/s  (1.111s,  921.90/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [ 300/1251 ( 24%)]  Loss:  3.235847 (3.1979)  Time: 1.097s,  933.39/s  (1.111s,  921.36/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [ 350/1251 ( 28%)]  Loss:  3.127807 (3.1891)  Time: 1.096s,  933.95/s  (1.111s,  921.85/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [ 400/1251 ( 32%)]  Loss:  2.856241 (3.1521)  Time: 1.094s,  935.97/s  (1.110s,  922.55/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 450/1251 ( 36%)]  Loss:  3.163246 (3.1532)  Time: 1.097s,  933.11/s  (1.110s,  922.44/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 500/1251 ( 40%)]  Loss:  3.434395 (3.1788)  Time: 1.092s,  937.53/s  (1.110s,  922.54/s)  LR: 1.711e-04  Data: 0.010 (0.012)
Train: 182 [ 550/1251 ( 44%)]  Loss:  3.057759 (3.1687)  Time: 1.205s,  849.67/s  (1.109s,  923.20/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 600/1251 ( 48%)]  Loss:  3.099911 (3.1634)  Time: 1.095s,  935.51/s  (1.110s,  922.76/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [ 650/1251 ( 52%)]  Loss:  3.147315 (3.1623)  Time: 1.105s,  926.44/s  (1.109s,  923.13/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [ 700/1251 ( 56%)]  Loss:  3.217185 (3.1659)  Time: 1.096s,  934.27/s  (1.109s,  923.37/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 750/1251 ( 60%)]  Loss:  3.153403 (3.1651)  Time: 1.095s,  935.57/s  (1.109s,  923.25/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 800/1251 ( 64%)]  Loss:  3.280818 (3.1719)  Time: 1.105s,  926.86/s  (1.109s,  923.38/s)  LR: 1.711e-04  Data: 0.010 (0.012)
Train: 182 [ 850/1251 ( 68%)]  Loss:  2.892896 (3.1564)  Time: 1.098s,  932.37/s  (1.109s,  923.61/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 900/1251 ( 72%)]  Loss:  2.879471 (3.1419)  Time: 1.099s,  931.52/s  (1.108s,  923.87/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [ 950/1251 ( 76%)]  Loss:  3.449155 (3.1572)  Time: 1.106s,  925.69/s  (1.108s,  923.91/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [1000/1251 ( 80%)]  Loss:  3.040320 (3.1517)  Time: 1.175s,  871.67/s  (1.108s,  924.11/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [1050/1251 ( 84%)]  Loss:  3.284150 (3.1577)  Time: 1.097s,  933.79/s  (1.108s,  924.16/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [1100/1251 ( 88%)]  Loss:  3.022350 (3.1518)  Time: 1.100s,  930.75/s  (1.108s,  924.39/s)  LR: 1.711e-04  Data: 0.014 (0.012)
Train: 182 [1150/1251 ( 92%)]  Loss:  3.300140 (3.1580)  Time: 1.098s,  932.26/s  (1.108s,  924.41/s)  LR: 1.711e-04  Data: 0.011 (0.012)
Train: 182 [1200/1251 ( 96%)]  Loss:  2.971742 (3.1505)  Time: 1.095s,  935.39/s  (1.108s,  924.57/s)  LR: 1.711e-04  Data: 0.012 (0.012)
Train: 182 [1250/1251 (100%)]  Loss:  3.020121 (3.1455)  Time: 1.082s,  946.43/s  (1.107s,  924.61/s)  LR: 1.711e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.301 (3.301)  Loss:  0.4261 (0.4261)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.5652 (0.9069)  Acc@1: 86.4387 (79.1920)  Acc@5: 97.7594 (94.7440)
Test (EMA): [   0/48]  Time: 3.260 (3.260)  Loss:  0.4040 (0.4040)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5424 (0.8405)  Acc@1: 87.6179 (80.5220)  Acc@5: 97.7594 (95.3760)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 80.30000005126954)

Train: 183 [   0/1251 (  0%)]  Loss:  3.064760 (3.0648)  Time: 1.105s,  926.79/s  (1.105s,  926.79/s)  LR: 1.687e-04  Data: 0.024 (0.024)
Train: 183 [  50/1251 (  4%)]  Loss:  2.871452 (2.9681)  Time: 1.126s,  909.73/s  (1.118s,  915.72/s)  LR: 1.687e-04  Data: 0.010 (0.012)
Train: 183 [ 100/1251 (  8%)]  Loss:  3.053277 (2.9965)  Time: 1.120s,  913.88/s  (1.114s,  918.81/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [ 150/1251 ( 12%)]  Loss:  3.144952 (3.0336)  Time: 1.100s,  930.91/s  (1.114s,  919.17/s)  LR: 1.687e-04  Data: 0.010 (0.012)
Train: 183 [ 200/1251 ( 16%)]  Loss:  3.196205 (3.0661)  Time: 1.095s,  935.18/s  (1.112s,  920.84/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [ 250/1251 ( 20%)]  Loss:  3.344986 (3.1126)  Time: 1.134s,  902.99/s  (1.112s,  921.27/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [ 300/1251 ( 24%)]  Loss:  3.240170 (3.1308)  Time: 1.115s,  918.52/s  (1.111s,  921.55/s)  LR: 1.687e-04  Data: 0.010 (0.012)
Train: 183 [ 350/1251 ( 28%)]  Loss:  3.164953 (3.1351)  Time: 1.106s,  926.21/s  (1.110s,  922.53/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Train: 183 [ 400/1251 ( 32%)]  Loss:  2.876761 (3.1064)  Time: 1.149s,  891.12/s  (1.110s,  922.57/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [ 450/1251 ( 36%)]  Loss:  3.024016 (3.0982)  Time: 1.097s,  933.13/s  (1.110s,  922.83/s)  LR: 1.687e-04  Data: 0.014 (0.012)
Train: 183 [ 500/1251 ( 40%)]  Loss:  2.981596 (3.0876)  Time: 1.209s,  846.84/s  (1.110s,  922.85/s)  LR: 1.687e-04  Data: 0.010 (0.012)
Train: 183 [ 550/1251 ( 44%)]  Loss:  3.134105 (3.0914)  Time: 1.127s,  908.40/s  (1.111s,  921.65/s)  LR: 1.687e-04  Data: 0.010 (0.012)
Train: 183 [ 600/1251 ( 48%)]  Loss:  3.109916 (3.0929)  Time: 1.101s,  930.31/s  (1.112s,  921.17/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [ 650/1251 ( 52%)]  Loss:  2.975160 (3.0845)  Time: 1.097s,  933.53/s  (1.111s,  921.48/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 183 [ 700/1251 ( 56%)]  Loss:  3.087892 (3.0847)  Time: 1.132s,  904.67/s  (1.111s,  921.30/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Train: 183 [ 750/1251 ( 60%)]  Loss:  2.981038 (3.0782)  Time: 1.096s,  934.43/s  (1.111s,  921.47/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Train: 183 [ 800/1251 ( 64%)]  Loss:  3.301172 (3.0913)  Time: 1.096s,  934.27/s  (1.111s,  921.67/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [ 850/1251 ( 68%)]  Loss:  3.046023 (3.0888)  Time: 1.097s,  933.26/s  (1.111s,  921.86/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Train: 183 [ 900/1251 ( 72%)]  Loss:  3.126309 (3.0908)  Time: 1.103s,  928.39/s  (1.111s,  921.97/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 183 [ 950/1251 ( 76%)]  Loss:  3.161404 (3.0943)  Time: 1.196s,  856.07/s  (1.110s,  922.14/s)  LR: 1.687e-04  Data: 0.010 (0.012)
Train: 183 [1000/1251 ( 80%)]  Loss:  3.224460 (3.1005)  Time: 1.100s,  930.96/s  (1.110s,  922.45/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Train: 183 [1050/1251 ( 84%)]  Loss:  3.155330 (3.1030)  Time: 1.097s,  933.25/s  (1.110s,  922.72/s)  LR: 1.687e-04  Data: 0.012 (0.012)
Train: 183 [1100/1251 ( 88%)]  Loss:  3.289271 (3.1111)  Time: 1.093s,  937.21/s  (1.110s,  922.48/s)  LR: 1.687e-04  Data: 0.009 (0.012)
Train: 183 [1150/1251 ( 92%)]  Loss:  2.872622 (3.1012)  Time: 1.096s,  934.69/s  (1.110s,  922.76/s)  LR: 1.687e-04  Data: 0.011 (0.012)
Train: 183 [1200/1251 ( 96%)]  Loss:  3.169024 (3.1039)  Time: 1.096s,  934.62/s  (1.110s,  922.74/s)  LR: 1.687e-04  Data: 0.013 (0.012)
Train: 183 [1250/1251 (100%)]  Loss:  2.954760 (3.0981)  Time: 1.080s,  947.95/s  (1.110s,  922.63/s)  LR: 1.687e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.232 (3.232)  Loss:  0.4560 (0.4560)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5852 (0.9133)  Acc@1: 87.0283 (79.2160)  Acc@5: 97.2877 (94.7820)
Test (EMA): [   0/48]  Time: 3.063 (3.063)  Loss:  0.4039 (0.4039)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5417 (0.8398)  Acc@1: 87.2641 (80.4960)  Acc@5: 97.7594 (95.3900)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 80.307999921875)

Train: 184 [   0/1251 (  0%)]  Loss:  2.614224 (2.6142)  Time: 1.108s,  923.89/s  (1.108s,  923.89/s)  LR: 1.662e-04  Data: 0.024 (0.024)
Train: 184 [  50/1251 (  4%)]  Loss:  3.293310 (2.9538)  Time: 1.104s,  927.61/s  (1.109s,  923.04/s)  LR: 1.662e-04  Data: 0.011 (0.011)
Train: 184 [ 100/1251 (  8%)]  Loss:  2.990959 (2.9662)  Time: 1.126s,  909.80/s  (1.107s,  924.76/s)  LR: 1.662e-04  Data: 0.011 (0.011)
Train: 184 [ 150/1251 ( 12%)]  Loss:  3.345695 (3.0610)  Time: 1.108s,  923.83/s  (1.109s,  923.66/s)  LR: 1.662e-04  Data: 0.010 (0.011)
Train: 184 [ 200/1251 ( 16%)]  Loss:  2.961924 (3.0412)  Time: 1.095s,  934.78/s  (1.108s,  924.56/s)  LR: 1.662e-04  Data: 0.012 (0.011)
Train: 184 [ 250/1251 ( 20%)]  Loss:  3.225374 (3.0719)  Time: 1.095s,  934.92/s  (1.108s,  923.99/s)  LR: 1.662e-04  Data: 0.012 (0.011)
Train: 184 [ 300/1251 ( 24%)]  Loss:  3.288211 (3.1028)  Time: 1.103s,  928.76/s  (1.108s,  923.91/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [ 350/1251 ( 28%)]  Loss:  2.915180 (3.0794)  Time: 1.176s,  870.72/s  (1.108s,  924.08/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [ 400/1251 ( 32%)]  Loss:  3.113486 (3.0832)  Time: 1.099s,  931.73/s  (1.108s,  924.58/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [ 450/1251 ( 36%)]  Loss:  3.302136 (3.1050)  Time: 1.094s,  936.05/s  (1.107s,  924.75/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [ 500/1251 ( 40%)]  Loss:  3.052165 (3.1002)  Time: 1.090s,  939.66/s  (1.107s,  924.94/s)  LR: 1.662e-04  Data: 0.010 (0.012)
Train: 184 [ 550/1251 ( 44%)]  Loss:  3.228770 (3.1110)  Time: 1.119s,  914.70/s  (1.108s,  924.55/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [ 600/1251 ( 48%)]  Loss:  3.246058 (3.1213)  Time: 1.134s,  903.40/s  (1.108s,  924.60/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [ 650/1251 ( 52%)]  Loss:  3.242010 (3.1300)  Time: 1.097s,  933.05/s  (1.108s,  924.21/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [ 700/1251 ( 56%)]  Loss:  3.225990 (3.1364)  Time: 1.101s,  930.23/s  (1.108s,  924.32/s)  LR: 1.662e-04  Data: 0.010 (0.012)
Train: 184 [ 750/1251 ( 60%)]  Loss:  3.145224 (3.1369)  Time: 1.121s,  913.10/s  (1.108s,  923.89/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [ 800/1251 ( 64%)]  Loss:  3.048551 (3.1317)  Time: 1.095s,  934.90/s  (1.108s,  923.90/s)  LR: 1.662e-04  Data: 0.013 (0.012)
Train: 184 [ 850/1251 ( 68%)]  Loss:  2.873223 (3.1174)  Time: 1.097s,  933.62/s  (1.108s,  924.06/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [ 900/1251 ( 72%)]  Loss:  3.099740 (3.1164)  Time: 1.098s,  932.69/s  (1.108s,  924.32/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [ 950/1251 ( 76%)]  Loss:  2.892728 (3.1052)  Time: 1.130s,  905.85/s  (1.108s,  924.25/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [1000/1251 ( 80%)]  Loss:  3.322611 (3.1156)  Time: 1.098s,  932.97/s  (1.108s,  924.11/s)  LR: 1.662e-04  Data: 0.013 (0.012)
Train: 184 [1050/1251 ( 84%)]  Loss:  3.216467 (3.1202)  Time: 1.188s,  861.72/s  (1.108s,  924.15/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [1100/1251 ( 88%)]  Loss:  3.324981 (3.1291)  Time: 1.098s,  932.24/s  (1.108s,  924.01/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [1150/1251 ( 92%)]  Loss:  3.380717 (3.1396)  Time: 1.130s,  906.46/s  (1.108s,  923.89/s)  LR: 1.662e-04  Data: 0.011 (0.012)
Train: 184 [1200/1251 ( 96%)]  Loss:  3.226273 (3.1430)  Time: 1.132s,  904.74/s  (1.109s,  923.30/s)  LR: 1.662e-04  Data: 0.012 (0.012)
Train: 184 [1250/1251 (100%)]  Loss:  3.151742 (3.1434)  Time: 1.078s,  949.49/s  (1.109s,  923.00/s)  LR: 1.662e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.345 (3.345)  Loss:  0.4453 (0.4453)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.5835 (0.9121)  Acc@1: 87.0283 (79.0320)  Acc@5: 96.8160 (94.7840)
Test (EMA): [   0/48]  Time: 3.133 (3.133)  Loss:  0.4030 (0.4030)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5411 (0.8390)  Acc@1: 87.2642 (80.5020)  Acc@5: 97.7594 (95.3900)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 80.358)

Train: 185 [   0/1251 (  0%)]  Loss:  3.248003 (3.2480)  Time: 1.111s,  921.73/s  (1.111s,  921.73/s)  LR: 1.638e-04  Data: 0.030 (0.030)
Train: 185 [  50/1251 (  4%)]  Loss:  3.093690 (3.1708)  Time: 1.097s,  933.22/s  (1.107s,  925.09/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 100/1251 (  8%)]  Loss:  3.420102 (3.2539)  Time: 1.120s,  914.08/s  (1.109s,  923.76/s)  LR: 1.638e-04  Data: 0.013 (0.012)
Train: 185 [ 150/1251 ( 12%)]  Loss:  3.469306 (3.3078)  Time: 1.097s,  933.18/s  (1.109s,  923.43/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 200/1251 ( 16%)]  Loss:  3.278444 (3.3019)  Time: 1.129s,  907.20/s  (1.111s,  921.57/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 250/1251 ( 20%)]  Loss:  3.192790 (3.2837)  Time: 1.099s,  932.17/s  (1.110s,  922.26/s)  LR: 1.638e-04  Data: 0.014 (0.012)
Train: 185 [ 300/1251 ( 24%)]  Loss:  2.971264 (3.2391)  Time: 1.190s,  860.78/s  (1.110s,  922.55/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 350/1251 ( 28%)]  Loss:  2.969974 (3.2054)  Time: 1.104s,  927.47/s  (1.110s,  922.54/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 400/1251 ( 32%)]  Loss:  3.052860 (3.1885)  Time: 1.099s,  932.09/s  (1.109s,  923.45/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 450/1251 ( 36%)]  Loss:  3.424474 (3.2121)  Time: 1.095s,  935.51/s  (1.110s,  922.88/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 500/1251 ( 40%)]  Loss:  3.135126 (3.2051)  Time: 1.099s,  931.81/s  (1.109s,  923.55/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 550/1251 ( 44%)]  Loss:  3.021666 (3.1898)  Time: 1.097s,  933.57/s  (1.108s,  924.00/s)  LR: 1.638e-04  Data: 0.013 (0.012)
Train: 185 [ 600/1251 ( 48%)]  Loss:  3.077247 (3.1811)  Time: 1.091s,  938.92/s  (1.108s,  924.17/s)  LR: 1.638e-04  Data: 0.010 (0.012)
Train: 185 [ 650/1251 ( 52%)]  Loss:  3.233069 (3.1849)  Time: 1.102s,  929.00/s  (1.108s,  924.39/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 700/1251 ( 56%)]  Loss:  2.981041 (3.1713)  Time: 1.138s,  899.47/s  (1.109s,  923.77/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 750/1251 ( 60%)]  Loss:  3.141600 (3.1694)  Time: 1.096s,  934.33/s  (1.108s,  924.00/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 800/1251 ( 64%)]  Loss:  2.986789 (3.1587)  Time: 1.119s,  914.99/s  (1.108s,  923.80/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 850/1251 ( 68%)]  Loss:  2.772555 (3.1372)  Time: 1.095s,  935.20/s  (1.108s,  924.08/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [ 900/1251 ( 72%)]  Loss:  2.792320 (3.1191)  Time: 1.131s,  905.30/s  (1.108s,  924.30/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [ 950/1251 ( 76%)]  Loss:  3.386896 (3.1325)  Time: 1.097s,  933.31/s  (1.108s,  924.20/s)  LR: 1.638e-04  Data: 0.012 (0.012)
Train: 185 [1000/1251 ( 80%)]  Loss:  3.057387 (3.1289)  Time: 1.096s,  934.67/s  (1.108s,  924.33/s)  LR: 1.638e-04  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 185 [1050/1251 ( 84%)]  Loss:  3.080193 (3.1267)  Time: 1.130s,  906.32/s  (1.109s,  923.71/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [1100/1251 ( 88%)]  Loss:  3.133939 (3.1270)  Time: 1.093s,  936.52/s  (1.109s,  923.77/s)  LR: 1.638e-04  Data: 0.010 (0.012)
Train: 185 [1150/1251 ( 92%)]  Loss:  3.153677 (3.1281)  Time: 1.097s,  933.39/s  (1.108s,  923.96/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [1200/1251 ( 96%)]  Loss:  3.137883 (3.1285)  Time: 1.096s,  934.36/s  (1.108s,  924.22/s)  LR: 1.638e-04  Data: 0.011 (0.012)
Train: 185 [1250/1251 (100%)]  Loss:  3.395966 (3.1388)  Time: 1.078s,  950.19/s  (1.108s,  924.25/s)  LR: 1.638e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.239 (3.239)  Loss:  0.4564 (0.4564)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6080 (0.9247)  Acc@1: 86.7925 (79.1140)  Acc@5: 97.4057 (94.8660)
Test (EMA): [   0/48]  Time: 3.166 (3.166)  Loss:  0.4026 (0.4026)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.5408 (0.8384)  Acc@1: 87.3821 (80.5400)  Acc@5: 97.8774 (95.4000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 80.379999921875)

Train: 186 [   0/1251 (  0%)]  Loss:  3.191074 (3.1911)  Time: 1.107s,  925.34/s  (1.107s,  925.34/s)  LR: 1.614e-04  Data: 0.023 (0.023)
Train: 186 [  50/1251 (  4%)]  Loss:  3.133953 (3.1625)  Time: 1.096s,  934.56/s  (1.109s,  923.48/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 100/1251 (  8%)]  Loss:  2.888100 (3.0710)  Time: 1.095s,  935.56/s  (1.107s,  924.70/s)  LR: 1.614e-04  Data: 0.010 (0.012)
Train: 186 [ 150/1251 ( 12%)]  Loss:  3.068485 (3.0704)  Time: 1.135s,  902.47/s  (1.109s,  923.60/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 200/1251 ( 16%)]  Loss:  3.106317 (3.0776)  Time: 1.101s,  929.80/s  (1.109s,  923.34/s)  LR: 1.614e-04  Data: 0.014 (0.012)
Train: 186 [ 250/1251 ( 20%)]  Loss:  3.167020 (3.0925)  Time: 1.098s,  932.34/s  (1.109s,  923.77/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 300/1251 ( 24%)]  Loss:  2.856051 (3.0587)  Time: 1.095s,  935.16/s  (1.108s,  924.23/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 350/1251 ( 28%)]  Loss:  3.044724 (3.0570)  Time: 1.121s,  913.65/s  (1.109s,  923.74/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 400/1251 ( 32%)]  Loss:  2.994586 (3.0500)  Time: 1.101s,  930.04/s  (1.109s,  923.44/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 450/1251 ( 36%)]  Loss:  2.938961 (3.0389)  Time: 1.094s,  935.61/s  (1.108s,  923.82/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 500/1251 ( 40%)]  Loss:  3.251220 (3.0582)  Time: 1.088s,  940.80/s  (1.109s,  923.51/s)  LR: 1.614e-04  Data: 0.009 (0.012)
Train: 186 [ 550/1251 ( 44%)]  Loss:  2.750233 (3.0326)  Time: 1.098s,  932.90/s  (1.109s,  923.19/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 600/1251 ( 48%)]  Loss:  3.098706 (3.0376)  Time: 1.098s,  932.94/s  (1.109s,  923.22/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 650/1251 ( 52%)]  Loss:  3.454104 (3.0674)  Time: 1.102s,  928.86/s  (1.109s,  923.39/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 700/1251 ( 56%)]  Loss:  2.938080 (3.0588)  Time: 1.095s,  934.95/s  (1.108s,  923.80/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 750/1251 ( 60%)]  Loss:  3.368373 (3.0781)  Time: 1.098s,  932.68/s  (1.108s,  923.98/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 800/1251 ( 64%)]  Loss:  2.980404 (3.0724)  Time: 1.118s,  916.05/s  (1.108s,  924.27/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 850/1251 ( 68%)]  Loss:  3.192630 (3.0791)  Time: 1.098s,  932.85/s  (1.108s,  923.86/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [ 900/1251 ( 72%)]  Loss:  3.100006 (3.0802)  Time: 1.095s,  935.24/s  (1.108s,  924.07/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [ 950/1251 ( 76%)]  Loss:  3.001840 (3.0762)  Time: 1.107s,  925.27/s  (1.108s,  923.88/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [1000/1251 ( 80%)]  Loss:  3.168659 (3.0806)  Time: 1.098s,  932.56/s  (1.109s,  923.62/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [1050/1251 ( 84%)]  Loss:  3.356231 (3.0932)  Time: 1.098s,  932.62/s  (1.109s,  923.67/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [1100/1251 ( 88%)]  Loss:  2.997314 (3.0890)  Time: 1.097s,  933.79/s  (1.108s,  923.90/s)  LR: 1.614e-04  Data: 0.013 (0.012)
Train: 186 [1150/1251 ( 92%)]  Loss:  3.307146 (3.0981)  Time: 1.097s,  933.17/s  (1.108s,  924.10/s)  LR: 1.614e-04  Data: 0.011 (0.012)
Train: 186 [1200/1251 ( 96%)]  Loss:  3.082680 (3.0975)  Time: 1.100s,  930.83/s  (1.108s,  923.94/s)  LR: 1.614e-04  Data: 0.012 (0.012)
Train: 186 [1250/1251 (100%)]  Loss:  2.962917 (3.0923)  Time: 1.081s,  947.67/s  (1.108s,  923.98/s)  LR: 1.614e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.234 (3.234)  Loss:  0.4547 (0.4547)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.6103 (0.9159)  Acc@1: 87.0283 (79.2320)  Acc@5: 97.5236 (94.8900)
Test (EMA): [   0/48]  Time: 3.247 (3.247)  Loss:  0.4025 (0.4025)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5399 (0.8375)  Acc@1: 87.2642 (80.5620)  Acc@5: 97.9953 (95.4140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 80.40799997314453)

Train: 187 [   0/1251 (  0%)]  Loss:  3.216482 (3.2165)  Time: 1.137s,  900.69/s  (1.137s,  900.69/s)  LR: 1.590e-04  Data: 0.029 (0.029)
Train: 187 [  50/1251 (  4%)]  Loss:  3.163586 (3.1900)  Time: 1.099s,  932.06/s  (1.109s,  923.07/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 100/1251 (  8%)]  Loss:  3.232260 (3.2041)  Time: 1.131s,  905.37/s  (1.114s,  919.56/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 150/1251 ( 12%)]  Loss:  3.091823 (3.1760)  Time: 1.097s,  933.15/s  (1.113s,  919.70/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [ 200/1251 ( 16%)]  Loss:  3.042612 (3.1494)  Time: 1.098s,  932.22/s  (1.110s,  922.24/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 250/1251 ( 20%)]  Loss:  3.014222 (3.1268)  Time: 1.133s,  904.12/s  (1.111s,  921.98/s)  LR: 1.590e-04  Data: 0.010 (0.012)
Train: 187 [ 300/1251 ( 24%)]  Loss:  3.069167 (3.1186)  Time: 1.094s,  936.08/s  (1.112s,  920.92/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 350/1251 ( 28%)]  Loss:  3.240181 (3.1338)  Time: 1.095s,  935.31/s  (1.112s,  920.53/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [ 400/1251 ( 32%)]  Loss:  2.616142 (3.0763)  Time: 1.099s,  931.71/s  (1.112s,  921.08/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [ 450/1251 ( 36%)]  Loss:  3.375367 (3.1062)  Time: 1.119s,  914.82/s  (1.112s,  920.71/s)  LR: 1.590e-04  Data: 0.010 (0.012)
Train: 187 [ 500/1251 ( 40%)]  Loss:  3.146893 (3.1099)  Time: 1.096s,  934.53/s  (1.113s,  920.35/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 550/1251 ( 44%)]  Loss:  3.050759 (3.1050)  Time: 1.098s,  932.21/s  (1.112s,  921.06/s)  LR: 1.590e-04  Data: 0.014 (0.012)
Train: 187 [ 600/1251 ( 48%)]  Loss:  2.740215 (3.0769)  Time: 1.096s,  934.06/s  (1.111s,  921.54/s)  LR: 1.590e-04  Data: 0.013 (0.012)
Train: 187 [ 650/1251 ( 52%)]  Loss:  3.005556 (3.0718)  Time: 1.101s,  929.94/s  (1.111s,  921.90/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 700/1251 ( 56%)]  Loss:  3.077983 (3.0722)  Time: 1.107s,  925.09/s  (1.110s,  922.30/s)  LR: 1.590e-04  Data: 0.010 (0.012)
Train: 187 [ 750/1251 ( 60%)]  Loss:  3.326484 (3.0881)  Time: 1.099s,  931.44/s  (1.110s,  922.65/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 800/1251 ( 64%)]  Loss:  3.229244 (3.0964)  Time: 1.198s,  854.98/s  (1.110s,  922.17/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 850/1251 ( 68%)]  Loss:  3.265928 (3.1058)  Time: 1.096s,  934.27/s  (1.110s,  922.20/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [ 900/1251 ( 72%)]  Loss:  2.924589 (3.0963)  Time: 1.125s,  910.38/s  (1.110s,  922.44/s)  LR: 1.590e-04  Data: 0.010 (0.012)
Train: 187 [ 950/1251 ( 76%)]  Loss:  2.955650 (3.0893)  Time: 1.097s,  933.59/s  (1.110s,  922.64/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [1000/1251 ( 80%)]  Loss:  2.860271 (3.0784)  Time: 1.118s,  915.60/s  (1.110s,  922.70/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [1050/1251 ( 84%)]  Loss:  2.904986 (3.0705)  Time: 1.097s,  933.29/s  (1.109s,  922.96/s)  LR: 1.590e-04  Data: 0.011 (0.012)
Train: 187 [1100/1251 ( 88%)]  Loss:  3.411367 (3.0853)  Time: 1.096s,  934.27/s  (1.109s,  923.08/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [1150/1251 ( 92%)]  Loss:  3.032198 (3.0831)  Time: 1.099s,  932.11/s  (1.109s,  923.21/s)  LR: 1.590e-04  Data: 0.012 (0.012)
Train: 187 [1200/1251 ( 96%)]  Loss:  3.376953 (3.0948)  Time: 1.099s,  932.15/s  (1.109s,  923.47/s)  LR: 1.590e-04  Data: 0.013 (0.012)
Train: 187 [1250/1251 (100%)]  Loss:  3.093436 (3.0948)  Time: 1.090s,  939.68/s  (1.109s,  923.51/s)  LR: 1.590e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.247 (3.247)  Loss:  0.4185 (0.4185)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5643 (0.8960)  Acc@1: 86.7925 (79.1560)  Acc@5: 97.5236 (94.8060)
Test (EMA): [   0/48]  Time: 3.117 (3.117)  Loss:  0.4020 (0.4020)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5392 (0.8366)  Acc@1: 87.2642 (80.5700)  Acc@5: 97.9953 (95.4200)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 80.443999921875)

Train: 188 [   0/1251 (  0%)]  Loss:  2.716131 (2.7161)  Time: 1.103s,  928.34/s  (1.103s,  928.34/s)  LR: 1.566e-04  Data: 0.023 (0.023)
Train: 188 [  50/1251 (  4%)]  Loss:  3.236671 (2.9764)  Time: 1.096s,  934.16/s  (1.101s,  930.27/s)  LR: 1.566e-04  Data: 0.012 (0.012)
Train: 188 [ 100/1251 (  8%)]  Loss:  3.389125 (3.1140)  Time: 1.113s,  919.70/s  (1.104s,  927.32/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 150/1251 ( 12%)]  Loss:  2.973322 (3.0788)  Time: 1.093s,  936.59/s  (1.105s,  926.83/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 200/1251 ( 16%)]  Loss:  3.276855 (3.1184)  Time: 1.102s,  928.86/s  (1.104s,  927.15/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 250/1251 ( 20%)]  Loss:  2.975634 (3.0946)  Time: 1.100s,  931.21/s  (1.104s,  927.63/s)  LR: 1.566e-04  Data: 0.013 (0.012)
Train: 188 [ 300/1251 ( 24%)]  Loss:  3.271495 (3.1199)  Time: 1.097s,  933.18/s  (1.107s,  925.36/s)  LR: 1.566e-04  Data: 0.012 (0.012)
Train: 188 [ 350/1251 ( 28%)]  Loss:  3.396930 (3.1545)  Time: 1.123s,  912.16/s  (1.106s,  925.64/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 400/1251 ( 32%)]  Loss:  2.835196 (3.1190)  Time: 1.112s,  920.50/s  (1.106s,  926.18/s)  LR: 1.566e-04  Data: 0.012 (0.012)
Train: 188 [ 450/1251 ( 36%)]  Loss:  2.992370 (3.1064)  Time: 1.105s,  927.04/s  (1.105s,  926.33/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 500/1251 ( 40%)]  Loss:  3.301753 (3.1241)  Time: 1.100s,  931.09/s  (1.106s,  926.23/s)  LR: 1.566e-04  Data: 0.014 (0.012)
Train: 188 [ 550/1251 ( 44%)]  Loss:  3.223603 (3.1324)  Time: 1.133s,  903.75/s  (1.106s,  925.99/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 600/1251 ( 48%)]  Loss:  3.107901 (3.1305)  Time: 1.098s,  932.19/s  (1.106s,  925.89/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 650/1251 ( 52%)]  Loss:  3.420219 (3.1512)  Time: 1.118s,  915.77/s  (1.106s,  925.76/s)  LR: 1.566e-04  Data: 0.012 (0.012)
Train: 188 [ 700/1251 ( 56%)]  Loss:  3.113420 (3.1487)  Time: 1.105s,  927.08/s  (1.106s,  925.94/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 750/1251 ( 60%)]  Loss:  2.893801 (3.1328)  Time: 1.099s,  931.79/s  (1.106s,  926.17/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 800/1251 ( 64%)]  Loss:  2.913880 (3.1199)  Time: 1.107s,  925.42/s  (1.105s,  926.36/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [ 850/1251 ( 68%)]  Loss:  3.284529 (3.1290)  Time: 1.096s,  933.99/s  (1.106s,  926.03/s)  LR: 1.566e-04  Data: 0.012 (0.012)
Train: 188 [ 900/1251 ( 72%)]  Loss:  2.942098 (3.1192)  Time: 1.096s,  934.59/s  (1.106s,  926.15/s)  LR: 1.566e-04  Data: 0.014 (0.012)
Train: 188 [ 950/1251 ( 76%)]  Loss:  3.156594 (3.1211)  Time: 1.094s,  936.05/s  (1.106s,  925.95/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [1000/1251 ( 80%)]  Loss:  3.308009 (3.1300)  Time: 1.099s,  931.95/s  (1.106s,  925.80/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [1050/1251 ( 84%)]  Loss:  3.054718 (3.1266)  Time: 1.099s,  931.91/s  (1.106s,  925.90/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [1100/1251 ( 88%)]  Loss:  3.238839 (3.1314)  Time: 1.098s,  932.82/s  (1.106s,  925.76/s)  LR: 1.566e-04  Data: 0.012 (0.012)
Train: 188 [1150/1251 ( 92%)]  Loss:  3.052509 (3.1282)  Time: 1.122s,  912.79/s  (1.106s,  925.72/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [1200/1251 ( 96%)]  Loss:  3.158196 (3.1294)  Time: 1.098s,  932.68/s  (1.106s,  925.73/s)  LR: 1.566e-04  Data: 0.011 (0.012)
Train: 188 [1250/1251 (100%)]  Loss:  3.185307 (3.1315)  Time: 1.080s,  948.08/s  (1.106s,  925.83/s)  LR: 1.566e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.319 (3.319)  Loss:  0.4688 (0.4688)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5922 (0.9257)  Acc@1: 86.9104 (79.2060)  Acc@5: 97.7594 (94.7720)
Test (EMA): [   0/48]  Time: 3.136 (3.136)  Loss:  0.4017 (0.4017)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5382 (0.8360)  Acc@1: 87.3821 (80.6280)  Acc@5: 97.9953 (95.4400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 80.48999997314453)

Train: 189 [   0/1251 (  0%)]  Loss:  3.035871 (3.0359)  Time: 1.104s,  927.82/s  (1.104s,  927.82/s)  LR: 1.542e-04  Data: 0.020 (0.020)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 189 [  50/1251 (  4%)]  Loss:  3.259037 (3.1475)  Time: 1.098s,  932.55/s  (1.106s,  926.01/s)  LR: 1.542e-04  Data: 0.011 (0.012)
Train: 189 [ 100/1251 (  8%)]  Loss:  3.162714 (3.1525)  Time: 1.142s,  896.37/s  (1.104s,  927.32/s)  LR: 1.542e-04  Data: 0.010 (0.012)
Train: 189 [ 150/1251 ( 12%)]  Loss:  3.174446 (3.1580)  Time: 1.095s,  934.86/s  (1.104s,  927.64/s)  LR: 1.542e-04  Data: 0.012 (0.012)
Train: 189 [ 200/1251 ( 16%)]  Loss:  3.116007 (3.1496)  Time: 1.119s,  915.15/s  (1.105s,  926.97/s)  LR: 1.542e-04  Data: 0.010 (0.011)
Train: 189 [ 250/1251 ( 20%)]  Loss:  3.055827 (3.1340)  Time: 1.098s,  933.01/s  (1.106s,  926.12/s)  LR: 1.542e-04  Data: 0.012 (0.011)
Train: 189 [ 300/1251 ( 24%)]  Loss:  3.214365 (3.1455)  Time: 1.119s,  915.36/s  (1.107s,  925.39/s)  LR: 1.542e-04  Data: 0.010 (0.011)
Train: 189 [ 350/1251 ( 28%)]  Loss:  3.175541 (3.1492)  Time: 1.096s,  934.55/s  (1.107s,  924.65/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [ 400/1251 ( 32%)]  Loss:  3.087561 (3.1424)  Time: 1.125s,  910.00/s  (1.110s,  922.79/s)  LR: 1.542e-04  Data: 0.010 (0.011)
Train: 189 [ 450/1251 ( 36%)]  Loss:  3.180077 (3.1461)  Time: 1.097s,  933.50/s  (1.110s,  922.92/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [ 500/1251 ( 40%)]  Loss:  2.987003 (3.1317)  Time: 1.097s,  933.84/s  (1.109s,  923.08/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [ 550/1251 ( 44%)]  Loss:  3.194805 (3.1369)  Time: 1.095s,  934.77/s  (1.109s,  923.56/s)  LR: 1.542e-04  Data: 0.012 (0.011)
Train: 189 [ 600/1251 ( 48%)]  Loss:  2.982114 (3.1250)  Time: 1.099s,  931.57/s  (1.109s,  923.55/s)  LR: 1.542e-04  Data: 0.015 (0.011)
Train: 189 [ 650/1251 ( 52%)]  Loss:  3.274070 (3.1357)  Time: 1.102s,  928.81/s  (1.108s,  923.78/s)  LR: 1.542e-04  Data: 0.014 (0.011)
Train: 189 [ 700/1251 ( 56%)]  Loss:  3.375533 (3.1517)  Time: 1.096s,  933.91/s  (1.108s,  923.84/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [ 750/1251 ( 60%)]  Loss:  3.048123 (3.1452)  Time: 1.106s,  926.16/s  (1.108s,  923.92/s)  LR: 1.542e-04  Data: 0.013 (0.011)
Train: 189 [ 800/1251 ( 64%)]  Loss:  3.173280 (3.1468)  Time: 1.134s,  902.91/s  (1.109s,  923.24/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [ 850/1251 ( 68%)]  Loss:  3.105509 (3.1445)  Time: 1.098s,  932.76/s  (1.109s,  923.02/s)  LR: 1.542e-04  Data: 0.012 (0.011)
Train: 189 [ 900/1251 ( 72%)]  Loss:  3.064823 (3.1404)  Time: 1.106s,  926.23/s  (1.109s,  923.10/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [ 950/1251 ( 76%)]  Loss:  2.939257 (3.1303)  Time: 1.097s,  933.71/s  (1.110s,  922.93/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [1000/1251 ( 80%)]  Loss:  2.973405 (3.1228)  Time: 1.097s,  933.27/s  (1.109s,  922.98/s)  LR: 1.542e-04  Data: 0.013 (0.011)
Train: 189 [1050/1251 ( 84%)]  Loss:  3.456235 (3.1380)  Time: 1.106s,  925.88/s  (1.109s,  923.08/s)  LR: 1.542e-04  Data: 0.012 (0.011)
Train: 189 [1100/1251 ( 88%)]  Loss:  3.419622 (3.1502)  Time: 1.212s,  844.62/s  (1.109s,  922.96/s)  LR: 1.542e-04  Data: 0.012 (0.011)
Train: 189 [1150/1251 ( 92%)]  Loss:  3.325366 (3.1575)  Time: 1.095s,  935.03/s  (1.109s,  923.11/s)  LR: 1.542e-04  Data: 0.011 (0.011)
Train: 189 [1200/1251 ( 96%)]  Loss:  3.128403 (3.1564)  Time: 1.176s,  870.51/s  (1.109s,  923.23/s)  LR: 1.542e-04  Data: 0.012 (0.011)
Train: 189 [1250/1251 (100%)]  Loss:  3.495485 (3.1694)  Time: 1.085s,  943.60/s  (1.109s,  923.33/s)  LR: 1.542e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.267 (3.267)  Loss:  0.4620 (0.4620)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.6049 (0.9191)  Acc@1: 86.5566 (79.5400)  Acc@5: 97.7594 (94.7780)
Test (EMA): [   0/48]  Time: 3.160 (3.160)  Loss:  0.4020 (0.4020)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5380 (0.8357)  Acc@1: 87.5000 (80.6860)  Acc@5: 97.9953 (95.4360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 80.49399997314453)

Train: 190 [   0/1251 (  0%)]  Loss:  3.105256 (3.1053)  Time: 1.106s,  925.70/s  (1.106s,  925.70/s)  LR: 1.518e-04  Data: 0.024 (0.024)
Train: 190 [  50/1251 (  4%)]  Loss:  3.203480 (3.1544)  Time: 1.101s,  930.14/s  (1.109s,  923.03/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 100/1251 (  8%)]  Loss:  3.261307 (3.1900)  Time: 1.099s,  931.70/s  (1.109s,  923.71/s)  LR: 1.518e-04  Data: 0.012 (0.012)
Train: 190 [ 150/1251 ( 12%)]  Loss:  3.039387 (3.1524)  Time: 1.096s,  934.65/s  (1.106s,  925.59/s)  LR: 1.518e-04  Data: 0.012 (0.012)
Train: 190 [ 200/1251 ( 16%)]  Loss:  3.365742 (3.1950)  Time: 1.100s,  931.21/s  (1.108s,  923.83/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 250/1251 ( 20%)]  Loss:  2.885265 (3.1434)  Time: 1.101s,  930.22/s  (1.109s,  923.42/s)  LR: 1.518e-04  Data: 0.017 (0.012)
Train: 190 [ 300/1251 ( 24%)]  Loss:  2.981864 (3.1203)  Time: 1.106s,  926.12/s  (1.109s,  923.23/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 350/1251 ( 28%)]  Loss:  2.991388 (3.1042)  Time: 1.125s,  910.26/s  (1.109s,  923.24/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 400/1251 ( 32%)]  Loss:  3.082808 (3.1018)  Time: 1.114s,  919.33/s  (1.109s,  923.17/s)  LR: 1.518e-04  Data: 0.015 (0.012)
Train: 190 [ 450/1251 ( 36%)]  Loss:  3.209787 (3.1126)  Time: 1.103s,  928.09/s  (1.109s,  923.44/s)  LR: 1.518e-04  Data: 0.013 (0.012)
Train: 190 [ 500/1251 ( 40%)]  Loss:  2.854943 (3.0892)  Time: 1.142s,  896.51/s  (1.109s,  923.60/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 550/1251 ( 44%)]  Loss:  3.035742 (3.0847)  Time: 1.101s,  930.06/s  (1.109s,  923.44/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 600/1251 ( 48%)]  Loss:  3.236628 (3.0964)  Time: 1.174s,  871.95/s  (1.109s,  923.56/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 650/1251 ( 52%)]  Loss:  3.079267 (3.0952)  Time: 1.097s,  933.04/s  (1.109s,  923.46/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 700/1251 ( 56%)]  Loss:  3.299380 (3.1088)  Time: 1.097s,  933.45/s  (1.109s,  923.59/s)  LR: 1.518e-04  Data: 0.014 (0.012)
Train: 190 [ 750/1251 ( 60%)]  Loss:  3.369211 (3.1251)  Time: 1.099s,  931.99/s  (1.109s,  923.52/s)  LR: 1.518e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 190 [ 800/1251 ( 64%)]  Loss:  3.073360 (3.1220)  Time: 1.105s,  927.12/s  (1.108s,  923.88/s)  LR: 1.518e-04  Data: 0.010 (0.012)
Train: 190 [ 850/1251 ( 68%)]  Loss:  3.097603 (3.1207)  Time: 1.100s,  931.17/s  (1.108s,  924.04/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [ 900/1251 ( 72%)]  Loss:  3.296071 (3.1299)  Time: 1.101s,  929.91/s  (1.108s,  924.28/s)  LR: 1.518e-04  Data: 0.012 (0.012)
Train: 190 [ 950/1251 ( 76%)]  Loss:  3.194676 (3.1332)  Time: 1.098s,  932.99/s  (1.108s,  924.22/s)  LR: 1.518e-04  Data: 0.011 (0.012)
Train: 190 [1000/1251 ( 80%)]  Loss:  3.235969 (3.1381)  Time: 1.095s,  935.46/s  (1.108s,  924.14/s)  LR: 1.518e-04  Data: 0.013 (0.012)
Train: 190 [1050/1251 ( 84%)]  Loss:  3.029053 (3.1331)  Time: 1.096s,  934.70/s  (1.108s,  924.06/s)  LR: 1.518e-04  Data: 0.010 (0.012)
Train: 190 [1100/1251 ( 88%)]  Loss:  2.906773 (3.1233)  Time: 1.098s,  932.60/s  (1.108s,  924.15/s)  LR: 1.518e-04  Data: 0.012 (0.012)
Train: 190 [1150/1251 ( 92%)]  Loss:  3.088510 (3.1218)  Time: 1.098s,  932.99/s  (1.108s,  924.02/s)  LR: 1.518e-04  Data: 0.014 (0.012)
Train: 190 [1200/1251 ( 96%)]  Loss:  2.810218 (3.1093)  Time: 1.096s,  934.41/s  (1.108s,  924.09/s)  LR: 1.518e-04  Data: 0.012 (0.012)
Train: 190 [1250/1251 (100%)]  Loss:  2.957195 (3.1035)  Time: 1.078s,  949.94/s  (1.108s,  923.84/s)  LR: 1.518e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.283 (3.283)  Loss:  0.4443 (0.4443)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.5698 (0.9089)  Acc@1: 87.0283 (79.5480)  Acc@5: 98.1132 (94.9040)
Test (EMA): [   0/48]  Time: 3.204 (3.204)  Loss:  0.4016 (0.4016)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5384 (0.8350)  Acc@1: 87.3821 (80.6740)  Acc@5: 97.9953 (95.4480)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 80.49599989746093)

Train: 191 [   0/1251 (  0%)]  Loss:  2.906646 (2.9066)  Time: 1.100s,  930.88/s  (1.100s,  930.88/s)  LR: 1.495e-04  Data: 0.019 (0.019)
Train: 191 [  50/1251 (  4%)]  Loss:  2.932656 (2.9197)  Time: 1.098s,  932.52/s  (1.111s,  921.56/s)  LR: 1.495e-04  Data: 0.014 (0.012)
Train: 191 [ 100/1251 (  8%)]  Loss:  3.339268 (3.0595)  Time: 1.097s,  933.27/s  (1.107s,  924.97/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [ 150/1251 ( 12%)]  Loss:  3.044731 (3.0558)  Time: 1.098s,  932.70/s  (1.108s,  923.98/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 200/1251 ( 16%)]  Loss:  3.150152 (3.0747)  Time: 1.098s,  932.67/s  (1.107s,  925.09/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 250/1251 ( 20%)]  Loss:  3.124310 (3.0830)  Time: 1.106s,  926.01/s  (1.107s,  924.61/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 300/1251 ( 24%)]  Loss:  3.239030 (3.1053)  Time: 1.098s,  933.00/s  (1.107s,  924.81/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [ 350/1251 ( 28%)]  Loss:  3.295762 (3.1291)  Time: 1.131s,  905.67/s  (1.107s,  925.01/s)  LR: 1.495e-04  Data: 0.010 (0.012)
Train: 191 [ 400/1251 ( 32%)]  Loss:  2.774313 (3.0897)  Time: 1.099s,  932.15/s  (1.107s,  924.87/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 450/1251 ( 36%)]  Loss:  2.796865 (3.0604)  Time: 1.098s,  932.53/s  (1.108s,  924.53/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [ 500/1251 ( 40%)]  Loss:  2.888828 (3.0448)  Time: 1.102s,  929.60/s  (1.107s,  924.72/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [ 550/1251 ( 44%)]  Loss:  2.995617 (3.0407)  Time: 1.193s,  858.16/s  (1.108s,  924.58/s)  LR: 1.495e-04  Data: 0.010 (0.012)
Train: 191 [ 600/1251 ( 48%)]  Loss:  3.001146 (3.0376)  Time: 1.097s,  933.18/s  (1.108s,  924.47/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [ 650/1251 ( 52%)]  Loss:  3.316006 (3.0575)  Time: 1.099s,  931.59/s  (1.108s,  924.47/s)  LR: 1.495e-04  Data: 0.014 (0.012)
Train: 191 [ 700/1251 ( 56%)]  Loss:  3.088113 (3.0596)  Time: 1.106s,  925.51/s  (1.108s,  924.46/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 750/1251 ( 60%)]  Loss:  3.009950 (3.0565)  Time: 1.133s,  904.12/s  (1.107s,  924.90/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 800/1251 ( 64%)]  Loss:  3.030121 (3.0549)  Time: 1.113s,  920.44/s  (1.107s,  924.89/s)  LR: 1.495e-04  Data: 0.013 (0.012)
Train: 191 [ 850/1251 ( 68%)]  Loss:  2.826106 (3.0422)  Time: 1.101s,  930.39/s  (1.107s,  925.16/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [ 900/1251 ( 72%)]  Loss:  3.140629 (3.0474)  Time: 1.098s,  933.00/s  (1.107s,  925.08/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [ 950/1251 ( 76%)]  Loss:  2.964594 (3.0432)  Time: 1.096s,  933.92/s  (1.107s,  925.07/s)  LR: 1.495e-04  Data: 0.011 (0.012)
Train: 191 [1000/1251 ( 80%)]  Loss:  3.162973 (3.0489)  Time: 1.097s,  933.31/s  (1.107s,  925.27/s)  LR: 1.495e-04  Data: 0.010 (0.012)
Train: 191 [1050/1251 ( 84%)]  Loss:  2.841936 (3.0395)  Time: 1.123s,  911.60/s  (1.107s,  924.98/s)  LR: 1.495e-04  Data: 0.012 (0.012)
Train: 191 [1100/1251 ( 88%)]  Loss:  2.917385 (3.0342)  Time: 1.099s,  931.73/s  (1.107s,  924.79/s)  LR: 1.495e-04  Data: 0.015 (0.012)
Train: 191 [1150/1251 ( 92%)]  Loss:  2.921075 (3.0295)  Time: 1.098s,  932.31/s  (1.107s,  924.72/s)  LR: 1.495e-04  Data: 0.010 (0.012)
Train: 191 [1200/1251 ( 96%)]  Loss:  3.102625 (3.0324)  Time: 1.176s,  870.85/s  (1.107s,  924.79/s)  LR: 1.495e-04  Data: 0.010 (0.012)
Train: 191 [1250/1251 (100%)]  Loss:  2.924463 (3.0283)  Time: 1.104s,  927.56/s  (1.108s,  924.46/s)  LR: 1.495e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.248 (3.248)  Loss:  0.4455 (0.4455)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.5721 (0.8946)  Acc@1: 86.5566 (79.7820)  Acc@5: 97.6415 (95.0160)
Test (EMA): [   0/48]  Time: 3.119 (3.119)  Loss:  0.4013 (0.4013)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5378 (0.8345)  Acc@1: 87.5000 (80.6940)  Acc@5: 97.9953 (95.4660)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 80.501999921875)

Train: 192 [   0/1251 (  0%)]  Loss:  3.235215 (3.2352)  Time: 1.103s,  928.80/s  (1.103s,  928.80/s)  LR: 1.471e-04  Data: 0.023 (0.023)
Train: 192 [  50/1251 (  4%)]  Loss:  2.973103 (3.1042)  Time: 1.097s,  933.44/s  (1.105s,  926.36/s)  LR: 1.471e-04  Data: 0.010 (0.012)
Train: 192 [ 100/1251 (  8%)]  Loss:  2.914332 (3.0409)  Time: 1.108s,  923.95/s  (1.108s,  924.27/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 150/1251 ( 12%)]  Loss:  3.026738 (3.0373)  Time: 1.108s,  924.53/s  (1.107s,  924.87/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 200/1251 ( 16%)]  Loss:  3.083622 (3.0466)  Time: 1.099s,  931.99/s  (1.109s,  923.31/s)  LR: 1.471e-04  Data: 0.012 (0.012)
Train: 192 [ 250/1251 ( 20%)]  Loss:  3.127895 (3.0602)  Time: 1.100s,  930.54/s  (1.111s,  921.94/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 300/1251 ( 24%)]  Loss:  3.145799 (3.0724)  Time: 1.098s,  932.49/s  (1.112s,  920.82/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 192 [ 350/1251 ( 28%)]  Loss:  3.050521 (3.0697)  Time: 1.131s,  905.30/s  (1.112s,  920.55/s)  LR: 1.471e-04  Data: 0.012 (0.012)
Train: 192 [ 400/1251 ( 32%)]  Loss:  3.141172 (3.0776)  Time: 1.094s,  936.22/s  (1.113s,  920.14/s)  LR: 1.471e-04  Data: 0.012 (0.012)
Train: 192 [ 450/1251 ( 36%)]  Loss:  2.981890 (3.0680)  Time: 1.190s,  860.67/s  (1.112s,  920.82/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 500/1251 ( 40%)]  Loss:  3.094753 (3.0705)  Time: 1.196s,  856.54/s  (1.112s,  921.10/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 550/1251 ( 44%)]  Loss:  3.007369 (3.0652)  Time: 1.098s,  932.85/s  (1.111s,  921.49/s)  LR: 1.471e-04  Data: 0.012 (0.012)
Train: 192 [ 600/1251 ( 48%)]  Loss:  2.742696 (3.0404)  Time: 1.108s,  923.91/s  (1.111s,  921.94/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 650/1251 ( 52%)]  Loss:  3.155742 (3.0486)  Time: 1.099s,  931.93/s  (1.110s,  922.27/s)  LR: 1.471e-04  Data: 0.010 (0.012)
Train: 192 [ 700/1251 ( 56%)]  Loss:  2.993545 (3.0450)  Time: 1.104s,  927.53/s  (1.111s,  922.07/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 750/1251 ( 60%)]  Loss:  2.982625 (3.0411)  Time: 1.098s,  932.97/s  (1.110s,  922.16/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 800/1251 ( 64%)]  Loss:  3.061247 (3.0423)  Time: 1.097s,  933.58/s  (1.111s,  921.86/s)  LR: 1.471e-04  Data: 0.012 (0.012)
Train: 192 [ 850/1251 ( 68%)]  Loss:  3.013622 (3.0407)  Time: 1.123s,  912.10/s  (1.111s,  921.78/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 900/1251 ( 72%)]  Loss:  3.117184 (3.0447)  Time: 1.129s,  907.30/s  (1.111s,  921.88/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [ 950/1251 ( 76%)]  Loss:  3.185748 (3.0517)  Time: 1.098s,  932.54/s  (1.110s,  922.20/s)  LR: 1.471e-04  Data: 0.014 (0.012)
Train: 192 [1000/1251 ( 80%)]  Loss:  3.230004 (3.0602)  Time: 1.098s,  932.85/s  (1.110s,  922.45/s)  LR: 1.471e-04  Data: 0.012 (0.012)
Train: 192 [1050/1251 ( 84%)]  Loss:  3.288709 (3.0706)  Time: 1.098s,  932.93/s  (1.110s,  922.42/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [1100/1251 ( 88%)]  Loss:  2.771605 (3.0576)  Time: 1.133s,  903.82/s  (1.110s,  922.47/s)  LR: 1.471e-04  Data: 0.011 (0.012)
Train: 192 [1150/1251 ( 92%)]  Loss:  2.947712 (3.0530)  Time: 1.093s,  936.81/s  (1.110s,  922.68/s)  LR: 1.471e-04  Data: 0.010 (0.012)
Train: 192 [1200/1251 ( 96%)]  Loss:  2.802636 (3.0430)  Time: 1.099s,  931.80/s  (1.110s,  922.65/s)  LR: 1.471e-04  Data: 0.013 (0.012)
Train: 192 [1250/1251 (100%)]  Loss:  2.848577 (3.0355)  Time: 1.079s,  949.27/s  (1.110s,  922.48/s)  LR: 1.471e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.252 (3.252)  Loss:  0.4397 (0.4397)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5934 (0.9000)  Acc@1: 87.1462 (79.5380)  Acc@5: 97.4057 (94.8820)
Test (EMA): [   0/48]  Time: 3.045 (3.045)  Loss:  0.4009 (0.4009)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5368 (0.8338)  Acc@1: 87.5000 (80.7240)  Acc@5: 97.9953 (95.4780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 80.50200002685547)

Train: 193 [   0/1251 (  0%)]  Loss:  3.045316 (3.0453)  Time: 1.101s,  930.11/s  (1.101s,  930.11/s)  LR: 1.448e-04  Data: 0.019 (0.019)
Train: 193 [  50/1251 (  4%)]  Loss:  3.002421 (3.0239)  Time: 1.097s,  933.20/s  (1.113s,  919.88/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 100/1251 (  8%)]  Loss:  2.958338 (3.0020)  Time: 1.123s,  911.50/s  (1.109s,  923.11/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 150/1251 ( 12%)]  Loss:  2.757206 (2.9408)  Time: 1.097s,  933.79/s  (1.109s,  923.57/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [ 200/1251 ( 16%)]  Loss:  3.137294 (2.9801)  Time: 1.104s,  927.66/s  (1.108s,  924.41/s)  LR: 1.448e-04  Data: 0.022 (0.012)
Train: 193 [ 250/1251 ( 20%)]  Loss:  3.032994 (2.9889)  Time: 1.120s,  913.96/s  (1.108s,  924.19/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 300/1251 ( 24%)]  Loss:  3.131193 (3.0093)  Time: 1.096s,  934.03/s  (1.109s,  923.17/s)  LR: 1.448e-04  Data: 0.013 (0.012)
Train: 193 [ 350/1251 ( 28%)]  Loss:  3.100507 (3.0207)  Time: 1.103s,  928.20/s  (1.110s,  922.42/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [ 400/1251 ( 32%)]  Loss:  3.184762 (3.0389)  Time: 1.199s,  854.36/s  (1.112s,  920.92/s)  LR: 1.448e-04  Data: 0.010 (0.012)
Train: 193 [ 450/1251 ( 36%)]  Loss:  3.295771 (3.0646)  Time: 1.189s,  861.34/s  (1.111s,  921.63/s)  LR: 1.448e-04  Data: 0.010 (0.012)
Train: 193 [ 500/1251 ( 40%)]  Loss:  2.966124 (3.0556)  Time: 1.129s,  906.95/s  (1.110s,  922.42/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 550/1251 ( 44%)]  Loss:  3.295250 (3.0756)  Time: 1.096s,  934.29/s  (1.110s,  922.88/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [ 600/1251 ( 48%)]  Loss:  3.115274 (3.0787)  Time: 1.101s,  929.82/s  (1.109s,  923.08/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 650/1251 ( 52%)]  Loss:  2.908586 (3.0665)  Time: 1.093s,  936.65/s  (1.109s,  923.29/s)  LR: 1.448e-04  Data: 0.010 (0.012)
Train: 193 [ 700/1251 ( 56%)]  Loss:  3.200690 (3.0754)  Time: 1.097s,  933.82/s  (1.109s,  923.60/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [ 750/1251 ( 60%)]  Loss:  3.362287 (3.0934)  Time: 1.096s,  933.92/s  (1.109s,  923.61/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [ 800/1251 ( 64%)]  Loss:  2.768660 (3.0743)  Time: 1.096s,  934.25/s  (1.109s,  923.65/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 850/1251 ( 68%)]  Loss:  3.377418 (3.0911)  Time: 1.125s,  910.02/s  (1.108s,  923.93/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Train: 193 [ 900/1251 ( 72%)]  Loss:  3.062093 (3.0896)  Time: 1.126s,  909.64/s  (1.108s,  923.95/s)  LR: 1.448e-04  Data: 0.013 (0.012)
Train: 193 [ 950/1251 ( 76%)]  Loss:  3.519399 (3.1111)  Time: 1.134s,  903.17/s  (1.109s,  923.38/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [1000/1251 ( 80%)]  Loss:  2.956490 (3.1037)  Time: 1.120s,  914.58/s  (1.109s,  923.11/s)  LR: 1.448e-04  Data: 0.013 (0.012)
Train: 193 [1050/1251 ( 84%)]  Loss:  3.172122 (3.1068)  Time: 1.129s,  907.04/s  (1.109s,  923.07/s)  LR: 1.448e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 193 [1100/1251 ( 88%)]  Loss:  3.230176 (3.1122)  Time: 1.098s,  932.71/s  (1.109s,  923.02/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [1150/1251 ( 92%)]  Loss:  3.278043 (3.1191)  Time: 1.096s,  934.48/s  (1.109s,  923.15/s)  LR: 1.448e-04  Data: 0.012 (0.012)
Train: 193 [1200/1251 ( 96%)]  Loss:  2.970657 (3.1132)  Time: 1.099s,  931.92/s  (1.109s,  923.25/s)  LR: 1.448e-04  Data: 0.013 (0.012)
Train: 193 [1250/1251 (100%)]  Loss:  3.227118 (3.1175)  Time: 1.161s,  882.13/s  (1.109s,  923.11/s)  LR: 1.448e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.286 (3.286)  Loss:  0.4366 (0.4366)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.400)  Loss:  0.5693 (0.8958)  Acc@1: 86.2028 (79.5040)  Acc@5: 97.6415 (94.8240)
Test (EMA): [   0/48]  Time: 3.142 (3.142)  Loss:  0.4005 (0.4005)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.5368 (0.8334)  Acc@1: 87.5000 (80.7440)  Acc@5: 98.1132 (95.4920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 80.521999921875)

Train: 194 [   0/1251 (  0%)]  Loss:  3.514380 (3.5144)  Time: 1.105s,  926.61/s  (1.105s,  926.61/s)  LR: 1.425e-04  Data: 0.024 (0.024)
Train: 194 [  50/1251 (  4%)]  Loss:  3.048971 (3.2817)  Time: 1.106s,  926.14/s  (1.106s,  926.01/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 100/1251 (  8%)]  Loss:  3.178530 (3.2473)  Time: 1.189s,  861.35/s  (1.106s,  925.97/s)  LR: 1.425e-04  Data: 0.010 (0.012)
Train: 194 [ 150/1251 ( 12%)]  Loss:  3.342276 (3.2710)  Time: 1.132s,  904.62/s  (1.107s,  924.82/s)  LR: 1.425e-04  Data: 0.013 (0.012)
Train: 194 [ 200/1251 ( 16%)]  Loss:  3.021150 (3.2211)  Time: 1.099s,  931.85/s  (1.107s,  924.80/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 250/1251 ( 20%)]  Loss:  3.478796 (3.2640)  Time: 1.102s,  929.09/s  (1.107s,  925.30/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 300/1251 ( 24%)]  Loss:  3.067877 (3.2360)  Time: 1.103s,  928.27/s  (1.107s,  925.42/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 350/1251 ( 28%)]  Loss:  2.971215 (3.2029)  Time: 1.099s,  932.12/s  (1.106s,  925.75/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 400/1251 ( 32%)]  Loss:  3.051609 (3.1861)  Time: 1.098s,  932.49/s  (1.106s,  925.81/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 450/1251 ( 36%)]  Loss:  2.894376 (3.1569)  Time: 1.096s,  934.26/s  (1.106s,  925.90/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 500/1251 ( 40%)]  Loss:  3.162160 (3.1574)  Time: 1.095s,  935.17/s  (1.106s,  926.03/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 550/1251 ( 44%)]  Loss:  2.995132 (3.1439)  Time: 1.133s,  903.70/s  (1.106s,  925.91/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 600/1251 ( 48%)]  Loss:  3.209200 (3.1489)  Time: 1.096s,  933.94/s  (1.107s,  925.40/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 650/1251 ( 52%)]  Loss:  3.023193 (3.1399)  Time: 1.102s,  929.05/s  (1.107s,  925.07/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 700/1251 ( 56%)]  Loss:  2.800950 (3.1173)  Time: 1.095s,  935.37/s  (1.107s,  925.17/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 750/1251 ( 60%)]  Loss:  3.208590 (3.1230)  Time: 1.103s,  928.29/s  (1.106s,  925.46/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 800/1251 ( 64%)]  Loss:  2.881338 (3.1088)  Time: 1.131s,  905.28/s  (1.107s,  924.65/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [ 850/1251 ( 68%)]  Loss:  2.849546 (3.0944)  Time: 1.101s,  930.00/s  (1.108s,  924.43/s)  LR: 1.425e-04  Data: 0.016 (0.012)
Train: 194 [ 900/1251 ( 72%)]  Loss:  2.816417 (3.0798)  Time: 1.131s,  905.25/s  (1.108s,  923.98/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [ 950/1251 ( 76%)]  Loss:  3.024396 (3.0770)  Time: 1.098s,  932.41/s  (1.108s,  923.98/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [1000/1251 ( 80%)]  Loss:  2.871610 (3.0672)  Time: 1.097s,  933.32/s  (1.108s,  924.10/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [1050/1251 ( 84%)]  Loss:  3.050061 (3.0664)  Time: 1.097s,  933.28/s  (1.108s,  924.35/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [1100/1251 ( 88%)]  Loss:  3.124793 (3.0690)  Time: 1.098s,  932.69/s  (1.108s,  924.16/s)  LR: 1.425e-04  Data: 0.010 (0.012)
Train: 194 [1150/1251 ( 92%)]  Loss:  3.158847 (3.0727)  Time: 1.123s,  911.71/s  (1.108s,  924.12/s)  LR: 1.425e-04  Data: 0.011 (0.012)
Train: 194 [1200/1251 ( 96%)]  Loss:  2.904913 (3.0660)  Time: 1.103s,  928.43/s  (1.108s,  924.11/s)  LR: 1.425e-04  Data: 0.012 (0.012)
Train: 194 [1250/1251 (100%)]  Loss:  3.095187 (3.0671)  Time: 1.075s,  952.20/s  (1.108s,  923.86/s)  LR: 1.425e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.281 (3.281)  Loss:  0.4618 (0.4618)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.5829 (0.9109)  Acc@1: 87.2641 (79.4040)  Acc@5: 97.8774 (94.9080)
Test (EMA): [   0/48]  Time: 3.247 (3.247)  Loss:  0.4002 (0.4002)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5356 (0.8330)  Acc@1: 87.7359 (80.7440)  Acc@5: 97.9953 (95.5080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 80.540000078125)

Train: 195 [   0/1251 (  0%)]  Loss:  2.860257 (2.8603)  Time: 1.105s,  926.51/s  (1.105s,  926.51/s)  LR: 1.401e-04  Data: 0.026 (0.026)
Train: 195 [  50/1251 (  4%)]  Loss:  3.334801 (3.0975)  Time: 1.094s,  936.30/s  (1.115s,  918.68/s)  LR: 1.401e-04  Data: 0.010 (0.012)
Train: 195 [ 100/1251 (  8%)]  Loss:  2.800002 (2.9984)  Time: 1.095s,  935.25/s  (1.113s,  919.99/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [ 150/1251 ( 12%)]  Loss:  3.036112 (3.0078)  Time: 1.102s,  928.83/s  (1.110s,  922.69/s)  LR: 1.401e-04  Data: 0.013 (0.012)
Train: 195 [ 200/1251 ( 16%)]  Loss:  3.300516 (3.0663)  Time: 1.192s,  858.97/s  (1.109s,  923.68/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [ 250/1251 ( 20%)]  Loss:  3.095862 (3.0713)  Time: 1.095s,  934.81/s  (1.109s,  923.66/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [ 300/1251 ( 24%)]  Loss:  3.261932 (3.0985)  Time: 1.096s,  934.33/s  (1.109s,  923.18/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [ 350/1251 ( 28%)]  Loss:  3.207384 (3.1121)  Time: 1.097s,  933.16/s  (1.109s,  923.36/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [ 400/1251 ( 32%)]  Loss:  3.044988 (3.1047)  Time: 1.135s,  902.39/s  (1.109s,  923.30/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [ 450/1251 ( 36%)]  Loss:  3.124173 (3.1066)  Time: 1.193s,  858.31/s  (1.109s,  923.28/s)  LR: 1.401e-04  Data: 0.010 (0.012)
Train: 195 [ 500/1251 ( 40%)]  Loss:  3.067179 (3.1030)  Time: 1.110s,  922.54/s  (1.109s,  923.62/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [ 550/1251 ( 44%)]  Loss:  2.869212 (3.0835)  Time: 1.099s,  931.82/s  (1.109s,  923.76/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 195 [ 600/1251 ( 48%)]  Loss:  3.116821 (3.0861)  Time: 1.197s,  855.68/s  (1.109s,  923.36/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [ 650/1251 ( 52%)]  Loss:  3.329784 (3.1035)  Time: 1.131s,  905.07/s  (1.109s,  923.43/s)  LR: 1.401e-04  Data: 0.010 (0.012)
Train: 195 [ 700/1251 ( 56%)]  Loss:  3.049519 (3.0999)  Time: 1.132s,  904.95/s  (1.109s,  923.65/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [ 750/1251 ( 60%)]  Loss:  3.248998 (3.1092)  Time: 1.097s,  933.32/s  (1.109s,  923.21/s)  LR: 1.401e-04  Data: 0.013 (0.012)
Train: 195 [ 800/1251 ( 64%)]  Loss:  3.025517 (3.1043)  Time: 1.097s,  933.51/s  (1.109s,  923.52/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [ 850/1251 ( 68%)]  Loss:  3.068815 (3.1023)  Time: 1.129s,  906.86/s  (1.109s,  923.72/s)  LR: 1.401e-04  Data: 0.010 (0.012)
Train: 195 [ 900/1251 ( 72%)]  Loss:  2.806854 (3.0868)  Time: 1.122s,  912.61/s  (1.109s,  923.45/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [ 950/1251 ( 76%)]  Loss:  2.932233 (3.0790)  Time: 1.096s,  934.28/s  (1.109s,  923.55/s)  LR: 1.401e-04  Data: 0.013 (0.012)
Train: 195 [1000/1251 ( 80%)]  Loss:  3.186099 (3.0841)  Time: 1.096s,  934.06/s  (1.109s,  923.67/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [1050/1251 ( 84%)]  Loss:  2.821645 (3.0722)  Time: 1.100s,  930.90/s  (1.109s,  923.72/s)  LR: 1.401e-04  Data: 0.011 (0.012)
Train: 195 [1100/1251 ( 88%)]  Loss:  3.119179 (3.0743)  Time: 1.097s,  933.42/s  (1.109s,  923.66/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [1150/1251 ( 92%)]  Loss:  3.319899 (3.0845)  Time: 1.096s,  934.17/s  (1.108s,  923.88/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [1200/1251 ( 96%)]  Loss:  3.093931 (3.0849)  Time: 1.097s,  933.66/s  (1.109s,  923.74/s)  LR: 1.401e-04  Data: 0.012 (0.012)
Train: 195 [1250/1251 (100%)]  Loss:  3.215727 (3.0899)  Time: 1.081s,  946.92/s  (1.109s,  923.68/s)  LR: 1.401e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.182 (3.182)  Loss:  0.4396 (0.4396)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.5824 (0.9047)  Acc@1: 86.7924 (79.5400)  Acc@5: 97.5236 (94.9080)
Test (EMA): [   0/48]  Time: 3.077 (3.077)  Loss:  0.4002 (0.4002)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5341 (0.8326)  Acc@1: 87.6179 (80.7540)  Acc@5: 97.9953 (95.5080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 80.56200002685547)

Train: 196 [   0/1251 (  0%)]  Loss:  3.237092 (3.2371)  Time: 1.126s,  909.39/s  (1.126s,  909.39/s)  LR: 1.378e-04  Data: 0.020 (0.020)
Train: 196 [  50/1251 (  4%)]  Loss:  2.856188 (3.0466)  Time: 1.097s,  933.58/s  (1.104s,  927.16/s)  LR: 1.378e-04  Data: 0.011 (0.011)
Train: 196 [ 100/1251 (  8%)]  Loss:  2.988574 (3.0273)  Time: 1.097s,  933.57/s  (1.105s,  926.52/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 150/1251 ( 12%)]  Loss:  3.208776 (3.0727)  Time: 1.092s,  937.33/s  (1.106s,  925.93/s)  LR: 1.378e-04  Data: 0.010 (0.012)
Train: 196 [ 200/1251 ( 16%)]  Loss:  2.902158 (3.0386)  Time: 1.097s,  933.54/s  (1.105s,  926.62/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [ 250/1251 ( 20%)]  Loss:  3.334486 (3.0879)  Time: 1.096s,  934.19/s  (1.105s,  926.75/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 300/1251 ( 24%)]  Loss:  3.062632 (3.0843)  Time: 1.101s,  930.42/s  (1.105s,  926.69/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [ 350/1251 ( 28%)]  Loss:  2.907556 (3.0622)  Time: 1.103s,  928.27/s  (1.105s,  926.57/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 400/1251 ( 32%)]  Loss:  3.075704 (3.0637)  Time: 1.201s,  852.87/s  (1.105s,  926.47/s)  LR: 1.378e-04  Data: 0.010 (0.012)
Train: 196 [ 450/1251 ( 36%)]  Loss:  2.980267 (3.0553)  Time: 1.111s,  921.32/s  (1.106s,  926.05/s)  LR: 1.378e-04  Data: 0.010 (0.012)
Train: 196 [ 500/1251 ( 40%)]  Loss:  3.303752 (3.0779)  Time: 1.138s,  899.66/s  (1.107s,  924.84/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 550/1251 ( 44%)]  Loss:  3.015125 (3.0727)  Time: 1.108s,  924.57/s  (1.107s,  924.94/s)  LR: 1.378e-04  Data: 0.015 (0.012)
Train: 196 [ 600/1251 ( 48%)]  Loss:  2.829892 (3.0540)  Time: 1.098s,  932.97/s  (1.107s,  924.88/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 650/1251 ( 52%)]  Loss:  3.043472 (3.0533)  Time: 1.096s,  933.93/s  (1.107s,  924.95/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [ 700/1251 ( 56%)]  Loss:  2.860196 (3.0404)  Time: 1.101s,  929.71/s  (1.107s,  924.89/s)  LR: 1.378e-04  Data: 0.013 (0.012)
Train: 196 [ 750/1251 ( 60%)]  Loss:  2.841569 (3.0280)  Time: 1.097s,  933.86/s  (1.108s,  924.25/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 800/1251 ( 64%)]  Loss:  3.086878 (3.0314)  Time: 1.100s,  930.95/s  (1.108s,  924.14/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 850/1251 ( 68%)]  Loss:  3.089124 (3.0346)  Time: 1.097s,  933.24/s  (1.108s,  924.29/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [ 900/1251 ( 72%)]  Loss:  3.010786 (3.0334)  Time: 1.107s,  925.02/s  (1.108s,  924.58/s)  LR: 1.378e-04  Data: 0.012 (0.012)
Train: 196 [ 950/1251 ( 76%)]  Loss:  3.204554 (3.0419)  Time: 1.101s,  930.41/s  (1.107s,  924.73/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [1000/1251 ( 80%)]  Loss:  3.122986 (3.0458)  Time: 1.100s,  931.03/s  (1.107s,  924.90/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [1050/1251 ( 84%)]  Loss:  3.268515 (3.0559)  Time: 1.105s,  926.74/s  (1.107s,  924.91/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [1100/1251 ( 88%)]  Loss:  2.986563 (3.0529)  Time: 1.117s,  916.59/s  (1.107s,  924.81/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [1150/1251 ( 92%)]  Loss:  3.052619 (3.0529)  Time: 1.125s,  910.63/s  (1.108s,  924.55/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [1200/1251 ( 96%)]  Loss:  2.863970 (3.0453)  Time: 1.204s,  850.69/s  (1.107s,  924.63/s)  LR: 1.378e-04  Data: 0.011 (0.012)
Train: 196 [1250/1251 (100%)]  Loss:  3.160007 (3.0497)  Time: 1.080s,  947.86/s  (1.107s,  924.61/s)  LR: 1.378e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.210 (3.210)  Loss:  0.4434 (0.4434)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.5980 (0.9138)  Acc@1: 86.4387 (79.4160)  Acc@5: 97.5236 (94.9100)
Test (EMA): [   0/48]  Time: 3.172 (3.172)  Loss:  0.4002 (0.4002)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5340 (0.8323)  Acc@1: 87.6179 (80.7960)  Acc@5: 97.9953 (95.5200)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 80.57000002685547)

Train: 197 [   0/1251 (  0%)]  Loss:  3.309178 (3.3092)  Time: 1.103s,  928.53/s  (1.103s,  928.53/s)  LR: 1.355e-04  Data: 0.020 (0.020)
Train: 197 [  50/1251 (  4%)]  Loss:  3.122645 (3.2159)  Time: 1.100s,  931.22/s  (1.104s,  927.93/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [ 100/1251 (  8%)]  Loss:  2.881590 (3.1045)  Time: 1.096s,  933.91/s  (1.105s,  926.47/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 197 [ 150/1251 ( 12%)]  Loss:  3.227939 (3.1353)  Time: 1.112s,  920.76/s  (1.105s,  926.66/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [ 200/1251 ( 16%)]  Loss:  2.849419 (3.0782)  Time: 1.098s,  932.69/s  (1.105s,  926.92/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [ 250/1251 ( 20%)]  Loss:  3.245991 (3.1061)  Time: 1.205s,  849.63/s  (1.107s,  925.20/s)  LR: 1.355e-04  Data: 0.010 (0.012)
Train: 197 [ 300/1251 ( 24%)]  Loss:  3.102309 (3.1056)  Time: 1.094s,  936.14/s  (1.106s,  925.67/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [ 350/1251 ( 28%)]  Loss:  3.037624 (3.0971)  Time: 1.101s,  930.18/s  (1.106s,  926.25/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [ 400/1251 ( 32%)]  Loss:  3.219278 (3.1107)  Time: 1.096s,  933.91/s  (1.105s,  926.46/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [ 450/1251 ( 36%)]  Loss:  3.082402 (3.1078)  Time: 1.094s,  936.27/s  (1.105s,  926.60/s)  LR: 1.355e-04  Data: 0.010 (0.012)
Train: 197 [ 500/1251 ( 40%)]  Loss:  3.130837 (3.1099)  Time: 1.128s,  907.70/s  (1.106s,  925.80/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [ 550/1251 ( 44%)]  Loss:  3.009212 (3.1015)  Time: 1.096s,  934.67/s  (1.106s,  925.65/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [ 600/1251 ( 48%)]  Loss:  3.120043 (3.1030)  Time: 1.102s,  929.46/s  (1.107s,  924.90/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [ 650/1251 ( 52%)]  Loss:  3.047297 (3.0990)  Time: 1.097s,  933.47/s  (1.107s,  924.75/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [ 700/1251 ( 56%)]  Loss:  3.006143 (3.0928)  Time: 1.101s,  930.40/s  (1.107s,  924.86/s)  LR: 1.355e-04  Data: 0.013 (0.012)
Train: 197 [ 750/1251 ( 60%)]  Loss:  2.730581 (3.0702)  Time: 1.190s,  860.65/s  (1.107s,  925.03/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [ 800/1251 ( 64%)]  Loss:  3.179670 (3.0766)  Time: 1.097s,  933.15/s  (1.107s,  925.16/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [ 850/1251 ( 68%)]  Loss:  2.946126 (3.0693)  Time: 1.097s,  933.12/s  (1.107s,  925.33/s)  LR: 1.355e-04  Data: 0.013 (0.012)
Train: 197 [ 900/1251 ( 72%)]  Loss:  2.884586 (3.0596)  Time: 1.098s,  932.84/s  (1.107s,  925.42/s)  LR: 1.355e-04  Data: 0.014 (0.012)
Train: 197 [ 950/1251 ( 76%)]  Loss:  3.209270 (3.0671)  Time: 1.130s,  906.59/s  (1.107s,  924.71/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [1000/1251 ( 80%)]  Loss:  3.269469 (3.0767)  Time: 1.097s,  933.68/s  (1.107s,  924.79/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [1050/1251 ( 84%)]  Loss:  3.147728 (3.0800)  Time: 1.101s,  930.15/s  (1.107s,  924.83/s)  LR: 1.355e-04  Data: 0.012 (0.012)
Train: 197 [1100/1251 ( 88%)]  Loss:  3.079506 (3.0799)  Time: 1.097s,  933.26/s  (1.107s,  924.95/s)  LR: 1.355e-04  Data: 0.011 (0.012)
Train: 197 [1150/1251 ( 92%)]  Loss:  3.097517 (3.0807)  Time: 1.127s,  908.30/s  (1.107s,  924.88/s)  LR: 1.355e-04  Data: 0.013 (0.012)
Train: 197 [1200/1251 ( 96%)]  Loss:  2.844653 (3.0712)  Time: 1.109s,  923.35/s  (1.107s,  924.90/s)  LR: 1.355e-04  Data: 0.013 (0.012)
Train: 197 [1250/1251 (100%)]  Loss:  3.359980 (3.0823)  Time: 1.080s,  947.78/s  (1.107s,  924.92/s)  LR: 1.355e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.330 (3.330)  Loss:  0.4414 (0.4414)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.398)  Loss:  0.5612 (0.8871)  Acc@1: 86.7924 (79.4820)  Acc@5: 97.7594 (95.0120)
Test (EMA): [   0/48]  Time: 3.180 (3.180)  Loss:  0.3997 (0.3997)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.413)  Loss:  0.5338 (0.8320)  Acc@1: 87.7358 (80.7680)  Acc@5: 97.8774 (95.5140)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 80.62799994873046)

Train: 198 [   0/1251 (  0%)]  Loss:  3.289092 (3.2891)  Time: 1.103s,  928.72/s  (1.103s,  928.72/s)  LR: 1.333e-04  Data: 0.020 (0.020)
Train: 198 [  50/1251 (  4%)]  Loss:  3.109674 (3.1994)  Time: 1.194s,  857.85/s  (1.109s,  923.73/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 100/1251 (  8%)]  Loss:  3.196530 (3.1984)  Time: 1.100s,  931.32/s  (1.112s,  920.62/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 150/1251 ( 12%)]  Loss:  3.425781 (3.2553)  Time: 1.125s,  910.11/s  (1.110s,  922.58/s)  LR: 1.333e-04  Data: 0.013 (0.012)
Train: 198 [ 200/1251 ( 16%)]  Loss:  3.200427 (3.2443)  Time: 1.097s,  933.42/s  (1.108s,  923.96/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 250/1251 ( 20%)]  Loss:  3.202644 (3.2374)  Time: 1.097s,  933.63/s  (1.107s,  924.61/s)  LR: 1.333e-04  Data: 0.013 (0.012)
Train: 198 [ 300/1251 ( 24%)]  Loss:  2.746389 (3.1672)  Time: 1.098s,  932.78/s  (1.106s,  925.53/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 350/1251 ( 28%)]  Loss:  2.849075 (3.1275)  Time: 1.214s,  843.38/s  (1.107s,  924.99/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 400/1251 ( 32%)]  Loss:  2.935347 (3.1061)  Time: 1.089s,  939.89/s  (1.107s,  925.31/s)  LR: 1.333e-04  Data: 0.010 (0.012)
Train: 198 [ 450/1251 ( 36%)]  Loss:  2.953927 (3.0909)  Time: 1.124s,  911.38/s  (1.107s,  925.35/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 500/1251 ( 40%)]  Loss:  3.161328 (3.0973)  Time: 1.097s,  933.29/s  (1.106s,  925.45/s)  LR: 1.333e-04  Data: 0.012 (0.012)
Train: 198 [ 550/1251 ( 44%)]  Loss:  2.877222 (3.0790)  Time: 1.097s,  933.70/s  (1.107s,  925.29/s)  LR: 1.333e-04  Data: 0.012 (0.012)
Train: 198 [ 600/1251 ( 48%)]  Loss:  2.877322 (3.0634)  Time: 1.095s,  935.05/s  (1.107s,  924.94/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 650/1251 ( 52%)]  Loss:  3.414266 (3.0885)  Time: 1.097s,  933.69/s  (1.107s,  925.02/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 700/1251 ( 56%)]  Loss:  2.924611 (3.0776)  Time: 1.099s,  932.04/s  (1.107s,  925.06/s)  LR: 1.333e-04  Data: 0.012 (0.012)
Train: 198 [ 750/1251 ( 60%)]  Loss:  3.208294 (3.0857)  Time: 1.126s,  909.57/s  (1.107s,  925.07/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [ 800/1251 ( 64%)]  Loss:  2.917468 (3.0758)  Time: 1.095s,  935.14/s  (1.107s,  925.01/s)  LR: 1.333e-04  Data: 0.010 (0.012)
Train: 198 [ 850/1251 ( 68%)]  Loss:  2.983637 (3.0707)  Time: 1.103s,  928.20/s  (1.107s,  925.35/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 198 [ 900/1251 ( 72%)]  Loss:  3.175759 (3.0763)  Time: 1.104s,  927.74/s  (1.107s,  925.43/s)  LR: 1.333e-04  Data: 0.018 (0.012)
Train: 198 [ 950/1251 ( 76%)]  Loss:  3.111244 (3.0780)  Time: 1.099s,  931.71/s  (1.107s,  925.42/s)  LR: 1.333e-04  Data: 0.014 (0.012)
Train: 198 [1000/1251 ( 80%)]  Loss:  3.046914 (3.0765)  Time: 1.096s,  934.44/s  (1.107s,  925.42/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [1050/1251 ( 84%)]  Loss:  2.953087 (3.0709)  Time: 1.109s,  923.45/s  (1.107s,  925.42/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [1100/1251 ( 88%)]  Loss:  3.156072 (3.0746)  Time: 1.098s,  933.03/s  (1.107s,  925.43/s)  LR: 1.333e-04  Data: 0.012 (0.012)
Train: 198 [1150/1251 ( 92%)]  Loss:  3.457369 (3.0906)  Time: 1.133s,  904.12/s  (1.106s,  925.51/s)  LR: 1.333e-04  Data: 0.012 (0.012)
Train: 198 [1200/1251 ( 96%)]  Loss:  3.413280 (3.1035)  Time: 1.094s,  935.85/s  (1.107s,  925.36/s)  LR: 1.333e-04  Data: 0.011 (0.012)
Train: 198 [1250/1251 (100%)]  Loss:  2.955180 (3.0978)  Time: 1.087s,  942.42/s  (1.107s,  925.25/s)  LR: 1.333e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.221 (3.221)  Loss:  0.4256 (0.4256)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.5607 (0.8842)  Acc@1: 86.3208 (79.5840)  Acc@5: 96.9340 (94.9980)
Test (EMA): [   0/48]  Time: 3.190 (3.190)  Loss:  0.3992 (0.3992)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.410)  Loss:  0.5340 (0.8318)  Acc@1: 87.6179 (80.8300)  Acc@5: 97.8774 (95.5080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 80.67399994873047)

Train: 199 [   0/1251 (  0%)]  Loss:  3.149144 (3.1491)  Time: 1.102s,  929.64/s  (1.102s,  929.64/s)  LR: 1.310e-04  Data: 0.022 (0.022)
Train: 199 [  50/1251 (  4%)]  Loss:  3.125414 (3.1373)  Time: 1.098s,  932.44/s  (1.110s,  922.82/s)  LR: 1.310e-04  Data: 0.012 (0.012)
Train: 199 [ 100/1251 (  8%)]  Loss:  3.048168 (3.1076)  Time: 1.097s,  933.80/s  (1.107s,  925.04/s)  LR: 1.310e-04  Data: 0.011 (0.012)
Train: 199 [ 150/1251 ( 12%)]  Loss:  2.645920 (2.9922)  Time: 1.104s,  927.26/s  (1.107s,  925.13/s)  LR: 1.310e-04  Data: 0.011 (0.012)
Train: 199 [ 200/1251 ( 16%)]  Loss:  2.901996 (2.9741)  Time: 1.096s,  933.90/s  (1.108s,  924.24/s)  LR: 1.310e-04  Data: 0.011 (0.011)
Train: 199 [ 250/1251 ( 20%)]  Loss:  2.948036 (2.9698)  Time: 1.098s,  932.97/s  (1.107s,  924.73/s)  LR: 1.310e-04  Data: 0.012 (0.011)
Train: 199 [ 300/1251 ( 24%)]  Loss:  3.016658 (2.9765)  Time: 1.100s,  930.57/s  (1.108s,  924.26/s)  LR: 1.310e-04  Data: 0.012 (0.011)
Train: 199 [ 350/1251 ( 28%)]  Loss:  2.960766 (2.9745)  Time: 1.098s,  932.84/s  (1.107s,  925.20/s)  LR: 1.310e-04  Data: 0.011 (0.011)
Train: 199 [ 400/1251 ( 32%)]  Loss:  2.911666 (2.9675)  Time: 1.095s,  934.84/s  (1.107s,  925.38/s)  LR: 1.310e-04  Data: 0.012 (0.011)
Train: 199 [ 450/1251 ( 36%)]  Loss:  3.162710 (2.9870)  Time: 1.094s,  936.34/s  (1.107s,  925.21/s)  LR: 1.310e-04  Data: 0.010 (0.011)
Train: 199 [ 500/1251 ( 40%)]  Loss:  2.863141 (2.9758)  Time: 1.096s,  934.03/s  (1.107s,  925.38/s)  LR: 1.310e-04  Data: 0.013 (0.011)
Train: 199 [ 550/1251 ( 44%)]  Loss:  3.124516 (2.9882)  Time: 1.097s,  933.80/s  (1.107s,  925.42/s)  LR: 1.310e-04  Data: 0.011 (0.011)
Train: 199 [ 600/1251 ( 48%)]  Loss:  3.210705 (3.0053)  Time: 1.094s,  935.61/s  (1.106s,  925.49/s)  LR: 1.310e-04  Data: 0.012 (0.012)
Train: 199 [ 650/1251 ( 52%)]  Loss:  2.965762 (3.0025)  Time: 1.101s,  930.11/s  (1.106s,  925.71/s)  LR: 1.310e-04  Data: 0.012 (0.012)
Train: 199 [ 700/1251 ( 56%)]  Loss:  3.230100 (3.0176)  Time: 1.101s,  930.46/s  (1.106s,  925.97/s)  LR: 1.310e-04  Data: 0.011 (0.012)
Train: 199 [ 750/1251 ( 60%)]  Loss:  2.993562 (3.0161)  Time: 1.128s,  908.03/s  (1.106s,  925.61/s)  LR: 1.310e-04  Data: 0.010 (0.011)
Train: 199 [ 800/1251 ( 64%)]  Loss:  2.994344 (3.0149)  Time: 1.098s,  932.30/s  (1.107s,  925.07/s)  LR: 1.310e-04  Data: 0.010 (0.011)
Train: 199 [ 850/1251 ( 68%)]  Loss:  3.170187 (3.0235)  Time: 1.099s,  931.41/s  (1.107s,  924.88/s)  LR: 1.310e-04  Data: 0.012 (0.011)
Train: 199 [ 900/1251 ( 72%)]  Loss:  3.220808 (3.0339)  Time: 1.127s,  908.64/s  (1.107s,  924.83/s)  LR: 1.310e-04  Data: 0.013 (0.011)
Train: 199 [ 950/1251 ( 76%)]  Loss:  3.162817 (3.0403)  Time: 1.105s,  926.98/s  (1.107s,  924.78/s)  LR: 1.310e-04  Data: 0.010 (0.012)
Train: 199 [1000/1251 ( 80%)]  Loss:  3.219319 (3.0488)  Time: 1.094s,  935.78/s  (1.107s,  924.91/s)  LR: 1.310e-04  Data: 0.012 (0.012)
Train: 199 [1050/1251 ( 84%)]  Loss:  3.017637 (3.0474)  Time: 1.099s,  931.73/s  (1.107s,  924.84/s)  LR: 1.310e-04  Data: 0.012 (0.012)
Train: 199 [1100/1251 ( 88%)]  Loss:  2.923206 (3.0420)  Time: 1.120s,  914.64/s  (1.107s,  924.78/s)  LR: 1.310e-04  Data: 0.010 (0.012)
Train: 199 [1150/1251 ( 92%)]  Loss:  3.038324 (3.0419)  Time: 1.097s,  933.22/s  (1.107s,  924.84/s)  LR: 1.310e-04  Data: 0.010 (0.012)
Train: 199 [1200/1251 ( 96%)]  Loss:  3.241838 (3.0499)  Time: 1.122s,  912.44/s  (1.107s,  924.79/s)  LR: 1.310e-04  Data: 0.011 (0.011)
Train: 199 [1250/1251 (100%)]  Loss:  3.035645 (3.0493)  Time: 1.080s,  947.81/s  (1.107s,  924.72/s)  LR: 1.310e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.238 (3.238)  Loss:  0.4499 (0.4499)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.409)  Loss:  0.5647 (0.8965)  Acc@1: 87.2641 (79.4800)  Acc@5: 97.6415 (95.0260)
Test (EMA): [   0/48]  Time: 3.056 (3.056)  Loss:  0.3988 (0.3988)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5333 (0.8315)  Acc@1: 87.6179 (80.8520)  Acc@5: 97.8774 (95.5180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 80.686)

Train: 200 [   0/1251 (  0%)]  Loss:  2.826221 (2.8262)  Time: 1.106s,  926.13/s  (1.106s,  926.13/s)  LR: 1.287e-04  Data: 0.019 (0.019)
Train: 200 [  50/1251 (  4%)]  Loss:  3.195919 (3.0111)  Time: 1.096s,  934.27/s  (1.110s,  922.86/s)  LR: 1.287e-04  Data: 0.011 (0.012)
Train: 200 [ 100/1251 (  8%)]  Loss:  3.347012 (3.1231)  Time: 1.099s,  931.67/s  (1.109s,  923.61/s)  LR: 1.287e-04  Data: 0.010 (0.012)
Train: 200 [ 150/1251 ( 12%)]  Loss:  3.260809 (3.1575)  Time: 1.105s,  926.38/s  (1.107s,  925.15/s)  LR: 1.287e-04  Data: 0.011 (0.011)
Train: 200 [ 200/1251 ( 16%)]  Loss:  2.833106 (3.0926)  Time: 1.103s,  928.13/s  (1.106s,  925.95/s)  LR: 1.287e-04  Data: 0.012 (0.012)
Train: 200 [ 250/1251 ( 20%)]  Loss:  3.359602 (3.1371)  Time: 1.092s,  937.72/s  (1.109s,  923.46/s)  LR: 1.287e-04  Data: 0.010 (0.011)
Train: 200 [ 300/1251 ( 24%)]  Loss:  3.170035 (3.1418)  Time: 1.099s,  931.94/s  (1.109s,  923.33/s)  LR: 1.287e-04  Data: 0.010 (0.011)
Train: 200 [ 350/1251 ( 28%)]  Loss:  3.089383 (3.1353)  Time: 1.099s,  931.54/s  (1.108s,  924.11/s)  LR: 1.287e-04  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 200 [ 400/1251 ( 32%)]  Loss:  3.218796 (3.1445)  Time: 1.098s,  932.25/s  (1.107s,  924.72/s)  LR: 1.287e-04  Data: 0.012 (0.011)
Train: 200 [ 450/1251 ( 36%)]  Loss:  2.967258 (3.1268)  Time: 1.091s,  938.64/s  (1.107s,  924.86/s)  LR: 1.287e-04  Data: 0.010 (0.011)
Train: 200 [ 500/1251 ( 40%)]  Loss:  2.815373 (3.0985)  Time: 1.100s,  930.84/s  (1.107s,  925.27/s)  LR: 1.287e-04  Data: 0.011 (0.011)
Train: 200 [ 550/1251 ( 44%)]  Loss:  2.928840 (3.0844)  Time: 1.128s,  907.69/s  (1.107s,  924.81/s)  LR: 1.287e-04  Data: 0.011 (0.011)
Train: 200 [ 600/1251 ( 48%)]  Loss:  3.352161 (3.1050)  Time: 1.097s,  933.49/s  (1.108s,  924.03/s)  LR: 1.287e-04  Data: 0.012 (0.011)
Train: 200 [ 650/1251 ( 52%)]  Loss:  3.123337 (3.1063)  Time: 1.098s,  932.99/s  (1.108s,  924.24/s)  LR: 1.287e-04  Data: 0.013 (0.012)
Train: 200 [ 700/1251 ( 56%)]  Loss:  3.292855 (3.1187)  Time: 1.097s,  933.15/s  (1.108s,  924.56/s)  LR: 1.287e-04  Data: 0.011 (0.012)
Train: 200 [ 750/1251 ( 60%)]  Loss:  2.912339 (3.1058)  Time: 1.096s,  934.01/s  (1.107s,  924.98/s)  LR: 1.287e-04  Data: 0.012 (0.012)
Train: 200 [ 800/1251 ( 64%)]  Loss:  3.209751 (3.1119)  Time: 1.098s,  932.75/s  (1.107s,  925.05/s)  LR: 1.287e-04  Data: 0.012 (0.012)
Train: 200 [ 850/1251 ( 68%)]  Loss:  3.189714 (3.1163)  Time: 1.095s,  934.75/s  (1.107s,  925.23/s)  LR: 1.287e-04  Data: 0.012 (0.012)
Train: 200 [ 900/1251 ( 72%)]  Loss:  2.847252 (3.1021)  Time: 1.104s,  927.44/s  (1.107s,  925.39/s)  LR: 1.287e-04  Data: 0.011 (0.012)
Train: 200 [ 950/1251 ( 76%)]  Loss:  3.127694 (3.1034)  Time: 1.098s,  932.28/s  (1.106s,  925.50/s)  LR: 1.287e-04  Data: 0.014 (0.012)
Train: 200 [1000/1251 ( 80%)]  Loss:  2.768940 (3.0874)  Time: 1.104s,  927.29/s  (1.107s,  925.32/s)  LR: 1.287e-04  Data: 0.011 (0.012)
Train: 200 [1050/1251 ( 84%)]  Loss:  2.855985 (3.0769)  Time: 1.094s,  936.07/s  (1.106s,  925.48/s)  LR: 1.287e-04  Data: 0.010 (0.012)
Train: 200 [1100/1251 ( 88%)]  Loss:  3.054650 (3.0760)  Time: 1.111s,  921.64/s  (1.106s,  925.48/s)  LR: 1.287e-04  Data: 0.011 (0.012)
Train: 200 [1150/1251 ( 92%)]  Loss:  2.663301 (3.0588)  Time: 1.096s,  933.89/s  (1.107s,  925.38/s)  LR: 1.287e-04  Data: 0.011 (0.012)
Train: 200 [1200/1251 ( 96%)]  Loss:  2.716235 (3.0451)  Time: 1.097s,  933.70/s  (1.106s,  925.60/s)  LR: 1.287e-04  Data: 0.012 (0.012)
Train: 200 [1250/1251 (100%)]  Loss:  2.853517 (3.0377)  Time: 1.087s,  942.31/s  (1.106s,  925.55/s)  LR: 1.287e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.235 (3.235)  Loss:  0.4298 (0.4298)  Acc@1: 93.4570 (93.4570)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5583 (0.9019)  Acc@1: 86.6745 (79.7220)  Acc@5: 97.8774 (94.9460)
Test (EMA): [   0/48]  Time: 3.083 (3.083)  Loss:  0.3972 (0.3972)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5328 (0.8310)  Acc@1: 87.5000 (80.8200)  Acc@5: 97.8774 (95.5360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 80.694)

Train: 201 [   0/1251 (  0%)]  Loss:  3.141064 (3.1411)  Time: 1.101s,  929.67/s  (1.101s,  929.67/s)  LR: 1.265e-04  Data: 0.020 (0.020)
Train: 201 [  50/1251 (  4%)]  Loss:  3.217860 (3.1795)  Time: 1.096s,  934.30/s  (1.108s,  924.03/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [ 100/1251 (  8%)]  Loss:  3.140409 (3.1664)  Time: 1.102s,  929.55/s  (1.112s,  920.86/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 150/1251 ( 12%)]  Loss:  3.053781 (3.1383)  Time: 1.097s,  933.82/s  (1.109s,  923.71/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 200/1251 ( 16%)]  Loss:  3.060866 (3.1228)  Time: 1.132s,  904.38/s  (1.111s,  921.86/s)  LR: 1.265e-04  Data: 0.010 (0.012)
Train: 201 [ 250/1251 ( 20%)]  Loss:  2.804577 (3.0698)  Time: 1.095s,  935.14/s  (1.111s,  922.03/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 300/1251 ( 24%)]  Loss:  3.079086 (3.0711)  Time: 1.105s,  926.59/s  (1.110s,  922.49/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 350/1251 ( 28%)]  Loss:  2.704314 (3.0252)  Time: 1.098s,  932.39/s  (1.110s,  922.45/s)  LR: 1.265e-04  Data: 0.013 (0.012)
Train: 201 [ 400/1251 ( 32%)]  Loss:  2.911047 (3.0126)  Time: 1.101s,  929.91/s  (1.109s,  923.15/s)  LR: 1.265e-04  Data: 0.013 (0.012)
Train: 201 [ 450/1251 ( 36%)]  Loss:  3.292965 (3.0406)  Time: 1.101s,  929.99/s  (1.109s,  923.51/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [ 500/1251 ( 40%)]  Loss:  2.639309 (3.0041)  Time: 1.123s,  911.78/s  (1.110s,  922.57/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 550/1251 ( 44%)]  Loss:  3.112453 (3.0131)  Time: 1.104s,  927.66/s  (1.109s,  923.18/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [ 600/1251 ( 48%)]  Loss:  2.988755 (3.0113)  Time: 1.096s,  934.24/s  (1.109s,  923.27/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [ 650/1251 ( 52%)]  Loss:  3.256442 (3.0288)  Time: 1.097s,  933.08/s  (1.109s,  923.34/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 700/1251 ( 56%)]  Loss:  3.329509 (3.0488)  Time: 1.136s,  901.10/s  (1.109s,  923.54/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [ 750/1251 ( 60%)]  Loss:  3.010124 (3.0464)  Time: 1.097s,  933.60/s  (1.109s,  923.76/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [ 800/1251 ( 64%)]  Loss:  3.137221 (3.0518)  Time: 1.101s,  929.87/s  (1.108s,  923.82/s)  LR: 1.265e-04  Data: 0.010 (0.012)
Train: 201 [ 850/1251 ( 68%)]  Loss:  3.188413 (3.0593)  Time: 1.096s,  933.90/s  (1.108s,  923.86/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 900/1251 ( 72%)]  Loss:  2.943303 (3.0532)  Time: 1.099s,  931.37/s  (1.108s,  924.14/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [ 950/1251 ( 76%)]  Loss:  3.113545 (3.0563)  Time: 1.118s,  915.88/s  (1.109s,  923.65/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [1000/1251 ( 80%)]  Loss:  3.113763 (3.0590)  Time: 1.196s,  856.05/s  (1.108s,  923.78/s)  LR: 1.265e-04  Data: 0.011 (0.012)
Train: 201 [1050/1251 ( 84%)]  Loss:  2.854710 (3.0497)  Time: 1.095s,  934.84/s  (1.108s,  924.11/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [1100/1251 ( 88%)]  Loss:  3.137648 (3.0535)  Time: 1.101s,  929.90/s  (1.108s,  924.17/s)  LR: 1.265e-04  Data: 0.012 (0.012)
Train: 201 [1150/1251 ( 92%)]  Loss:  3.248026 (3.0616)  Time: 1.096s,  934.08/s  (1.108s,  924.27/s)  LR: 1.265e-04  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 201 [1200/1251 ( 96%)]  Loss:  3.076811 (3.0622)  Time: 1.096s,  934.26/s  (1.108s,  924.29/s)  LR: 1.265e-04  Data: 0.010 (0.012)
Train: 201 [1250/1251 (100%)]  Loss:  3.282307 (3.0707)  Time: 1.080s,  948.51/s  (1.108s,  924.39/s)  LR: 1.265e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.227 (3.227)  Loss:  0.4343 (0.4343)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.5866 (0.8995)  Acc@1: 86.5566 (79.6700)  Acc@5: 97.5236 (94.8760)
Test (EMA): [   0/48]  Time: 3.277 (3.277)  Loss:  0.3965 (0.3965)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5330 (0.8309)  Acc@1: 87.5000 (80.8280)  Acc@5: 97.8774 (95.5360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 80.724)

Train: 202 [   0/1251 (  0%)]  Loss:  2.926859 (2.9269)  Time: 1.112s,  920.64/s  (1.112s,  920.64/s)  LR: 1.243e-04  Data: 0.023 (0.023)
Train: 202 [  50/1251 (  4%)]  Loss:  3.323952 (3.1254)  Time: 1.104s,  927.25/s  (1.114s,  919.15/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 100/1251 (  8%)]  Loss:  3.167325 (3.1394)  Time: 1.101s,  930.25/s  (1.112s,  921.20/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 150/1251 ( 12%)]  Loss:  3.246207 (3.1661)  Time: 1.102s,  928.90/s  (1.112s,  921.18/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 200/1251 ( 16%)]  Loss:  3.062433 (3.1454)  Time: 1.141s,  897.14/s  (1.109s,  923.42/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 250/1251 ( 20%)]  Loss:  2.994098 (3.1201)  Time: 1.094s,  935.69/s  (1.108s,  923.90/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 300/1251 ( 24%)]  Loss:  3.018222 (3.1056)  Time: 1.096s,  934.71/s  (1.108s,  924.27/s)  LR: 1.243e-04  Data: 0.012 (0.012)
Train: 202 [ 350/1251 ( 28%)]  Loss:  3.055241 (3.0993)  Time: 1.098s,  932.31/s  (1.108s,  924.34/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 400/1251 ( 32%)]  Loss:  3.215811 (3.1122)  Time: 1.097s,  933.15/s  (1.107s,  925.03/s)  LR: 1.243e-04  Data: 0.013 (0.012)
Train: 202 [ 450/1251 ( 36%)]  Loss:  3.226326 (3.1236)  Time: 1.098s,  932.65/s  (1.107s,  924.78/s)  LR: 1.243e-04  Data: 0.012 (0.012)
Train: 202 [ 500/1251 ( 40%)]  Loss:  3.154490 (3.1265)  Time: 1.097s,  933.52/s  (1.107s,  924.87/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 550/1251 ( 44%)]  Loss:  3.126942 (3.1265)  Time: 1.102s,  929.55/s  (1.107s,  924.63/s)  LR: 1.243e-04  Data: 0.012 (0.012)
Train: 202 [ 600/1251 ( 48%)]  Loss:  2.781578 (3.1000)  Time: 1.097s,  933.17/s  (1.108s,  924.33/s)  LR: 1.243e-04  Data: 0.014 (0.012)
Train: 202 [ 650/1251 ( 52%)]  Loss:  2.950647 (3.0893)  Time: 1.094s,  936.23/s  (1.108s,  924.07/s)  LR: 1.243e-04  Data: 0.010 (0.012)
Train: 202 [ 700/1251 ( 56%)]  Loss:  2.887083 (3.0758)  Time: 1.098s,  932.41/s  (1.108s,  924.23/s)  LR: 1.243e-04  Data: 0.014 (0.012)
Train: 202 [ 750/1251 ( 60%)]  Loss:  3.185107 (3.0826)  Time: 1.097s,  933.15/s  (1.108s,  924.32/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 800/1251 ( 64%)]  Loss:  3.143905 (3.0862)  Time: 1.097s,  933.17/s  (1.108s,  924.32/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [ 850/1251 ( 68%)]  Loss:  2.724070 (3.0661)  Time: 1.160s,  882.87/s  (1.108s,  924.10/s)  LR: 1.243e-04  Data: 0.012 (0.012)
Train: 202 [ 900/1251 ( 72%)]  Loss:  3.014810 (3.0634)  Time: 1.098s,  932.54/s  (1.108s,  924.31/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 202 [ 950/1251 ( 76%)]  Loss:  2.698892 (3.0452)  Time: 1.192s,  859.29/s  (1.108s,  923.99/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [1000/1251 ( 80%)]  Loss:  3.008248 (3.0434)  Time: 1.101s,  930.48/s  (1.108s,  924.22/s)  LR: 1.243e-04  Data: 0.012 (0.012)
Train: 202 [1050/1251 ( 84%)]  Loss:  3.277757 (3.0541)  Time: 1.096s,  934.46/s  (1.108s,  924.21/s)  LR: 1.243e-04  Data: 0.013 (0.012)
Train: 202 [1100/1251 ( 88%)]  Loss:  3.013753 (3.0523)  Time: 1.130s,  906.52/s  (1.108s,  924.08/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [1150/1251 ( 92%)]  Loss:  3.187374 (3.0580)  Time: 1.102s,  928.85/s  (1.108s,  923.93/s)  LR: 1.243e-04  Data: 0.011 (0.012)
Train: 202 [1200/1251 ( 96%)]  Loss:  3.132380 (3.0609)  Time: 1.124s,  911.32/s  (1.108s,  923.79/s)  LR: 1.243e-04  Data: 0.012 (0.012)
Train: 202 [1250/1251 (100%)]  Loss:  2.914342 (3.0553)  Time: 1.079s,  948.69/s  (1.109s,  923.73/s)  LR: 1.243e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.288 (3.288)  Loss:  0.4419 (0.4419)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.413)  Loss:  0.5858 (0.9020)  Acc@1: 86.7925 (79.7780)  Acc@5: 97.6415 (94.9760)
Test (EMA): [   0/48]  Time: 3.227 (3.227)  Loss:  0.3961 (0.3961)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.411)  Loss:  0.5330 (0.8307)  Acc@1: 87.5000 (80.8620)  Acc@5: 97.9953 (95.5420)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 80.744)

Train: 203 [   0/1251 (  0%)]  Loss:  3.089428 (3.0894)  Time: 1.129s,  906.89/s  (1.129s,  906.89/s)  LR: 1.221e-04  Data: 0.020 (0.020)
Train: 203 [  50/1251 (  4%)]  Loss:  3.109688 (3.0996)  Time: 1.097s,  933.64/s  (1.108s,  923.79/s)  LR: 1.221e-04  Data: 0.011 (0.011)
Train: 203 [ 100/1251 (  8%)]  Loss:  3.207931 (3.1357)  Time: 1.100s,  931.24/s  (1.108s,  924.15/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [ 150/1251 ( 12%)]  Loss:  2.883080 (3.0725)  Time: 1.106s,  925.47/s  (1.108s,  924.58/s)  LR: 1.221e-04  Data: 0.012 (0.012)
Train: 203 [ 200/1251 ( 16%)]  Loss:  2.920803 (3.0422)  Time: 1.101s,  930.46/s  (1.109s,  923.56/s)  LR: 1.221e-04  Data: 0.017 (0.012)
Train: 203 [ 250/1251 ( 20%)]  Loss:  2.846518 (3.0096)  Time: 1.105s,  926.96/s  (1.109s,  923.49/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [ 300/1251 ( 24%)]  Loss:  3.288129 (3.0494)  Time: 1.096s,  934.45/s  (1.108s,  924.11/s)  LR: 1.221e-04  Data: 0.012 (0.011)
Train: 203 [ 350/1251 ( 28%)]  Loss:  2.937951 (3.0354)  Time: 1.098s,  932.75/s  (1.108s,  924.40/s)  LR: 1.221e-04  Data: 0.012 (0.011)
Train: 203 [ 400/1251 ( 32%)]  Loss:  3.265770 (3.0610)  Time: 1.099s,  932.04/s  (1.107s,  924.83/s)  LR: 1.221e-04  Data: 0.013 (0.011)
Train: 203 [ 450/1251 ( 36%)]  Loss:  3.402289 (3.0952)  Time: 1.130s,  906.33/s  (1.108s,  924.39/s)  LR: 1.221e-04  Data: 0.010 (0.011)
Train: 203 [ 500/1251 ( 40%)]  Loss:  2.847086 (3.0726)  Time: 1.121s,  913.55/s  (1.109s,  923.55/s)  LR: 1.221e-04  Data: 0.012 (0.011)
Train: 203 [ 550/1251 ( 44%)]  Loss:  3.229483 (3.0857)  Time: 1.097s,  933.49/s  (1.110s,  922.18/s)  LR: 1.221e-04  Data: 0.013 (0.011)
Train: 203 [ 600/1251 ( 48%)]  Loss:  3.033684 (3.0817)  Time: 1.204s,  850.70/s  (1.110s,  922.54/s)  LR: 1.221e-04  Data: 0.011 (0.011)
Train: 203 [ 650/1251 ( 52%)]  Loss:  3.038544 (3.0786)  Time: 1.096s,  934.73/s  (1.110s,  922.72/s)  LR: 1.221e-04  Data: 0.013 (0.011)
Train: 203 [ 700/1251 ( 56%)]  Loss:  3.190154 (3.0860)  Time: 1.105s,  927.06/s  (1.109s,  923.35/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [ 750/1251 ( 60%)]  Loss:  2.765712 (3.0660)  Time: 1.099s,  932.01/s  (1.109s,  923.55/s)  LR: 1.221e-04  Data: 0.013 (0.012)
Train: 203 [ 800/1251 ( 64%)]  Loss:  2.861912 (3.0540)  Time: 1.096s,  934.47/s  (1.109s,  923.07/s)  LR: 1.221e-04  Data: 0.012 (0.012)
Train: 203 [ 850/1251 ( 68%)]  Loss:  2.893786 (3.0451)  Time: 1.198s,  855.00/s  (1.109s,  923.05/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [ 900/1251 ( 72%)]  Loss:  2.977144 (3.0415)  Time: 1.096s,  933.90/s  (1.109s,  923.25/s)  LR: 1.221e-04  Data: 0.013 (0.012)
Train: 203 [ 950/1251 ( 76%)]  Loss:  3.171040 (3.0480)  Time: 1.119s,  915.32/s  (1.109s,  923.36/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [1000/1251 ( 80%)]  Loss:  3.138927 (3.0523)  Time: 1.099s,  931.96/s  (1.109s,  922.97/s)  LR: 1.221e-04  Data: 0.014 (0.012)
Train: 203 [1050/1251 ( 84%)]  Loss:  3.243926 (3.0610)  Time: 1.120s,  914.61/s  (1.109s,  923.19/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [1100/1251 ( 88%)]  Loss:  3.004645 (3.0586)  Time: 1.103s,  928.14/s  (1.110s,  922.82/s)  LR: 1.221e-04  Data: 0.011 (0.012)
Train: 203 [1150/1251 ( 92%)]  Loss:  2.850511 (3.0499)  Time: 1.102s,  929.14/s  (1.110s,  922.80/s)  LR: 1.221e-04  Data: 0.012 (0.012)
Train: 203 [1200/1251 ( 96%)]  Loss:  3.062217 (3.0504)  Time: 1.097s,  933.36/s  (1.110s,  922.84/s)  LR: 1.221e-04  Data: 0.013 (0.012)
Train: 203 [1250/1251 (100%)]  Loss:  3.391387 (3.0635)  Time: 1.080s,  948.07/s  (1.110s,  922.81/s)  LR: 1.221e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.358 (3.358)  Loss:  0.4228 (0.4228)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.410)  Loss:  0.6066 (0.8912)  Acc@1: 86.0849 (79.7120)  Acc@5: 97.5236 (94.9940)
Test (EMA): [   0/48]  Time: 3.098 (3.098)  Loss:  0.3956 (0.3956)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.408)  Loss:  0.5334 (0.8304)  Acc@1: 87.5000 (80.8700)  Acc@5: 98.1132 (95.5480)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 80.74400010253906)

Train: 204 [   0/1251 (  0%)]  Loss:  2.968390 (2.9684)  Time: 1.103s,  928.32/s  (1.103s,  928.32/s)  LR: 1.199e-04  Data: 0.021 (0.021)
Train: 204 [  50/1251 (  4%)]  Loss:  3.127097 (3.0477)  Time: 1.096s,  933.89/s  (1.110s,  922.48/s)  LR: 1.199e-04  Data: 0.012 (0.012)
Train: 204 [ 100/1251 (  8%)]  Loss:  3.111440 (3.0690)  Time: 1.103s,  928.45/s  (1.116s,  917.82/s)  LR: 1.199e-04  Data: 0.013 (0.012)
Train: 204 [ 150/1251 ( 12%)]  Loss:  2.989253 (3.0490)  Time: 1.101s,  929.79/s  (1.112s,  920.57/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 200/1251 ( 16%)]  Loss:  3.051096 (3.0495)  Time: 1.099s,  931.48/s  (1.110s,  922.16/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 250/1251 ( 20%)]  Loss:  3.096573 (3.0573)  Time: 1.097s,  933.43/s  (1.109s,  923.09/s)  LR: 1.199e-04  Data: 0.012 (0.012)
Train: 204 [ 300/1251 ( 24%)]  Loss:  3.171099 (3.0736)  Time: 1.124s,  910.96/s  (1.109s,  923.57/s)  LR: 1.199e-04  Data: 0.012 (0.012)
Train: 204 [ 350/1251 ( 28%)]  Loss:  2.944565 (3.0574)  Time: 1.126s,  909.60/s  (1.110s,  922.22/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 400/1251 ( 32%)]  Loss:  3.265993 (3.0806)  Time: 1.100s,  930.67/s  (1.111s,  921.37/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 450/1251 ( 36%)]  Loss:  2.972981 (3.0698)  Time: 1.097s,  933.52/s  (1.112s,  921.11/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 500/1251 ( 40%)]  Loss:  2.909429 (3.0553)  Time: 1.136s,  901.65/s  (1.113s,  920.23/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 550/1251 ( 44%)]  Loss:  3.046546 (3.0545)  Time: 1.243s,  823.84/s  (1.114s,  918.87/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 600/1251 ( 48%)]  Loss:  2.893565 (3.0422)  Time: 1.106s,  926.22/s  (1.114s,  919.62/s)  LR: 1.199e-04  Data: 0.010 (0.012)
Train: 204 [ 650/1251 ( 52%)]  Loss:  2.698087 (3.0176)  Time: 1.096s,  934.03/s  (1.112s,  920.57/s)  LR: 1.199e-04  Data: 0.012 (0.012)
Train: 204 [ 700/1251 ( 56%)]  Loss:  3.263715 (3.0340)  Time: 1.097s,  933.57/s  (1.112s,  920.93/s)  LR: 1.199e-04  Data: 0.013 (0.012)
Train: 204 [ 750/1251 ( 60%)]  Loss:  2.909310 (3.0262)  Time: 1.094s,  935.72/s  (1.112s,  921.25/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [ 800/1251 ( 64%)]  Loss:  3.069816 (3.0288)  Time: 1.195s,  856.97/s  (1.111s,  921.52/s)  LR: 1.199e-04  Data: 0.010 (0.012)
Train: 204 [ 850/1251 ( 68%)]  Loss:  3.244795 (3.0408)  Time: 1.103s,  928.40/s  (1.111s,  921.55/s)  LR: 1.199e-04  Data: 0.010 (0.012)
Train: 204 [ 900/1251 ( 72%)]  Loss:  3.013759 (3.0393)  Time: 1.096s,  933.96/s  (1.111s,  921.96/s)  LR: 1.199e-04  Data: 0.012 (0.012)
Train: 204 [ 950/1251 ( 76%)]  Loss:  2.887985 (3.0318)  Time: 1.108s,  924.38/s  (1.110s,  922.19/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [1000/1251 ( 80%)]  Loss:  3.009320 (3.0307)  Time: 1.099s,  931.35/s  (1.110s,  922.35/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [1050/1251 ( 84%)]  Loss:  2.944684 (3.0268)  Time: 1.098s,  932.58/s  (1.110s,  922.29/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [1100/1251 ( 88%)]  Loss:  3.163433 (3.0327)  Time: 1.100s,  930.87/s  (1.110s,  922.56/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [1150/1251 ( 92%)]  Loss:  2.979013 (3.0305)  Time: 1.094s,  935.66/s  (1.110s,  922.57/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [1200/1251 ( 96%)]  Loss:  2.867186 (3.0240)  Time: 1.097s,  933.25/s  (1.110s,  922.92/s)  LR: 1.199e-04  Data: 0.011 (0.012)
Train: 204 [1250/1251 (100%)]  Loss:  3.232251 (3.0320)  Time: 1.109s,  923.17/s  (1.110s,  922.86/s)  LR: 1.199e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.291 (3.291)  Loss:  0.4260 (0.4260)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5524 (0.8929)  Acc@1: 87.6179 (79.7840)  Acc@5: 97.7594 (94.9960)
Test (EMA): [   0/48]  Time: 3.092 (3.092)  Loss:  0.3955 (0.3955)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.408)  Loss:  0.5340 (0.8301)  Acc@1: 87.3821 (80.9220)  Acc@5: 97.8774 (95.5580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 80.75400005126953)

Train: 205 [   0/1251 (  0%)]  Loss:  3.132620 (3.1326)  Time: 1.103s,  928.76/s  (1.103s,  928.76/s)  LR: 1.177e-04  Data: 0.022 (0.022)
Train: 205 [  50/1251 (  4%)]  Loss:  2.905917 (3.0193)  Time: 1.119s,  914.86/s  (1.106s,  925.73/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [ 100/1251 (  8%)]  Loss:  2.993528 (3.0107)  Time: 1.096s,  934.29/s  (1.108s,  924.38/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [ 150/1251 ( 12%)]  Loss:  3.090126 (3.0305)  Time: 1.097s,  933.15/s  (1.107s,  925.02/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 200/1251 ( 16%)]  Loss:  3.215901 (3.0676)  Time: 1.099s,  931.44/s  (1.107s,  925.34/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 250/1251 ( 20%)]  Loss:  2.705649 (3.0073)  Time: 1.101s,  930.49/s  (1.109s,  923.66/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 300/1251 ( 24%)]  Loss:  3.183378 (3.0324)  Time: 1.096s,  934.22/s  (1.108s,  923.95/s)  LR: 1.177e-04  Data: 0.016 (0.012)
Train: 205 [ 350/1251 ( 28%)]  Loss:  3.152770 (3.0475)  Time: 1.101s,  929.95/s  (1.108s,  923.94/s)  LR: 1.177e-04  Data: 0.013 (0.012)
Train: 205 [ 400/1251 ( 32%)]  Loss:  3.391154 (3.0857)  Time: 1.099s,  931.87/s  (1.108s,  924.27/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [ 450/1251 ( 36%)]  Loss:  3.150700 (3.0922)  Time: 1.101s,  930.25/s  (1.108s,  924.24/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [ 500/1251 ( 40%)]  Loss:  3.197011 (3.1017)  Time: 1.127s,  908.49/s  (1.108s,  924.21/s)  LR: 1.177e-04  Data: 0.010 (0.012)
Train: 205 [ 550/1251 ( 44%)]  Loss:  2.821434 (3.0783)  Time: 1.103s,  928.48/s  (1.108s,  924.07/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 600/1251 ( 48%)]  Loss:  2.721631 (3.0509)  Time: 1.098s,  932.39/s  (1.108s,  924.37/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 650/1251 ( 52%)]  Loss:  2.806126 (3.0334)  Time: 1.098s,  932.50/s  (1.108s,  924.28/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 700/1251 ( 56%)]  Loss:  3.141403 (3.0406)  Time: 1.104s,  927.40/s  (1.107s,  924.73/s)  LR: 1.177e-04  Data: 0.010 (0.012)
Train: 205 [ 750/1251 ( 60%)]  Loss:  3.253421 (3.0539)  Time: 1.102s,  929.27/s  (1.107s,  924.83/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 205 [ 800/1251 ( 64%)]  Loss:  3.218032 (3.0636)  Time: 1.129s,  906.81/s  (1.108s,  924.44/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 850/1251 ( 68%)]  Loss:  3.223973 (3.0725)  Time: 1.100s,  931.33/s  (1.108s,  924.36/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [ 900/1251 ( 72%)]  Loss:  3.105980 (3.0743)  Time: 1.097s,  933.69/s  (1.108s,  924.60/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [ 950/1251 ( 76%)]  Loss:  3.269365 (3.0840)  Time: 1.094s,  936.27/s  (1.107s,  924.67/s)  LR: 1.177e-04  Data: 0.010 (0.012)
Train: 205 [1000/1251 ( 80%)]  Loss:  3.052968 (3.0825)  Time: 1.096s,  933.99/s  (1.107s,  924.90/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [1050/1251 ( 84%)]  Loss:  3.014003 (3.0794)  Time: 1.101s,  930.37/s  (1.107s,  925.15/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [1100/1251 ( 88%)]  Loss:  3.366499 (3.0919)  Time: 1.096s,  934.13/s  (1.107s,  925.09/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [1150/1251 ( 92%)]  Loss:  3.315318 (3.1012)  Time: 1.099s,  932.17/s  (1.107s,  924.81/s)  LR: 1.177e-04  Data: 0.011 (0.012)
Train: 205 [1200/1251 ( 96%)]  Loss:  3.096761 (3.1010)  Time: 1.097s,  933.48/s  (1.107s,  924.89/s)  LR: 1.177e-04  Data: 0.012 (0.012)
Train: 205 [1250/1251 (100%)]  Loss:  2.760695 (3.0879)  Time: 1.104s,  927.34/s  (1.107s,  924.64/s)  LR: 1.177e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.176 (3.176)  Loss:  0.4563 (0.4563)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.407)  Loss:  0.5765 (0.9060)  Acc@1: 87.5000 (79.7360)  Acc@5: 97.6415 (94.9680)
Test (EMA): [   0/48]  Time: 3.175 (3.175)  Loss:  0.3961 (0.3961)  Acc@1: 93.4570 (93.4570)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.410)  Loss:  0.5348 (0.8299)  Acc@1: 87.3821 (80.9280)  Acc@5: 97.8774 (95.5680)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 80.76799997314453)

Train: 206 [   0/1251 (  0%)]  Loss:  2.920863 (2.9209)  Time: 1.105s,  926.94/s  (1.105s,  926.94/s)  LR: 1.155e-04  Data: 0.023 (0.023)
Train: 206 [  50/1251 (  4%)]  Loss:  2.837473 (2.8792)  Time: 1.094s,  935.90/s  (1.105s,  926.76/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 206 [ 100/1251 (  8%)]  Loss:  3.175794 (2.9780)  Time: 1.121s,  913.69/s  (1.105s,  926.70/s)  LR: 1.155e-04  Data: 0.012 (0.011)
Train: 206 [ 150/1251 ( 12%)]  Loss:  2.928153 (2.9656)  Time: 1.100s,  930.78/s  (1.108s,  923.87/s)  LR: 1.155e-04  Data: 0.012 (0.011)
Train: 206 [ 200/1251 ( 16%)]  Loss:  3.095464 (2.9915)  Time: 1.098s,  932.71/s  (1.108s,  924.48/s)  LR: 1.155e-04  Data: 0.012 (0.011)
Train: 206 [ 250/1251 ( 20%)]  Loss:  3.070781 (3.0048)  Time: 1.099s,  931.34/s  (1.108s,  924.43/s)  LR: 1.155e-04  Data: 0.016 (0.012)
Train: 206 [ 300/1251 ( 24%)]  Loss:  2.562762 (2.9416)  Time: 1.097s,  933.41/s  (1.108s,  923.87/s)  LR: 1.155e-04  Data: 0.012 (0.012)
Train: 206 [ 350/1251 ( 28%)]  Loss:  3.187655 (2.9724)  Time: 1.189s,  861.04/s  (1.108s,  924.33/s)  LR: 1.155e-04  Data: 0.013 (0.012)
Train: 206 [ 400/1251 ( 32%)]  Loss:  2.988994 (2.9742)  Time: 1.099s,  931.34/s  (1.109s,  923.68/s)  LR: 1.155e-04  Data: 0.012 (0.012)
Train: 206 [ 450/1251 ( 36%)]  Loss:  3.020240 (2.9788)  Time: 1.101s,  930.46/s  (1.108s,  923.81/s)  LR: 1.155e-04  Data: 0.013 (0.012)
Train: 206 [ 500/1251 ( 40%)]  Loss:  3.066721 (2.9868)  Time: 1.098s,  932.88/s  (1.109s,  923.53/s)  LR: 1.155e-04  Data: 0.012 (0.012)
Train: 206 [ 550/1251 ( 44%)]  Loss:  3.183124 (3.0032)  Time: 1.096s,  933.97/s  (1.108s,  923.82/s)  LR: 1.155e-04  Data: 0.012 (0.012)
Train: 206 [ 600/1251 ( 48%)]  Loss:  2.727439 (2.9820)  Time: 1.097s,  933.28/s  (1.108s,  923.84/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [ 650/1251 ( 52%)]  Loss:  3.263344 (3.0021)  Time: 1.099s,  931.83/s  (1.108s,  924.31/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [ 700/1251 ( 56%)]  Loss:  3.190140 (3.0146)  Time: 1.099s,  931.68/s  (1.108s,  924.59/s)  LR: 1.155e-04  Data: 0.014 (0.012)
Train: 206 [ 750/1251 ( 60%)]  Loss:  3.063569 (3.0177)  Time: 1.101s,  930.27/s  (1.108s,  924.58/s)  LR: 1.155e-04  Data: 0.012 (0.012)
Train: 206 [ 800/1251 ( 64%)]  Loss:  2.765486 (3.0028)  Time: 1.096s,  934.52/s  (1.107s,  924.77/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [ 850/1251 ( 68%)]  Loss:  2.846431 (2.9941)  Time: 1.100s,  930.87/s  (1.108s,  924.53/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [ 900/1251 ( 72%)]  Loss:  2.954567 (2.9921)  Time: 1.098s,  932.21/s  (1.108s,  924.48/s)  LR: 1.155e-04  Data: 0.013 (0.012)
Train: 206 [ 950/1251 ( 76%)]  Loss:  3.024523 (2.9937)  Time: 1.099s,  931.56/s  (1.107s,  924.62/s)  LR: 1.155e-04  Data: 0.014 (0.012)
Train: 206 [1000/1251 ( 80%)]  Loss:  3.245663 (3.0057)  Time: 1.100s,  930.69/s  (1.107s,  924.91/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [1050/1251 ( 84%)]  Loss:  2.978873 (3.0045)  Time: 1.125s,  910.22/s  (1.107s,  924.98/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [1100/1251 ( 88%)]  Loss:  3.231118 (3.0143)  Time: 1.092s,  938.07/s  (1.107s,  925.15/s)  LR: 1.155e-04  Data: 0.010 (0.012)
Train: 206 [1150/1251 ( 92%)]  Loss:  2.818735 (3.0062)  Time: 1.096s,  934.16/s  (1.107s,  925.13/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [1200/1251 ( 96%)]  Loss:  3.090008 (3.0095)  Time: 1.134s,  903.24/s  (1.107s,  925.22/s)  LR: 1.155e-04  Data: 0.011 (0.012)
Train: 206 [1250/1251 (100%)]  Loss:  3.073479 (3.0120)  Time: 1.081s,  947.40/s  (1.107s,  924.70/s)  LR: 1.155e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.234 (3.234)  Loss:  0.4414 (0.4414)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.5598 (0.8925)  Acc@1: 86.7925 (79.6360)  Acc@5: 97.7594 (94.9320)
Test (EMA): [   0/48]  Time: 3.257 (3.257)  Loss:  0.3969 (0.3969)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5354 (0.8300)  Acc@1: 87.6179 (80.9100)  Acc@5: 97.8774 (95.5760)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 80.79600005126953)

Train: 207 [   0/1251 (  0%)]  Loss:  3.129396 (3.1294)  Time: 1.104s,  927.27/s  (1.104s,  927.27/s)  LR: 1.134e-04  Data: 0.024 (0.024)
Train: 207 [  50/1251 (  4%)]  Loss:  3.123758 (3.1266)  Time: 1.097s,  933.13/s  (1.111s,  921.41/s)  LR: 1.134e-04  Data: 0.014 (0.012)
Train: 207 [ 100/1251 (  8%)]  Loss:  2.949868 (3.0677)  Time: 1.096s,  934.14/s  (1.110s,  922.41/s)  LR: 1.134e-04  Data: 0.015 (0.012)
Train: 207 [ 150/1251 ( 12%)]  Loss:  3.031389 (3.0586)  Time: 1.122s,  912.50/s  (1.109s,  922.95/s)  LR: 1.134e-04  Data: 0.010 (0.012)
Train: 207 [ 200/1251 ( 16%)]  Loss:  2.991965 (3.0453)  Time: 1.113s,  919.97/s  (1.112s,  920.89/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Train: 207 [ 250/1251 ( 20%)]  Loss:  2.946674 (3.0288)  Time: 1.098s,  932.78/s  (1.110s,  922.22/s)  LR: 1.134e-04  Data: 0.010 (0.012)
Train: 207 [ 300/1251 ( 24%)]  Loss:  2.744218 (2.9882)  Time: 1.215s,  842.54/s  (1.112s,  920.58/s)  LR: 1.134e-04  Data: 0.009 (0.012)
Train: 207 [ 350/1251 ( 28%)]  Loss:  3.378755 (3.0370)  Time: 1.097s,  933.71/s  (1.111s,  921.46/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [ 400/1251 ( 32%)]  Loss:  3.024829 (3.0357)  Time: 1.095s,  935.19/s  (1.110s,  922.20/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Train: 207 [ 450/1251 ( 36%)]  Loss:  2.952617 (3.0273)  Time: 1.127s,  908.67/s  (1.111s,  921.36/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Train: 207 [ 500/1251 ( 40%)]  Loss:  2.950938 (3.0204)  Time: 1.123s,  912.22/s  (1.110s,  922.16/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Train: 207 [ 550/1251 ( 44%)]  Loss:  3.011916 (3.0197)  Time: 1.097s,  933.67/s  (1.110s,  922.30/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [ 600/1251 ( 48%)]  Loss:  2.789273 (3.0020)  Time: 1.103s,  928.54/s  (1.110s,  922.28/s)  LR: 1.134e-04  Data: 0.013 (0.012)
Train: 207 [ 650/1251 ( 52%)]  Loss:  3.116725 (3.0102)  Time: 1.122s,  912.62/s  (1.110s,  922.42/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [ 700/1251 ( 56%)]  Loss:  3.181832 (3.0216)  Time: 1.135s,  902.31/s  (1.110s,  922.23/s)  LR: 1.134e-04  Data: 0.014 (0.012)
Train: 207 [ 750/1251 ( 60%)]  Loss:  2.905356 (3.0143)  Time: 1.099s,  932.01/s  (1.110s,  922.44/s)  LR: 1.134e-04  Data: 0.010 (0.012)
Train: 207 [ 800/1251 ( 64%)]  Loss:  2.791456 (3.0012)  Time: 1.131s,  905.32/s  (1.110s,  922.80/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [ 850/1251 ( 68%)]  Loss:  3.013108 (3.0019)  Time: 1.129s,  906.77/s  (1.110s,  922.17/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [ 900/1251 ( 72%)]  Loss:  3.093072 (3.0067)  Time: 1.098s,  932.23/s  (1.111s,  921.94/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [ 950/1251 ( 76%)]  Loss:  3.228377 (3.0178)  Time: 1.102s,  929.25/s  (1.110s,  922.47/s)  LR: 1.134e-04  Data: 0.010 (0.012)
Train: 207 [1000/1251 ( 80%)]  Loss:  2.988223 (3.0164)  Time: 1.094s,  935.73/s  (1.110s,  922.50/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Train: 207 [1050/1251 ( 84%)]  Loss:  3.049962 (3.0179)  Time: 1.100s,  930.53/s  (1.110s,  922.86/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Train: 207 [1100/1251 ( 88%)]  Loss:  3.140009 (3.0232)  Time: 1.096s,  934.16/s  (1.109s,  923.05/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [1150/1251 ( 92%)]  Loss:  2.867984 (3.0167)  Time: 1.097s,  933.45/s  (1.109s,  923.17/s)  LR: 1.134e-04  Data: 0.011 (0.012)
Train: 207 [1200/1251 ( 96%)]  Loss:  2.964018 (3.0146)  Time: 1.096s,  934.08/s  (1.109s,  923.27/s)  LR: 1.134e-04  Data: 0.012 (0.012)
Train: 207 [1250/1251 (100%)]  Loss:  3.047220 (3.0159)  Time: 1.081s,  947.01/s  (1.110s,  922.91/s)  LR: 1.134e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.290 (3.290)  Loss:  0.4342 (0.4342)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5553 (0.8954)  Acc@1: 87.3821 (79.8980)  Acc@5: 97.8774 (95.0260)
Test (EMA): [   0/48]  Time: 3.037 (3.037)  Loss:  0.3968 (0.3968)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5355 (0.8298)  Acc@1: 87.6179 (80.9380)  Acc@5: 97.8774 (95.5660)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 80.82)

Train: 208 [   0/1251 (  0%)]  Loss:  3.308934 (3.3089)  Time: 1.108s,  924.14/s  (1.108s,  924.14/s)  LR: 1.112e-04  Data: 0.024 (0.024)
Train: 208 [  50/1251 (  4%)]  Loss:  2.849705 (3.0793)  Time: 1.106s,  926.07/s  (1.113s,  920.26/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 100/1251 (  8%)]  Loss:  2.961694 (3.0401)  Time: 1.097s,  933.45/s  (1.109s,  923.07/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 150/1251 ( 12%)]  Loss:  3.016346 (3.0342)  Time: 1.098s,  932.80/s  (1.107s,  925.41/s)  LR: 1.112e-04  Data: 0.013 (0.012)
Train: 208 [ 200/1251 ( 16%)]  Loss:  3.030754 (3.0335)  Time: 1.103s,  928.25/s  (1.106s,  926.11/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 250/1251 ( 20%)]  Loss:  3.376827 (3.0907)  Time: 1.099s,  931.43/s  (1.107s,  925.39/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 300/1251 ( 24%)]  Loss:  3.199610 (3.1063)  Time: 1.101s,  929.72/s  (1.106s,  925.76/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 350/1251 ( 28%)]  Loss:  2.985468 (3.0912)  Time: 1.138s,  900.10/s  (1.106s,  926.26/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 400/1251 ( 32%)]  Loss:  3.096054 (3.0917)  Time: 1.098s,  932.63/s  (1.106s,  925.94/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 450/1251 ( 36%)]  Loss:  2.583789 (3.0409)  Time: 1.095s,  935.13/s  (1.105s,  926.51/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 500/1251 ( 40%)]  Loss:  3.044746 (3.0413)  Time: 1.098s,  932.99/s  (1.105s,  926.63/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 550/1251 ( 44%)]  Loss:  3.327067 (3.0651)  Time: 1.120s,  914.60/s  (1.105s,  926.64/s)  LR: 1.112e-04  Data: 0.010 (0.012)
Train: 208 [ 600/1251 ( 48%)]  Loss:  3.140171 (3.0709)  Time: 1.135s,  902.20/s  (1.106s,  925.82/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 650/1251 ( 52%)]  Loss:  3.161625 (3.0773)  Time: 1.094s,  935.60/s  (1.106s,  925.58/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 700/1251 ( 56%)]  Loss:  2.831413 (3.0609)  Time: 1.097s,  933.67/s  (1.107s,  924.98/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 750/1251 ( 60%)]  Loss:  2.961317 (3.0547)  Time: 1.104s,  927.92/s  (1.107s,  924.61/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [ 800/1251 ( 64%)]  Loss:  3.293283 (3.0688)  Time: 1.098s,  932.34/s  (1.108s,  924.60/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 850/1251 ( 68%)]  Loss:  2.946120 (3.0619)  Time: 1.119s,  914.83/s  (1.108s,  924.35/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [ 900/1251 ( 72%)]  Loss:  2.868739 (3.0518)  Time: 1.103s,  928.02/s  (1.109s,  923.73/s)  LR: 1.112e-04  Data: 0.010 (0.012)
Train: 208 [ 950/1251 ( 76%)]  Loss:  3.152037 (3.0568)  Time: 1.121s,  913.44/s  (1.109s,  923.27/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [1000/1251 ( 80%)]  Loss:  3.215219 (3.0643)  Time: 1.120s,  913.94/s  (1.109s,  923.26/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [1050/1251 ( 84%)]  Loss:  3.238000 (3.0722)  Time: 1.097s,  933.53/s  (1.109s,  923.11/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [1100/1251 ( 88%)]  Loss:  3.180268 (3.0769)  Time: 1.100s,  931.33/s  (1.109s,  923.42/s)  LR: 1.112e-04  Data: 0.011 (0.012)
Train: 208 [1150/1251 ( 92%)]  Loss:  2.847203 (3.0673)  Time: 1.097s,  933.04/s  (1.109s,  923.50/s)  LR: 1.112e-04  Data: 0.012 (0.012)
Train: 208 [1200/1251 ( 96%)]  Loss:  3.242802 (3.0744)  Time: 1.098s,  932.96/s  (1.109s,  923.56/s)  LR: 1.112e-04  Data: 0.013 (0.012)
Train: 208 [1250/1251 (100%)]  Loss:  2.723994 (3.0609)  Time: 1.079s,  949.10/s  (1.109s,  923.48/s)  LR: 1.112e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.265 (3.265)  Loss:  0.4291 (0.4291)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.5743 (0.8781)  Acc@1: 86.7925 (79.6900)  Acc@5: 97.2877 (95.0760)
Test (EMA): [   0/48]  Time: 3.126 (3.126)  Loss:  0.3965 (0.3965)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5348 (0.8297)  Acc@1: 87.7358 (80.9520)  Acc@5: 97.8774 (95.5780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 80.828)

Train: 209 [   0/1251 (  0%)]  Loss:  2.849842 (2.8498)  Time: 1.105s,  926.67/s  (1.105s,  926.67/s)  LR: 1.091e-04  Data: 0.022 (0.022)
Train: 209 [  50/1251 (  4%)]  Loss:  3.076442 (2.9631)  Time: 1.102s,  929.43/s  (1.108s,  923.88/s)  LR: 1.091e-04  Data: 0.011 (0.012)
Train: 209 [ 100/1251 (  8%)]  Loss:  3.402637 (3.1096)  Time: 1.098s,  932.98/s  (1.109s,  923.61/s)  LR: 1.091e-04  Data: 0.012 (0.012)
Train: 209 [ 150/1251 ( 12%)]  Loss:  2.954759 (3.0709)  Time: 1.118s,  915.69/s  (1.108s,  924.28/s)  LR: 1.091e-04  Data: 0.010 (0.012)
Train: 209 [ 200/1251 ( 16%)]  Loss:  3.020684 (3.0609)  Time: 1.097s,  933.78/s  (1.111s,  921.74/s)  LR: 1.091e-04  Data: 0.010 (0.011)
Train: 209 [ 250/1251 ( 20%)]  Loss:  3.277755 (3.0970)  Time: 1.095s,  935.22/s  (1.110s,  922.63/s)  LR: 1.091e-04  Data: 0.012 (0.011)
Train: 209 [ 300/1251 ( 24%)]  Loss:  3.010324 (3.0846)  Time: 1.097s,  933.87/s  (1.109s,  923.11/s)  LR: 1.091e-04  Data: 0.011 (0.011)
Train: 209 [ 350/1251 ( 28%)]  Loss:  3.346075 (3.1173)  Time: 1.098s,  932.29/s  (1.110s,  922.11/s)  LR: 1.091e-04  Data: 0.012 (0.011)
Train: 209 [ 400/1251 ( 32%)]  Loss:  3.056704 (3.1106)  Time: 1.097s,  933.87/s  (1.110s,  922.69/s)  LR: 1.091e-04  Data: 0.009 (0.011)
Train: 209 [ 450/1251 ( 36%)]  Loss:  3.121948 (3.1117)  Time: 1.095s,  934.88/s  (1.110s,  922.52/s)  LR: 1.091e-04  Data: 0.012 (0.011)
Train: 209 [ 500/1251 ( 40%)]  Loss:  3.175469 (3.1175)  Time: 1.097s,  933.63/s  (1.110s,  922.77/s)  LR: 1.091e-04  Data: 0.012 (0.011)
Train: 209 [ 550/1251 ( 44%)]  Loss:  3.165711 (3.1215)  Time: 1.108s,  924.31/s  (1.110s,  922.34/s)  LR: 1.091e-04  Data: 0.010 (0.011)
Train: 209 [ 600/1251 ( 48%)]  Loss:  3.099085 (3.1198)  Time: 1.130s,  906.05/s  (1.111s,  921.96/s)  LR: 1.091e-04  Data: 0.011 (0.011)
Train: 209 [ 650/1251 ( 52%)]  Loss:  3.329941 (3.1348)  Time: 1.096s,  934.37/s  (1.110s,  922.35/s)  LR: 1.091e-04  Data: 0.011 (0.011)
Train: 209 [ 700/1251 ( 56%)]  Loss:  3.222237 (3.1406)  Time: 1.099s,  931.80/s  (1.110s,  922.72/s)  LR: 1.091e-04  Data: 0.015 (0.011)
Train: 209 [ 750/1251 ( 60%)]  Loss:  2.690491 (3.1125)  Time: 1.096s,  934.12/s  (1.110s,  922.38/s)  LR: 1.091e-04  Data: 0.011 (0.011)
Train: 209 [ 800/1251 ( 64%)]  Loss:  2.908439 (3.1005)  Time: 1.122s,  912.26/s  (1.110s,  922.49/s)  LR: 1.091e-04  Data: 0.010 (0.011)
Train: 209 [ 850/1251 ( 68%)]  Loss:  3.252172 (3.1089)  Time: 1.096s,  934.57/s  (1.109s,  922.95/s)  LR: 1.091e-04  Data: 0.012 (0.011)
Train: 209 [ 900/1251 ( 72%)]  Loss:  2.978779 (3.1021)  Time: 1.096s,  934.10/s  (1.110s,  922.77/s)  LR: 1.091e-04  Data: 0.012 (0.011)
Train: 209 [ 950/1251 ( 76%)]  Loss:  3.232470 (3.1086)  Time: 1.095s,  935.08/s  (1.110s,  922.75/s)  LR: 1.091e-04  Data: 0.011 (0.012)
Train: 209 [1000/1251 ( 80%)]  Loss:  3.040302 (3.1053)  Time: 1.174s,  872.29/s  (1.110s,  922.87/s)  LR: 1.091e-04  Data: 0.011 (0.012)
Train: 209 [1050/1251 ( 84%)]  Loss:  3.191667 (3.1093)  Time: 1.095s,  935.00/s  (1.109s,  923.18/s)  LR: 1.091e-04  Data: 0.012 (0.012)
Train: 209 [1100/1251 ( 88%)]  Loss:  3.204484 (3.1134)  Time: 1.105s,  927.02/s  (1.109s,  922.97/s)  LR: 1.091e-04  Data: 0.012 (0.012)
Train: 209 [1150/1251 ( 92%)]  Loss:  3.252526 (3.1192)  Time: 1.100s,  930.83/s  (1.109s,  923.17/s)  LR: 1.091e-04  Data: 0.014 (0.012)
Train: 209 [1200/1251 ( 96%)]  Loss:  3.020133 (3.1152)  Time: 1.095s,  935.38/s  (1.109s,  923.14/s)  LR: 1.091e-04  Data: 0.010 (0.012)
Train: 209 [1250/1251 (100%)]  Loss:  2.931413 (3.1082)  Time: 1.088s,  940.76/s  (1.109s,  923.22/s)  LR: 1.091e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.316 (3.316)  Loss:  0.4394 (0.4394)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.5446 (0.8865)  Acc@1: 87.9717 (79.9640)  Acc@5: 97.9953 (94.9240)
Test (EMA): [   0/48]  Time: 3.320 (3.320)  Loss:  0.3965 (0.3965)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  0.5345 (0.8297)  Acc@1: 87.6179 (80.9400)  Acc@5: 97.8774 (95.5920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 80.83000005126954)

Train: 210 [   0/1251 (  0%)]  Loss:  3.080981 (3.0810)  Time: 1.103s,  928.76/s  (1.103s,  928.76/s)  LR: 1.070e-04  Data: 0.020 (0.020)
Train: 210 [  50/1251 (  4%)]  Loss:  2.980475 (3.0307)  Time: 1.097s,  933.34/s  (1.108s,  924.24/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [ 100/1251 (  8%)]  Loss:  2.834426 (2.9653)  Time: 1.097s,  933.61/s  (1.110s,  922.47/s)  LR: 1.070e-04  Data: 0.010 (0.012)
Train: 210 [ 150/1251 ( 12%)]  Loss:  3.174852 (3.0177)  Time: 1.112s,  920.88/s  (1.108s,  924.28/s)  LR: 1.070e-04  Data: 0.010 (0.012)
Train: 210 [ 200/1251 ( 16%)]  Loss:  2.542654 (2.9227)  Time: 1.095s,  934.94/s  (1.106s,  925.55/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 250/1251 ( 20%)]  Loss:  3.019328 (2.9388)  Time: 1.095s,  935.06/s  (1.107s,  925.11/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 300/1251 ( 24%)]  Loss:  3.012266 (2.9493)  Time: 1.098s,  932.58/s  (1.107s,  924.93/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [ 350/1251 ( 28%)]  Loss:  3.119349 (2.9705)  Time: 1.124s,  911.24/s  (1.107s,  924.73/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [ 400/1251 ( 32%)]  Loss:  2.712054 (2.9418)  Time: 1.119s,  915.22/s  (1.107s,  924.63/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [ 450/1251 ( 36%)]  Loss:  3.139897 (2.9616)  Time: 1.102s,  928.85/s  (1.108s,  924.51/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 500/1251 ( 40%)]  Loss:  3.166158 (2.9802)  Time: 1.096s,  934.68/s  (1.107s,  924.85/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 550/1251 ( 44%)]  Loss:  2.729851 (2.9594)  Time: 1.098s,  932.62/s  (1.107s,  925.21/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 600/1251 ( 48%)]  Loss:  3.136554 (2.9730)  Time: 1.100s,  931.28/s  (1.107s,  924.96/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 650/1251 ( 52%)]  Loss:  3.011750 (2.9758)  Time: 1.120s,  914.57/s  (1.108s,  924.37/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 700/1251 ( 56%)]  Loss:  3.053588 (2.9809)  Time: 1.118s,  915.63/s  (1.108s,  924.48/s)  LR: 1.070e-04  Data: 0.010 (0.012)
Train: 210 [ 750/1251 ( 60%)]  Loss:  2.940116 (2.9784)  Time: 1.096s,  933.95/s  (1.108s,  924.54/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 210 [ 800/1251 ( 64%)]  Loss:  2.950316 (2.9767)  Time: 1.104s,  927.32/s  (1.107s,  924.71/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [ 850/1251 ( 68%)]  Loss:  3.062280 (2.9815)  Time: 1.096s,  933.99/s  (1.107s,  924.84/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [ 900/1251 ( 72%)]  Loss:  3.012681 (2.9831)  Time: 1.099s,  932.02/s  (1.107s,  925.18/s)  LR: 1.070e-04  Data: 0.010 (0.012)
Train: 210 [ 950/1251 ( 76%)]  Loss:  2.794402 (2.9737)  Time: 1.204s,  850.82/s  (1.107s,  924.62/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [1000/1251 ( 80%)]  Loss:  2.712911 (2.9613)  Time: 1.122s,  913.06/s  (1.107s,  924.67/s)  LR: 1.070e-04  Data: 0.012 (0.012)
Train: 210 [1050/1251 ( 84%)]  Loss:  3.187296 (2.9716)  Time: 1.105s,  926.90/s  (1.107s,  924.88/s)  LR: 1.070e-04  Data: 0.010 (0.012)
Train: 210 [1100/1251 ( 88%)]  Loss:  2.876910 (2.9674)  Time: 1.098s,  932.75/s  (1.107s,  924.72/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [1150/1251 ( 92%)]  Loss:  2.938556 (2.9662)  Time: 1.101s,  929.81/s  (1.108s,  924.56/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [1200/1251 ( 96%)]  Loss:  3.073512 (2.9705)  Time: 1.098s,  932.94/s  (1.108s,  924.47/s)  LR: 1.070e-04  Data: 0.011 (0.012)
Train: 210 [1250/1251 (100%)]  Loss:  3.135850 (2.9769)  Time: 1.081s,  947.03/s  (1.108s,  924.46/s)  LR: 1.070e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.345 (3.345)  Loss:  0.4546 (0.4546)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5899 (0.9284)  Acc@1: 86.3208 (79.7340)  Acc@5: 97.4057 (94.9600)
Test (EMA): [   0/48]  Time: 3.199 (3.199)  Loss:  0.3964 (0.3964)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.408)  Loss:  0.5341 (0.8297)  Acc@1: 87.6179 (80.9400)  Acc@5: 97.7594 (95.6180)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 80.85200005126953)

Train: 211 [   0/1251 (  0%)]  Loss:  3.140853 (3.1409)  Time: 1.108s,  924.08/s  (1.108s,  924.08/s)  LR: 1.049e-04  Data: 0.023 (0.023)
Train: 211 [  50/1251 (  4%)]  Loss:  2.908723 (3.0248)  Time: 1.098s,  932.59/s  (1.104s,  927.37/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 100/1251 (  8%)]  Loss:  3.155369 (3.0683)  Time: 1.095s,  935.16/s  (1.107s,  925.07/s)  LR: 1.049e-04  Data: 0.011 (0.012)
Train: 211 [ 150/1251 ( 12%)]  Loss:  3.082511 (3.0719)  Time: 1.098s,  932.41/s  (1.106s,  926.05/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 200/1251 ( 16%)]  Loss:  3.203040 (3.0981)  Time: 1.116s,  917.70/s  (1.106s,  926.11/s)  LR: 1.049e-04  Data: 0.013 (0.012)
Train: 211 [ 250/1251 ( 20%)]  Loss:  3.110779 (3.1002)  Time: 1.098s,  932.76/s  (1.106s,  925.92/s)  LR: 1.049e-04  Data: 0.013 (0.012)
Train: 211 [ 300/1251 ( 24%)]  Loss:  2.939628 (3.0773)  Time: 1.092s,  937.54/s  (1.107s,  925.34/s)  LR: 1.049e-04  Data: 0.011 (0.012)
Train: 211 [ 350/1251 ( 28%)]  Loss:  2.755646 (3.0371)  Time: 1.205s,  849.80/s  (1.107s,  925.09/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 400/1251 ( 32%)]  Loss:  2.934683 (3.0257)  Time: 1.097s,  933.07/s  (1.106s,  925.52/s)  LR: 1.049e-04  Data: 0.013 (0.012)
Train: 211 [ 450/1251 ( 36%)]  Loss:  3.028814 (3.0260)  Time: 1.104s,  927.13/s  (1.107s,  925.15/s)  LR: 1.049e-04  Data: 0.010 (0.012)
Train: 211 [ 500/1251 ( 40%)]  Loss:  3.120432 (3.0346)  Time: 1.096s,  934.06/s  (1.107s,  925.31/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 550/1251 ( 44%)]  Loss:  2.925951 (3.0255)  Time: 1.096s,  934.08/s  (1.107s,  925.24/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 600/1251 ( 48%)]  Loss:  2.843343 (3.0115)  Time: 1.132s,  904.94/s  (1.108s,  924.41/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 650/1251 ( 52%)]  Loss:  2.903901 (3.0038)  Time: 1.096s,  933.91/s  (1.108s,  923.89/s)  LR: 1.049e-04  Data: 0.011 (0.012)
Train: 211 [ 700/1251 ( 56%)]  Loss:  3.103798 (3.0105)  Time: 1.095s,  934.76/s  (1.108s,  923.90/s)  LR: 1.049e-04  Data: 0.011 (0.012)
Train: 211 [ 750/1251 ( 60%)]  Loss:  2.951409 (3.0068)  Time: 1.102s,  929.20/s  (1.108s,  924.01/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 800/1251 ( 64%)]  Loss:  2.967167 (3.0045)  Time: 1.096s,  934.07/s  (1.108s,  924.04/s)  LR: 1.049e-04  Data: 0.013 (0.012)
Train: 211 [ 850/1251 ( 68%)]  Loss:  3.015363 (3.0051)  Time: 1.103s,  928.01/s  (1.108s,  924.23/s)  LR: 1.049e-04  Data: 0.011 (0.012)
Train: 211 [ 900/1251 ( 72%)]  Loss:  3.272013 (3.0191)  Time: 1.099s,  931.76/s  (1.108s,  924.26/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [ 950/1251 ( 76%)]  Loss:  3.125179 (3.0244)  Time: 1.098s,  932.34/s  (1.108s,  924.37/s)  LR: 1.049e-04  Data: 0.014 (0.012)
Train: 211 [1000/1251 ( 80%)]  Loss:  2.780515 (3.0128)  Time: 1.098s,  932.25/s  (1.108s,  924.53/s)  LR: 1.049e-04  Data: 0.012 (0.012)
Train: 211 [1050/1251 ( 84%)]  Loss:  3.167511 (3.0198)  Time: 1.097s,  933.43/s  (1.108s,  924.31/s)  LR: 1.049e-04  Data: 0.016 (0.012)
Train: 211 [1100/1251 ( 88%)]  Loss:  2.915904 (3.0153)  Time: 1.097s,  933.16/s  (1.108s,  924.27/s)  LR: 1.049e-04  Data: 0.013 (0.012)
Train: 211 [1150/1251 ( 92%)]  Loss:  3.045196 (3.0166)  Time: 1.136s,  901.28/s  (1.108s,  924.22/s)  LR: 1.049e-04  Data: 0.011 (0.012)
Train: 211 [1200/1251 ( 96%)]  Loss:  2.976481 (3.0150)  Time: 1.097s,  933.43/s  (1.108s,  924.26/s)  LR: 1.049e-04  Data: 0.013 (0.012)
Train: 211 [1250/1251 (100%)]  Loss:  3.167137 (3.0208)  Time: 1.081s,  947.03/s  (1.108s,  924.26/s)  LR: 1.049e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.220 (3.220)  Loss:  0.4148 (0.4148)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.5758 (0.8889)  Acc@1: 86.7924 (79.8340)  Acc@5: 96.9340 (95.1080)
Test (EMA): [   0/48]  Time: 3.125 (3.125)  Loss:  0.3961 (0.3961)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5343 (0.8298)  Acc@1: 87.7358 (80.9560)  Acc@5: 97.7594 (95.6320)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 80.86200012939453)

Train: 212 [   0/1251 (  0%)]  Loss:  2.921867 (2.9219)  Time: 1.102s,  929.03/s  (1.102s,  929.03/s)  LR: 1.029e-04  Data: 0.020 (0.020)
Train: 212 [  50/1251 (  4%)]  Loss:  3.054983 (2.9884)  Time: 1.101s,  930.47/s  (1.104s,  927.17/s)  LR: 1.029e-04  Data: 0.012 (0.012)
Train: 212 [ 100/1251 (  8%)]  Loss:  3.097081 (3.0246)  Time: 1.096s,  934.68/s  (1.104s,  927.36/s)  LR: 1.029e-04  Data: 0.012 (0.012)
Train: 212 [ 150/1251 ( 12%)]  Loss:  2.857546 (2.9829)  Time: 1.095s,  935.47/s  (1.104s,  927.83/s)  LR: 1.029e-04  Data: 0.012 (0.012)
Train: 212 [ 200/1251 ( 16%)]  Loss:  3.134248 (3.0131)  Time: 1.132s,  904.71/s  (1.103s,  928.20/s)  LR: 1.029e-04  Data: 0.011 (0.012)
Train: 212 [ 250/1251 ( 20%)]  Loss:  3.105731 (3.0286)  Time: 1.093s,  936.46/s  (1.104s,  927.41/s)  LR: 1.029e-04  Data: 0.009 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 212 [ 300/1251 ( 24%)]  Loss:  2.706754 (2.9826)  Time: 1.180s,  867.73/s  (1.104s,  927.12/s)  LR: 1.029e-04  Data: 0.011 (0.012)
Train: 212 [ 350/1251 ( 28%)]  Loss:  2.791009 (2.9587)  Time: 1.094s,  935.80/s  (1.105s,  927.06/s)  LR: 1.029e-04  Data: 0.010 (0.012)
Train: 212 [ 400/1251 ( 32%)]  Loss:  3.173408 (2.9825)  Time: 1.098s,  932.68/s  (1.105s,  927.06/s)  LR: 1.029e-04  Data: 0.010 (0.012)
Train: 212 [ 450/1251 ( 36%)]  Loss:  3.211086 (3.0054)  Time: 1.105s,  926.64/s  (1.104s,  927.23/s)  LR: 1.029e-04  Data: 0.011 (0.012)
Train: 212 [ 500/1251 ( 40%)]  Loss:  3.068449 (3.0111)  Time: 1.097s,  933.32/s  (1.104s,  927.30/s)  LR: 1.029e-04  Data: 0.012 (0.012)
Train: 212 [ 550/1251 ( 44%)]  Loss:  3.184607 (3.0256)  Time: 1.095s,  935.43/s  (1.105s,  927.08/s)  LR: 1.029e-04  Data: 0.012 (0.012)
Train: 212 [ 600/1251 ( 48%)]  Loss:  2.980291 (3.0221)  Time: 1.180s,  867.94/s  (1.105s,  926.79/s)  LR: 1.029e-04  Data: 0.016 (0.012)
Train: 212 [ 650/1251 ( 52%)]  Loss:  3.009559 (3.0212)  Time: 1.103s,  928.04/s  (1.105s,  926.34/s)  LR: 1.029e-04  Data: 0.012 (0.011)
Train: 212 [ 700/1251 ( 56%)]  Loss:  3.071337 (3.0245)  Time: 1.105s,  926.70/s  (1.105s,  926.35/s)  LR: 1.029e-04  Data: 0.011 (0.011)
Train: 212 [ 750/1251 ( 60%)]  Loss:  2.935060 (3.0189)  Time: 1.120s,  913.96/s  (1.106s,  925.85/s)  LR: 1.029e-04  Data: 0.012 (0.011)
Train: 212 [ 800/1251 ( 64%)]  Loss:  2.882613 (3.0109)  Time: 1.096s,  934.35/s  (1.107s,  925.33/s)  LR: 1.029e-04  Data: 0.012 (0.011)
Train: 212 [ 850/1251 ( 68%)]  Loss:  2.820078 (3.0003)  Time: 1.103s,  928.12/s  (1.106s,  925.48/s)  LR: 1.029e-04  Data: 0.011 (0.011)
Train: 212 [ 900/1251 ( 72%)]  Loss:  2.872808 (2.9936)  Time: 1.098s,  932.36/s  (1.106s,  925.54/s)  LR: 1.029e-04  Data: 0.012 (0.011)
Train: 212 [ 950/1251 ( 76%)]  Loss:  3.130011 (3.0004)  Time: 1.099s,  931.79/s  (1.107s,  925.22/s)  LR: 1.029e-04  Data: 0.011 (0.011)
Train: 212 [1000/1251 ( 80%)]  Loss:  2.598544 (2.9813)  Time: 1.119s,  914.76/s  (1.108s,  924.52/s)  LR: 1.029e-04  Data: 0.010 (0.011)
Train: 212 [1050/1251 ( 84%)]  Loss:  2.914398 (2.9782)  Time: 1.098s,  932.68/s  (1.108s,  924.56/s)  LR: 1.029e-04  Data: 0.011 (0.011)
Train: 212 [1100/1251 ( 88%)]  Loss:  3.059656 (2.9818)  Time: 1.098s,  932.89/s  (1.107s,  924.64/s)  LR: 1.029e-04  Data: 0.011 (0.011)
Train: 212 [1150/1251 ( 92%)]  Loss:  2.945574 (2.9803)  Time: 1.110s,  922.79/s  (1.108s,  924.33/s)  LR: 1.029e-04  Data: 0.009 (0.011)
Train: 212 [1200/1251 ( 96%)]  Loss:  2.884742 (2.9765)  Time: 1.098s,  932.57/s  (1.108s,  924.24/s)  LR: 1.029e-04  Data: 0.012 (0.011)
Train: 212 [1250/1251 (100%)]  Loss:  2.848291 (2.9715)  Time: 1.083s,  945.13/s  (1.108s,  924.17/s)  LR: 1.029e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.258 (3.258)  Loss:  0.4267 (0.4267)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5363 (0.8894)  Acc@1: 88.0896 (80.0080)  Acc@5: 97.9953 (95.0520)
Test (EMA): [   0/48]  Time: 3.188 (3.188)  Loss:  0.3957 (0.3957)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.5347 (0.8297)  Acc@1: 87.8538 (80.9680)  Acc@5: 97.7594 (95.6300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 80.87000012939453)

Train: 213 [   0/1251 (  0%)]  Loss:  2.723428 (2.7234)  Time: 1.102s,  929.48/s  (1.102s,  929.48/s)  LR: 1.008e-04  Data: 0.019 (0.019)
Train: 213 [  50/1251 (  4%)]  Loss:  3.230984 (2.9772)  Time: 1.188s,  861.86/s  (1.104s,  927.29/s)  LR: 1.008e-04  Data: 0.011 (0.011)
Train: 213 [ 100/1251 (  8%)]  Loss:  3.067766 (3.0074)  Time: 1.101s,  930.13/s  (1.105s,  926.55/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 150/1251 ( 12%)]  Loss:  3.059190 (3.0203)  Time: 1.100s,  931.18/s  (1.109s,  923.12/s)  LR: 1.008e-04  Data: 0.013 (0.011)
Train: 213 [ 200/1251 ( 16%)]  Loss:  3.086341 (3.0335)  Time: 1.100s,  930.67/s  (1.112s,  921.21/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 250/1251 ( 20%)]  Loss:  2.966220 (3.0223)  Time: 1.135s,  902.28/s  (1.115s,  918.21/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 300/1251 ( 24%)]  Loss:  3.195933 (3.0471)  Time: 1.104s,  927.19/s  (1.114s,  919.09/s)  LR: 1.008e-04  Data: 0.010 (0.012)
Train: 213 [ 350/1251 ( 28%)]  Loss:  3.123890 (3.0567)  Time: 1.097s,  933.16/s  (1.113s,  919.75/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [ 400/1251 ( 32%)]  Loss:  3.192211 (3.0718)  Time: 1.098s,  932.45/s  (1.112s,  920.79/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [ 450/1251 ( 36%)]  Loss:  3.220240 (3.0866)  Time: 1.099s,  931.98/s  (1.111s,  921.79/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [ 500/1251 ( 40%)]  Loss:  2.858156 (3.0659)  Time: 1.117s,  916.62/s  (1.111s,  921.54/s)  LR: 1.008e-04  Data: 0.010 (0.012)
Train: 213 [ 550/1251 ( 44%)]  Loss:  2.794834 (3.0433)  Time: 1.175s,  871.18/s  (1.111s,  921.39/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [ 600/1251 ( 48%)]  Loss:  2.852937 (3.0286)  Time: 1.104s,  927.60/s  (1.111s,  921.85/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 650/1251 ( 52%)]  Loss:  2.935771 (3.0220)  Time: 1.096s,  934.06/s  (1.111s,  921.84/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 700/1251 ( 56%)]  Loss:  3.080375 (3.0259)  Time: 1.117s,  916.96/s  (1.110s,  922.42/s)  LR: 1.008e-04  Data: 0.010 (0.012)
Train: 213 [ 750/1251 ( 60%)]  Loss:  2.716342 (3.0065)  Time: 1.102s,  928.83/s  (1.110s,  922.70/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 800/1251 ( 64%)]  Loss:  2.923833 (3.0017)  Time: 1.097s,  933.30/s  (1.110s,  922.88/s)  LR: 1.008e-04  Data: 0.013 (0.012)
Train: 213 [ 850/1251 ( 68%)]  Loss:  3.055799 (3.0047)  Time: 1.101s,  929.92/s  (1.109s,  923.20/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [ 900/1251 ( 72%)]  Loss:  2.718855 (2.9896)  Time: 1.114s,  919.51/s  (1.109s,  923.33/s)  LR: 1.008e-04  Data: 0.010 (0.012)
Train: 213 [ 950/1251 ( 76%)]  Loss:  2.919191 (2.9861)  Time: 1.096s,  934.07/s  (1.109s,  923.38/s)  LR: 1.008e-04  Data: 0.011 (0.012)
Train: 213 [1000/1251 ( 80%)]  Loss:  2.971736 (2.9854)  Time: 1.098s,  933.01/s  (1.109s,  923.26/s)  LR: 1.008e-04  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 213 [1050/1251 ( 84%)]  Loss:  3.011101 (2.9866)  Time: 1.098s,  932.27/s  (1.109s,  923.36/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [1100/1251 ( 88%)]  Loss:  2.887676 (2.9823)  Time: 1.119s,  914.71/s  (1.109s,  923.43/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [1150/1251 ( 92%)]  Loss:  3.124775 (2.9882)  Time: 1.122s,  912.56/s  (1.109s,  923.24/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [1200/1251 ( 96%)]  Loss:  2.750984 (2.9787)  Time: 1.098s,  933.02/s  (1.109s,  923.15/s)  LR: 1.008e-04  Data: 0.012 (0.012)
Train: 213 [1250/1251 (100%)]  Loss:  3.425463 (2.9959)  Time: 1.090s,  939.60/s  (1.109s,  923.22/s)  LR: 1.008e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.205 (3.205)  Loss:  0.4239 (0.4239)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.5393 (0.8727)  Acc@1: 87.2642 (80.0100)  Acc@5: 97.7594 (95.1400)
Test (EMA): [   0/48]  Time: 3.193 (3.193)  Loss:  0.3952 (0.3952)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5344 (0.8294)  Acc@1: 87.6179 (80.9260)  Acc@5: 97.8774 (95.6280)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 80.925999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 80.909999921875)

Train: 214 [   0/1251 (  0%)]  Loss:  2.979020 (2.9790)  Time: 1.111s,  921.60/s  (1.111s,  921.60/s)  LR: 9.877e-05  Data: 0.030 (0.030)
Train: 214 [  50/1251 (  4%)]  Loss:  2.707778 (2.8434)  Time: 1.124s,  911.29/s  (1.122s,  912.47/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 100/1251 (  8%)]  Loss:  3.258836 (2.9819)  Time: 1.091s,  938.39/s  (1.117s,  917.04/s)  LR: 9.877e-05  Data: 0.010 (0.012)
Train: 214 [ 150/1251 ( 12%)]  Loss:  2.880624 (2.9566)  Time: 1.129s,  906.60/s  (1.118s,  916.11/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 200/1251 ( 16%)]  Loss:  2.957204 (2.9567)  Time: 1.122s,  912.84/s  (1.118s,  915.99/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 250/1251 ( 20%)]  Loss:  2.756403 (2.9233)  Time: 1.122s,  912.32/s  (1.118s,  915.99/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 300/1251 ( 24%)]  Loss:  3.010474 (2.9358)  Time: 1.102s,  929.20/s  (1.117s,  917.03/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 350/1251 ( 28%)]  Loss:  3.121154 (2.9589)  Time: 1.096s,  934.06/s  (1.115s,  918.72/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 400/1251 ( 32%)]  Loss:  2.703330 (2.9305)  Time: 1.097s,  933.38/s  (1.115s,  918.56/s)  LR: 9.877e-05  Data: 0.014 (0.012)
Train: 214 [ 450/1251 ( 36%)]  Loss:  3.155082 (2.9530)  Time: 1.096s,  934.06/s  (1.114s,  919.06/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 500/1251 ( 40%)]  Loss:  3.267137 (2.9815)  Time: 1.099s,  932.04/s  (1.114s,  919.12/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 550/1251 ( 44%)]  Loss:  2.832094 (2.9691)  Time: 1.136s,  901.19/s  (1.115s,  918.18/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 600/1251 ( 48%)]  Loss:  3.167892 (2.9844)  Time: 1.099s,  931.44/s  (1.115s,  918.04/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 650/1251 ( 52%)]  Loss:  3.012667 (2.9864)  Time: 1.099s,  931.41/s  (1.115s,  918.60/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 700/1251 ( 56%)]  Loss:  3.070202 (2.9920)  Time: 1.097s,  933.49/s  (1.114s,  919.25/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [ 750/1251 ( 60%)]  Loss:  2.639792 (2.9700)  Time: 1.101s,  930.38/s  (1.114s,  919.52/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 800/1251 ( 64%)]  Loss:  2.880870 (2.9647)  Time: 1.092s,  937.72/s  (1.113s,  920.00/s)  LR: 9.877e-05  Data: 0.010 (0.012)
Train: 214 [ 850/1251 ( 68%)]  Loss:  2.847718 (2.9582)  Time: 1.098s,  932.61/s  (1.112s,  920.70/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [ 900/1251 ( 72%)]  Loss:  2.856287 (2.9529)  Time: 1.105s,  926.63/s  (1.112s,  920.63/s)  LR: 9.877e-05  Data: 0.018 (0.012)
Train: 214 [ 950/1251 ( 76%)]  Loss:  3.119789 (2.9612)  Time: 1.096s,  934.43/s  (1.112s,  920.75/s)  LR: 9.877e-05  Data: 0.013 (0.012)
Train: 214 [1000/1251 ( 80%)]  Loss:  2.838849 (2.9554)  Time: 1.132s,  904.36/s  (1.112s,  920.89/s)  LR: 9.877e-05  Data: 0.011 (0.012)
Train: 214 [1050/1251 ( 84%)]  Loss:  3.067496 (2.9605)  Time: 1.122s,  912.86/s  (1.112s,  920.53/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [1100/1251 ( 88%)]  Loss:  3.152191 (2.9688)  Time: 1.136s,  901.48/s  (1.112s,  920.54/s)  LR: 9.877e-05  Data: 0.016 (0.012)
Train: 214 [1150/1251 ( 92%)]  Loss:  3.124832 (2.9753)  Time: 1.100s,  931.29/s  (1.112s,  920.59/s)  LR: 9.877e-05  Data: 0.012 (0.012)
Train: 214 [1200/1251 ( 96%)]  Loss:  3.181952 (2.9836)  Time: 1.121s,  913.19/s  (1.112s,  920.85/s)  LR: 9.877e-05  Data: 0.010 (0.012)
Train: 214 [1250/1251 (100%)]  Loss:  3.117710 (2.9887)  Time: 1.087s,  941.89/s  (1.112s,  920.89/s)  LR: 9.877e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.215 (3.215)  Loss:  0.4419 (0.4419)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.5580 (0.9003)  Acc@1: 87.9717 (80.0280)  Acc@5: 97.7594 (95.1400)
Test (EMA): [   0/48]  Time: 3.253 (3.253)  Loss:  0.3953 (0.3953)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5349 (0.8297)  Acc@1: 87.7358 (80.9180)  Acc@5: 97.8774 (95.6080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 80.925999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 80.91799997314453)

Train: 215 [   0/1251 (  0%)]  Loss:  3.190082 (3.1901)  Time: 1.104s,  927.31/s  (1.104s,  927.31/s)  LR: 9.674e-05  Data: 0.020 (0.020)
Train: 215 [  50/1251 (  4%)]  Loss:  2.912046 (3.0511)  Time: 1.101s,  929.95/s  (1.110s,  922.55/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [ 100/1251 (  8%)]  Loss:  3.190504 (3.0975)  Time: 1.102s,  929.29/s  (1.107s,  925.13/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [ 150/1251 ( 12%)]  Loss:  2.920072 (3.0532)  Time: 1.115s,  918.17/s  (1.106s,  926.03/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [ 200/1251 ( 16%)]  Loss:  3.005518 (3.0436)  Time: 1.095s,  935.23/s  (1.106s,  925.48/s)  LR: 9.674e-05  Data: 0.011 (0.011)
Train: 215 [ 250/1251 ( 20%)]  Loss:  2.806909 (3.0042)  Time: 1.097s,  933.50/s  (1.106s,  925.58/s)  LR: 9.674e-05  Data: 0.012 (0.011)
Train: 215 [ 300/1251 ( 24%)]  Loss:  3.064815 (3.0128)  Time: 1.091s,  938.29/s  (1.106s,  925.48/s)  LR: 9.674e-05  Data: 0.009 (0.011)
Train: 215 [ 350/1251 ( 28%)]  Loss:  3.114610 (3.0256)  Time: 1.096s,  934.20/s  (1.106s,  926.13/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [ 400/1251 ( 32%)]  Loss:  2.968010 (3.0192)  Time: 1.094s,  935.84/s  (1.106s,  926.00/s)  LR: 9.674e-05  Data: 0.010 (0.012)
Train: 215 [ 450/1251 ( 36%)]  Loss:  2.968591 (3.0141)  Time: 1.126s,  909.54/s  (1.106s,  926.25/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [ 500/1251 ( 40%)]  Loss:  2.715993 (2.9870)  Time: 1.096s,  934.72/s  (1.106s,  925.67/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [ 550/1251 ( 44%)]  Loss:  2.917461 (2.9812)  Time: 1.101s,  930.42/s  (1.107s,  924.75/s)  LR: 9.674e-05  Data: 0.010 (0.012)
Train: 215 [ 600/1251 ( 48%)]  Loss:  2.857440 (2.9717)  Time: 1.096s,  934.32/s  (1.108s,  924.34/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [ 650/1251 ( 52%)]  Loss:  3.218177 (2.9893)  Time: 1.097s,  933.48/s  (1.107s,  924.66/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [ 700/1251 ( 56%)]  Loss:  2.951741 (2.9868)  Time: 1.106s,  925.67/s  (1.107s,  924.69/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [ 750/1251 ( 60%)]  Loss:  3.034824 (2.9898)  Time: 1.123s,  911.73/s  (1.107s,  924.76/s)  LR: 9.674e-05  Data: 0.010 (0.012)
Train: 215 [ 800/1251 ( 64%)]  Loss:  3.324431 (3.0095)  Time: 1.130s,  905.84/s  (1.108s,  924.48/s)  LR: 9.674e-05  Data: 0.010 (0.012)
Train: 215 [ 850/1251 ( 68%)]  Loss:  2.783646 (2.9969)  Time: 1.101s,  930.28/s  (1.108s,  924.33/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [ 900/1251 ( 72%)]  Loss:  3.121675 (3.0035)  Time: 1.103s,  928.18/s  (1.108s,  924.46/s)  LR: 9.674e-05  Data: 0.010 (0.012)
Train: 215 [ 950/1251 ( 76%)]  Loss:  2.730069 (2.9898)  Time: 1.097s,  933.65/s  (1.108s,  924.23/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [1000/1251 ( 80%)]  Loss:  2.968525 (2.9888)  Time: 1.096s,  934.20/s  (1.108s,  924.33/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [1050/1251 ( 84%)]  Loss:  2.906229 (2.9851)  Time: 1.120s,  914.62/s  (1.109s,  923.73/s)  LR: 9.674e-05  Data: 0.012 (0.012)
Train: 215 [1100/1251 ( 88%)]  Loss:  3.008335 (2.9861)  Time: 1.098s,  932.21/s  (1.108s,  923.86/s)  LR: 9.674e-05  Data: 0.013 (0.012)
Train: 215 [1150/1251 ( 92%)]  Loss:  3.022323 (2.9876)  Time: 1.096s,  934.07/s  (1.108s,  923.88/s)  LR: 9.674e-05  Data: 0.013 (0.012)
Train: 215 [1200/1251 ( 96%)]  Loss:  2.945296 (2.9859)  Time: 1.114s,  919.18/s  (1.109s,  923.74/s)  LR: 9.674e-05  Data: 0.011 (0.012)
Train: 215 [1250/1251 (100%)]  Loss:  2.935759 (2.9840)  Time: 1.079s,  948.73/s  (1.109s,  923.69/s)  LR: 9.674e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.217 (3.217)  Loss:  0.4502 (0.4502)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.5776 (0.8921)  Acc@1: 87.6179 (79.7660)  Acc@5: 97.5236 (95.0880)
Test (EMA): [   0/48]  Time: 3.088 (3.088)  Loss:  0.3953 (0.3953)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5357 (0.8298)  Acc@1: 87.9717 (80.9440)  Acc@5: 97.8774 (95.5960)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 80.925999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 80.922000078125)

Train: 216 [   0/1251 (  0%)]  Loss:  3.217248 (3.2172)  Time: 1.119s,  914.96/s  (1.119s,  914.96/s)  LR: 9.474e-05  Data: 0.028 (0.028)
Train: 216 [  50/1251 (  4%)]  Loss:  3.175761 (3.1965)  Time: 1.095s,  934.89/s  (1.110s,  922.63/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 100/1251 (  8%)]  Loss:  3.224993 (3.2060)  Time: 1.102s,  928.92/s  (1.111s,  921.75/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 216 [ 150/1251 ( 12%)]  Loss:  3.002000 (3.1550)  Time: 1.098s,  932.53/s  (1.109s,  923.34/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 200/1251 ( 16%)]  Loss:  3.066275 (3.1373)  Time: 1.097s,  933.26/s  (1.107s,  924.70/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 250/1251 ( 20%)]  Loss:  2.795368 (3.0803)  Time: 1.094s,  936.19/s  (1.107s,  925.14/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 300/1251 ( 24%)]  Loss:  2.982627 (3.0663)  Time: 1.101s,  930.26/s  (1.108s,  924.54/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 350/1251 ( 28%)]  Loss:  3.236892 (3.0876)  Time: 1.100s,  931.10/s  (1.109s,  923.57/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 400/1251 ( 32%)]  Loss:  2.806395 (3.0564)  Time: 1.098s,  932.48/s  (1.108s,  924.29/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 450/1251 ( 36%)]  Loss:  2.782367 (3.0290)  Time: 1.094s,  935.96/s  (1.107s,  924.83/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 500/1251 ( 40%)]  Loss:  2.922808 (3.0193)  Time: 1.098s,  932.87/s  (1.107s,  924.78/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 550/1251 ( 44%)]  Loss:  2.923561 (3.0114)  Time: 1.103s,  928.40/s  (1.107s,  924.96/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 600/1251 ( 48%)]  Loss:  2.767026 (2.9926)  Time: 1.098s,  932.75/s  (1.107s,  924.87/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [ 650/1251 ( 52%)]  Loss:  3.075229 (2.9985)  Time: 1.120s,  914.58/s  (1.108s,  924.33/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 700/1251 ( 56%)]  Loss:  2.680364 (2.9773)  Time: 1.100s,  930.89/s  (1.108s,  924.21/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 750/1251 ( 60%)]  Loss:  2.787289 (2.9654)  Time: 1.112s,  920.79/s  (1.108s,  924.07/s)  LR: 9.474e-05  Data: 0.009 (0.012)
Train: 216 [ 800/1251 ( 64%)]  Loss:  2.814395 (2.9565)  Time: 1.099s,  931.90/s  (1.108s,  924.27/s)  LR: 9.474e-05  Data: 0.017 (0.012)
Train: 216 [ 850/1251 ( 68%)]  Loss:  2.871493 (2.9518)  Time: 1.100s,  931.21/s  (1.108s,  924.50/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 900/1251 ( 72%)]  Loss:  2.975372 (2.9530)  Time: 1.097s,  933.19/s  (1.108s,  924.52/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [ 950/1251 ( 76%)]  Loss:  2.985356 (2.9546)  Time: 1.099s,  931.45/s  (1.107s,  924.64/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [1000/1251 ( 80%)]  Loss:  3.105342 (2.9618)  Time: 1.096s,  934.21/s  (1.107s,  924.65/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [1050/1251 ( 84%)]  Loss:  2.802954 (2.9546)  Time: 1.096s,  933.97/s  (1.108s,  924.31/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [1100/1251 ( 88%)]  Loss:  2.931104 (2.9536)  Time: 1.122s,  912.81/s  (1.108s,  924.28/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [1150/1251 ( 92%)]  Loss:  2.936980 (2.9529)  Time: 1.102s,  928.90/s  (1.108s,  924.27/s)  LR: 9.474e-05  Data: 0.011 (0.012)
Train: 216 [1200/1251 ( 96%)]  Loss:  2.979210 (2.9539)  Time: 1.101s,  929.81/s  (1.108s,  924.21/s)  LR: 9.474e-05  Data: 0.012 (0.012)
Train: 216 [1250/1251 (100%)]  Loss:  2.527453 (2.9375)  Time: 1.087s,  942.29/s  (1.108s,  924.28/s)  LR: 9.474e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.211 (3.211)  Loss:  0.4356 (0.4356)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.5979 (0.8916)  Acc@1: 87.0283 (80.1060)  Acc@5: 97.4057 (95.2040)
Test (EMA): [   0/48]  Time: 3.086 (3.086)  Loss:  0.3954 (0.3954)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.408)  Loss:  0.5364 (0.8297)  Acc@1: 87.9717 (80.9660)  Acc@5: 98.1132 (95.5800)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 80.925999921875)

Train: 217 [   0/1251 (  0%)]  Loss:  2.969995 (2.9700)  Time: 1.110s,  922.15/s  (1.110s,  922.15/s)  LR: 9.275e-05  Data: 0.022 (0.022)
Train: 217 [  50/1251 (  4%)]  Loss:  3.061209 (3.0156)  Time: 1.174s,  872.18/s  (1.107s,  924.99/s)  LR: 9.275e-05  Data: 0.010 (0.012)
Train: 217 [ 100/1251 (  8%)]  Loss:  3.010163 (3.0138)  Time: 1.096s,  933.95/s  (1.106s,  926.15/s)  LR: 9.275e-05  Data: 0.011 (0.012)
Train: 217 [ 150/1251 ( 12%)]  Loss:  2.714050 (2.9389)  Time: 1.097s,  933.51/s  (1.105s,  926.97/s)  LR: 9.275e-05  Data: 0.011 (0.012)
Train: 217 [ 200/1251 ( 16%)]  Loss:  2.958220 (2.9427)  Time: 1.120s,  914.40/s  (1.108s,  924.24/s)  LR: 9.275e-05  Data: 0.010 (0.011)
Train: 217 [ 250/1251 ( 20%)]  Loss:  3.037197 (2.9585)  Time: 1.102s,  929.14/s  (1.108s,  923.89/s)  LR: 9.275e-05  Data: 0.011 (0.011)
Train: 217 [ 300/1251 ( 24%)]  Loss:  2.905650 (2.9509)  Time: 1.105s,  926.95/s  (1.108s,  924.00/s)  LR: 9.275e-05  Data: 0.011 (0.012)
Train: 217 [ 350/1251 ( 28%)]  Loss:  2.834095 (2.9363)  Time: 1.097s,  933.80/s  (1.108s,  924.41/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 400/1251 ( 32%)]  Loss:  3.095537 (2.9540)  Time: 1.098s,  932.82/s  (1.108s,  924.01/s)  LR: 9.275e-05  Data: 0.013 (0.012)
Train: 217 [ 450/1251 ( 36%)]  Loss:  2.688004 (2.9274)  Time: 1.096s,  934.39/s  (1.109s,  923.11/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 500/1251 ( 40%)]  Loss:  2.953717 (2.9298)  Time: 1.097s,  933.79/s  (1.109s,  923.52/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 550/1251 ( 44%)]  Loss:  3.046801 (2.9396)  Time: 1.096s,  933.98/s  (1.108s,  924.04/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 600/1251 ( 48%)]  Loss:  2.981602 (2.9428)  Time: 1.104s,  927.51/s  (1.108s,  924.01/s)  LR: 9.275e-05  Data: 0.011 (0.012)
Train: 217 [ 650/1251 ( 52%)]  Loss:  2.866000 (2.9373)  Time: 1.121s,  913.35/s  (1.108s,  924.25/s)  LR: 9.275e-05  Data: 0.010 (0.012)
Train: 217 [ 700/1251 ( 56%)]  Loss:  2.991718 (2.9409)  Time: 1.097s,  933.37/s  (1.108s,  924.58/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 750/1251 ( 60%)]  Loss:  3.129998 (2.9527)  Time: 1.095s,  934.96/s  (1.108s,  924.58/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 800/1251 ( 64%)]  Loss:  2.945255 (2.9523)  Time: 1.103s,  928.45/s  (1.108s,  924.59/s)  LR: 9.275e-05  Data: 0.010 (0.012)
Train: 217 [ 850/1251 ( 68%)]  Loss:  2.907389 (2.9498)  Time: 1.093s,  937.15/s  (1.107s,  924.72/s)  LR: 9.275e-05  Data: 0.010 (0.012)
Train: 217 [ 900/1251 ( 72%)]  Loss:  3.020406 (2.9535)  Time: 1.191s,  859.85/s  (1.107s,  924.70/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [ 950/1251 ( 76%)]  Loss:  3.218247 (2.9668)  Time: 1.119s,  914.99/s  (1.108s,  924.27/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [1000/1251 ( 80%)]  Loss:  2.914406 (2.9643)  Time: 1.123s,  911.71/s  (1.108s,  924.54/s)  LR: 9.275e-05  Data: 0.011 (0.012)
Train: 217 [1050/1251 ( 84%)]  Loss:  3.286339 (2.9789)  Time: 1.095s,  934.94/s  (1.107s,  924.70/s)  LR: 9.275e-05  Data: 0.011 (0.012)
Train: 217 [1100/1251 ( 88%)]  Loss:  3.180464 (2.9877)  Time: 1.125s,  910.16/s  (1.108s,  924.60/s)  LR: 9.275e-05  Data: 0.015 (0.012)
Train: 217 [1150/1251 ( 92%)]  Loss:  3.122541 (2.9933)  Time: 1.101s,  930.33/s  (1.108s,  924.45/s)  LR: 9.275e-05  Data: 0.012 (0.012)
Train: 217 [1200/1251 ( 96%)]  Loss:  3.279777 (3.0048)  Time: 1.118s,  916.12/s  (1.108s,  924.32/s)  LR: 9.275e-05  Data: 0.010 (0.012)
Train: 217 [1250/1251 (100%)]  Loss:  3.039022 (3.0061)  Time: 1.080s,  947.91/s  (1.108s,  924.30/s)  LR: 9.275e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.375 (3.375)  Loss:  0.4224 (0.4224)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.398)  Loss:  0.5608 (0.8804)  Acc@1: 87.8538 (80.1320)  Acc@5: 97.7594 (95.1320)
Test (EMA): [   0/48]  Time: 3.277 (3.277)  Loss:  0.3953 (0.3953)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  0.5372 (0.8295)  Acc@1: 87.7358 (80.9800)  Acc@5: 98.1132 (95.5960)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 80.928000078125)

Train: 218 [   0/1251 (  0%)]  Loss:  2.639189 (2.6392)  Time: 1.115s,  918.52/s  (1.115s,  918.52/s)  LR: 9.078e-05  Data: 0.030 (0.030)
Train: 218 [  50/1251 (  4%)]  Loss:  3.220737 (2.9300)  Time: 1.097s,  933.39/s  (1.104s,  927.63/s)  LR: 9.078e-05  Data: 0.013 (0.012)
Train: 218 [ 100/1251 (  8%)]  Loss:  2.779287 (2.8797)  Time: 1.122s,  912.81/s  (1.106s,  925.67/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 150/1251 ( 12%)]  Loss:  3.083219 (2.9306)  Time: 1.123s,  911.79/s  (1.107s,  925.24/s)  LR: 9.078e-05  Data: 0.010 (0.012)
Train: 218 [ 200/1251 ( 16%)]  Loss:  3.129930 (2.9705)  Time: 1.100s,  930.77/s  (1.108s,  923.95/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 250/1251 ( 20%)]  Loss:  2.570529 (2.9038)  Time: 1.107s,  924.61/s  (1.108s,  924.53/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 300/1251 ( 24%)]  Loss:  2.972856 (2.9137)  Time: 1.119s,  914.93/s  (1.108s,  924.21/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 350/1251 ( 28%)]  Loss:  2.982246 (2.9222)  Time: 1.098s,  932.62/s  (1.108s,  924.03/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 400/1251 ( 32%)]  Loss:  3.093833 (2.9413)  Time: 1.101s,  930.30/s  (1.108s,  924.26/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Train: 218 [ 450/1251 ( 36%)]  Loss:  3.298911 (2.9771)  Time: 1.099s,  931.96/s  (1.108s,  924.17/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Train: 218 [ 500/1251 ( 40%)]  Loss:  2.804410 (2.9614)  Time: 1.124s,  911.24/s  (1.108s,  924.49/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 550/1251 ( 44%)]  Loss:  2.923925 (2.9583)  Time: 1.133s,  903.86/s  (1.108s,  924.18/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 600/1251 ( 48%)]  Loss:  2.943109 (2.9571)  Time: 1.099s,  931.47/s  (1.108s,  924.49/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Train: 218 [ 650/1251 ( 52%)]  Loss:  3.119581 (2.9687)  Time: 1.096s,  934.19/s  (1.108s,  924.58/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 218 [ 700/1251 ( 56%)]  Loss:  3.207201 (2.9846)  Time: 1.095s,  935.29/s  (1.108s,  924.00/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [ 750/1251 ( 60%)]  Loss:  3.159343 (2.9955)  Time: 1.097s,  933.86/s  (1.108s,  924.08/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Train: 218 [ 800/1251 ( 64%)]  Loss:  2.953288 (2.9930)  Time: 1.102s,  929.61/s  (1.108s,  923.93/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Train: 218 [ 850/1251 ( 68%)]  Loss:  2.878944 (2.9867)  Time: 1.189s,  861.57/s  (1.108s,  924.25/s)  LR: 9.078e-05  Data: 0.013 (0.012)
Train: 218 [ 900/1251 ( 72%)]  Loss:  3.157430 (2.9957)  Time: 1.108s,  923.99/s  (1.108s,  924.32/s)  LR: 9.078e-05  Data: 0.012 (0.012)
Train: 218 [ 950/1251 ( 76%)]  Loss:  2.944444 (2.9931)  Time: 1.101s,  930.35/s  (1.108s,  924.60/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [1000/1251 ( 80%)]  Loss:  3.281422 (3.0068)  Time: 1.120s,  914.42/s  (1.108s,  924.39/s)  LR: 9.078e-05  Data: 0.010 (0.012)
Train: 218 [1050/1251 ( 84%)]  Loss:  3.010264 (3.0070)  Time: 1.098s,  932.85/s  (1.108s,  924.53/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [1100/1251 ( 88%)]  Loss:  3.045310 (3.0087)  Time: 1.096s,  934.57/s  (1.108s,  924.41/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [1150/1251 ( 92%)]  Loss:  2.990495 (3.0079)  Time: 1.128s,  907.79/s  (1.108s,  924.47/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [1200/1251 ( 96%)]  Loss:  2.902671 (3.0037)  Time: 1.100s,  930.97/s  (1.107s,  924.64/s)  LR: 9.078e-05  Data: 0.011 (0.012)
Train: 218 [1250/1251 (100%)]  Loss:  2.986511 (3.0030)  Time: 1.106s,  926.19/s  (1.108s,  924.10/s)  LR: 9.078e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.307 (3.307)  Loss:  0.4022 (0.4022)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5434 (0.8749)  Acc@1: 86.7925 (80.1060)  Acc@5: 97.2877 (95.1260)
Test (EMA): [   0/48]  Time: 3.267 (3.267)  Loss:  0.3954 (0.3954)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5370 (0.8294)  Acc@1: 87.5000 (80.9760)  Acc@5: 97.9953 (95.5820)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 80.937999921875)

Train: 219 [   0/1251 (  0%)]  Loss:  2.961676 (2.9617)  Time: 1.102s,  929.45/s  (1.102s,  929.45/s)  LR: 8.883e-05  Data: 0.020 (0.020)
Train: 219 [  50/1251 (  4%)]  Loss:  2.939914 (2.9508)  Time: 1.096s,  934.58/s  (1.102s,  929.13/s)  LR: 8.883e-05  Data: 0.010 (0.012)
Train: 219 [ 100/1251 (  8%)]  Loss:  2.797366 (2.8997)  Time: 1.145s,  894.33/s  (1.108s,  924.14/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [ 150/1251 ( 12%)]  Loss:  2.950142 (2.9123)  Time: 1.097s,  933.49/s  (1.108s,  923.82/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 200/1251 ( 16%)]  Loss:  3.143020 (2.9584)  Time: 1.102s,  928.85/s  (1.109s,  923.23/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 250/1251 ( 20%)]  Loss:  2.631932 (2.9040)  Time: 1.094s,  936.42/s  (1.109s,  923.20/s)  LR: 8.883e-05  Data: 0.010 (0.012)
Train: 219 [ 300/1251 ( 24%)]  Loss:  3.110123 (2.9335)  Time: 1.098s,  932.22/s  (1.109s,  923.27/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [ 350/1251 ( 28%)]  Loss:  2.997580 (2.9415)  Time: 1.095s,  934.90/s  (1.108s,  923.92/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 400/1251 ( 32%)]  Loss:  2.897177 (2.9365)  Time: 1.091s,  938.39/s  (1.109s,  923.69/s)  LR: 8.883e-05  Data: 0.010 (0.012)
Train: 219 [ 450/1251 ( 36%)]  Loss:  3.191049 (2.9620)  Time: 1.119s,  915.17/s  (1.109s,  923.60/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [ 500/1251 ( 40%)]  Loss:  3.173402 (2.9812)  Time: 1.098s,  932.40/s  (1.109s,  922.94/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 550/1251 ( 44%)]  Loss:  3.108824 (2.9919)  Time: 1.103s,  928.41/s  (1.110s,  922.12/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 600/1251 ( 48%)]  Loss:  2.858517 (2.9816)  Time: 1.120s,  914.58/s  (1.110s,  922.17/s)  LR: 8.883e-05  Data: 0.010 (0.012)
Train: 219 [ 650/1251 ( 52%)]  Loss:  2.995241 (2.9826)  Time: 1.103s,  928.29/s  (1.110s,  922.21/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [ 700/1251 ( 56%)]  Loss:  2.918331 (2.9783)  Time: 1.120s,  913.91/s  (1.111s,  921.73/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 750/1251 ( 60%)]  Loss:  3.228569 (2.9939)  Time: 1.104s,  927.92/s  (1.111s,  921.64/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 800/1251 ( 64%)]  Loss:  3.045416 (2.9970)  Time: 1.201s,  852.55/s  (1.111s,  921.93/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [ 850/1251 ( 68%)]  Loss:  2.906105 (2.9919)  Time: 1.096s,  933.96/s  (1.111s,  922.00/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [ 900/1251 ( 72%)]  Loss:  3.173489 (3.0015)  Time: 1.134s,  902.85/s  (1.111s,  921.92/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [ 950/1251 ( 76%)]  Loss:  3.025281 (3.0027)  Time: 1.099s,  932.17/s  (1.111s,  921.60/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [1000/1251 ( 80%)]  Loss:  3.142193 (3.0093)  Time: 1.100s,  931.25/s  (1.111s,  921.66/s)  LR: 8.883e-05  Data: 0.014 (0.012)
Train: 219 [1050/1251 ( 84%)]  Loss:  3.077446 (3.0124)  Time: 1.102s,  929.11/s  (1.111s,  921.41/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [1100/1251 ( 88%)]  Loss:  3.302849 (3.0250)  Time: 1.097s,  933.14/s  (1.111s,  921.32/s)  LR: 8.883e-05  Data: 0.011 (0.012)
Train: 219 [1150/1251 ( 92%)]  Loss:  3.091225 (3.0278)  Time: 1.103s,  928.43/s  (1.112s,  921.20/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [1200/1251 ( 96%)]  Loss:  2.693028 (3.0144)  Time: 1.099s,  931.93/s  (1.112s,  921.16/s)  LR: 8.883e-05  Data: 0.012 (0.012)
Train: 219 [1250/1251 (100%)]  Loss:  2.927296 (3.0110)  Time: 1.108s,  924.42/s  (1.112s,  921.17/s)  LR: 8.883e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.257 (3.257)  Loss:  0.4256 (0.4256)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5897 (0.8833)  Acc@1: 87.1462 (79.9980)  Acc@5: 97.6415 (95.0820)
Test (EMA): [   0/48]  Time: 3.055 (3.055)  Loss:  0.3948 (0.3948)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.405)  Loss:  0.5375 (0.8292)  Acc@1: 87.5000 (81.0080)  Acc@5: 97.9953 (95.6000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 80.939999921875)

Train: 220 [   0/1251 (  0%)]  Loss:  2.880196 (2.8802)  Time: 1.102s,  928.94/s  (1.102s,  928.94/s)  LR: 8.689e-05  Data: 0.020 (0.020)
Train: 220 [  50/1251 (  4%)]  Loss:  2.742666 (2.8114)  Time: 1.097s,  933.10/s  (1.110s,  922.44/s)  LR: 8.689e-05  Data: 0.014 (0.012)
Train: 220 [ 100/1251 (  8%)]  Loss:  3.189948 (2.9376)  Time: 1.103s,  928.02/s  (1.107s,  924.99/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [ 150/1251 ( 12%)]  Loss:  2.979327 (2.9480)  Time: 1.094s,  936.31/s  (1.107s,  924.96/s)  LR: 8.689e-05  Data: 0.012 (0.012)
Train: 220 [ 200/1251 ( 16%)]  Loss:  3.067703 (2.9720)  Time: 1.099s,  932.00/s  (1.106s,  925.73/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [ 250/1251 ( 20%)]  Loss:  3.082289 (2.9904)  Time: 1.131s,  905.57/s  (1.109s,  923.66/s)  LR: 8.689e-05  Data: 0.013 (0.012)
Train: 220 [ 300/1251 ( 24%)]  Loss:  2.814953 (2.9653)  Time: 1.098s,  932.34/s  (1.110s,  922.61/s)  LR: 8.689e-05  Data: 0.013 (0.012)
Train: 220 [ 350/1251 ( 28%)]  Loss:  2.867001 (2.9530)  Time: 1.094s,  936.40/s  (1.112s,  920.93/s)  LR: 8.689e-05  Data: 0.010 (0.012)
Train: 220 [ 400/1251 ( 32%)]  Loss:  3.022495 (2.9607)  Time: 1.097s,  933.72/s  (1.111s,  921.68/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [ 450/1251 ( 36%)]  Loss:  2.952000 (2.9599)  Time: 1.108s,  924.58/s  (1.110s,  922.19/s)  LR: 8.689e-05  Data: 0.012 (0.012)
Train: 220 [ 500/1251 ( 40%)]  Loss:  3.092593 (2.9719)  Time: 1.117s,  917.00/s  (1.110s,  922.73/s)  LR: 8.689e-05  Data: 0.010 (0.012)
Train: 220 [ 550/1251 ( 44%)]  Loss:  2.886014 (2.9648)  Time: 1.128s,  907.40/s  (1.109s,  923.16/s)  LR: 8.689e-05  Data: 0.010 (0.012)
Train: 220 [ 600/1251 ( 48%)]  Loss:  3.087440 (2.9742)  Time: 1.097s,  933.70/s  (1.109s,  923.30/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [ 650/1251 ( 52%)]  Loss:  2.931205 (2.9711)  Time: 1.096s,  934.18/s  (1.109s,  923.61/s)  LR: 8.689e-05  Data: 0.012 (0.012)
Train: 220 [ 700/1251 ( 56%)]  Loss:  2.977645 (2.9716)  Time: 1.097s,  933.21/s  (1.109s,  923.67/s)  LR: 8.689e-05  Data: 0.013 (0.012)
Train: 220 [ 750/1251 ( 60%)]  Loss:  2.898439 (2.9670)  Time: 1.095s,  935.29/s  (1.108s,  924.15/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [ 800/1251 ( 64%)]  Loss:  2.968184 (2.9671)  Time: 1.099s,  931.85/s  (1.108s,  924.03/s)  LR: 8.689e-05  Data: 0.011 (0.011)
Train: 220 [ 850/1251 ( 68%)]  Loss:  3.200549 (2.9800)  Time: 1.097s,  933.53/s  (1.108s,  924.38/s)  LR: 8.689e-05  Data: 0.012 (0.012)
Train: 220 [ 900/1251 ( 72%)]  Loss:  3.082248 (2.9854)  Time: 1.099s,  931.69/s  (1.108s,  924.19/s)  LR: 8.689e-05  Data: 0.014 (0.012)
Train: 220 [ 950/1251 ( 76%)]  Loss:  3.127925 (2.9925)  Time: 1.094s,  935.78/s  (1.108s,  924.06/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [1000/1251 ( 80%)]  Loss:  2.796378 (2.9832)  Time: 1.100s,  930.97/s  (1.108s,  924.22/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [1050/1251 ( 84%)]  Loss:  3.163530 (2.9914)  Time: 1.097s,  933.38/s  (1.108s,  923.92/s)  LR: 8.689e-05  Data: 0.012 (0.012)
Train: 220 [1100/1251 ( 88%)]  Loss:  3.069989 (2.9948)  Time: 1.097s,  933.65/s  (1.108s,  924.06/s)  LR: 8.689e-05  Data: 0.012 (0.012)
Train: 220 [1150/1251 ( 92%)]  Loss:  2.908143 (2.9912)  Time: 1.100s,  930.65/s  (1.108s,  924.22/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [1200/1251 ( 96%)]  Loss:  3.097275 (2.9954)  Time: 1.118s,  916.12/s  (1.108s,  924.24/s)  LR: 8.689e-05  Data: 0.011 (0.012)
Train: 220 [1250/1251 (100%)]  Loss:  3.156169 (3.0016)  Time: 1.081s,  947.56/s  (1.108s,  924.29/s)  LR: 8.689e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.237 (3.237)  Loss:  0.4274 (0.4274)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5625 (0.8920)  Acc@1: 87.7359 (80.2160)  Acc@5: 97.5236 (95.1560)
Test (EMA): [   0/48]  Time: 3.095 (3.095)  Loss:  0.3944 (0.3944)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.399)  Loss:  0.5378 (0.8293)  Acc@1: 87.6179 (81.0160)  Acc@5: 98.1132 (95.6200)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 80.939999921875)

Train: 221 [   0/1251 (  0%)]  Loss:  3.073581 (3.0736)  Time: 1.130s,  906.13/s  (1.130s,  906.13/s)  LR: 8.497e-05  Data: 0.020 (0.020)
Train: 221 [  50/1251 (  4%)]  Loss:  3.042981 (3.0583)  Time: 1.121s,  913.74/s  (1.108s,  923.79/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 100/1251 (  8%)]  Loss:  2.849298 (2.9886)  Time: 1.099s,  931.70/s  (1.109s,  923.46/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 150/1251 ( 12%)]  Loss:  2.919401 (2.9713)  Time: 1.098s,  932.54/s  (1.106s,  925.48/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 200/1251 ( 16%)]  Loss:  3.044834 (2.9860)  Time: 1.119s,  914.79/s  (1.107s,  925.09/s)  LR: 8.497e-05  Data: 0.012 (0.012)
Train: 221 [ 250/1251 ( 20%)]  Loss:  2.914173 (2.9740)  Time: 1.097s,  933.54/s  (1.106s,  926.05/s)  LR: 8.497e-05  Data: 0.012 (0.012)
Train: 221 [ 300/1251 ( 24%)]  Loss:  2.838332 (2.9547)  Time: 1.120s,  914.17/s  (1.107s,  925.00/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 350/1251 ( 28%)]  Loss:  2.989353 (2.9590)  Time: 1.095s,  935.48/s  (1.107s,  924.93/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 400/1251 ( 32%)]  Loss:  2.542253 (2.9127)  Time: 1.100s,  930.91/s  (1.106s,  925.61/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 450/1251 ( 36%)]  Loss:  3.211148 (2.9425)  Time: 1.095s,  934.85/s  (1.106s,  926.08/s)  LR: 8.497e-05  Data: 0.012 (0.012)
Train: 221 [ 500/1251 ( 40%)]  Loss:  2.961922 (2.9443)  Time: 1.104s,  927.81/s  (1.105s,  926.32/s)  LR: 8.497e-05  Data: 0.010 (0.012)
Train: 221 [ 550/1251 ( 44%)]  Loss:  3.171996 (2.9633)  Time: 1.095s,  935.46/s  (1.106s,  926.22/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 600/1251 ( 48%)]  Loss:  3.255912 (2.9858)  Time: 1.102s,  929.07/s  (1.106s,  926.09/s)  LR: 8.497e-05  Data: 0.012 (0.012)
Train: 221 [ 650/1251 ( 52%)]  Loss:  2.896220 (2.9794)  Time: 1.130s,  906.17/s  (1.107s,  925.34/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 700/1251 ( 56%)]  Loss:  2.829922 (2.9694)  Time: 1.120s,  914.35/s  (1.107s,  924.98/s)  LR: 8.497e-05  Data: 0.010 (0.012)
Train: 221 [ 750/1251 ( 60%)]  Loss:  2.808233 (2.9593)  Time: 1.097s,  933.16/s  (1.107s,  925.03/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 800/1251 ( 64%)]  Loss:  3.141955 (2.9701)  Time: 1.097s,  933.43/s  (1.107s,  925.30/s)  LR: 8.497e-05  Data: 0.012 (0.012)
Train: 221 [ 850/1251 ( 68%)]  Loss:  2.761645 (2.9585)  Time: 1.119s,  914.70/s  (1.107s,  924.99/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [ 900/1251 ( 72%)]  Loss:  2.841398 (2.9523)  Time: 1.121s,  913.46/s  (1.107s,  924.86/s)  LR: 8.497e-05  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 221 [ 950/1251 ( 76%)]  Loss:  2.713299 (2.9404)  Time: 1.259s,  813.11/s  (1.108s,  924.44/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [1000/1251 ( 80%)]  Loss:  3.233449 (2.9543)  Time: 1.098s,  932.39/s  (1.108s,  924.53/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [1050/1251 ( 84%)]  Loss:  2.647910 (2.9404)  Time: 1.109s,  923.13/s  (1.107s,  924.64/s)  LR: 8.497e-05  Data: 0.012 (0.012)
Train: 221 [1100/1251 ( 88%)]  Loss:  2.813832 (2.9349)  Time: 1.097s,  933.84/s  (1.107s,  924.87/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [1150/1251 ( 92%)]  Loss:  3.274718 (2.9491)  Time: 1.096s,  934.34/s  (1.107s,  924.63/s)  LR: 8.497e-05  Data: 0.011 (0.012)
Train: 221 [1200/1251 ( 96%)]  Loss:  3.020342 (2.9519)  Time: 1.101s,  930.34/s  (1.108s,  924.42/s)  LR: 8.497e-05  Data: 0.010 (0.012)
Train: 221 [1250/1251 (100%)]  Loss:  3.233062 (2.9627)  Time: 1.081s,  947.51/s  (1.108s,  924.40/s)  LR: 8.497e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.323 (3.323)  Loss:  0.4362 (0.4362)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.5475 (0.8906)  Acc@1: 87.2641 (80.1740)  Acc@5: 98.2311 (95.1060)
Test (EMA): [   0/48]  Time: 3.276 (3.276)  Loss:  0.3942 (0.3942)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5376 (0.8292)  Acc@1: 87.6179 (80.9980)  Acc@5: 97.9953 (95.5880)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 80.94400007568359)

Train: 222 [   0/1251 (  0%)]  Loss:  2.762350 (2.7623)  Time: 1.127s,  908.93/s  (1.127s,  908.93/s)  LR: 8.307e-05  Data: 0.019 (0.019)
Train: 222 [  50/1251 (  4%)]  Loss:  3.178684 (2.9705)  Time: 1.096s,  933.92/s  (1.109s,  923.37/s)  LR: 8.307e-05  Data: 0.012 (0.012)
Train: 222 [ 100/1251 (  8%)]  Loss:  2.765515 (2.9022)  Time: 1.211s,  845.89/s  (1.114s,  919.29/s)  LR: 8.307e-05  Data: 0.010 (0.012)
Train: 222 [ 150/1251 ( 12%)]  Loss:  3.069636 (2.9440)  Time: 1.135s,  902.38/s  (1.116s,  917.71/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 200/1251 ( 16%)]  Loss:  3.073322 (2.9699)  Time: 1.103s,  928.53/s  (1.112s,  921.05/s)  LR: 8.307e-05  Data: 0.010 (0.012)
Train: 222 [ 250/1251 ( 20%)]  Loss:  2.962395 (2.9687)  Time: 1.099s,  931.70/s  (1.110s,  922.32/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 222 [ 300/1251 ( 24%)]  Loss:  3.014067 (2.9751)  Time: 1.095s,  935.40/s  (1.110s,  922.71/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 350/1251 ( 28%)]  Loss:  3.029612 (2.9819)  Time: 1.099s,  931.87/s  (1.110s,  922.93/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 400/1251 ( 32%)]  Loss:  2.916732 (2.9747)  Time: 1.098s,  932.60/s  (1.109s,  923.59/s)  LR: 8.307e-05  Data: 0.010 (0.012)
Train: 222 [ 450/1251 ( 36%)]  Loss:  2.909211 (2.9682)  Time: 1.179s,  868.36/s  (1.109s,  923.61/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 500/1251 ( 40%)]  Loss:  2.764448 (2.9496)  Time: 1.097s,  933.33/s  (1.108s,  924.19/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 550/1251 ( 44%)]  Loss:  2.871089 (2.9431)  Time: 1.098s,  932.53/s  (1.107s,  924.64/s)  LR: 8.307e-05  Data: 0.012 (0.012)
Train: 222 [ 600/1251 ( 48%)]  Loss:  2.950569 (2.9437)  Time: 1.098s,  932.88/s  (1.107s,  924.89/s)  LR: 8.307e-05  Data: 0.010 (0.012)
Train: 222 [ 650/1251 ( 52%)]  Loss:  2.439792 (2.9077)  Time: 1.128s,  908.08/s  (1.107s,  925.13/s)  LR: 8.307e-05  Data: 0.018 (0.012)
Train: 222 [ 700/1251 ( 56%)]  Loss:  2.975489 (2.9122)  Time: 1.194s,  857.50/s  (1.107s,  924.81/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 750/1251 ( 60%)]  Loss:  3.061662 (2.9215)  Time: 1.097s,  933.49/s  (1.107s,  925.27/s)  LR: 8.307e-05  Data: 0.012 (0.012)
Train: 222 [ 800/1251 ( 64%)]  Loss:  3.281443 (2.9427)  Time: 1.106s,  925.81/s  (1.107s,  925.01/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 850/1251 ( 68%)]  Loss:  2.929654 (2.9420)  Time: 1.100s,  930.71/s  (1.107s,  925.30/s)  LR: 8.307e-05  Data: 0.013 (0.012)
Train: 222 [ 900/1251 ( 72%)]  Loss:  2.934068 (2.9416)  Time: 1.096s,  934.09/s  (1.107s,  925.24/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [ 950/1251 ( 76%)]  Loss:  2.677299 (2.9284)  Time: 1.099s,  931.96/s  (1.107s,  925.29/s)  LR: 8.307e-05  Data: 0.012 (0.012)
Train: 222 [1000/1251 ( 80%)]  Loss:  3.057021 (2.9345)  Time: 1.100s,  930.90/s  (1.107s,  925.19/s)  LR: 8.307e-05  Data: 0.012 (0.012)
Train: 222 [1050/1251 ( 84%)]  Loss:  3.283177 (2.9503)  Time: 1.099s,  932.08/s  (1.106s,  925.44/s)  LR: 8.307e-05  Data: 0.010 (0.012)
Train: 222 [1100/1251 ( 88%)]  Loss:  2.970813 (2.9512)  Time: 1.118s,  916.01/s  (1.107s,  925.23/s)  LR: 8.307e-05  Data: 0.011 (0.012)
Train: 222 [1150/1251 ( 92%)]  Loss:  3.109476 (2.9578)  Time: 1.098s,  932.67/s  (1.107s,  925.27/s)  LR: 8.307e-05  Data: 0.014 (0.012)
Train: 222 [1200/1251 ( 96%)]  Loss:  2.532201 (2.9408)  Time: 1.119s,  914.84/s  (1.106s,  925.46/s)  LR: 8.307e-05  Data: 0.012 (0.012)
Train: 222 [1250/1251 (100%)]  Loss:  2.822858 (2.9363)  Time: 1.104s,  927.65/s  (1.107s,  924.94/s)  LR: 8.307e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.205 (3.205)  Loss:  0.4009 (0.4009)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.5573 (0.8707)  Acc@1: 86.7925 (80.0840)  Acc@5: 97.2877 (95.1820)
Test (EMA): [   0/48]  Time: 3.164 (3.164)  Loss:  0.3940 (0.3940)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5373 (0.8293)  Acc@1: 87.5000 (81.0040)  Acc@5: 97.9953 (95.6000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 80.95199997314454)

Train: 223 [   0/1251 (  0%)]  Loss:  2.957232 (2.9572)  Time: 1.112s,  921.23/s  (1.112s,  921.23/s)  LR: 8.119e-05  Data: 0.022 (0.022)
Train: 223 [  50/1251 (  4%)]  Loss:  2.859129 (2.9082)  Time: 1.122s,  912.80/s  (1.105s,  926.38/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 100/1251 (  8%)]  Loss:  2.917679 (2.9113)  Time: 1.096s,  934.70/s  (1.111s,  921.66/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 150/1251 ( 12%)]  Loss:  2.868078 (2.9005)  Time: 1.099s,  932.17/s  (1.110s,  922.68/s)  LR: 8.119e-05  Data: 0.013 (0.012)
Train: 223 [ 200/1251 ( 16%)]  Loss:  3.040680 (2.9286)  Time: 1.096s,  933.98/s  (1.110s,  922.32/s)  LR: 8.119e-05  Data: 0.011 (0.011)
Train: 223 [ 250/1251 ( 20%)]  Loss:  3.271918 (2.9858)  Time: 1.106s,  926.22/s  (1.109s,  923.51/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 300/1251 ( 24%)]  Loss:  2.800443 (2.9593)  Time: 1.112s,  920.96/s  (1.109s,  923.00/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 350/1251 ( 28%)]  Loss:  2.934461 (2.9562)  Time: 1.099s,  931.40/s  (1.108s,  923.82/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 400/1251 ( 32%)]  Loss:  2.873123 (2.9470)  Time: 1.094s,  935.88/s  (1.108s,  924.06/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 450/1251 ( 36%)]  Loss:  2.939958 (2.9463)  Time: 1.104s,  927.74/s  (1.108s,  923.97/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 500/1251 ( 40%)]  Loss:  3.204531 (2.9697)  Time: 1.098s,  932.83/s  (1.108s,  924.20/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 550/1251 ( 44%)]  Loss:  3.060827 (2.9773)  Time: 1.103s,  928.10/s  (1.108s,  924.41/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 600/1251 ( 48%)]  Loss:  2.994969 (2.9787)  Time: 1.185s,  864.31/s  (1.107s,  924.76/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 650/1251 ( 52%)]  Loss:  3.243867 (2.9976)  Time: 1.099s,  931.43/s  (1.109s,  923.64/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 700/1251 ( 56%)]  Loss:  2.941182 (2.9939)  Time: 1.122s,  912.70/s  (1.108s,  924.01/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 750/1251 ( 60%)]  Loss:  2.794811 (2.9814)  Time: 1.120s,  914.59/s  (1.109s,  923.77/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 800/1251 ( 64%)]  Loss:  3.005988 (2.9829)  Time: 1.095s,  934.78/s  (1.108s,  924.07/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [ 850/1251 ( 68%)]  Loss:  2.993725 (2.9835)  Time: 1.106s,  925.65/s  (1.108s,  924.22/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [ 900/1251 ( 72%)]  Loss:  3.086704 (2.9889)  Time: 1.125s,  910.55/s  (1.108s,  924.39/s)  LR: 8.119e-05  Data: 0.015 (0.012)
Train: 223 [ 950/1251 ( 76%)]  Loss:  3.135040 (2.9962)  Time: 1.103s,  928.40/s  (1.108s,  924.39/s)  LR: 8.119e-05  Data: 0.010 (0.012)
Train: 223 [1000/1251 ( 80%)]  Loss:  2.914279 (2.9923)  Time: 1.096s,  933.95/s  (1.108s,  924.41/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [1050/1251 ( 84%)]  Loss:  3.017703 (2.9935)  Time: 1.099s,  932.10/s  (1.108s,  924.55/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [1100/1251 ( 88%)]  Loss:  2.990724 (2.9934)  Time: 1.100s,  930.63/s  (1.108s,  924.37/s)  LR: 8.119e-05  Data: 0.018 (0.012)
Train: 223 [1150/1251 ( 92%)]  Loss:  3.274081 (3.0050)  Time: 1.096s,  933.95/s  (1.108s,  924.49/s)  LR: 8.119e-05  Data: 0.011 (0.012)
Train: 223 [1200/1251 ( 96%)]  Loss:  2.897436 (3.0007)  Time: 1.096s,  934.63/s  (1.107s,  924.64/s)  LR: 8.119e-05  Data: 0.012 (0.012)
Train: 223 [1250/1251 (100%)]  Loss:  2.915411 (2.9975)  Time: 1.103s,  928.03/s  (1.108s,  924.55/s)  LR: 8.119e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.389 (3.389)  Loss:  0.4250 (0.4250)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.5895 (0.8914)  Acc@1: 86.4387 (80.1760)  Acc@5: 97.7594 (95.1780)
Test (EMA): [   0/48]  Time: 3.095 (3.095)  Loss:  0.3940 (0.3940)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5372 (0.8293)  Acc@1: 87.6179 (81.0000)  Acc@5: 97.9953 (95.6000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 80.95599997314453)

Train: 224 [   0/1251 (  0%)]  Loss:  2.890506 (2.8905)  Time: 1.134s,  902.98/s  (1.134s,  902.98/s)  LR: 7.933e-05  Data: 0.025 (0.025)
Train: 224 [  50/1251 (  4%)]  Loss:  3.042317 (2.9664)  Time: 1.099s,  931.96/s  (1.109s,  923.18/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 100/1251 (  8%)]  Loss:  3.172753 (3.0352)  Time: 1.130s,  906.28/s  (1.105s,  926.90/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 150/1251 ( 12%)]  Loss:  3.115321 (3.0552)  Time: 1.101s,  930.42/s  (1.108s,  924.42/s)  LR: 7.933e-05  Data: 0.012 (0.011)
Train: 224 [ 200/1251 ( 16%)]  Loss:  2.986282 (3.0414)  Time: 1.097s,  933.45/s  (1.107s,  925.02/s)  LR: 7.933e-05  Data: 0.014 (0.012)
Train: 224 [ 250/1251 ( 20%)]  Loss:  2.978176 (3.0309)  Time: 1.186s,  863.65/s  (1.108s,  924.09/s)  LR: 7.933e-05  Data: 0.017 (0.012)
Train: 224 [ 300/1251 ( 24%)]  Loss:  2.715100 (2.9858)  Time: 1.106s,  925.76/s  (1.108s,  924.36/s)  LR: 7.933e-05  Data: 0.012 (0.012)
Train: 224 [ 350/1251 ( 28%)]  Loss:  2.992490 (2.9866)  Time: 1.097s,  933.82/s  (1.107s,  924.95/s)  LR: 7.933e-05  Data: 0.012 (0.012)
Train: 224 [ 400/1251 ( 32%)]  Loss:  2.894604 (2.9764)  Time: 1.097s,  933.49/s  (1.107s,  925.27/s)  LR: 7.933e-05  Data: 0.012 (0.012)
Train: 224 [ 450/1251 ( 36%)]  Loss:  2.972450 (2.9760)  Time: 1.096s,  934.53/s  (1.106s,  925.68/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 500/1251 ( 40%)]  Loss:  2.746482 (2.9551)  Time: 1.097s,  933.70/s  (1.106s,  925.68/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 550/1251 ( 44%)]  Loss:  2.770696 (2.9398)  Time: 1.196s,  856.27/s  (1.106s,  925.99/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 600/1251 ( 48%)]  Loss:  2.996727 (2.9441)  Time: 1.093s,  936.69/s  (1.106s,  925.69/s)  LR: 7.933e-05  Data: 0.010 (0.012)
Train: 224 [ 650/1251 ( 52%)]  Loss:  3.050959 (2.9518)  Time: 1.094s,  935.74/s  (1.106s,  925.72/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 700/1251 ( 56%)]  Loss:  3.043893 (2.9579)  Time: 1.121s,  913.22/s  (1.106s,  925.62/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 750/1251 ( 60%)]  Loss:  3.072891 (2.9651)  Time: 1.096s,  934.33/s  (1.107s,  925.27/s)  LR: 7.933e-05  Data: 0.012 (0.012)
Train: 224 [ 800/1251 ( 64%)]  Loss:  3.039177 (2.9695)  Time: 1.108s,  924.48/s  (1.107s,  924.99/s)  LR: 7.933e-05  Data: 0.010 (0.012)
Train: 224 [ 850/1251 ( 68%)]  Loss:  3.069751 (2.9750)  Time: 1.099s,  931.39/s  (1.107s,  925.02/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [ 900/1251 ( 72%)]  Loss:  2.826686 (2.9672)  Time: 1.098s,  932.24/s  (1.107s,  924.94/s)  LR: 7.933e-05  Data: 0.018 (0.012)
Train: 224 [ 950/1251 ( 76%)]  Loss:  3.011481 (2.9694)  Time: 1.099s,  932.14/s  (1.107s,  925.13/s)  LR: 7.933e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 224 [1000/1251 ( 80%)]  Loss:  2.964214 (2.9692)  Time: 1.195s,  856.86/s  (1.107s,  925.05/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [1050/1251 ( 84%)]  Loss:  3.040303 (2.9724)  Time: 1.095s,  935.27/s  (1.107s,  925.17/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [1100/1251 ( 88%)]  Loss:  2.918144 (2.9701)  Time: 1.098s,  932.85/s  (1.107s,  925.29/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [1150/1251 ( 92%)]  Loss:  3.095383 (2.9753)  Time: 1.097s,  933.69/s  (1.107s,  925.30/s)  LR: 7.933e-05  Data: 0.011 (0.012)
Train: 224 [1200/1251 ( 96%)]  Loss:  2.823865 (2.9692)  Time: 1.123s,  911.84/s  (1.107s,  925.29/s)  LR: 7.933e-05  Data: 0.012 (0.012)
Train: 224 [1250/1251 (100%)]  Loss:  3.208142 (2.9784)  Time: 1.097s,  933.52/s  (1.107s,  925.17/s)  LR: 7.933e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.254 (3.254)  Loss:  0.4349 (0.4349)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.230 (0.411)  Loss:  0.5613 (0.8752)  Acc@1: 86.2028 (80.2380)  Acc@5: 97.7594 (95.1120)
Test (EMA): [   0/48]  Time: 3.128 (3.128)  Loss:  0.3941 (0.3941)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.405)  Loss:  0.5375 (0.8294)  Acc@1: 87.3821 (81.0200)  Acc@5: 97.9953 (95.5860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 80.9660000756836)

Train: 225 [   0/1251 (  0%)]  Loss:  2.507526 (2.5075)  Time: 1.104s,  927.67/s  (1.104s,  927.67/s)  LR: 7.749e-05  Data: 0.022 (0.022)
Train: 225 [  50/1251 (  4%)]  Loss:  2.748682 (2.6281)  Time: 1.095s,  935.00/s  (1.103s,  928.79/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 100/1251 (  8%)]  Loss:  3.036081 (2.7641)  Time: 1.098s,  932.38/s  (1.107s,  925.29/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 150/1251 ( 12%)]  Loss:  2.946027 (2.8096)  Time: 1.097s,  933.46/s  (1.106s,  925.89/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 200/1251 ( 16%)]  Loss:  3.111790 (2.8700)  Time: 1.190s,  860.82/s  (1.106s,  925.45/s)  LR: 7.749e-05  Data: 0.010 (0.012)
Train: 225 [ 250/1251 ( 20%)]  Loss:  2.940596 (2.8818)  Time: 1.095s,  934.83/s  (1.106s,  925.91/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 300/1251 ( 24%)]  Loss:  3.280448 (2.9387)  Time: 1.130s,  906.12/s  (1.107s,  925.07/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 350/1251 ( 28%)]  Loss:  3.114349 (2.9607)  Time: 1.102s,  929.40/s  (1.107s,  925.35/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 400/1251 ( 32%)]  Loss:  3.015985 (2.9668)  Time: 1.096s,  934.35/s  (1.107s,  924.89/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 450/1251 ( 36%)]  Loss:  3.132566 (2.9834)  Time: 1.094s,  936.18/s  (1.108s,  924.38/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 500/1251 ( 40%)]  Loss:  2.970952 (2.9823)  Time: 1.129s,  906.74/s  (1.108s,  924.04/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 550/1251 ( 44%)]  Loss:  3.063341 (2.9890)  Time: 1.096s,  934.24/s  (1.108s,  923.81/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 600/1251 ( 48%)]  Loss:  2.918440 (2.9836)  Time: 1.099s,  931.59/s  (1.108s,  924.43/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 650/1251 ( 52%)]  Loss:  3.087097 (2.9910)  Time: 1.096s,  934.15/s  (1.107s,  924.70/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 700/1251 ( 56%)]  Loss:  3.166191 (3.0027)  Time: 1.106s,  925.86/s  (1.107s,  925.05/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 750/1251 ( 60%)]  Loss:  2.896527 (2.9960)  Time: 1.103s,  928.68/s  (1.107s,  924.82/s)  LR: 7.749e-05  Data: 0.013 (0.012)
Train: 225 [ 800/1251 ( 64%)]  Loss:  2.923463 (2.9918)  Time: 1.097s,  933.52/s  (1.107s,  925.10/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [ 850/1251 ( 68%)]  Loss:  3.139474 (3.0000)  Time: 1.091s,  938.51/s  (1.108s,  924.46/s)  LR: 7.749e-05  Data: 0.009 (0.012)
Train: 225 [ 900/1251 ( 72%)]  Loss:  3.017049 (3.0009)  Time: 1.096s,  934.33/s  (1.108s,  924.33/s)  LR: 7.749e-05  Data: 0.012 (0.012)
Train: 225 [ 950/1251 ( 76%)]  Loss:  3.022726 (3.0020)  Time: 1.098s,  932.61/s  (1.108s,  924.50/s)  LR: 7.749e-05  Data: 0.014 (0.012)
Train: 225 [1000/1251 ( 80%)]  Loss:  3.008220 (3.0023)  Time: 1.096s,  934.09/s  (1.108s,  924.45/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [1050/1251 ( 84%)]  Loss:  2.896615 (2.9975)  Time: 1.092s,  937.45/s  (1.108s,  924.57/s)  LR: 7.749e-05  Data: 0.010 (0.012)
Train: 225 [1100/1251 ( 88%)]  Loss:  2.805827 (2.9891)  Time: 1.098s,  932.26/s  (1.107s,  924.61/s)  LR: 7.749e-05  Data: 0.013 (0.012)
Train: 225 [1150/1251 ( 92%)]  Loss:  2.956944 (2.9878)  Time: 1.098s,  932.88/s  (1.107s,  924.98/s)  LR: 7.749e-05  Data: 0.013 (0.012)
Train: 225 [1200/1251 ( 96%)]  Loss:  3.170767 (2.9951)  Time: 1.097s,  933.47/s  (1.107s,  924.94/s)  LR: 7.749e-05  Data: 0.011 (0.012)
Train: 225 [1250/1251 (100%)]  Loss:  2.950008 (2.9934)  Time: 1.079s,  948.77/s  (1.107s,  924.96/s)  LR: 7.749e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.266 (3.266)  Loss:  0.4295 (0.4295)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.5869 (0.8779)  Acc@1: 87.0283 (80.2540)  Acc@5: 97.4057 (95.2240)
Test (EMA): [   0/48]  Time: 3.325 (3.325)  Loss:  0.3941 (0.3941)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5381 (0.8292)  Acc@1: 87.1462 (81.0220)  Acc@5: 97.7594 (95.5920)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 80.96800002441407)

Train: 226 [   0/1251 (  0%)]  Loss:  2.876001 (2.8760)  Time: 1.101s,  929.94/s  (1.101s,  929.94/s)  LR: 7.567e-05  Data: 0.020 (0.020)
Train: 226 [  50/1251 (  4%)]  Loss:  3.007299 (2.9417)  Time: 1.121s,  913.12/s  (1.114s,  919.45/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 100/1251 (  8%)]  Loss:  2.895706 (2.9263)  Time: 1.097s,  933.57/s  (1.112s,  920.52/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 150/1251 ( 12%)]  Loss:  3.028089 (2.9518)  Time: 1.183s,  865.92/s  (1.110s,  922.34/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 200/1251 ( 16%)]  Loss:  3.016855 (2.9648)  Time: 1.097s,  933.67/s  (1.109s,  923.13/s)  LR: 7.567e-05  Data: 0.012 (0.012)
Train: 226 [ 250/1251 ( 20%)]  Loss:  2.888195 (2.9520)  Time: 1.095s,  934.98/s  (1.108s,  923.81/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 300/1251 ( 24%)]  Loss:  2.854909 (2.9382)  Time: 1.095s,  935.35/s  (1.107s,  924.84/s)  LR: 7.567e-05  Data: 0.013 (0.012)
Train: 226 [ 350/1251 ( 28%)]  Loss:  3.186529 (2.9692)  Time: 1.101s,  929.86/s  (1.107s,  925.03/s)  LR: 7.567e-05  Data: 0.014 (0.012)
Train: 226 [ 400/1251 ( 32%)]  Loss:  3.320500 (3.0082)  Time: 1.097s,  933.09/s  (1.107s,  924.64/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 450/1251 ( 36%)]  Loss:  2.936331 (3.0010)  Time: 1.104s,  927.31/s  (1.107s,  925.12/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 500/1251 ( 40%)]  Loss:  2.886374 (2.9906)  Time: 1.101s,  930.25/s  (1.107s,  925.27/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 226 [ 550/1251 ( 44%)]  Loss:  2.898933 (2.9830)  Time: 1.104s,  927.53/s  (1.106s,  925.91/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 600/1251 ( 48%)]  Loss:  3.040477 (2.9874)  Time: 1.104s,  927.62/s  (1.106s,  925.67/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 650/1251 ( 52%)]  Loss:  2.867695 (2.9788)  Time: 1.096s,  933.94/s  (1.106s,  925.91/s)  LR: 7.567e-05  Data: 0.013 (0.012)
Train: 226 [ 700/1251 ( 56%)]  Loss:  3.072160 (2.9851)  Time: 1.119s,  915.22/s  (1.107s,  925.16/s)  LR: 7.567e-05  Data: 0.010 (0.012)
Train: 226 [ 750/1251 ( 60%)]  Loss:  2.994158 (2.9856)  Time: 1.108s,  924.05/s  (1.107s,  924.62/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 800/1251 ( 64%)]  Loss:  2.816213 (2.9757)  Time: 1.100s,  930.64/s  (1.107s,  924.64/s)  LR: 7.567e-05  Data: 0.012 (0.012)
Train: 226 [ 850/1251 ( 68%)]  Loss:  2.804659 (2.9662)  Time: 1.094s,  935.87/s  (1.108s,  924.45/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [ 900/1251 ( 72%)]  Loss:  2.991201 (2.9675)  Time: 1.097s,  933.55/s  (1.108s,  924.21/s)  LR: 7.567e-05  Data: 0.012 (0.012)
Train: 226 [ 950/1251 ( 76%)]  Loss:  2.468130 (2.9425)  Time: 1.099s,  931.78/s  (1.108s,  924.22/s)  LR: 7.567e-05  Data: 0.012 (0.012)
Train: 226 [1000/1251 ( 80%)]  Loss:  2.709249 (2.9314)  Time: 1.097s,  933.40/s  (1.108s,  924.37/s)  LR: 7.567e-05  Data: 0.012 (0.012)
Train: 226 [1050/1251 ( 84%)]  Loss:  2.919001 (2.9308)  Time: 1.120s,  914.36/s  (1.108s,  924.18/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [1100/1251 ( 88%)]  Loss:  3.065145 (2.9367)  Time: 1.098s,  932.20/s  (1.108s,  924.40/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [1150/1251 ( 92%)]  Loss:  2.711092 (2.9273)  Time: 1.098s,  932.57/s  (1.108s,  924.35/s)  LR: 7.567e-05  Data: 0.011 (0.012)
Train: 226 [1200/1251 ( 96%)]  Loss:  2.996789 (2.9301)  Time: 1.098s,  932.60/s  (1.108s,  924.36/s)  LR: 7.567e-05  Data: 0.014 (0.012)
Train: 226 [1250/1251 (100%)]  Loss:  2.706041 (2.9215)  Time: 1.103s,  928.09/s  (1.108s,  924.23/s)  LR: 7.567e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.286 (3.286)  Loss:  0.4308 (0.4308)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5800 (0.8806)  Acc@1: 86.3208 (80.1980)  Acc@5: 97.2877 (95.1080)
Test (EMA): [   0/48]  Time: 3.230 (3.230)  Loss:  0.3941 (0.3941)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5390 (0.8292)  Acc@1: 87.2641 (81.0280)  Acc@5: 97.7594 (95.5640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 80.976)

Train: 227 [   0/1251 (  0%)]  Loss:  2.808267 (2.8083)  Time: 1.102s,  929.16/s  (1.102s,  929.16/s)  LR: 7.386e-05  Data: 0.023 (0.023)
Train: 227 [  50/1251 (  4%)]  Loss:  2.995313 (2.9018)  Time: 1.096s,  934.36/s  (1.113s,  919.64/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 100/1251 (  8%)]  Loss:  3.107583 (2.9704)  Time: 1.177s,  870.17/s  (1.113s,  919.78/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 150/1251 ( 12%)]  Loss:  3.160265 (3.0179)  Time: 1.097s,  933.29/s  (1.114s,  919.42/s)  LR: 7.386e-05  Data: 0.012 (0.011)
Train: 227 [ 200/1251 ( 16%)]  Loss:  2.830468 (2.9804)  Time: 1.110s,  922.31/s  (1.111s,  921.86/s)  LR: 7.386e-05  Data: 0.011 (0.011)
Train: 227 [ 250/1251 ( 20%)]  Loss:  2.862197 (2.9607)  Time: 1.096s,  934.51/s  (1.110s,  922.12/s)  LR: 7.386e-05  Data: 0.012 (0.011)
Train: 227 [ 300/1251 ( 24%)]  Loss:  3.058437 (2.9746)  Time: 1.100s,  930.99/s  (1.110s,  922.76/s)  LR: 7.386e-05  Data: 0.013 (0.011)
Train: 227 [ 350/1251 ( 28%)]  Loss:  2.747418 (2.9462)  Time: 1.097s,  933.10/s  (1.109s,  923.29/s)  LR: 7.386e-05  Data: 0.012 (0.011)
Train: 227 [ 400/1251 ( 32%)]  Loss:  2.715009 (2.9206)  Time: 1.098s,  932.22/s  (1.108s,  923.87/s)  LR: 7.386e-05  Data: 0.012 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 227 [ 450/1251 ( 36%)]  Loss:  2.774768 (2.9060)  Time: 1.101s,  930.00/s  (1.109s,  923.69/s)  LR: 7.386e-05  Data: 0.011 (0.011)
Train: 227 [ 500/1251 ( 40%)]  Loss:  2.862689 (2.9020)  Time: 1.096s,  934.72/s  (1.108s,  923.88/s)  LR: 7.386e-05  Data: 0.012 (0.012)
Train: 227 [ 550/1251 ( 44%)]  Loss:  2.877352 (2.9000)  Time: 1.099s,  932.00/s  (1.108s,  924.08/s)  LR: 7.386e-05  Data: 0.012 (0.012)
Train: 227 [ 600/1251 ( 48%)]  Loss:  2.947896 (2.9037)  Time: 1.096s,  933.94/s  (1.108s,  924.21/s)  LR: 7.386e-05  Data: 0.013 (0.012)
Train: 227 [ 650/1251 ( 52%)]  Loss:  3.306941 (2.9325)  Time: 1.097s,  933.12/s  (1.108s,  923.80/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 700/1251 ( 56%)]  Loss:  2.937405 (2.9328)  Time: 1.123s,  911.69/s  (1.109s,  923.61/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 750/1251 ( 60%)]  Loss:  2.901252 (2.9308)  Time: 1.096s,  934.00/s  (1.109s,  923.24/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 800/1251 ( 64%)]  Loss:  2.689622 (2.9166)  Time: 1.103s,  928.42/s  (1.109s,  923.38/s)  LR: 7.386e-05  Data: 0.020 (0.012)
Train: 227 [ 850/1251 ( 68%)]  Loss:  2.802324 (2.9103)  Time: 1.104s,  927.27/s  (1.109s,  923.60/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 900/1251 ( 72%)]  Loss:  3.043510 (2.9173)  Time: 1.096s,  934.59/s  (1.109s,  923.64/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [ 950/1251 ( 76%)]  Loss:  3.034416 (2.9232)  Time: 1.096s,  934.53/s  (1.108s,  923.92/s)  LR: 7.386e-05  Data: 0.012 (0.012)
Train: 227 [1000/1251 ( 80%)]  Loss:  3.002266 (2.9269)  Time: 1.116s,  917.77/s  (1.109s,  923.68/s)  LR: 7.386e-05  Data: 0.013 (0.012)
Train: 227 [1050/1251 ( 84%)]  Loss:  3.181705 (2.9385)  Time: 1.099s,  931.96/s  (1.108s,  923.99/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [1100/1251 ( 88%)]  Loss:  2.754215 (2.9305)  Time: 1.122s,  912.29/s  (1.108s,  923.84/s)  LR: 7.386e-05  Data: 0.010 (0.012)
Train: 227 [1150/1251 ( 92%)]  Loss:  3.159065 (2.9400)  Time: 1.097s,  933.51/s  (1.108s,  923.98/s)  LR: 7.386e-05  Data: 0.012 (0.012)
Train: 227 [1200/1251 ( 96%)]  Loss:  2.969009 (2.9412)  Time: 1.115s,  918.16/s  (1.108s,  923.93/s)  LR: 7.386e-05  Data: 0.011 (0.012)
Train: 227 [1250/1251 (100%)]  Loss:  3.135479 (2.9486)  Time: 1.077s,  950.77/s  (1.108s,  923.90/s)  LR: 7.386e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.260 (3.260)  Loss:  0.4197 (0.4197)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5761 (0.8819)  Acc@1: 87.5000 (80.3020)  Acc@5: 97.5236 (95.1620)
Test (EMA): [   0/48]  Time: 3.148 (3.148)  Loss:  0.3949 (0.3949)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5398 (0.8292)  Acc@1: 87.2642 (81.0620)  Acc@5: 97.6415 (95.5860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 80.97999997314453)

Train: 228 [   0/1251 (  0%)]  Loss:  2.903487 (2.9035)  Time: 1.135s,  902.58/s  (1.135s,  902.58/s)  LR: 7.208e-05  Data: 0.026 (0.026)
Train: 228 [  50/1251 (  4%)]  Loss:  2.805294 (2.8544)  Time: 1.214s,  843.39/s  (1.111s,  922.03/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 100/1251 (  8%)]  Loss:  2.806262 (2.8383)  Time: 1.098s,  932.29/s  (1.108s,  924.15/s)  LR: 7.208e-05  Data: 0.013 (0.012)
Train: 228 [ 150/1251 ( 12%)]  Loss:  2.614433 (2.7824)  Time: 1.094s,  935.64/s  (1.108s,  923.98/s)  LR: 7.208e-05  Data: 0.010 (0.012)
Train: 228 [ 200/1251 ( 16%)]  Loss:  2.891599 (2.8042)  Time: 1.104s,  927.37/s  (1.110s,  922.35/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [ 250/1251 ( 20%)]  Loss:  2.904489 (2.8209)  Time: 1.097s,  933.50/s  (1.109s,  923.46/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 300/1251 ( 24%)]  Loss:  3.062789 (2.8555)  Time: 1.101s,  929.83/s  (1.109s,  923.55/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [ 350/1251 ( 28%)]  Loss:  3.021242 (2.8762)  Time: 1.193s,  858.20/s  (1.108s,  923.92/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 400/1251 ( 32%)]  Loss:  2.966396 (2.8862)  Time: 1.096s,  934.47/s  (1.109s,  923.53/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 450/1251 ( 36%)]  Loss:  2.755462 (2.8731)  Time: 1.114s,  918.97/s  (1.108s,  924.46/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 500/1251 ( 40%)]  Loss:  2.796536 (2.8662)  Time: 1.100s,  931.17/s  (1.107s,  924.65/s)  LR: 7.208e-05  Data: 0.015 (0.012)
Train: 228 [ 550/1251 ( 44%)]  Loss:  2.968105 (2.8747)  Time: 1.096s,  933.90/s  (1.107s,  924.91/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 600/1251 ( 48%)]  Loss:  3.103391 (2.8923)  Time: 1.098s,  932.67/s  (1.107s,  924.61/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 650/1251 ( 52%)]  Loss:  3.043782 (2.9031)  Time: 1.102s,  929.54/s  (1.108s,  924.29/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [ 700/1251 ( 56%)]  Loss:  2.925341 (2.9046)  Time: 1.097s,  933.12/s  (1.108s,  924.34/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [ 750/1251 ( 60%)]  Loss:  2.586932 (2.8847)  Time: 1.100s,  931.07/s  (1.108s,  924.55/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 800/1251 ( 64%)]  Loss:  2.838325 (2.8820)  Time: 1.097s,  933.19/s  (1.108s,  924.59/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 850/1251 ( 68%)]  Loss:  3.113919 (2.8949)  Time: 1.106s,  925.48/s  (1.107s,  924.63/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [ 900/1251 ( 72%)]  Loss:  2.998784 (2.9003)  Time: 1.101s,  929.82/s  (1.107s,  924.83/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [ 950/1251 ( 76%)]  Loss:  2.712057 (2.8909)  Time: 1.094s,  935.60/s  (1.107s,  924.93/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [1000/1251 ( 80%)]  Loss:  3.060663 (2.8990)  Time: 1.099s,  931.53/s  (1.107s,  925.05/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [1050/1251 ( 84%)]  Loss:  3.052241 (2.9060)  Time: 1.100s,  930.59/s  (1.107s,  924.67/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [1100/1251 ( 88%)]  Loss:  2.893228 (2.9054)  Time: 1.105s,  926.55/s  (1.107s,  924.95/s)  LR: 7.208e-05  Data: 0.013 (0.012)
Train: 228 [1150/1251 ( 92%)]  Loss:  2.958839 (2.9076)  Time: 1.096s,  934.65/s  (1.107s,  924.76/s)  LR: 7.208e-05  Data: 0.012 (0.012)
Train: 228 [1200/1251 ( 96%)]  Loss:  2.991063 (2.9110)  Time: 1.097s,  933.74/s  (1.107s,  924.85/s)  LR: 7.208e-05  Data: 0.011 (0.012)
Train: 228 [1250/1251 (100%)]  Loss:  2.723397 (2.9038)  Time: 1.080s,  948.08/s  (1.107s,  924.72/s)  LR: 7.208e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.214 (3.214)  Loss:  0.4319 (0.4319)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.414)  Loss:  0.6091 (0.8835)  Acc@1: 86.4387 (80.3400)  Acc@5: 97.0519 (95.1360)
Test (EMA): [   0/48]  Time: 3.117 (3.117)  Loss:  0.3954 (0.3954)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5403 (0.8294)  Acc@1: 87.2642 (81.0320)  Acc@5: 97.7594 (95.5580)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 80.997999921875)

Train: 229 [   0/1251 (  0%)]  Loss:  2.705296 (2.7053)  Time: 1.100s,  930.98/s  (1.100s,  930.98/s)  LR: 7.032e-05  Data: 0.020 (0.020)
Train: 229 [  50/1251 (  4%)]  Loss:  3.116531 (2.9109)  Time: 1.100s,  931.02/s  (1.106s,  926.10/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [ 100/1251 (  8%)]  Loss:  3.053277 (2.9584)  Time: 1.198s,  855.03/s  (1.107s,  924.71/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [ 150/1251 ( 12%)]  Loss:  2.925203 (2.9501)  Time: 1.096s,  934.05/s  (1.107s,  925.16/s)  LR: 7.032e-05  Data: 0.013 (0.012)
Train: 229 [ 200/1251 ( 16%)]  Loss:  2.758191 (2.9117)  Time: 1.094s,  935.73/s  (1.107s,  924.86/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 250/1251 ( 20%)]  Loss:  3.032164 (2.9318)  Time: 1.101s,  930.44/s  (1.109s,  923.68/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 300/1251 ( 24%)]  Loss:  3.085582 (2.9537)  Time: 1.202s,  852.26/s  (1.109s,  923.74/s)  LR: 7.032e-05  Data: 0.010 (0.012)
Train: 229 [ 350/1251 ( 28%)]  Loss:  3.104876 (2.9726)  Time: 1.128s,  907.83/s  (1.109s,  923.44/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 400/1251 ( 32%)]  Loss:  2.876241 (2.9619)  Time: 1.097s,  933.73/s  (1.108s,  923.79/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 450/1251 ( 36%)]  Loss:  3.028631 (2.9686)  Time: 1.101s,  930.09/s  (1.108s,  924.06/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [ 500/1251 ( 40%)]  Loss:  3.023563 (2.9736)  Time: 1.102s,  928.81/s  (1.107s,  924.64/s)  LR: 7.032e-05  Data: 0.010 (0.012)
Train: 229 [ 550/1251 ( 44%)]  Loss:  2.927586 (2.9698)  Time: 1.095s,  935.21/s  (1.108s,  924.56/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 600/1251 ( 48%)]  Loss:  3.077204 (2.9780)  Time: 1.112s,  921.20/s  (1.108s,  924.40/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 650/1251 ( 52%)]  Loss:  2.721549 (2.9597)  Time: 1.095s,  935.55/s  (1.108s,  924.21/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 700/1251 ( 56%)]  Loss:  2.632462 (2.9379)  Time: 1.097s,  933.60/s  (1.108s,  924.23/s)  LR: 7.032e-05  Data: 0.010 (0.012)
Train: 229 [ 750/1251 ( 60%)]  Loss:  2.985587 (2.9409)  Time: 1.095s,  935.18/s  (1.108s,  924.46/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 800/1251 ( 64%)]  Loss:  2.572009 (2.9192)  Time: 1.098s,  932.48/s  (1.108s,  924.54/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [ 850/1251 ( 68%)]  Loss:  3.042426 (2.9260)  Time: 1.210s,  846.47/s  (1.107s,  924.71/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [ 900/1251 ( 72%)]  Loss:  3.084242 (2.9343)  Time: 1.093s,  936.54/s  (1.108s,  924.59/s)  LR: 7.032e-05  Data: 0.010 (0.012)
Train: 229 [ 950/1251 ( 76%)]  Loss:  3.214401 (2.9484)  Time: 1.097s,  933.23/s  (1.107s,  924.62/s)  LR: 7.032e-05  Data: 0.011 (0.012)
Train: 229 [1000/1251 ( 80%)]  Loss:  2.618030 (2.9326)  Time: 1.098s,  932.52/s  (1.108s,  924.48/s)  LR: 7.032e-05  Data: 0.013 (0.012)
Train: 229 [1050/1251 ( 84%)]  Loss:  3.230332 (2.9462)  Time: 1.097s,  933.25/s  (1.107s,  924.78/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [1100/1251 ( 88%)]  Loss:  3.232588 (2.9586)  Time: 1.098s,  932.55/s  (1.107s,  924.71/s)  LR: 7.032e-05  Data: 0.010 (0.012)
Train: 229 [1150/1251 ( 92%)]  Loss:  2.711785 (2.9483)  Time: 1.095s,  934.81/s  (1.108s,  924.35/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [1200/1251 ( 96%)]  Loss:  2.591020 (2.9340)  Time: 1.132s,  904.38/s  (1.109s,  923.76/s)  LR: 7.032e-05  Data: 0.012 (0.012)
Train: 229 [1250/1251 (100%)]  Loss:  3.052829 (2.9386)  Time: 1.079s,  948.73/s  (1.109s,  923.19/s)  LR: 7.032e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.256 (3.256)  Loss:  0.4415 (0.4415)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.5825 (0.8881)  Acc@1: 87.3821 (80.2960)  Acc@5: 97.4057 (95.2380)
Test (EMA): [   0/48]  Time: 3.206 (3.206)  Loss:  0.3951 (0.3951)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5406 (0.8290)  Acc@1: 87.5000 (81.0560)  Acc@5: 97.7594 (95.5700)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 80.999999921875)

Train: 230 [   0/1251 (  0%)]  Loss:  2.863014 (2.8630)  Time: 1.112s,  920.82/s  (1.112s,  920.82/s)  LR: 6.857e-05  Data: 0.021 (0.021)
Train: 230 [  50/1251 (  4%)]  Loss:  2.697286 (2.7801)  Time: 1.202s,  851.98/s  (1.105s,  926.61/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 100/1251 (  8%)]  Loss:  3.049716 (2.8700)  Time: 1.097s,  933.66/s  (1.105s,  926.32/s)  LR: 6.857e-05  Data: 0.012 (0.012)
Train: 230 [ 150/1251 ( 12%)]  Loss:  2.768518 (2.8446)  Time: 1.096s,  934.20/s  (1.106s,  925.82/s)  LR: 6.857e-05  Data: 0.012 (0.012)
Train: 230 [ 200/1251 ( 16%)]  Loss:  3.195772 (2.9149)  Time: 1.098s,  932.50/s  (1.107s,  924.67/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 250/1251 ( 20%)]  Loss:  2.896596 (2.9118)  Time: 1.099s,  931.59/s  (1.106s,  925.66/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 300/1251 ( 24%)]  Loss:  3.196979 (2.9526)  Time: 1.122s,  912.96/s  (1.108s,  924.52/s)  LR: 6.857e-05  Data: 0.012 (0.012)
Train: 230 [ 350/1251 ( 28%)]  Loss:  2.788815 (2.9321)  Time: 1.096s,  934.04/s  (1.108s,  924.21/s)  LR: 6.857e-05  Data: 0.012 (0.012)
Train: 230 [ 400/1251 ( 32%)]  Loss:  3.155444 (2.9569)  Time: 1.100s,  930.62/s  (1.108s,  924.29/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 450/1251 ( 36%)]  Loss:  2.794227 (2.9406)  Time: 1.136s,  901.34/s  (1.108s,  923.92/s)  LR: 6.857e-05  Data: 0.010 (0.012)
Train: 230 [ 500/1251 ( 40%)]  Loss:  2.825735 (2.9302)  Time: 1.098s,  932.45/s  (1.108s,  924.12/s)  LR: 6.857e-05  Data: 0.015 (0.012)
Train: 230 [ 550/1251 ( 44%)]  Loss:  2.801574 (2.9195)  Time: 1.187s,  862.84/s  (1.108s,  924.55/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 600/1251 ( 48%)]  Loss:  3.061032 (2.9304)  Time: 1.094s,  935.87/s  (1.108s,  923.88/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 650/1251 ( 52%)]  Loss:  2.764791 (2.9185)  Time: 1.097s,  933.07/s  (1.109s,  923.76/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 700/1251 ( 56%)]  Loss:  2.714068 (2.9049)  Time: 1.117s,  916.53/s  (1.108s,  923.81/s)  LR: 6.857e-05  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 230 [ 750/1251 ( 60%)]  Loss:  2.833994 (2.9005)  Time: 1.093s,  936.71/s  (1.109s,  923.47/s)  LR: 6.857e-05  Data: 0.010 (0.012)
Train: 230 [ 800/1251 ( 64%)]  Loss:  3.152501 (2.9153)  Time: 1.201s,  852.43/s  (1.109s,  922.99/s)  LR: 6.857e-05  Data: 0.009 (0.012)
Train: 230 [ 850/1251 ( 68%)]  Loss:  2.719363 (2.9044)  Time: 1.195s,  856.65/s  (1.109s,  923.11/s)  LR: 6.857e-05  Data: 0.013 (0.012)
Train: 230 [ 900/1251 ( 72%)]  Loss:  3.239209 (2.9220)  Time: 1.131s,  905.33/s  (1.109s,  923.42/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [ 950/1251 ( 76%)]  Loss:  2.827924 (2.9173)  Time: 1.124s,  911.29/s  (1.109s,  923.13/s)  LR: 6.857e-05  Data: 0.012 (0.012)
Train: 230 [1000/1251 ( 80%)]  Loss:  2.845361 (2.9139)  Time: 1.094s,  935.91/s  (1.109s,  923.43/s)  LR: 6.857e-05  Data: 0.013 (0.012)
Train: 230 [1050/1251 ( 84%)]  Loss:  3.088418 (2.9218)  Time: 1.180s,  868.16/s  (1.109s,  923.60/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [1100/1251 ( 88%)]  Loss:  2.750263 (2.9144)  Time: 1.109s,  923.32/s  (1.109s,  923.67/s)  LR: 6.857e-05  Data: 0.011 (0.012)
Train: 230 [1150/1251 ( 92%)]  Loss:  2.825540 (2.9107)  Time: 1.100s,  930.84/s  (1.109s,  923.64/s)  LR: 6.857e-05  Data: 0.012 (0.012)
Train: 230 [1200/1251 ( 96%)]  Loss:  2.713577 (2.9028)  Time: 1.117s,  916.35/s  (1.109s,  923.59/s)  LR: 6.857e-05  Data: 0.010 (0.012)
Train: 230 [1250/1251 (100%)]  Loss:  3.189494 (2.9138)  Time: 1.082s,  946.26/s  (1.109s,  923.38/s)  LR: 6.857e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.236 (3.236)  Loss:  0.4161 (0.4161)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5487 (0.8705)  Acc@1: 88.2076 (80.4980)  Acc@5: 97.9953 (95.1760)
Test (EMA): [   0/48]  Time: 3.154 (3.154)  Loss:  0.3946 (0.3946)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5411 (0.8291)  Acc@1: 87.3821 (81.0280)  Acc@5: 97.7594 (95.5800)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 81.00399987060547)

Train: 231 [   0/1251 (  0%)]  Loss:  2.825801 (2.8258)  Time: 1.100s,  931.22/s  (1.100s,  931.22/s)  LR: 6.685e-05  Data: 0.020 (0.020)
Train: 231 [  50/1251 (  4%)]  Loss:  3.117065 (2.9714)  Time: 1.091s,  938.21/s  (1.110s,  922.28/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [ 100/1251 (  8%)]  Loss:  3.060359 (3.0011)  Time: 1.123s,  912.22/s  (1.116s,  917.59/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 150/1251 ( 12%)]  Loss:  2.947994 (2.9878)  Time: 1.095s,  934.88/s  (1.111s,  921.43/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [ 200/1251 ( 16%)]  Loss:  3.149618 (3.0202)  Time: 1.095s,  935.08/s  (1.109s,  923.51/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 250/1251 ( 20%)]  Loss:  3.048600 (3.0249)  Time: 1.101s,  930.29/s  (1.109s,  923.75/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [ 300/1251 ( 24%)]  Loss:  2.973425 (3.0176)  Time: 1.098s,  932.23/s  (1.108s,  924.57/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 350/1251 ( 28%)]  Loss:  2.587172 (2.9638)  Time: 1.101s,  929.68/s  (1.107s,  924.72/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [ 400/1251 ( 32%)]  Loss:  2.874843 (2.9539)  Time: 1.130s,  905.84/s  (1.108s,  924.33/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [ 450/1251 ( 36%)]  Loss:  2.843154 (2.9428)  Time: 1.103s,  928.19/s  (1.108s,  924.41/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [ 500/1251 ( 40%)]  Loss:  2.909688 (2.9398)  Time: 1.097s,  933.61/s  (1.108s,  924.09/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 550/1251 ( 44%)]  Loss:  3.022380 (2.9467)  Time: 1.096s,  934.22/s  (1.108s,  924.19/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [ 600/1251 ( 48%)]  Loss:  2.949999 (2.9469)  Time: 1.102s,  928.90/s  (1.109s,  923.31/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [ 650/1251 ( 52%)]  Loss:  2.703094 (2.9295)  Time: 1.122s,  912.86/s  (1.109s,  923.34/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 700/1251 ( 56%)]  Loss:  3.024348 (2.9358)  Time: 1.097s,  933.53/s  (1.109s,  923.28/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 750/1251 ( 60%)]  Loss:  3.222445 (2.9537)  Time: 1.214s,  843.78/s  (1.109s,  923.36/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [ 800/1251 ( 64%)]  Loss:  3.033995 (2.9585)  Time: 1.095s,  935.22/s  (1.109s,  923.13/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 850/1251 ( 68%)]  Loss:  2.909172 (2.9557)  Time: 1.099s,  931.67/s  (1.109s,  923.43/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 900/1251 ( 72%)]  Loss:  2.988362 (2.9574)  Time: 1.096s,  934.56/s  (1.109s,  923.11/s)  LR: 6.685e-05  Data: 0.012 (0.012)
Train: 231 [ 950/1251 ( 76%)]  Loss:  2.753091 (2.9472)  Time: 1.099s,  932.08/s  (1.109s,  923.00/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [1000/1251 ( 80%)]  Loss:  2.880584 (2.9441)  Time: 1.096s,  934.59/s  (1.109s,  923.17/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [1050/1251 ( 84%)]  Loss:  2.803649 (2.9377)  Time: 1.091s,  938.76/s  (1.109s,  923.37/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [1100/1251 ( 88%)]  Loss:  3.069399 (2.9434)  Time: 1.104s,  927.12/s  (1.109s,  923.62/s)  LR: 6.685e-05  Data: 0.014 (0.012)
Train: 231 [1150/1251 ( 92%)]  Loss:  3.111157 (2.9504)  Time: 1.096s,  934.15/s  (1.108s,  923.85/s)  LR: 6.685e-05  Data: 0.010 (0.012)
Train: 231 [1200/1251 ( 96%)]  Loss:  2.640244 (2.9380)  Time: 1.100s,  931.10/s  (1.108s,  923.96/s)  LR: 6.685e-05  Data: 0.011 (0.012)
Train: 231 [1250/1251 (100%)]  Loss:  2.995756 (2.9402)  Time: 1.080s,  948.14/s  (1.108s,  923.99/s)  LR: 6.685e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.246 (3.246)  Loss:  0.4146 (0.4146)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.5605 (0.8664)  Acc@1: 87.2641 (80.5560)  Acc@5: 97.6415 (95.2720)
Test (EMA): [   0/48]  Time: 3.070 (3.070)  Loss:  0.3946 (0.3946)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5410 (0.8292)  Acc@1: 87.3821 (81.0740)  Acc@5: 97.8774 (95.5620)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 81.008)

Train: 232 [   0/1251 (  0%)]  Loss:  3.013520 (3.0135)  Time: 1.131s,  905.77/s  (1.131s,  905.77/s)  LR: 6.514e-05  Data: 0.020 (0.020)
Train: 232 [  50/1251 (  4%)]  Loss:  2.793745 (2.9036)  Time: 1.099s,  931.83/s  (1.112s,  921.12/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 100/1251 (  8%)]  Loss:  2.808251 (2.8718)  Time: 1.107s,  924.99/s  (1.109s,  923.65/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 150/1251 ( 12%)]  Loss:  2.975992 (2.8979)  Time: 1.097s,  933.88/s  (1.106s,  925.59/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 200/1251 ( 16%)]  Loss:  3.082658 (2.9348)  Time: 1.191s,  859.76/s  (1.109s,  923.61/s)  LR: 6.514e-05  Data: 0.010 (0.012)
Train: 232 [ 250/1251 ( 20%)]  Loss:  2.743209 (2.9029)  Time: 1.094s,  935.73/s  (1.107s,  924.67/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 300/1251 ( 24%)]  Loss:  3.005246 (2.9175)  Time: 1.102s,  929.06/s  (1.107s,  924.92/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 350/1251 ( 28%)]  Loss:  2.949024 (2.9215)  Time: 1.137s,  900.84/s  (1.108s,  924.25/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 400/1251 ( 32%)]  Loss:  3.060464 (2.9369)  Time: 1.096s,  934.62/s  (1.107s,  924.72/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [ 450/1251 ( 36%)]  Loss:  2.828098 (2.9260)  Time: 1.095s,  934.77/s  (1.107s,  925.32/s)  LR: 6.514e-05  Data: 0.010 (0.012)
Train: 232 [ 500/1251 ( 40%)]  Loss:  2.943974 (2.9277)  Time: 1.101s,  929.80/s  (1.106s,  925.49/s)  LR: 6.514e-05  Data: 0.010 (0.012)
Train: 232 [ 550/1251 ( 44%)]  Loss:  2.760980 (2.9138)  Time: 1.142s,  896.84/s  (1.107s,  925.01/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [ 600/1251 ( 48%)]  Loss:  2.972913 (2.9183)  Time: 1.098s,  932.22/s  (1.107s,  924.75/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [ 650/1251 ( 52%)]  Loss:  3.037410 (2.9268)  Time: 1.095s,  935.38/s  (1.108s,  924.00/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 700/1251 ( 56%)]  Loss:  2.996564 (2.9315)  Time: 1.208s,  847.85/s  (1.108s,  924.05/s)  LR: 6.514e-05  Data: 0.010 (0.012)
Train: 232 [ 750/1251 ( 60%)]  Loss:  2.807719 (2.9237)  Time: 1.098s,  932.54/s  (1.108s,  924.04/s)  LR: 6.514e-05  Data: 0.013 (0.012)
Train: 232 [ 800/1251 ( 64%)]  Loss:  3.097435 (2.9340)  Time: 1.098s,  932.27/s  (1.108s,  924.44/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [ 850/1251 ( 68%)]  Loss:  2.944467 (2.9345)  Time: 1.096s,  934.36/s  (1.108s,  924.43/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 900/1251 ( 72%)]  Loss:  2.807169 (2.9278)  Time: 1.103s,  928.48/s  (1.108s,  924.53/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [ 950/1251 ( 76%)]  Loss:  2.885300 (2.9257)  Time: 1.102s,  929.13/s  (1.108s,  924.57/s)  LR: 6.514e-05  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 232 [1000/1251 ( 80%)]  Loss:  3.069430 (2.9326)  Time: 1.100s,  931.14/s  (1.107s,  924.76/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [1050/1251 ( 84%)]  Loss:  3.078703 (2.9392)  Time: 1.123s,  912.03/s  (1.107s,  924.83/s)  LR: 6.514e-05  Data: 0.011 (0.012)
Train: 232 [1100/1251 ( 88%)]  Loss:  3.054720 (2.9442)  Time: 1.092s,  937.31/s  (1.107s,  924.90/s)  LR: 6.514e-05  Data: 0.009 (0.012)
Train: 232 [1150/1251 ( 92%)]  Loss:  3.079504 (2.9499)  Time: 1.099s,  932.14/s  (1.107s,  924.63/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [1200/1251 ( 96%)]  Loss:  3.096601 (2.9557)  Time: 1.098s,  932.61/s  (1.108s,  924.60/s)  LR: 6.514e-05  Data: 0.012 (0.012)
Train: 232 [1250/1251 (100%)]  Loss:  2.933487 (2.9549)  Time: 1.082s,  946.19/s  (1.108s,  924.44/s)  LR: 6.514e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.283 (3.283)  Loss:  0.4020 (0.4020)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5614 (0.8669)  Acc@1: 87.2642 (80.3220)  Acc@5: 97.8774 (95.2320)
Test (EMA): [   0/48]  Time: 3.068 (3.068)  Loss:  0.3941 (0.3941)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5407 (0.8293)  Acc@1: 87.6179 (81.0860)  Acc@5: 97.9953 (95.5860)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 81.015999921875)

Train: 233 [   0/1251 (  0%)]  Loss:  2.682226 (2.6822)  Time: 1.133s,  904.03/s  (1.133s,  904.03/s)  LR: 6.346e-05  Data: 0.020 (0.020)
Train: 233 [  50/1251 (  4%)]  Loss:  2.727099 (2.7047)  Time: 1.100s,  930.84/s  (1.119s,  915.25/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [ 100/1251 (  8%)]  Loss:  2.914492 (2.7746)  Time: 1.199s,  853.69/s  (1.113s,  920.21/s)  LR: 6.346e-05  Data: 0.011 (0.012)
Train: 233 [ 150/1251 ( 12%)]  Loss:  3.074260 (2.8495)  Time: 1.099s,  932.15/s  (1.111s,  921.41/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [ 200/1251 ( 16%)]  Loss:  2.806732 (2.8410)  Time: 1.119s,  915.30/s  (1.110s,  922.13/s)  LR: 6.346e-05  Data: 0.010 (0.012)
Train: 233 [ 250/1251 ( 20%)]  Loss:  2.775936 (2.8301)  Time: 1.098s,  932.26/s  (1.109s,  923.05/s)  LR: 6.346e-05  Data: 0.013 (0.012)
Train: 233 [ 300/1251 ( 24%)]  Loss:  2.810882 (2.8274)  Time: 1.096s,  934.27/s  (1.108s,  924.30/s)  LR: 6.346e-05  Data: 0.013 (0.012)
Train: 233 [ 350/1251 ( 28%)]  Loss:  2.919541 (2.8389)  Time: 1.096s,  934.09/s  (1.108s,  923.81/s)  LR: 6.346e-05  Data: 0.013 (0.012)
Train: 233 [ 400/1251 ( 32%)]  Loss:  3.175918 (2.8763)  Time: 1.104s,  927.69/s  (1.109s,  923.64/s)  LR: 6.346e-05  Data: 0.010 (0.012)
Train: 233 [ 450/1251 ( 36%)]  Loss:  2.999408 (2.8886)  Time: 1.098s,  932.87/s  (1.109s,  923.42/s)  LR: 6.346e-05  Data: 0.011 (0.012)
Train: 233 [ 500/1251 ( 40%)]  Loss:  3.007715 (2.8995)  Time: 1.102s,  929.20/s  (1.109s,  923.37/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [ 550/1251 ( 44%)]  Loss:  2.970512 (2.9054)  Time: 1.098s,  932.87/s  (1.109s,  923.13/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [ 600/1251 ( 48%)]  Loss:  3.126583 (2.9224)  Time: 1.129s,  907.26/s  (1.110s,  922.54/s)  LR: 6.346e-05  Data: 0.011 (0.012)
Train: 233 [ 650/1251 ( 52%)]  Loss:  3.296292 (2.9491)  Time: 1.095s,  935.41/s  (1.110s,  922.53/s)  LR: 6.346e-05  Data: 0.011 (0.012)
Train: 233 [ 700/1251 ( 56%)]  Loss:  2.851174 (2.9426)  Time: 1.104s,  927.13/s  (1.110s,  922.49/s)  LR: 6.346e-05  Data: 0.011 (0.012)
Train: 233 [ 750/1251 ( 60%)]  Loss:  2.915308 (2.9409)  Time: 1.097s,  933.62/s  (1.110s,  922.88/s)  LR: 6.346e-05  Data: 0.013 (0.012)
Train: 233 [ 800/1251 ( 64%)]  Loss:  2.560697 (2.9185)  Time: 1.122s,  912.70/s  (1.110s,  922.85/s)  LR: 6.346e-05  Data: 0.010 (0.012)
Train: 233 [ 850/1251 ( 68%)]  Loss:  2.722433 (2.9076)  Time: 1.096s,  934.32/s  (1.109s,  923.14/s)  LR: 6.346e-05  Data: 0.013 (0.012)
Train: 233 [ 900/1251 ( 72%)]  Loss:  2.972934 (2.9111)  Time: 1.098s,  932.31/s  (1.110s,  922.79/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [ 950/1251 ( 76%)]  Loss:  2.834973 (2.9073)  Time: 1.130s,  906.02/s  (1.109s,  922.96/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [1000/1251 ( 80%)]  Loss:  2.541212 (2.8898)  Time: 1.194s,  857.47/s  (1.110s,  922.79/s)  LR: 6.346e-05  Data: 0.010 (0.012)
Train: 233 [1050/1251 ( 84%)]  Loss:  3.050144 (2.8971)  Time: 1.129s,  907.15/s  (1.111s,  921.95/s)  LR: 6.346e-05  Data: 0.011 (0.012)
Train: 233 [1100/1251 ( 88%)]  Loss:  2.528353 (2.8811)  Time: 1.102s,  929.01/s  (1.111s,  921.88/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [1150/1251 ( 92%)]  Loss:  2.825397 (2.8788)  Time: 1.098s,  932.70/s  (1.111s,  921.95/s)  LR: 6.346e-05  Data: 0.012 (0.012)
Train: 233 [1200/1251 ( 96%)]  Loss:  3.015496 (2.8842)  Time: 1.180s,  868.15/s  (1.110s,  922.14/s)  LR: 6.346e-05  Data: 0.010 (0.012)
Train: 233 [1250/1251 (100%)]  Loss:  2.981217 (2.8880)  Time: 1.083s,  945.85/s  (1.111s,  921.84/s)  LR: 6.346e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.218 (3.218)  Loss:  0.4129 (0.4129)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.403)  Loss:  0.5782 (0.8749)  Acc@1: 86.6745 (80.3160)  Acc@5: 97.8774 (95.2400)
Test (EMA): [   0/48]  Time: 3.330 (3.330)  Loss:  0.3938 (0.3938)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.7305 (98.7305)
Test (EMA): [  48/48]  Time: 0.230 (0.409)  Loss:  0.5412 (0.8294)  Acc@1: 87.5000 (81.1200)  Acc@5: 97.8774 (95.5900)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 81.01999994873047)

Train: 234 [   0/1251 (  0%)]  Loss:  3.022729 (3.0227)  Time: 1.104s,  927.31/s  (1.104s,  927.31/s)  LR: 6.180e-05  Data: 0.022 (0.022)
Train: 234 [  50/1251 (  4%)]  Loss:  3.088402 (3.0556)  Time: 1.213s,  844.31/s  (1.104s,  927.53/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [ 100/1251 (  8%)]  Loss:  2.726193 (2.9458)  Time: 1.126s,  909.08/s  (1.106s,  925.77/s)  LR: 6.180e-05  Data: 0.017 (0.012)
Train: 234 [ 150/1251 ( 12%)]  Loss:  2.966234 (2.9509)  Time: 1.117s,  916.75/s  (1.111s,  921.84/s)  LR: 6.180e-05  Data: 0.017 (0.012)
Train: 234 [ 200/1251 ( 16%)]  Loss:  3.032155 (2.9671)  Time: 1.121s,  913.39/s  (1.110s,  922.16/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [ 250/1251 ( 20%)]  Loss:  2.586655 (2.9037)  Time: 1.096s,  934.19/s  (1.110s,  922.77/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [ 300/1251 ( 24%)]  Loss:  2.956244 (2.9112)  Time: 1.100s,  930.88/s  (1.109s,  923.18/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [ 350/1251 ( 28%)]  Loss:  2.802278 (2.8976)  Time: 1.098s,  933.02/s  (1.109s,  923.37/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [ 400/1251 ( 32%)]  Loss:  3.103672 (2.9205)  Time: 1.097s,  933.29/s  (1.108s,  924.23/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [ 450/1251 ( 36%)]  Loss:  2.930962 (2.9216)  Time: 1.104s,  927.40/s  (1.108s,  924.33/s)  LR: 6.180e-05  Data: 0.013 (0.012)
Train: 234 [ 500/1251 ( 40%)]  Loss:  3.086659 (2.9366)  Time: 1.096s,  934.25/s  (1.108s,  924.53/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 234 [ 550/1251 ( 44%)]  Loss:  2.763169 (2.9221)  Time: 1.099s,  931.37/s  (1.107s,  924.84/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [ 600/1251 ( 48%)]  Loss:  2.747306 (2.9087)  Time: 1.107s,  925.35/s  (1.107s,  925.14/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [ 650/1251 ( 52%)]  Loss:  3.072010 (2.9203)  Time: 1.103s,  928.23/s  (1.107s,  925.00/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [ 700/1251 ( 56%)]  Loss:  2.889257 (2.9183)  Time: 1.100s,  930.87/s  (1.107s,  924.92/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [ 750/1251 ( 60%)]  Loss:  2.908665 (2.9177)  Time: 1.098s,  932.67/s  (1.108s,  924.59/s)  LR: 6.180e-05  Data: 0.010 (0.012)
Train: 234 [ 800/1251 ( 64%)]  Loss:  2.967825 (2.9206)  Time: 1.096s,  934.02/s  (1.107s,  924.75/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [ 850/1251 ( 68%)]  Loss:  2.600677 (2.9028)  Time: 1.100s,  930.79/s  (1.107s,  924.93/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [ 900/1251 ( 72%)]  Loss:  2.865368 (2.9009)  Time: 1.101s,  930.03/s  (1.107s,  924.61/s)  LR: 6.180e-05  Data: 0.010 (0.012)
Train: 234 [ 950/1251 ( 76%)]  Loss:  2.951207 (2.9034)  Time: 1.196s,  855.85/s  (1.108s,  924.43/s)  LR: 6.180e-05  Data: 0.013 (0.012)
Train: 234 [1000/1251 ( 80%)]  Loss:  3.019408 (2.9089)  Time: 1.094s,  935.70/s  (1.108s,  924.12/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [1050/1251 ( 84%)]  Loss:  2.786324 (2.9033)  Time: 1.120s,  914.06/s  (1.108s,  924.20/s)  LR: 6.180e-05  Data: 0.012 (0.012)
Train: 234 [1100/1251 ( 88%)]  Loss:  2.781903 (2.8981)  Time: 1.094s,  935.65/s  (1.108s,  924.05/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [1150/1251 ( 92%)]  Loss:  2.735305 (2.8913)  Time: 1.097s,  933.66/s  (1.108s,  923.92/s)  LR: 6.180e-05  Data: 0.011 (0.012)
Train: 234 [1200/1251 ( 96%)]  Loss:  2.913324 (2.8922)  Time: 1.094s,  936.29/s  (1.108s,  923.90/s)  LR: 6.180e-05  Data: 0.010 (0.012)
Train: 234 [1250/1251 (100%)]  Loss:  2.680796 (2.8840)  Time: 1.091s,  938.31/s  (1.108s,  924.03/s)  LR: 6.180e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.293 (3.293)  Loss:  0.4104 (0.4104)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.5652 (0.8661)  Acc@1: 86.5566 (80.4880)  Acc@5: 97.6415 (95.3060)
Test (EMA): [   0/48]  Time: 3.242 (3.242)  Loss:  0.3935 (0.3935)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.405)  Loss:  0.5414 (0.8295)  Acc@1: 87.3821 (81.1720)  Acc@5: 97.8774 (95.6000)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 81.02199997558594)

Train: 235 [   0/1251 (  0%)]  Loss:  2.860889 (2.8609)  Time: 1.106s,  925.75/s  (1.106s,  925.75/s)  LR: 6.016e-05  Data: 0.024 (0.024)
Train: 235 [  50/1251 (  4%)]  Loss:  3.085841 (2.9734)  Time: 1.100s,  930.84/s  (1.112s,  920.48/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 100/1251 (  8%)]  Loss:  3.088184 (3.0116)  Time: 1.096s,  934.33/s  (1.108s,  924.12/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 150/1251 ( 12%)]  Loss:  2.693818 (2.9322)  Time: 1.098s,  932.19/s  (1.108s,  924.15/s)  LR: 6.016e-05  Data: 0.014 (0.012)
Train: 235 [ 200/1251 ( 16%)]  Loss:  2.754381 (2.8966)  Time: 1.120s,  914.21/s  (1.108s,  923.87/s)  LR: 6.016e-05  Data: 0.013 (0.012)
Train: 235 [ 250/1251 ( 20%)]  Loss:  2.608650 (2.8486)  Time: 1.102s,  928.98/s  (1.110s,  922.21/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [ 300/1251 ( 24%)]  Loss:  3.014394 (2.8723)  Time: 1.107s,  924.89/s  (1.110s,  922.69/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 350/1251 ( 28%)]  Loss:  3.119992 (2.9033)  Time: 1.132s,  904.75/s  (1.111s,  921.98/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [ 400/1251 ( 32%)]  Loss:  2.570132 (2.8663)  Time: 1.099s,  931.36/s  (1.111s,  921.72/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [ 450/1251 ( 36%)]  Loss:  3.223034 (2.9019)  Time: 1.104s,  927.72/s  (1.110s,  922.42/s)  LR: 6.016e-05  Data: 0.010 (0.012)
Train: 235 [ 500/1251 ( 40%)]  Loss:  2.916349 (2.9032)  Time: 1.095s,  935.26/s  (1.109s,  923.02/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 550/1251 ( 44%)]  Loss:  2.945200 (2.9067)  Time: 1.099s,  932.13/s  (1.109s,  923.60/s)  LR: 6.016e-05  Data: 0.013 (0.012)
Train: 235 [ 600/1251 ( 48%)]  Loss:  3.090815 (2.9209)  Time: 1.099s,  932.17/s  (1.109s,  923.49/s)  LR: 6.016e-05  Data: 0.013 (0.012)
Train: 235 [ 650/1251 ( 52%)]  Loss:  2.981664 (2.9252)  Time: 1.128s,  907.79/s  (1.109s,  923.67/s)  LR: 6.016e-05  Data: 0.010 (0.012)
Train: 235 [ 700/1251 ( 56%)]  Loss:  3.033761 (2.9325)  Time: 1.122s,  912.63/s  (1.109s,  923.37/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 750/1251 ( 60%)]  Loss:  2.948177 (2.9335)  Time: 1.097s,  933.56/s  (1.109s,  923.63/s)  LR: 6.016e-05  Data: 0.013 (0.012)
Train: 235 [ 800/1251 ( 64%)]  Loss:  2.813097 (2.9264)  Time: 1.094s,  935.88/s  (1.109s,  923.40/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [ 850/1251 ( 68%)]  Loss:  2.834145 (2.9213)  Time: 1.097s,  933.20/s  (1.109s,  923.65/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 900/1251 ( 72%)]  Loss:  3.038123 (2.9274)  Time: 1.095s,  935.26/s  (1.109s,  923.49/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [ 950/1251 ( 76%)]  Loss:  2.690449 (2.9156)  Time: 1.132s,  904.46/s  (1.109s,  923.03/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [1000/1251 ( 80%)]  Loss:  2.841214 (2.9120)  Time: 1.096s,  934.29/s  (1.109s,  923.30/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [1050/1251 ( 84%)]  Loss:  3.177588 (2.9241)  Time: 1.191s,  859.55/s  (1.109s,  923.37/s)  LR: 6.016e-05  Data: 0.011 (0.012)
Train: 235 [1100/1251 ( 88%)]  Loss:  3.014761 (2.9280)  Time: 1.123s,  911.87/s  (1.109s,  923.61/s)  LR: 6.016e-05  Data: 0.010 (0.012)
Train: 235 [1150/1251 ( 92%)]  Loss:  3.142894 (2.9370)  Time: 1.137s,  900.48/s  (1.109s,  923.61/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [1200/1251 ( 96%)]  Loss:  2.750687 (2.9295)  Time: 1.100s,  930.67/s  (1.108s,  923.85/s)  LR: 6.016e-05  Data: 0.012 (0.012)
Train: 235 [1250/1251 (100%)]  Loss:  2.740986 (2.9223)  Time: 1.081s,  947.27/s  (1.109s,  923.59/s)  LR: 6.016e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.196 (3.196)  Loss:  0.4134 (0.4134)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.5651 (0.8642)  Acc@1: 86.9104 (80.4500)  Acc@5: 97.6415 (95.3120)
Test (EMA): [   0/48]  Time: 3.092 (3.092)  Loss:  0.3937 (0.3937)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5422 (0.8297)  Acc@1: 87.6179 (81.1780)  Acc@5: 97.8774 (95.5980)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 81.02799989746094)

Train: 236 [   0/1251 (  0%)]  Loss:  3.175036 (3.1750)  Time: 1.106s,  926.02/s  (1.106s,  926.02/s)  LR: 5.854e-05  Data: 0.022 (0.022)
Train: 236 [  50/1251 (  4%)]  Loss:  2.863183 (3.0191)  Time: 1.132s,  904.94/s  (1.111s,  921.41/s)  LR: 5.854e-05  Data: 0.012 (0.012)
Train: 236 [ 100/1251 (  8%)]  Loss:  2.930359 (2.9895)  Time: 1.098s,  932.77/s  (1.110s,  922.29/s)  LR: 5.854e-05  Data: 0.016 (0.012)
Train: 236 [ 150/1251 ( 12%)]  Loss:  2.888720 (2.9643)  Time: 1.101s,  930.06/s  (1.108s,  924.41/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 200/1251 ( 16%)]  Loss:  2.851953 (2.9419)  Time: 1.097s,  933.22/s  (1.108s,  924.59/s)  LR: 5.854e-05  Data: 0.013 (0.012)
Train: 236 [ 250/1251 ( 20%)]  Loss:  3.010335 (2.9533)  Time: 1.097s,  933.13/s  (1.107s,  925.32/s)  LR: 5.854e-05  Data: 0.013 (0.012)
Train: 236 [ 300/1251 ( 24%)]  Loss:  2.980369 (2.9571)  Time: 1.104s,  927.12/s  (1.107s,  924.73/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 350/1251 ( 28%)]  Loss:  2.938571 (2.9548)  Time: 1.095s,  935.08/s  (1.108s,  924.27/s)  LR: 5.854e-05  Data: 0.010 (0.012)
Train: 236 [ 400/1251 ( 32%)]  Loss:  2.806202 (2.9383)  Time: 1.098s,  932.83/s  (1.107s,  924.97/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 450/1251 ( 36%)]  Loss:  2.822105 (2.9267)  Time: 1.096s,  934.65/s  (1.107s,  924.67/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 500/1251 ( 40%)]  Loss:  3.030523 (2.9361)  Time: 1.186s,  863.75/s  (1.107s,  925.24/s)  LR: 5.854e-05  Data: 0.012 (0.012)
Train: 236 [ 550/1251 ( 44%)]  Loss:  3.089573 (2.9489)  Time: 1.175s,  871.17/s  (1.107s,  925.20/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 600/1251 ( 48%)]  Loss:  2.966892 (2.9503)  Time: 1.097s,  933.20/s  (1.106s,  925.85/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 650/1251 ( 52%)]  Loss:  2.982041 (2.9526)  Time: 1.124s,  911.31/s  (1.107s,  924.97/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 700/1251 ( 56%)]  Loss:  2.948079 (2.9523)  Time: 1.123s,  911.63/s  (1.107s,  924.79/s)  LR: 5.854e-05  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 236 [ 750/1251 ( 60%)]  Loss:  2.978318 (2.9539)  Time: 1.105s,  927.10/s  (1.108s,  924.39/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 800/1251 ( 64%)]  Loss:  2.814591 (2.9457)  Time: 1.108s,  924.56/s  (1.108s,  924.31/s)  LR: 5.854e-05  Data: 0.010 (0.012)
Train: 236 [ 850/1251 ( 68%)]  Loss:  2.841524 (2.9399)  Time: 1.097s,  933.32/s  (1.108s,  924.25/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [ 900/1251 ( 72%)]  Loss:  2.783756 (2.9317)  Time: 1.098s,  932.47/s  (1.108s,  924.21/s)  LR: 5.854e-05  Data: 0.012 (0.012)
Train: 236 [ 950/1251 ( 76%)]  Loss:  2.897629 (2.9300)  Time: 1.095s,  935.46/s  (1.108s,  924.33/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [1000/1251 ( 80%)]  Loss:  2.872205 (2.9272)  Time: 1.098s,  932.47/s  (1.108s,  924.28/s)  LR: 5.854e-05  Data: 0.012 (0.012)
Train: 236 [1050/1251 ( 84%)]  Loss:  2.831034 (2.9229)  Time: 1.096s,  933.93/s  (1.108s,  924.39/s)  LR: 5.854e-05  Data: 0.012 (0.012)
Train: 236 [1100/1251 ( 88%)]  Loss:  3.066622 (2.9291)  Time: 1.098s,  932.96/s  (1.108s,  924.41/s)  LR: 5.854e-05  Data: 0.013 (0.012)
Train: 236 [1150/1251 ( 92%)]  Loss:  2.990312 (2.9317)  Time: 1.096s,  934.09/s  (1.107s,  924.74/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [1200/1251 ( 96%)]  Loss:  2.973439 (2.9333)  Time: 1.102s,  929.52/s  (1.108s,  924.49/s)  LR: 5.854e-05  Data: 0.011 (0.012)
Train: 236 [1250/1251 (100%)]  Loss:  2.700999 (2.9244)  Time: 1.103s,  928.74/s  (1.108s,  924.39/s)  LR: 5.854e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.263 (3.263)  Loss:  0.4192 (0.4192)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.5756 (0.8691)  Acc@1: 87.2642 (80.4440)  Acc@5: 97.6415 (95.2100)
Test (EMA): [   0/48]  Time: 3.153 (3.153)  Loss:  0.3936 (0.3936)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.402)  Loss:  0.5422 (0.8297)  Acc@1: 87.5000 (81.1720)  Acc@5: 97.8774 (95.6220)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 81.02799994873047)

Train: 237 [   0/1251 (  0%)]  Loss:  2.828251 (2.8283)  Time: 1.108s,  924.22/s  (1.108s,  924.22/s)  LR: 5.694e-05  Data: 0.021 (0.021)
Train: 237 [  50/1251 (  4%)]  Loss:  3.147468 (2.9879)  Time: 1.096s,  934.23/s  (1.109s,  923.49/s)  LR: 5.694e-05  Data: 0.011 (0.012)
Train: 237 [ 100/1251 (  8%)]  Loss:  2.816653 (2.9308)  Time: 1.097s,  933.34/s  (1.107s,  925.34/s)  LR: 5.694e-05  Data: 0.011 (0.012)
Train: 237 [ 150/1251 ( 12%)]  Loss:  3.114988 (2.9768)  Time: 1.100s,  930.91/s  (1.110s,  922.24/s)  LR: 5.694e-05  Data: 0.012 (0.012)
Train: 237 [ 200/1251 ( 16%)]  Loss:  2.692323 (2.9199)  Time: 1.097s,  933.43/s  (1.108s,  924.37/s)  LR: 5.694e-05  Data: 0.012 (0.012)
Train: 237 [ 250/1251 ( 20%)]  Loss:  2.970275 (2.9283)  Time: 1.100s,  930.65/s  (1.108s,  924.37/s)  LR: 5.694e-05  Data: 0.013 (0.012)
Train: 237 [ 300/1251 ( 24%)]  Loss:  3.133508 (2.9576)  Time: 1.102s,  929.47/s  (1.108s,  924.23/s)  LR: 5.694e-05  Data: 0.010 (0.012)
Train: 237 [ 350/1251 ( 28%)]  Loss:  2.873997 (2.9472)  Time: 1.091s,  938.82/s  (1.108s,  924.43/s)  LR: 5.694e-05  Data: 0.010 (0.012)
Train: 237 [ 400/1251 ( 32%)]  Loss:  2.882452 (2.9400)  Time: 1.134s,  902.70/s  (1.108s,  924.01/s)  LR: 5.694e-05  Data: 0.011 (0.012)
Train: 237 [ 450/1251 ( 36%)]  Loss:  2.799466 (2.9259)  Time: 1.096s,  934.58/s  (1.110s,  922.60/s)  LR: 5.694e-05  Data: 0.012 (0.012)
Train: 237 [ 500/1251 ( 40%)]  Loss:  3.115905 (2.9432)  Time: 1.128s,  907.78/s  (1.110s,  922.79/s)  LR: 5.694e-05  Data: 0.010 (0.012)
Train: 237 [ 550/1251 ( 44%)]  Loss:  2.914108 (2.9408)  Time: 1.095s,  934.86/s  (1.109s,  923.32/s)  LR: 5.694e-05  Data: 0.012 (0.012)
Train: 237 [ 600/1251 ( 48%)]  Loss:  3.009051 (2.9460)  Time: 1.098s,  932.62/s  (1.109s,  923.00/s)  LR: 5.694e-05  Data: 0.012 (0.012)
Train: 237 [ 650/1251 ( 52%)]  Loss:  3.022058 (2.9515)  Time: 1.097s,  933.69/s  (1.110s,  922.87/s)  LR: 5.694e-05  Data: 0.013 (0.012)
Train: 237 [ 700/1251 ( 56%)]  Loss:  2.952447 (2.9515)  Time: 1.096s,  934.24/s  (1.109s,  922.99/s)  LR: 5.694e-05  Data: 0.011 (0.011)
Train: 237 [ 750/1251 ( 60%)]  Loss:  3.125769 (2.9624)  Time: 1.095s,  934.89/s  (1.109s,  923.13/s)  LR: 5.694e-05  Data: 0.012 (0.011)
Train: 237 [ 800/1251 ( 64%)]  Loss:  2.849511 (2.9558)  Time: 1.095s,  935.54/s  (1.109s,  923.40/s)  LR: 5.694e-05  Data: 0.011 (0.011)
Train: 237 [ 850/1251 ( 68%)]  Loss:  2.892439 (2.9523)  Time: 1.124s,  910.88/s  (1.109s,  923.52/s)  LR: 5.694e-05  Data: 0.011 (0.011)
Train: 237 [ 900/1251 ( 72%)]  Loss:  2.855559 (2.9472)  Time: 1.123s,  912.17/s  (1.109s,  923.41/s)  LR: 5.694e-05  Data: 0.010 (0.011)
Train: 237 [ 950/1251 ( 76%)]  Loss:  3.025999 (2.9511)  Time: 1.095s,  934.85/s  (1.109s,  922.96/s)  LR: 5.694e-05  Data: 0.012 (0.011)
Train: 237 [1000/1251 ( 80%)]  Loss:  2.840388 (2.9458)  Time: 1.135s,  901.97/s  (1.109s,  923.07/s)  LR: 5.694e-05  Data: 0.012 (0.011)
Train: 237 [1050/1251 ( 84%)]  Loss:  2.858115 (2.9419)  Time: 1.093s,  936.63/s  (1.110s,  922.76/s)  LR: 5.694e-05  Data: 0.010 (0.011)
Train: 237 [1100/1251 ( 88%)]  Loss:  2.768167 (2.9343)  Time: 1.211s,  845.85/s  (1.110s,  922.81/s)  LR: 5.694e-05  Data: 0.010 (0.011)
Train: 237 [1150/1251 ( 92%)]  Loss:  3.068385 (2.9399)  Time: 1.095s,  934.87/s  (1.110s,  922.84/s)  LR: 5.694e-05  Data: 0.012 (0.011)
Train: 237 [1200/1251 ( 96%)]  Loss:  2.866872 (2.9370)  Time: 1.097s,  933.18/s  (1.109s,  923.10/s)  LR: 5.694e-05  Data: 0.011 (0.011)
Train: 237 [1250/1251 (100%)]  Loss:  2.799118 (2.9317)  Time: 1.079s,  949.15/s  (1.109s,  923.11/s)  LR: 5.694e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.252 (3.252)  Loss:  0.4159 (0.4159)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.5650 (0.8651)  Acc@1: 87.2642 (80.5200)  Acc@5: 97.8774 (95.2980)
Test (EMA): [   0/48]  Time: 3.228 (3.228)  Loss:  0.3939 (0.3939)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.415)  Loss:  0.5426 (0.8299)  Acc@1: 87.5000 (81.1940)  Acc@5: 97.8774 (95.6220)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 81.03200002685547)

Train: 238 [   0/1251 (  0%)]  Loss:  2.938355 (2.9384)  Time: 1.100s,  930.66/s  (1.100s,  930.66/s)  LR: 5.536e-05  Data: 0.019 (0.019)
Train: 238 [  50/1251 (  4%)]  Loss:  2.962753 (2.9506)  Time: 1.098s,  932.22/s  (1.101s,  930.07/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 100/1251 (  8%)]  Loss:  2.903640 (2.9349)  Time: 1.094s,  936.14/s  (1.105s,  927.12/s)  LR: 5.536e-05  Data: 0.010 (0.012)
Train: 238 [ 150/1251 ( 12%)]  Loss:  2.823090 (2.9070)  Time: 1.132s,  904.52/s  (1.105s,  927.05/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 200/1251 ( 16%)]  Loss:  2.996194 (2.9248)  Time: 1.094s,  935.73/s  (1.105s,  926.83/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [ 250/1251 ( 20%)]  Loss:  2.988922 (2.9355)  Time: 1.099s,  931.70/s  (1.105s,  926.35/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 300/1251 ( 24%)]  Loss:  2.795552 (2.9155)  Time: 1.096s,  934.11/s  (1.106s,  926.06/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 350/1251 ( 28%)]  Loss:  3.080784 (2.9362)  Time: 1.098s,  932.25/s  (1.106s,  926.27/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [ 400/1251 ( 32%)]  Loss:  3.207124 (2.9663)  Time: 1.097s,  933.77/s  (1.105s,  926.32/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 450/1251 ( 36%)]  Loss:  3.126633 (2.9823)  Time: 1.102s,  929.42/s  (1.106s,  925.91/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [ 500/1251 ( 40%)]  Loss:  3.069685 (2.9902)  Time: 1.104s,  927.30/s  (1.106s,  925.94/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 238 [ 550/1251 ( 44%)]  Loss:  3.052109 (2.9954)  Time: 1.101s,  930.38/s  (1.106s,  926.00/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 600/1251 ( 48%)]  Loss:  2.848476 (2.9841)  Time: 1.143s,  895.76/s  (1.107s,  925.07/s)  LR: 5.536e-05  Data: 0.010 (0.012)
Train: 238 [ 650/1251 ( 52%)]  Loss:  3.009667 (2.9859)  Time: 1.122s,  912.32/s  (1.108s,  924.45/s)  LR: 5.536e-05  Data: 0.009 (0.012)
Train: 238 [ 700/1251 ( 56%)]  Loss:  3.077657 (2.9920)  Time: 1.125s,  909.91/s  (1.107s,  924.68/s)  LR: 5.536e-05  Data: 0.014 (0.012)
Train: 238 [ 750/1251 ( 60%)]  Loss:  3.112346 (2.9996)  Time: 1.099s,  931.95/s  (1.108s,  924.60/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [ 800/1251 ( 64%)]  Loss:  3.032314 (3.0015)  Time: 1.095s,  935.47/s  (1.107s,  924.69/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [ 850/1251 ( 68%)]  Loss:  3.079888 (3.0058)  Time: 1.100s,  931.06/s  (1.107s,  924.91/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [ 900/1251 ( 72%)]  Loss:  3.010421 (3.0061)  Time: 1.134s,  902.91/s  (1.107s,  924.70/s)  LR: 5.536e-05  Data: 0.010 (0.012)
Train: 238 [ 950/1251 ( 76%)]  Loss:  2.785248 (2.9950)  Time: 1.101s,  930.26/s  (1.108s,  924.45/s)  LR: 5.536e-05  Data: 0.013 (0.012)
Train: 238 [1000/1251 ( 80%)]  Loss:  2.914974 (2.9912)  Time: 1.103s,  928.27/s  (1.108s,  924.50/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [1050/1251 ( 84%)]  Loss:  2.580602 (2.9726)  Time: 1.098s,  932.91/s  (1.107s,  924.84/s)  LR: 5.536e-05  Data: 0.011 (0.012)
Train: 238 [1100/1251 ( 88%)]  Loss:  2.966501 (2.9723)  Time: 1.097s,  933.85/s  (1.107s,  924.96/s)  LR: 5.536e-05  Data: 0.013 (0.012)
Train: 238 [1150/1251 ( 92%)]  Loss:  2.719920 (2.9618)  Time: 1.095s,  934.79/s  (1.107s,  925.10/s)  LR: 5.536e-05  Data: 0.012 (0.012)
Train: 238 [1200/1251 ( 96%)]  Loss:  3.065954 (2.9660)  Time: 1.102s,  929.29/s  (1.107s,  925.17/s)  LR: 5.536e-05  Data: 0.010 (0.012)
Train: 238 [1250/1251 (100%)]  Loss:  2.873726 (2.9624)  Time: 1.081s,  947.22/s  (1.107s,  925.15/s)  LR: 5.536e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.288 (3.288)  Loss:  0.4243 (0.4243)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.410)  Loss:  0.5594 (0.8697)  Acc@1: 87.1462 (80.5720)  Acc@5: 97.7594 (95.1840)
Test (EMA): [   0/48]  Time: 3.182 (3.182)  Loss:  0.3940 (0.3940)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5426 (0.8301)  Acc@1: 87.2641 (81.1880)  Acc@5: 97.9953 (95.6100)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 81.056)

Train: 239 [   0/1251 (  0%)]  Loss:  2.948202 (2.9482)  Time: 1.103s,  928.07/s  (1.103s,  928.07/s)  LR: 5.380e-05  Data: 0.022 (0.022)
Train: 239 [  50/1251 (  4%)]  Loss:  2.999603 (2.9739)  Time: 1.096s,  934.53/s  (1.103s,  928.39/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 100/1251 (  8%)]  Loss:  2.584183 (2.8440)  Time: 1.096s,  934.16/s  (1.109s,  923.13/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 150/1251 ( 12%)]  Loss:  2.997644 (2.8824)  Time: 1.194s,  857.67/s  (1.108s,  924.51/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 200/1251 ( 16%)]  Loss:  2.966474 (2.8992)  Time: 1.097s,  933.51/s  (1.107s,  924.92/s)  LR: 5.380e-05  Data: 0.014 (0.012)
Train: 239 [ 250/1251 ( 20%)]  Loss:  2.734213 (2.8717)  Time: 1.194s,  857.91/s  (1.107s,  924.95/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 300/1251 ( 24%)]  Loss:  2.931836 (2.8803)  Time: 1.096s,  934.33/s  (1.108s,  924.41/s)  LR: 5.380e-05  Data: 0.014 (0.012)
Train: 239 [ 350/1251 ( 28%)]  Loss:  2.995763 (2.8947)  Time: 1.100s,  931.25/s  (1.107s,  925.15/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 400/1251 ( 32%)]  Loss:  3.001124 (2.9066)  Time: 1.096s,  934.07/s  (1.108s,  924.44/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 450/1251 ( 36%)]  Loss:  2.989974 (2.9149)  Time: 1.095s,  935.25/s  (1.107s,  924.93/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 500/1251 ( 40%)]  Loss:  2.686390 (2.8941)  Time: 1.096s,  934.45/s  (1.107s,  924.91/s)  LR: 5.380e-05  Data: 0.014 (0.012)
Train: 239 [ 550/1251 ( 44%)]  Loss:  2.750031 (2.8821)  Time: 1.097s,  933.73/s  (1.107s,  925.18/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 600/1251 ( 48%)]  Loss:  2.776478 (2.8740)  Time: 1.103s,  928.14/s  (1.107s,  925.32/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 650/1251 ( 52%)]  Loss:  3.048165 (2.8864)  Time: 1.093s,  936.47/s  (1.107s,  925.42/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 700/1251 ( 56%)]  Loss:  2.647319 (2.8705)  Time: 1.124s,  911.11/s  (1.107s,  925.41/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 750/1251 ( 60%)]  Loss:  3.031761 (2.8806)  Time: 1.097s,  933.79/s  (1.107s,  925.21/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 800/1251 ( 64%)]  Loss:  2.845803 (2.8785)  Time: 1.095s,  934.76/s  (1.107s,  925.36/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [ 850/1251 ( 68%)]  Loss:  2.736405 (2.8706)  Time: 1.122s,  913.05/s  (1.107s,  924.79/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 900/1251 ( 72%)]  Loss:  2.789091 (2.8663)  Time: 1.099s,  931.78/s  (1.108s,  924.55/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [ 950/1251 ( 76%)]  Loss:  2.832665 (2.8647)  Time: 1.096s,  934.72/s  (1.108s,  924.55/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [1000/1251 ( 80%)]  Loss:  3.093178 (2.8755)  Time: 1.098s,  932.31/s  (1.107s,  924.77/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [1050/1251 ( 84%)]  Loss:  3.141824 (2.8876)  Time: 1.109s,  922.97/s  (1.107s,  924.73/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [1100/1251 ( 88%)]  Loss:  2.773845 (2.8827)  Time: 1.120s,  914.52/s  (1.107s,  924.62/s)  LR: 5.380e-05  Data: 0.012 (0.012)
Train: 239 [1150/1251 ( 92%)]  Loss:  2.835343 (2.8807)  Time: 1.104s,  927.95/s  (1.108s,  924.54/s)  LR: 5.380e-05  Data: 0.011 (0.012)
Train: 239 [1200/1251 ( 96%)]  Loss:  3.040995 (2.8871)  Time: 1.099s,  932.11/s  (1.108s,  924.60/s)  LR: 5.380e-05  Data: 0.013 (0.012)
Train: 239 [1250/1251 (100%)]  Loss:  2.817888 (2.8845)  Time: 1.103s,  928.78/s  (1.108s,  924.44/s)  LR: 5.380e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.190 (3.190)  Loss:  0.4233 (0.4233)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.5641 (0.8672)  Acc@1: 87.3821 (80.3720)  Acc@5: 97.8774 (95.2920)
Test (EMA): [   0/48]  Time: 3.090 (3.090)  Loss:  0.3941 (0.3941)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.5431 (0.8302)  Acc@1: 87.3821 (81.1920)  Acc@5: 97.9953 (95.6020)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 81.06200002685547)

Train: 240 [   0/1251 (  0%)]  Loss:  2.922174 (2.9222)  Time: 1.134s,  903.12/s  (1.134s,  903.12/s)  LR: 5.227e-05  Data: 0.024 (0.024)
Train: 240 [  50/1251 (  4%)]  Loss:  2.765864 (2.8440)  Time: 1.094s,  935.87/s  (1.104s,  927.76/s)  LR: 5.227e-05  Data: 0.010 (0.012)
Train: 240 [ 100/1251 (  8%)]  Loss:  3.292809 (2.9936)  Time: 1.101s,  929.93/s  (1.105s,  926.72/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 240 [ 150/1251 ( 12%)]  Loss:  2.539804 (2.8802)  Time: 1.096s,  933.91/s  (1.105s,  926.51/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [ 200/1251 ( 16%)]  Loss:  2.872048 (2.8785)  Time: 1.117s,  916.35/s  (1.108s,  924.21/s)  LR: 5.227e-05  Data: 0.010 (0.012)
Train: 240 [ 250/1251 ( 20%)]  Loss:  2.820052 (2.8688)  Time: 1.123s,  911.57/s  (1.110s,  922.54/s)  LR: 5.227e-05  Data: 0.013 (0.012)
Train: 240 [ 300/1251 ( 24%)]  Loss:  2.998558 (2.8873)  Time: 1.107s,  925.13/s  (1.108s,  923.79/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [ 350/1251 ( 28%)]  Loss:  2.755332 (2.8708)  Time: 1.107s,  925.33/s  (1.109s,  923.20/s)  LR: 5.227e-05  Data: 0.010 (0.012)
Train: 240 [ 400/1251 ( 32%)]  Loss:  2.948133 (2.8794)  Time: 1.097s,  933.16/s  (1.108s,  924.17/s)  LR: 5.227e-05  Data: 0.013 (0.012)
Train: 240 [ 450/1251 ( 36%)]  Loss:  3.067664 (2.8982)  Time: 1.095s,  935.04/s  (1.109s,  923.68/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [ 500/1251 ( 40%)]  Loss:  2.849594 (2.8938)  Time: 1.098s,  932.54/s  (1.109s,  923.26/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [ 550/1251 ( 44%)]  Loss:  3.114490 (2.9122)  Time: 1.102s,  929.47/s  (1.109s,  923.56/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [ 600/1251 ( 48%)]  Loss:  2.885157 (2.9101)  Time: 1.103s,  928.48/s  (1.109s,  923.69/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [ 650/1251 ( 52%)]  Loss:  2.899670 (2.9094)  Time: 1.095s,  935.41/s  (1.108s,  923.86/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [ 700/1251 ( 56%)]  Loss:  2.818978 (2.9034)  Time: 1.126s,  909.73/s  (1.109s,  923.65/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [ 750/1251 ( 60%)]  Loss:  2.937946 (2.9055)  Time: 1.121s,  913.42/s  (1.109s,  923.68/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [ 800/1251 ( 64%)]  Loss:  3.019558 (2.9122)  Time: 1.098s,  932.92/s  (1.108s,  923.84/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [ 850/1251 ( 68%)]  Loss:  3.052587 (2.9200)  Time: 1.109s,  923.07/s  (1.109s,  923.68/s)  LR: 5.227e-05  Data: 0.009 (0.012)
Train: 240 [ 900/1251 ( 72%)]  Loss:  2.877525 (2.9178)  Time: 1.099s,  932.07/s  (1.109s,  923.50/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [ 950/1251 ( 76%)]  Loss:  3.060365 (2.9249)  Time: 1.098s,  932.64/s  (1.109s,  923.43/s)  LR: 5.227e-05  Data: 0.013 (0.012)
Train: 240 [1000/1251 ( 80%)]  Loss:  2.931896 (2.9252)  Time: 1.098s,  932.22/s  (1.109s,  923.53/s)  LR: 5.227e-05  Data: 0.013 (0.012)
Train: 240 [1050/1251 ( 84%)]  Loss:  2.711563 (2.9155)  Time: 1.095s,  934.92/s  (1.109s,  923.65/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [1100/1251 ( 88%)]  Loss:  2.929468 (2.9161)  Time: 1.102s,  929.56/s  (1.109s,  923.66/s)  LR: 5.227e-05  Data: 0.011 (0.012)
Train: 240 [1150/1251 ( 92%)]  Loss:  3.105324 (2.9240)  Time: 1.134s,  902.98/s  (1.109s,  923.59/s)  LR: 5.227e-05  Data: 0.013 (0.012)
Train: 240 [1200/1251 ( 96%)]  Loss:  3.104645 (2.9312)  Time: 1.109s,  923.11/s  (1.109s,  923.53/s)  LR: 5.227e-05  Data: 0.012 (0.012)
Train: 240 [1250/1251 (100%)]  Loss:  2.973339 (2.9329)  Time: 1.106s,  926.15/s  (1.109s,  923.54/s)  LR: 5.227e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.302 (3.302)  Loss:  0.4144 (0.4144)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5662 (0.8658)  Acc@1: 87.8538 (80.5640)  Acc@5: 97.7594 (95.2700)
Test (EMA): [   0/48]  Time: 3.089 (3.089)  Loss:  0.3947 (0.3947)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5434 (0.8307)  Acc@1: 87.3821 (81.1580)  Acc@5: 97.9953 (95.5880)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 81.15799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 81.07399994873047)

Train: 241 [   0/1251 (  0%)]  Loss:  2.950515 (2.9505)  Time: 1.102s,  928.97/s  (1.102s,  928.97/s)  LR: 5.076e-05  Data: 0.020 (0.020)
Train: 241 [  50/1251 (  4%)]  Loss:  3.092816 (3.0217)  Time: 1.105s,  926.51/s  (1.106s,  925.86/s)  LR: 5.076e-05  Data: 0.012 (0.012)
Train: 241 [ 100/1251 (  8%)]  Loss:  2.988100 (3.0105)  Time: 1.121s,  913.07/s  (1.108s,  924.04/s)  LR: 5.076e-05  Data: 0.012 (0.012)
Train: 241 [ 150/1251 ( 12%)]  Loss:  3.190055 (3.0554)  Time: 1.191s,  860.02/s  (1.109s,  923.66/s)  LR: 5.076e-05  Data: 0.012 (0.012)
Train: 241 [ 200/1251 ( 16%)]  Loss:  2.973727 (3.0390)  Time: 1.123s,  911.85/s  (1.111s,  921.73/s)  LR: 5.076e-05  Data: 0.011 (0.012)
Train: 241 [ 250/1251 ( 20%)]  Loss:  2.954433 (3.0249)  Time: 1.100s,  930.91/s  (1.109s,  923.44/s)  LR: 5.076e-05  Data: 0.013 (0.012)
Train: 241 [ 300/1251 ( 24%)]  Loss:  3.168266 (3.0454)  Time: 1.121s,  913.77/s  (1.111s,  921.46/s)  LR: 5.076e-05  Data: 0.010 (0.012)
Train: 241 [ 350/1251 ( 28%)]  Loss:  2.896568 (3.0268)  Time: 1.095s,  935.33/s  (1.113s,  920.25/s)  LR: 5.076e-05  Data: 0.010 (0.011)
Train: 241 [ 400/1251 ( 32%)]  Loss:  3.277358 (3.0546)  Time: 1.096s,  933.90/s  (1.112s,  921.15/s)  LR: 5.076e-05  Data: 0.011 (0.011)
Train: 241 [ 450/1251 ( 36%)]  Loss:  2.693429 (3.0185)  Time: 1.096s,  934.33/s  (1.111s,  921.78/s)  LR: 5.076e-05  Data: 0.012 (0.011)
Train: 241 [ 500/1251 ( 40%)]  Loss:  2.968635 (3.0140)  Time: 1.091s,  938.47/s  (1.111s,  921.29/s)  LR: 5.076e-05  Data: 0.010 (0.011)
Train: 241 [ 550/1251 ( 44%)]  Loss:  2.824980 (2.9982)  Time: 1.097s,  933.56/s  (1.111s,  921.79/s)  LR: 5.076e-05  Data: 0.013 (0.011)
Train: 241 [ 600/1251 ( 48%)]  Loss:  2.708477 (2.9760)  Time: 1.105s,  926.80/s  (1.111s,  921.88/s)  LR: 5.076e-05  Data: 0.010 (0.011)
Train: 241 [ 650/1251 ( 52%)]  Loss:  2.767839 (2.9611)  Time: 1.096s,  934.70/s  (1.111s,  922.01/s)  LR: 5.076e-05  Data: 0.012 (0.011)
Train: 241 [ 700/1251 ( 56%)]  Loss:  3.155286 (2.9740)  Time: 1.102s,  929.28/s  (1.110s,  922.39/s)  LR: 5.076e-05  Data: 0.011 (0.011)
Train: 241 [ 750/1251 ( 60%)]  Loss:  2.910593 (2.9701)  Time: 1.097s,  933.16/s  (1.110s,  922.83/s)  LR: 5.076e-05  Data: 0.012 (0.011)
Train: 241 [ 800/1251 ( 64%)]  Loss:  3.142375 (2.9802)  Time: 1.123s,  912.03/s  (1.109s,  922.97/s)  LR: 5.076e-05  Data: 0.010 (0.011)
Train: 241 [ 850/1251 ( 68%)]  Loss:  3.057009 (2.9845)  Time: 1.192s,  858.85/s  (1.110s,  922.47/s)  LR: 5.076e-05  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 241 [ 900/1251 ( 72%)]  Loss:  2.938208 (2.9820)  Time: 1.099s,  931.89/s  (1.110s,  922.43/s)  LR: 5.076e-05  Data: 0.013 (0.011)
Train: 241 [ 950/1251 ( 76%)]  Loss:  2.972354 (2.9816)  Time: 1.190s,  860.46/s  (1.110s,  922.16/s)  LR: 5.076e-05  Data: 0.012 (0.011)
Train: 241 [1000/1251 ( 80%)]  Loss:  2.927176 (2.9790)  Time: 1.112s,  920.48/s  (1.110s,  922.37/s)  LR: 5.076e-05  Data: 0.010 (0.011)
Train: 241 [1050/1251 ( 84%)]  Loss:  2.764050 (2.9692)  Time: 1.102s,  929.40/s  (1.110s,  922.59/s)  LR: 5.076e-05  Data: 0.012 (0.011)
Train: 241 [1100/1251 ( 88%)]  Loss:  2.732116 (2.9589)  Time: 1.100s,  930.70/s  (1.110s,  922.83/s)  LR: 5.076e-05  Data: 0.012 (0.012)
Train: 241 [1150/1251 ( 92%)]  Loss:  2.914153 (2.9570)  Time: 1.099s,  932.05/s  (1.109s,  923.03/s)  LR: 5.076e-05  Data: 0.012 (0.012)
Train: 241 [1200/1251 ( 96%)]  Loss:  3.058150 (2.9611)  Time: 1.100s,  931.24/s  (1.109s,  923.14/s)  LR: 5.076e-05  Data: 0.012 (0.012)
Train: 241 [1250/1251 (100%)]  Loss:  2.508049 (2.9436)  Time: 1.080s,  948.17/s  (1.110s,  922.88/s)  LR: 5.076e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.213 (3.213)  Loss:  0.4303 (0.4303)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.402)  Loss:  0.5718 (0.8712)  Acc@1: 87.2641 (80.6360)  Acc@5: 97.7594 (95.3080)
Test (EMA): [   0/48]  Time: 3.334 (3.334)  Loss:  0.3949 (0.3949)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.5434 (0.8309)  Acc@1: 87.3821 (81.1860)  Acc@5: 97.9953 (95.5780)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 81.15799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 81.08600005126954)

Train: 242 [   0/1251 (  0%)]  Loss:  2.957985 (2.9580)  Time: 1.107s,  925.23/s  (1.107s,  925.23/s)  LR: 4.927e-05  Data: 0.024 (0.024)
Train: 242 [  50/1251 (  4%)]  Loss:  2.835555 (2.8968)  Time: 1.097s,  933.79/s  (1.107s,  925.28/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 100/1251 (  8%)]  Loss:  3.005795 (2.9331)  Time: 1.092s,  937.41/s  (1.108s,  923.87/s)  LR: 4.927e-05  Data: 0.010 (0.012)
Train: 242 [ 150/1251 ( 12%)]  Loss:  2.922925 (2.9306)  Time: 1.097s,  933.40/s  (1.109s,  923.20/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 200/1251 ( 16%)]  Loss:  3.071789 (2.9588)  Time: 1.093s,  936.75/s  (1.107s,  924.70/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 250/1251 ( 20%)]  Loss:  2.565896 (2.8933)  Time: 1.097s,  933.77/s  (1.108s,  924.27/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [ 300/1251 ( 24%)]  Loss:  2.736912 (2.8710)  Time: 1.118s,  915.66/s  (1.107s,  925.41/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 350/1251 ( 28%)]  Loss:  2.869839 (2.8708)  Time: 1.121s,  913.25/s  (1.107s,  924.98/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 400/1251 ( 32%)]  Loss:  2.713204 (2.8533)  Time: 1.119s,  915.06/s  (1.107s,  924.90/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 450/1251 ( 36%)]  Loss:  2.730074 (2.8410)  Time: 1.097s,  933.49/s  (1.107s,  924.66/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [ 500/1251 ( 40%)]  Loss:  2.718474 (2.8299)  Time: 1.099s,  931.55/s  (1.107s,  925.02/s)  LR: 4.927e-05  Data: 0.010 (0.012)
Train: 242 [ 550/1251 ( 44%)]  Loss:  3.052648 (2.8484)  Time: 1.099s,  931.82/s  (1.107s,  925.15/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 600/1251 ( 48%)]  Loss:  3.063643 (2.8650)  Time: 1.099s,  931.45/s  (1.107s,  924.73/s)  LR: 4.927e-05  Data: 0.013 (0.012)
Train: 242 [ 650/1251 ( 52%)]  Loss:  2.999975 (2.8746)  Time: 1.101s,  930.07/s  (1.108s,  924.00/s)  LR: 4.927e-05  Data: 0.014 (0.012)
Train: 242 [ 700/1251 ( 56%)]  Loss:  2.935702 (2.8787)  Time: 1.103s,  928.07/s  (1.108s,  924.02/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [ 750/1251 ( 60%)]  Loss:  3.027328 (2.8880)  Time: 1.108s,  924.34/s  (1.108s,  924.43/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [ 800/1251 ( 64%)]  Loss:  2.844556 (2.8854)  Time: 1.196s,  856.40/s  (1.108s,  924.42/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [ 850/1251 ( 68%)]  Loss:  2.802877 (2.8808)  Time: 1.095s,  935.14/s  (1.107s,  924.90/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [ 900/1251 ( 72%)]  Loss:  2.669876 (2.8697)  Time: 1.226s,  835.25/s  (1.107s,  924.74/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [ 950/1251 ( 76%)]  Loss:  3.049948 (2.8787)  Time: 1.100s,  931.13/s  (1.107s,  925.03/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [1000/1251 ( 80%)]  Loss:  3.067222 (2.8877)  Time: 1.121s,  913.83/s  (1.107s,  924.94/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [1050/1251 ( 84%)]  Loss:  3.096463 (2.8972)  Time: 1.097s,  933.27/s  (1.107s,  924.62/s)  LR: 4.927e-05  Data: 0.012 (0.012)
Train: 242 [1100/1251 ( 88%)]  Loss:  3.049247 (2.9038)  Time: 1.091s,  938.48/s  (1.107s,  924.75/s)  LR: 4.927e-05  Data: 0.010 (0.012)
Train: 242 [1150/1251 ( 92%)]  Loss:  3.208475 (2.9165)  Time: 1.109s,  923.53/s  (1.107s,  924.87/s)  LR: 4.927e-05  Data: 0.011 (0.012)
Train: 242 [1200/1251 ( 96%)]  Loss:  2.951194 (2.9179)  Time: 1.097s,  933.13/s  (1.107s,  925.04/s)  LR: 4.927e-05  Data: 0.013 (0.012)
Train: 242 [1250/1251 (100%)]  Loss:  2.497006 (2.9017)  Time: 1.079s,  948.73/s  (1.107s,  924.70/s)  LR: 4.927e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.236 (3.236)  Loss:  0.4171 (0.4171)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.5719 (0.8682)  Acc@1: 87.6179 (80.5360)  Acc@5: 97.4057 (95.2900)
Test (EMA): [   0/48]  Time: 3.251 (3.251)  Loss:  0.3951 (0.3951)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5444 (0.8314)  Acc@1: 87.2642 (81.2000)  Acc@5: 97.9953 (95.5760)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 81.15799994873046)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 81.12)

Train: 243 [   0/1251 (  0%)]  Loss:  3.057656 (3.0577)  Time: 1.102s,  929.26/s  (1.102s,  929.26/s)  LR: 4.780e-05  Data: 0.020 (0.020)
Train: 243 [  50/1251 (  4%)]  Loss:  2.654388 (2.8560)  Time: 1.120s,  913.95/s  (1.111s,  921.54/s)  LR: 4.780e-05  Data: 0.010 (0.012)
Train: 243 [ 100/1251 (  8%)]  Loss:  2.886163 (2.8661)  Time: 1.097s,  933.83/s  (1.109s,  923.53/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 150/1251 ( 12%)]  Loss:  2.758195 (2.8391)  Time: 1.100s,  931.32/s  (1.106s,  925.65/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 200/1251 ( 16%)]  Loss:  3.124859 (2.8963)  Time: 1.097s,  933.14/s  (1.107s,  925.20/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 250/1251 ( 20%)]  Loss:  2.626673 (2.8513)  Time: 1.097s,  933.31/s  (1.106s,  925.73/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 300/1251 ( 24%)]  Loss:  2.952719 (2.8658)  Time: 1.100s,  930.69/s  (1.107s,  924.81/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 350/1251 ( 28%)]  Loss:  3.013528 (2.8843)  Time: 1.131s,  905.62/s  (1.107s,  924.95/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 400/1251 ( 32%)]  Loss:  2.583465 (2.8508)  Time: 1.119s,  915.17/s  (1.108s,  923.84/s)  LR: 4.780e-05  Data: 0.009 (0.012)
Train: 243 [ 450/1251 ( 36%)]  Loss:  3.033870 (2.8692)  Time: 1.097s,  933.73/s  (1.108s,  924.22/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 500/1251 ( 40%)]  Loss:  2.758275 (2.8591)  Time: 1.092s,  937.53/s  (1.108s,  924.34/s)  LR: 4.780e-05  Data: 0.009 (0.012)
Train: 243 [ 550/1251 ( 44%)]  Loss:  2.783688 (2.8528)  Time: 1.103s,  928.18/s  (1.108s,  924.30/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 600/1251 ( 48%)]  Loss:  2.883863 (2.8552)  Time: 1.122s,  912.31/s  (1.108s,  924.09/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 650/1251 ( 52%)]  Loss:  2.511006 (2.8306)  Time: 1.097s,  933.35/s  (1.109s,  923.55/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 700/1251 ( 56%)]  Loss:  2.870049 (2.8332)  Time: 1.097s,  933.47/s  (1.108s,  923.85/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 750/1251 ( 60%)]  Loss:  2.997721 (2.8435)  Time: 1.101s,  930.40/s  (1.108s,  924.02/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 800/1251 ( 64%)]  Loss:  2.900151 (2.8468)  Time: 1.201s,  852.33/s  (1.108s,  924.35/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 850/1251 ( 68%)]  Loss:  2.621868 (2.8343)  Time: 1.118s,  915.88/s  (1.108s,  924.34/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [ 900/1251 ( 72%)]  Loss:  2.984268 (2.8422)  Time: 1.100s,  931.11/s  (1.108s,  924.49/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [ 950/1251 ( 76%)]  Loss:  2.861343 (2.8432)  Time: 1.100s,  930.72/s  (1.108s,  924.38/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [1000/1251 ( 80%)]  Loss:  2.943084 (2.8479)  Time: 1.127s,  908.95/s  (1.108s,  924.44/s)  LR: 4.780e-05  Data: 0.012 (0.012)
Train: 243 [1050/1251 ( 84%)]  Loss:  2.612900 (2.8373)  Time: 1.098s,  932.97/s  (1.108s,  924.42/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Train: 243 [1100/1251 ( 88%)]  Loss:  2.871698 (2.8388)  Time: 1.101s,  930.08/s  (1.108s,  924.45/s)  LR: 4.780e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 243 [1150/1251 ( 92%)]  Loss:  2.920496 (2.8422)  Time: 1.115s,  918.67/s  (1.108s,  924.52/s)  LR: 4.780e-05  Data: 0.015 (0.012)
Train: 243 [1200/1251 ( 96%)]  Loss:  3.126211 (2.8535)  Time: 1.101s,  929.69/s  (1.108s,  924.49/s)  LR: 4.780e-05  Data: 0.013 (0.012)
Train: 243 [1250/1251 (100%)]  Loss:  2.869613 (2.8541)  Time: 1.081s,  947.35/s  (1.108s,  924.59/s)  LR: 4.780e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.197 (3.197)  Loss:  0.4229 (0.4229)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.230 (0.406)  Loss:  0.5896 (0.8747)  Acc@1: 86.9104 (80.5800)  Acc@5: 97.2877 (95.3080)
Test (EMA): [   0/48]  Time: 3.131 (3.131)  Loss:  0.3951 (0.3951)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5445 (0.8317)  Acc@1: 87.3821 (81.1840)  Acc@5: 97.9953 (95.5700)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 81.15799994873046)

Train: 244 [   0/1251 (  0%)]  Loss:  2.947101 (2.9471)  Time: 1.101s,  929.84/s  (1.101s,  929.84/s)  LR: 4.635e-05  Data: 0.020 (0.020)
Train: 244 [  50/1251 (  4%)]  Loss:  2.944767 (2.9459)  Time: 1.103s,  928.00/s  (1.107s,  925.36/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 100/1251 (  8%)]  Loss:  3.043871 (2.9786)  Time: 1.099s,  931.54/s  (1.108s,  924.18/s)  LR: 4.635e-05  Data: 0.014 (0.012)
Train: 244 [ 150/1251 ( 12%)]  Loss:  2.823959 (2.9399)  Time: 1.096s,  934.27/s  (1.108s,  924.59/s)  LR: 4.635e-05  Data: 0.010 (0.012)
Train: 244 [ 200/1251 ( 16%)]  Loss:  3.104405 (2.9728)  Time: 1.100s,  930.77/s  (1.106s,  925.73/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 250/1251 ( 20%)]  Loss:  3.149246 (3.0022)  Time: 1.100s,  930.84/s  (1.107s,  925.33/s)  LR: 4.635e-05  Data: 0.016 (0.012)
Train: 244 [ 300/1251 ( 24%)]  Loss:  3.023884 (3.0053)  Time: 1.096s,  933.91/s  (1.109s,  923.18/s)  LR: 4.635e-05  Data: 0.013 (0.012)
Train: 244 [ 350/1251 ( 28%)]  Loss:  2.805069 (2.9803)  Time: 1.103s,  927.98/s  (1.111s,  921.91/s)  LR: 4.635e-05  Data: 0.012 (0.012)
Train: 244 [ 400/1251 ( 32%)]  Loss:  2.906551 (2.9721)  Time: 1.094s,  936.40/s  (1.110s,  922.46/s)  LR: 4.635e-05  Data: 0.009 (0.012)
Train: 244 [ 450/1251 ( 36%)]  Loss:  2.955871 (2.9705)  Time: 1.096s,  934.07/s  (1.110s,  922.79/s)  LR: 4.635e-05  Data: 0.012 (0.012)
Train: 244 [ 500/1251 ( 40%)]  Loss:  2.764740 (2.9518)  Time: 1.104s,  927.61/s  (1.109s,  923.07/s)  LR: 4.635e-05  Data: 0.012 (0.012)
Train: 244 [ 550/1251 ( 44%)]  Loss:  2.784250 (2.9378)  Time: 1.096s,  933.97/s  (1.110s,  922.89/s)  LR: 4.635e-05  Data: 0.012 (0.012)
Train: 244 [ 600/1251 ( 48%)]  Loss:  3.201287 (2.9581)  Time: 1.099s,  931.72/s  (1.109s,  923.27/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 650/1251 ( 52%)]  Loss:  2.616390 (2.9337)  Time: 1.097s,  933.11/s  (1.108s,  923.83/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 700/1251 ( 56%)]  Loss:  2.835959 (2.9272)  Time: 1.195s,  856.83/s  (1.109s,  923.53/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 750/1251 ( 60%)]  Loss:  2.835215 (2.9214)  Time: 1.209s,  847.17/s  (1.109s,  923.76/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 800/1251 ( 64%)]  Loss:  3.052861 (2.9291)  Time: 1.101s,  930.20/s  (1.108s,  923.82/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 850/1251 ( 68%)]  Loss:  2.729150 (2.9180)  Time: 1.100s,  931.19/s  (1.108s,  924.18/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [ 900/1251 ( 72%)]  Loss:  2.667419 (2.9048)  Time: 1.125s,  909.82/s  (1.108s,  923.97/s)  LR: 4.635e-05  Data: 0.012 (0.012)
Train: 244 [ 950/1251 ( 76%)]  Loss:  3.023611 (2.9108)  Time: 1.097s,  933.70/s  (1.108s,  924.19/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [1000/1251 ( 80%)]  Loss:  2.817670 (2.9063)  Time: 1.101s,  929.69/s  (1.108s,  924.19/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [1050/1251 ( 84%)]  Loss:  2.687709 (2.8964)  Time: 1.096s,  933.96/s  (1.108s,  924.37/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [1100/1251 ( 88%)]  Loss:  2.966433 (2.8995)  Time: 1.097s,  933.49/s  (1.108s,  924.43/s)  LR: 4.635e-05  Data: 0.013 (0.012)
Train: 244 [1150/1251 ( 92%)]  Loss:  3.105278 (2.9080)  Time: 1.096s,  933.91/s  (1.108s,  924.07/s)  LR: 4.635e-05  Data: 0.012 (0.012)
Train: 244 [1200/1251 ( 96%)]  Loss:  2.884436 (2.9071)  Time: 1.232s,  831.49/s  (1.108s,  924.21/s)  LR: 4.635e-05  Data: 0.011 (0.012)
Train: 244 [1250/1251 (100%)]  Loss:  3.241759 (2.9200)  Time: 1.089s,  940.60/s  (1.108s,  923.98/s)  LR: 4.635e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.318 (3.318)  Loss:  0.4275 (0.4275)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5816 (0.8754)  Acc@1: 87.0283 (80.5600)  Acc@5: 97.7594 (95.2940)
Test (EMA): [   0/48]  Time: 3.086 (3.086)  Loss:  0.3953 (0.3953)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.403)  Loss:  0.5444 (0.8319)  Acc@1: 87.3821 (81.1860)  Acc@5: 97.9953 (95.5640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 81.17199994873047)

Train: 245 [   0/1251 (  0%)]  Loss:  2.959564 (2.9596)  Time: 1.103s,  928.60/s  (1.103s,  928.60/s)  LR: 4.493e-05  Data: 0.019 (0.019)
Train: 245 [  50/1251 (  4%)]  Loss:  2.790119 (2.8748)  Time: 1.102s,  928.93/s  (1.105s,  926.88/s)  LR: 4.493e-05  Data: 0.013 (0.012)
Train: 245 [ 100/1251 (  8%)]  Loss:  2.779023 (2.8429)  Time: 1.120s,  914.14/s  (1.108s,  924.58/s)  LR: 4.493e-05  Data: 0.010 (0.012)
Train: 245 [ 150/1251 ( 12%)]  Loss:  3.169536 (2.9246)  Time: 1.096s,  934.49/s  (1.105s,  926.91/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 200/1251 ( 16%)]  Loss:  2.936314 (2.9269)  Time: 1.099s,  931.48/s  (1.107s,  925.40/s)  LR: 4.493e-05  Data: 0.011 (0.012)
Train: 245 [ 250/1251 ( 20%)]  Loss:  3.123857 (2.9597)  Time: 1.098s,  932.80/s  (1.106s,  925.87/s)  LR: 4.493e-05  Data: 0.014 (0.012)
Train: 245 [ 300/1251 ( 24%)]  Loss:  2.878132 (2.9481)  Time: 1.103s,  928.46/s  (1.108s,  924.46/s)  LR: 4.493e-05  Data: 0.011 (0.012)
Train: 245 [ 350/1251 ( 28%)]  Loss:  2.689846 (2.9158)  Time: 1.122s,  912.40/s  (1.108s,  924.49/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 400/1251 ( 32%)]  Loss:  2.798334 (2.9027)  Time: 1.097s,  933.72/s  (1.108s,  923.91/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 450/1251 ( 36%)]  Loss:  2.923450 (2.9048)  Time: 1.097s,  933.60/s  (1.108s,  924.50/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 500/1251 ( 40%)]  Loss:  2.830616 (2.8981)  Time: 1.102s,  928.87/s  (1.107s,  924.73/s)  LR: 4.493e-05  Data: 0.010 (0.012)
Train: 245 [ 550/1251 ( 44%)]  Loss:  2.541417 (2.8684)  Time: 1.102s,  929.41/s  (1.107s,  924.86/s)  LR: 4.493e-05  Data: 0.011 (0.012)
Train: 245 [ 600/1251 ( 48%)]  Loss:  3.023154 (2.8803)  Time: 1.098s,  932.35/s  (1.108s,  924.51/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 650/1251 ( 52%)]  Loss:  3.035546 (2.8914)  Time: 1.098s,  932.31/s  (1.108s,  924.44/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 245 [ 700/1251 ( 56%)]  Loss:  2.909336 (2.8925)  Time: 1.205s,  849.59/s  (1.108s,  924.26/s)  LR: 4.493e-05  Data: 0.010 (0.012)
Train: 245 [ 750/1251 ( 60%)]  Loss:  2.968386 (2.8973)  Time: 1.096s,  934.27/s  (1.108s,  924.57/s)  LR: 4.493e-05  Data: 0.011 (0.012)
Train: 245 [ 800/1251 ( 64%)]  Loss:  3.057867 (2.9067)  Time: 1.127s,  908.99/s  (1.107s,  924.78/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 850/1251 ( 68%)]  Loss:  2.799739 (2.9008)  Time: 1.122s,  912.76/s  (1.108s,  924.16/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [ 900/1251 ( 72%)]  Loss:  2.900782 (2.9008)  Time: 1.096s,  934.73/s  (1.108s,  923.86/s)  LR: 4.493e-05  Data: 0.010 (0.012)
Train: 245 [ 950/1251 ( 76%)]  Loss:  2.840814 (2.8978)  Time: 1.099s,  931.94/s  (1.108s,  923.86/s)  LR: 4.493e-05  Data: 0.011 (0.012)
Train: 245 [1000/1251 ( 80%)]  Loss:  2.871675 (2.8965)  Time: 1.101s,  929.76/s  (1.109s,  923.77/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [1050/1251 ( 84%)]  Loss:  2.496796 (2.8784)  Time: 1.097s,  933.25/s  (1.108s,  923.83/s)  LR: 4.493e-05  Data: 0.013 (0.012)
Train: 245 [1100/1251 ( 88%)]  Loss:  2.893894 (2.8791)  Time: 1.097s,  933.28/s  (1.108s,  924.04/s)  LR: 4.493e-05  Data: 0.011 (0.012)
Train: 245 [1150/1251 ( 92%)]  Loss:  2.463187 (2.8617)  Time: 1.097s,  933.09/s  (1.108s,  924.17/s)  LR: 4.493e-05  Data: 0.012 (0.012)
Train: 245 [1200/1251 ( 96%)]  Loss:  2.954285 (2.8654)  Time: 1.101s,  929.85/s  (1.108s,  924.24/s)  LR: 4.493e-05  Data: 0.014 (0.012)
Train: 245 [1250/1251 (100%)]  Loss:  2.956292 (2.8689)  Time: 1.088s,  941.30/s  (1.108s,  924.47/s)  LR: 4.493e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.184 (3.184)  Loss:  0.4049 (0.4049)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.229 (0.407)  Loss:  0.5792 (0.8672)  Acc@1: 87.0283 (80.7120)  Acc@5: 97.6415 (95.3320)
Test (EMA): [   0/48]  Time: 3.174 (3.174)  Loss:  0.3953 (0.3953)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.406)  Loss:  0.5445 (0.8321)  Acc@1: 87.3821 (81.1840)  Acc@5: 97.9953 (95.5560)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 81.172)

Train: 246 [   0/1251 (  0%)]  Loss:  3.029742 (3.0297)  Time: 1.106s,  925.98/s  (1.106s,  925.98/s)  LR: 4.353e-05  Data: 0.024 (0.024)
Train: 246 [  50/1251 (  4%)]  Loss:  2.833002 (2.9314)  Time: 1.100s,  930.50/s  (1.109s,  922.97/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 100/1251 (  8%)]  Loss:  3.100346 (2.9877)  Time: 1.100s,  930.95/s  (1.106s,  926.04/s)  LR: 4.353e-05  Data: 0.014 (0.012)
Train: 246 [ 150/1251 ( 12%)]  Loss:  2.924879 (2.9720)  Time: 1.096s,  934.60/s  (1.107s,  925.42/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 200/1251 ( 16%)]  Loss:  3.024630 (2.9825)  Time: 1.120s,  914.12/s  (1.108s,  924.59/s)  LR: 4.353e-05  Data: 0.010 (0.012)
Train: 246 [ 250/1251 ( 20%)]  Loss:  3.396012 (3.0514)  Time: 1.106s,  926.17/s  (1.108s,  923.90/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 300/1251 ( 24%)]  Loss:  3.121433 (3.0614)  Time: 1.099s,  931.68/s  (1.108s,  924.53/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 350/1251 ( 28%)]  Loss:  2.513874 (2.9930)  Time: 1.098s,  932.77/s  (1.107s,  924.75/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [ 400/1251 ( 32%)]  Loss:  2.790470 (2.9705)  Time: 1.100s,  931.07/s  (1.107s,  925.12/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 450/1251 ( 36%)]  Loss:  2.895565 (2.9630)  Time: 1.099s,  932.10/s  (1.108s,  924.53/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [ 500/1251 ( 40%)]  Loss:  2.940415 (2.9609)  Time: 1.099s,  931.53/s  (1.107s,  924.81/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [ 550/1251 ( 44%)]  Loss:  2.822196 (2.9494)  Time: 1.195s,  856.74/s  (1.107s,  924.84/s)  LR: 4.353e-05  Data: 0.010 (0.012)
Train: 246 [ 600/1251 ( 48%)]  Loss:  2.774579 (2.9359)  Time: 1.102s,  929.36/s  (1.107s,  925.07/s)  LR: 4.353e-05  Data: 0.010 (0.012)
Train: 246 [ 650/1251 ( 52%)]  Loss:  3.034707 (2.9430)  Time: 1.098s,  932.96/s  (1.107s,  925.34/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 700/1251 ( 56%)]  Loss:  2.619569 (2.9214)  Time: 1.097s,  933.81/s  (1.107s,  925.27/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [ 750/1251 ( 60%)]  Loss:  2.905107 (2.9204)  Time: 1.136s,  901.69/s  (1.107s,  925.20/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [ 800/1251 ( 64%)]  Loss:  2.613361 (2.9023)  Time: 1.094s,  936.15/s  (1.107s,  925.04/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [ 850/1251 ( 68%)]  Loss:  2.845797 (2.8992)  Time: 1.102s,  929.55/s  (1.107s,  925.42/s)  LR: 4.353e-05  Data: 0.010 (0.012)
Train: 246 [ 900/1251 ( 72%)]  Loss:  2.438376 (2.8750)  Time: 1.122s,  912.76/s  (1.107s,  925.13/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [ 950/1251 ( 76%)]  Loss:  3.005991 (2.8815)  Time: 1.096s,  933.89/s  (1.107s,  925.09/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [1000/1251 ( 80%)]  Loss:  2.889201 (2.8819)  Time: 1.106s,  926.12/s  (1.107s,  925.24/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [1050/1251 ( 84%)]  Loss:  3.072077 (2.8905)  Time: 1.098s,  932.89/s  (1.107s,  925.10/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [1100/1251 ( 88%)]  Loss:  3.035738 (2.8968)  Time: 1.097s,  933.34/s  (1.107s,  925.16/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [1150/1251 ( 92%)]  Loss:  2.883062 (2.8963)  Time: 1.097s,  933.48/s  (1.107s,  925.06/s)  LR: 4.353e-05  Data: 0.012 (0.012)
Train: 246 [1200/1251 ( 96%)]  Loss:  2.924781 (2.8974)  Time: 1.102s,  929.37/s  (1.107s,  925.06/s)  LR: 4.353e-05  Data: 0.011 (0.012)
Train: 246 [1250/1251 (100%)]  Loss:  2.905877 (2.8977)  Time: 1.093s,  936.48/s  (1.107s,  925.06/s)  LR: 4.353e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.284 (3.284)  Loss:  0.4241 (0.4241)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.230 (0.408)  Loss:  0.5794 (0.8693)  Acc@1: 87.1462 (80.7360)  Acc@5: 97.6415 (95.3060)
Test (EMA): [   0/48]  Time: 3.189 (3.189)  Loss:  0.3954 (0.3954)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5448 (0.8321)  Acc@1: 87.6179 (81.2040)  Acc@5: 97.8774 (95.5640)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 81.177999921875)

Train: 247 [   0/1251 (  0%)]  Loss:  2.795181 (2.7952)  Time: 1.100s,  931.24/s  (1.100s,  931.24/s)  LR: 4.215e-05  Data: 0.020 (0.020)
Train: 247 [  50/1251 (  4%)]  Loss:  3.049602 (2.9224)  Time: 1.101s,  929.79/s  (1.104s,  927.73/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 100/1251 (  8%)]  Loss:  3.157588 (3.0008)  Time: 1.103s,  928.75/s  (1.104s,  927.83/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 150/1251 ( 12%)]  Loss:  2.761957 (2.9411)  Time: 1.120s,  914.68/s  (1.103s,  928.30/s)  LR: 4.215e-05  Data: 0.010 (0.012)
Train: 247 [ 200/1251 ( 16%)]  Loss:  2.830914 (2.9190)  Time: 1.119s,  914.78/s  (1.106s,  926.19/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 250/1251 ( 20%)]  Loss:  3.019604 (2.9358)  Time: 1.097s,  933.77/s  (1.106s,  925.54/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 300/1251 ( 24%)]  Loss:  2.999856 (2.9450)  Time: 1.096s,  934.33/s  (1.107s,  924.92/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 350/1251 ( 28%)]  Loss:  3.034621 (2.9562)  Time: 1.099s,  932.01/s  (1.106s,  925.52/s)  LR: 4.215e-05  Data: 0.010 (0.012)
Train: 247 [ 400/1251 ( 32%)]  Loss:  2.799304 (2.9387)  Time: 1.095s,  935.11/s  (1.108s,  924.51/s)  LR: 4.215e-05  Data: 0.010 (0.012)
Train: 247 [ 450/1251 ( 36%)]  Loss:  2.792600 (2.9241)  Time: 1.126s,  909.34/s  (1.109s,  923.34/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 500/1251 ( 40%)]  Loss:  3.019011 (2.9327)  Time: 1.103s,  928.27/s  (1.109s,  923.23/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 550/1251 ( 44%)]  Loss:  3.015899 (2.9397)  Time: 1.097s,  933.82/s  (1.109s,  923.22/s)  LR: 4.215e-05  Data: 0.011 (0.012)
Train: 247 [ 600/1251 ( 48%)]  Loss:  2.845969 (2.9325)  Time: 1.098s,  932.64/s  (1.109s,  923.28/s)  LR: 4.215e-05  Data: 0.011 (0.012)
Train: 247 [ 650/1251 ( 52%)]  Loss:  2.667466 (2.9135)  Time: 1.096s,  933.93/s  (1.110s,  922.82/s)  LR: 4.215e-05  Data: 0.011 (0.012)
Train: 247 [ 700/1251 ( 56%)]  Loss:  2.736591 (2.9017)  Time: 1.097s,  933.88/s  (1.109s,  923.22/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 750/1251 ( 60%)]  Loss:  3.072260 (2.9124)  Time: 1.096s,  934.13/s  (1.109s,  923.26/s)  LR: 4.215e-05  Data: 0.011 (0.012)
Train: 247 [ 800/1251 ( 64%)]  Loss:  2.429002 (2.8840)  Time: 1.096s,  934.21/s  (1.109s,  923.24/s)  LR: 4.215e-05  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 247 [ 850/1251 ( 68%)]  Loss:  2.946014 (2.8874)  Time: 1.142s,  896.54/s  (1.110s,  922.84/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 900/1251 ( 72%)]  Loss:  2.660125 (2.8755)  Time: 1.097s,  933.12/s  (1.109s,  922.95/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [ 950/1251 ( 76%)]  Loss:  2.948143 (2.8791)  Time: 1.096s,  934.01/s  (1.109s,  923.13/s)  LR: 4.215e-05  Data: 0.012 (0.012)
Train: 247 [1000/1251 ( 80%)]  Loss:  2.681898 (2.8697)  Time: 1.103s,  928.68/s  (1.109s,  923.24/s)  LR: 4.215e-05  Data: 0.010 (0.012)
Train: 247 [1050/1251 ( 84%)]  Loss:  2.850514 (2.8688)  Time: 1.097s,  933.63/s  (1.109s,  923.47/s)  LR: 4.215e-05  Data: 0.011 (0.012)
Train: 247 [1100/1251 ( 88%)]  Loss:  2.976038 (2.8735)  Time: 1.097s,  933.53/s  (1.109s,  923.32/s)  LR: 4.215e-05  Data: 0.013 (0.012)
Train: 247 [1150/1251 ( 92%)]  Loss:  2.917573 (2.8753)  Time: 1.095s,  935.14/s  (1.109s,  923.24/s)  LR: 4.215e-05  Data: 0.010 (0.012)
Train: 247 [1200/1251 ( 96%)]  Loss:  2.652401 (2.8664)  Time: 1.129s,  906.86/s  (1.109s,  923.13/s)  LR: 4.215e-05  Data: 0.011 (0.012)
Train: 247 [1250/1251 (100%)]  Loss:  2.805734 (2.8641)  Time: 1.081s,  947.54/s  (1.109s,  922.97/s)  LR: 4.215e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.228 (3.228)  Loss:  0.4160 (0.4160)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.5694 (0.8583)  Acc@1: 87.9717 (80.7640)  Acc@5: 97.7594 (95.3580)
Test (EMA): [   0/48]  Time: 3.163 (3.163)  Loss:  0.3963 (0.3963)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.230 (0.407)  Loss:  0.5457 (0.8323)  Acc@1: 87.6179 (81.2220)  Acc@5: 97.7594 (95.5460)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 81.18399994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 81.18399994873047)

Train: 248 [   0/1251 (  0%)]  Loss:  3.061424 (3.0614)  Time: 1.104s,  927.77/s  (1.104s,  927.77/s)  LR: 4.080e-05  Data: 0.021 (0.021)
Train: 248 [  50/1251 (  4%)]  Loss:  3.123840 (3.0926)  Time: 1.098s,  932.50/s  (1.109s,  923.08/s)  LR: 4.080e-05  Data: 0.016 (0.012)
Train: 248 [ 100/1251 (  8%)]  Loss:  2.689497 (2.9583)  Time: 1.097s,  933.32/s  (1.107s,  925.16/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 150/1251 ( 12%)]  Loss:  3.196023 (3.0177)  Time: 1.096s,  934.04/s  (1.109s,  923.06/s)  LR: 4.080e-05  Data: 0.012 (0.012)
Train: 248 [ 200/1251 ( 16%)]  Loss:  3.096966 (3.0335)  Time: 1.100s,  930.90/s  (1.109s,  923.11/s)  LR: 4.080e-05  Data: 0.013 (0.012)
Train: 248 [ 250/1251 ( 20%)]  Loss:  3.039096 (3.0345)  Time: 1.101s,  929.89/s  (1.111s,  921.28/s)  LR: 4.080e-05  Data: 0.010 (0.012)
Train: 248 [ 300/1251 ( 24%)]  Loss:  3.010091 (3.0310)  Time: 1.125s,  909.84/s  (1.111s,  921.82/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 350/1251 ( 28%)]  Loss:  2.964857 (3.0227)  Time: 1.105s,  926.91/s  (1.112s,  921.22/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 400/1251 ( 32%)]  Loss:  2.954290 (3.0151)  Time: 1.098s,  932.18/s  (1.111s,  921.67/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 450/1251 ( 36%)]  Loss:  3.024048 (3.0160)  Time: 1.096s,  934.64/s  (1.110s,  922.42/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 500/1251 ( 40%)]  Loss:  2.884663 (3.0041)  Time: 1.102s,  929.48/s  (1.110s,  922.89/s)  LR: 4.080e-05  Data: 0.009 (0.012)
Train: 248 [ 550/1251 ( 44%)]  Loss:  2.979663 (3.0020)  Time: 1.110s,  922.89/s  (1.109s,  923.43/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 600/1251 ( 48%)]  Loss:  2.852567 (2.9905)  Time: 1.097s,  933.40/s  (1.109s,  923.60/s)  LR: 4.080e-05  Data: 0.013 (0.012)
Train: 248 [ 650/1251 ( 52%)]  Loss:  2.772912 (2.9750)  Time: 1.095s,  934.85/s  (1.109s,  923.69/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 700/1251 ( 56%)]  Loss:  3.028940 (2.9786)  Time: 1.094s,  936.20/s  (1.109s,  923.51/s)  LR: 4.080e-05  Data: 0.010 (0.012)
Train: 248 [ 750/1251 ( 60%)]  Loss:  3.050172 (2.9831)  Time: 1.104s,  927.31/s  (1.109s,  923.66/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [ 800/1251 ( 64%)]  Loss:  2.628662 (2.9622)  Time: 1.099s,  931.81/s  (1.109s,  923.69/s)  LR: 4.080e-05  Data: 0.012 (0.012)
Train: 248 [ 850/1251 ( 68%)]  Loss:  2.747205 (2.9503)  Time: 1.096s,  934.31/s  (1.108s,  924.10/s)  LR: 4.080e-05  Data: 0.012 (0.012)
Train: 248 [ 900/1251 ( 72%)]  Loss:  3.196346 (2.9632)  Time: 1.103s,  928.34/s  (1.108s,  924.26/s)  LR: 4.080e-05  Data: 0.010 (0.012)
Train: 248 [ 950/1251 ( 76%)]  Loss:  2.973685 (2.9637)  Time: 1.126s,  909.46/s  (1.109s,  923.68/s)  LR: 4.080e-05  Data: 0.012 (0.012)
Train: 248 [1000/1251 ( 80%)]  Loss:  2.971692 (2.9641)  Time: 1.096s,  934.49/s  (1.109s,  923.71/s)  LR: 4.080e-05  Data: 0.012 (0.012)
Train: 248 [1050/1251 ( 84%)]  Loss:  2.706356 (2.9524)  Time: 1.205s,  849.91/s  (1.109s,  923.69/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [1100/1251 ( 88%)]  Loss:  2.557174 (2.9352)  Time: 1.101s,  930.15/s  (1.109s,  923.68/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [1150/1251 ( 92%)]  Loss:  2.929466 (2.9350)  Time: 1.099s,  931.93/s  (1.108s,  923.96/s)  LR: 4.080e-05  Data: 0.011 (0.012)
Train: 248 [1200/1251 ( 96%)]  Loss:  3.000663 (2.9376)  Time: 1.120s,  914.48/s  (1.108s,  924.16/s)  LR: 4.080e-05  Data: 0.010 (0.012)
Train: 248 [1250/1251 (100%)]  Loss:  2.899181 (2.9361)  Time: 1.079s,  948.93/s  (1.108s,  923.98/s)  LR: 4.080e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.313 (3.313)  Loss:  0.4257 (0.4257)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5690 (0.8659)  Acc@1: 87.1462 (80.6460)  Acc@5: 97.7594 (95.1980)
Test (EMA): [   0/48]  Time: 3.126 (3.126)  Loss:  0.3969 (0.3969)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5464 (0.8327)  Acc@1: 87.6179 (81.2200)  Acc@5: 97.7594 (95.5420)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 81.18399994873047)

Train: 249 [   0/1251 (  0%)]  Loss:  2.680543 (2.6805)  Time: 1.102s,  928.96/s  (1.102s,  928.96/s)  LR: 3.947e-05  Data: 0.019 (0.019)
Train: 249 [  50/1251 (  4%)]  Loss:  2.979872 (2.8302)  Time: 1.095s,  935.30/s  (1.103s,  928.76/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 100/1251 (  8%)]  Loss:  2.984060 (2.8815)  Time: 1.099s,  932.10/s  (1.105s,  926.62/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [ 150/1251 ( 12%)]  Loss:  2.904180 (2.8872)  Time: 1.097s,  933.54/s  (1.106s,  926.02/s)  LR: 3.947e-05  Data: 0.015 (0.012)
Train: 249 [ 200/1251 ( 16%)]  Loss:  2.958169 (2.9014)  Time: 1.096s,  934.45/s  (1.106s,  925.75/s)  LR: 3.947e-05  Data: 0.010 (0.012)
Train: 249 [ 250/1251 ( 20%)]  Loss:  2.996600 (2.9172)  Time: 1.102s,  928.88/s  (1.108s,  924.45/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 300/1251 ( 24%)]  Loss:  2.871130 (2.9107)  Time: 1.096s,  934.35/s  (1.108s,  924.50/s)  LR: 3.947e-05  Data: 0.013 (0.012)
Train: 249 [ 350/1251 ( 28%)]  Loss:  3.019832 (2.9243)  Time: 1.130s,  906.02/s  (1.107s,  924.69/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 400/1251 ( 32%)]  Loss:  2.855676 (2.9167)  Time: 1.095s,  935.23/s  (1.108s,  924.39/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 450/1251 ( 36%)]  Loss:  2.892960 (2.9143)  Time: 1.191s,  860.07/s  (1.107s,  924.75/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 500/1251 ( 40%)]  Loss:  2.951966 (2.9177)  Time: 1.097s,  933.28/s  (1.107s,  924.98/s)  LR: 3.947e-05  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 249 [ 550/1251 ( 44%)]  Loss:  2.943650 (2.9199)  Time: 1.131s,  905.58/s  (1.107s,  924.99/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [ 600/1251 ( 48%)]  Loss:  2.798867 (2.9106)  Time: 1.096s,  934.12/s  (1.107s,  925.18/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [ 650/1251 ( 52%)]  Loss:  3.121179 (2.9256)  Time: 1.097s,  933.03/s  (1.107s,  924.99/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [ 700/1251 ( 56%)]  Loss:  2.923512 (2.9255)  Time: 1.099s,  931.64/s  (1.107s,  924.66/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 750/1251 ( 60%)]  Loss:  2.791629 (2.9171)  Time: 1.097s,  933.78/s  (1.108s,  924.49/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [ 800/1251 ( 64%)]  Loss:  3.036418 (2.9241)  Time: 1.100s,  931.26/s  (1.107s,  924.75/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 850/1251 ( 68%)]  Loss:  2.954638 (2.9258)  Time: 1.095s,  935.09/s  (1.107s,  924.68/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [ 900/1251 ( 72%)]  Loss:  2.813668 (2.9199)  Time: 1.097s,  933.74/s  (1.108s,  924.24/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [ 950/1251 ( 76%)]  Loss:  2.998217 (2.9238)  Time: 1.097s,  933.12/s  (1.108s,  924.07/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [1000/1251 ( 80%)]  Loss:  2.966668 (2.9259)  Time: 1.130s,  906.55/s  (1.108s,  924.02/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [1050/1251 ( 84%)]  Loss:  2.875755 (2.9236)  Time: 1.096s,  934.33/s  (1.108s,  924.36/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [1100/1251 ( 88%)]  Loss:  2.980196 (2.9261)  Time: 1.104s,  927.14/s  (1.108s,  924.27/s)  LR: 3.947e-05  Data: 0.015 (0.012)
Train: 249 [1150/1251 ( 92%)]  Loss:  3.123266 (2.9343)  Time: 1.097s,  933.80/s  (1.108s,  924.24/s)  LR: 3.947e-05  Data: 0.012 (0.012)
Train: 249 [1200/1251 ( 96%)]  Loss:  2.968945 (2.9357)  Time: 1.121s,  913.60/s  (1.108s,  924.24/s)  LR: 3.947e-05  Data: 0.011 (0.012)
Train: 249 [1250/1251 (100%)]  Loss:  3.020626 (2.9389)  Time: 1.081s,  946.95/s  (1.108s,  924.27/s)  LR: 3.947e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.346 (3.346)  Loss:  0.4135 (0.4135)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.404)  Loss:  0.5469 (0.8590)  Acc@1: 87.7359 (80.7580)  Acc@5: 97.8774 (95.3140)
Test (EMA): [   0/48]  Time: 3.170 (3.170)  Loss:  0.3970 (0.3970)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.4375 (98.4375)
Test (EMA): [  48/48]  Time: 0.229 (0.407)  Loss:  0.5466 (0.8329)  Acc@1: 87.6179 (81.2280)  Acc@5: 97.7594 (95.5400)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 81.227999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 81.18599994873047)

Train: 250 [   0/1251 (  0%)]  Loss:  2.970849 (2.9708)  Time: 1.114s,  919.22/s  (1.114s,  919.22/s)  LR: 3.816e-05  Data: 0.029 (0.029)
Train: 250 [  50/1251 (  4%)]  Loss:  2.770114 (2.8705)  Time: 1.097s,  933.18/s  (1.105s,  926.38/s)  LR: 3.816e-05  Data: 0.012 (0.012)
Train: 250 [ 100/1251 (  8%)]  Loss:  2.829468 (2.8568)  Time: 1.096s,  934.08/s  (1.108s,  923.98/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 150/1251 ( 12%)]  Loss:  2.907528 (2.8695)  Time: 1.102s,  929.47/s  (1.110s,  922.67/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 200/1251 ( 16%)]  Loss:  3.065288 (2.9086)  Time: 1.121s,  913.58/s  (1.110s,  922.46/s)  LR: 3.816e-05  Data: 0.010 (0.012)
Train: 250 [ 250/1251 ( 20%)]  Loss:  2.962288 (2.9176)  Time: 1.096s,  933.95/s  (1.111s,  921.82/s)  LR: 3.816e-05  Data: 0.014 (0.012)
Train: 250 [ 300/1251 ( 24%)]  Loss:  2.797869 (2.9005)  Time: 1.101s,  930.37/s  (1.111s,  921.77/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 350/1251 ( 28%)]  Loss:  2.969306 (2.9091)  Time: 1.096s,  934.12/s  (1.110s,  922.79/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 400/1251 ( 32%)]  Loss:  3.008563 (2.9201)  Time: 1.224s,  836.42/s  (1.109s,  923.45/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 450/1251 ( 36%)]  Loss:  3.091643 (2.9373)  Time: 1.099s,  931.80/s  (1.109s,  923.73/s)  LR: 3.816e-05  Data: 0.013 (0.012)
Train: 250 [ 500/1251 ( 40%)]  Loss:  2.625096 (2.9089)  Time: 1.096s,  934.53/s  (1.110s,  922.90/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 550/1251 ( 44%)]  Loss:  2.855527 (2.9045)  Time: 1.098s,  932.31/s  (1.110s,  922.35/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 600/1251 ( 48%)]  Loss:  2.680393 (2.8872)  Time: 1.097s,  933.28/s  (1.110s,  922.23/s)  LR: 3.816e-05  Data: 0.012 (0.012)
Train: 250 [ 650/1251 ( 52%)]  Loss:  2.848459 (2.8845)  Time: 1.120s,  914.16/s  (1.110s,  922.37/s)  LR: 3.816e-05  Data: 0.013 (0.012)
Train: 250 [ 700/1251 ( 56%)]  Loss:  2.687609 (2.8713)  Time: 1.095s,  934.81/s  (1.110s,  922.54/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 750/1251 ( 60%)]  Loss:  2.819751 (2.8681)  Time: 1.121s,  913.85/s  (1.110s,  922.62/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 800/1251 ( 64%)]  Loss:  2.587613 (2.8516)  Time: 1.097s,  933.06/s  (1.110s,  922.61/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 850/1251 ( 68%)]  Loss:  2.978850 (2.8587)  Time: 1.096s,  934.07/s  (1.110s,  922.12/s)  LR: 3.816e-05  Data: 0.014 (0.012)
Train: 250 [ 900/1251 ( 72%)]  Loss:  2.810339 (2.8561)  Time: 1.187s,  862.81/s  (1.110s,  922.37/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [ 950/1251 ( 76%)]  Loss:  2.903301 (2.8585)  Time: 1.193s,  858.07/s  (1.110s,  922.62/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [1000/1251 ( 80%)]  Loss:  2.892874 (2.8601)  Time: 1.096s,  934.13/s  (1.110s,  922.92/s)  LR: 3.816e-05  Data: 0.013 (0.012)
Train: 250 [1050/1251 ( 84%)]  Loss:  2.775212 (2.8563)  Time: 1.100s,  930.92/s  (1.109s,  923.07/s)  LR: 3.816e-05  Data: 0.012 (0.012)
Train: 250 [1100/1251 ( 88%)]  Loss:  2.793471 (2.8535)  Time: 1.096s,  934.03/s  (1.109s,  923.21/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [1150/1251 ( 92%)]  Loss:  3.024581 (2.8607)  Time: 1.097s,  933.80/s  (1.109s,  923.26/s)  LR: 3.816e-05  Data: 0.011 (0.012)
Train: 250 [1200/1251 ( 96%)]  Loss:  2.856162 (2.8605)  Time: 1.094s,  936.38/s  (1.109s,  923.50/s)  LR: 3.816e-05  Data: 0.010 (0.012)
Train: 250 [1250/1251 (100%)]  Loss:  2.572160 (2.8494)  Time: 1.083s,  945.91/s  (1.109s,  923.47/s)  LR: 3.816e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.304 (3.304)  Loss:  0.4260 (0.4260)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.413)  Loss:  0.5634 (0.8719)  Acc@1: 87.2642 (80.6640)  Acc@5: 97.7594 (95.3300)
Test (EMA): [   0/48]  Time: 3.181 (3.181)  Loss:  0.3976 (0.3976)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.404)  Loss:  0.5470 (0.8334)  Acc@1: 87.3821 (81.2400)  Acc@5: 97.7594 (95.5300)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 81.23999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 81.227999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 81.18599994873047)

Train: 251 [   0/1251 (  0%)]  Loss:  3.016664 (3.0167)  Time: 1.107s,  924.65/s  (1.107s,  924.65/s)  LR: 3.687e-05  Data: 0.024 (0.024)
Train: 251 [  50/1251 (  4%)]  Loss:  2.813793 (2.9152)  Time: 1.098s,  932.86/s  (1.101s,  929.81/s)  LR: 3.687e-05  Data: 0.011 (0.011)
Train: 251 [ 100/1251 (  8%)]  Loss:  3.005096 (2.9452)  Time: 1.098s,  932.65/s  (1.106s,  925.98/s)  LR: 3.687e-05  Data: 0.011 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 251 [ 150/1251 ( 12%)]  Loss:  2.811288 (2.9117)  Time: 1.098s,  932.86/s  (1.104s,  927.74/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [ 200/1251 ( 16%)]  Loss:  2.710370 (2.8714)  Time: 1.101s,  930.09/s  (1.105s,  927.08/s)  LR: 3.687e-05  Data: 0.014 (0.012)
Train: 251 [ 250/1251 ( 20%)]  Loss:  2.931404 (2.8814)  Time: 1.118s,  915.81/s  (1.105s,  926.77/s)  LR: 3.687e-05  Data: 0.010 (0.012)
Train: 251 [ 300/1251 ( 24%)]  Loss:  2.440891 (2.8185)  Time: 1.095s,  934.83/s  (1.106s,  926.26/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [ 350/1251 ( 28%)]  Loss:  2.965637 (2.8369)  Time: 1.197s,  855.52/s  (1.106s,  926.20/s)  LR: 3.687e-05  Data: 0.011 (0.012)
Train: 251 [ 400/1251 ( 32%)]  Loss:  2.899816 (2.8439)  Time: 1.093s,  936.94/s  (1.105s,  926.42/s)  LR: 3.687e-05  Data: 0.010 (0.012)
Train: 251 [ 450/1251 ( 36%)]  Loss:  3.109416 (2.8704)  Time: 1.103s,  928.20/s  (1.105s,  926.43/s)  LR: 3.687e-05  Data: 0.011 (0.012)
Train: 251 [ 500/1251 ( 40%)]  Loss:  2.954792 (2.8781)  Time: 1.097s,  933.58/s  (1.106s,  926.05/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [ 550/1251 ( 44%)]  Loss:  2.900941 (2.8800)  Time: 1.097s,  933.29/s  (1.106s,  926.05/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [ 600/1251 ( 48%)]  Loss:  2.949485 (2.8854)  Time: 1.120s,  913.91/s  (1.106s,  925.97/s)  LR: 3.687e-05  Data: 0.011 (0.012)
Train: 251 [ 650/1251 ( 52%)]  Loss:  2.957984 (2.8905)  Time: 1.104s,  927.78/s  (1.107s,  924.94/s)  LR: 3.687e-05  Data: 0.010 (0.012)
Train: 251 [ 700/1251 ( 56%)]  Loss:  3.200446 (2.9112)  Time: 1.095s,  935.10/s  (1.107s,  925.05/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [ 750/1251 ( 60%)]  Loss:  2.799674 (2.9042)  Time: 1.100s,  930.86/s  (1.107s,  925.20/s)  LR: 3.687e-05  Data: 0.015 (0.012)
Train: 251 [ 800/1251 ( 64%)]  Loss:  2.612682 (2.8871)  Time: 1.131s,  905.31/s  (1.107s,  924.74/s)  LR: 3.687e-05  Data: 0.011 (0.012)
Train: 251 [ 850/1251 ( 68%)]  Loss:  2.773189 (2.8808)  Time: 1.132s,  904.39/s  (1.107s,  924.65/s)  LR: 3.687e-05  Data: 0.010 (0.012)
Train: 251 [ 900/1251 ( 72%)]  Loss:  2.692199 (2.8708)  Time: 1.097s,  933.59/s  (1.108s,  924.55/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [ 950/1251 ( 76%)]  Loss:  2.880631 (2.8713)  Time: 1.096s,  934.20/s  (1.107s,  924.75/s)  LR: 3.687e-05  Data: 0.013 (0.012)
Train: 251 [1000/1251 ( 80%)]  Loss:  3.066136 (2.8806)  Time: 1.122s,  912.26/s  (1.108s,  924.03/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [1050/1251 ( 84%)]  Loss:  3.012726 (2.8866)  Time: 1.097s,  933.25/s  (1.109s,  923.49/s)  LR: 3.687e-05  Data: 0.010 (0.012)
Train: 251 [1100/1251 ( 88%)]  Loss:  2.929680 (2.8885)  Time: 1.095s,  935.47/s  (1.109s,  923.60/s)  LR: 3.687e-05  Data: 0.011 (0.012)
Train: 251 [1150/1251 ( 92%)]  Loss:  2.968159 (2.8918)  Time: 1.202s,  851.84/s  (1.109s,  923.64/s)  LR: 3.687e-05  Data: 0.012 (0.012)
Train: 251 [1200/1251 ( 96%)]  Loss:  2.835067 (2.8895)  Time: 1.099s,  931.96/s  (1.109s,  923.69/s)  LR: 3.687e-05  Data: 0.013 (0.012)
Train: 251 [1250/1251 (100%)]  Loss:  3.177581 (2.9006)  Time: 1.083s,  945.55/s  (1.109s,  923.40/s)  LR: 3.687e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.340 (3.340)  Loss:  0.4200 (0.4200)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.406)  Loss:  0.5672 (0.8695)  Acc@1: 86.9104 (80.7660)  Acc@5: 97.4057 (95.3100)
Test (EMA): [   0/48]  Time: 3.091 (3.091)  Loss:  0.3980 (0.3980)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.404)  Loss:  0.5473 (0.8339)  Acc@1: 87.6179 (81.2680)  Acc@5: 97.7594 (95.5360)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 81.267999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 81.23999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 81.227999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 81.18799989746094)

Train: 252 [   0/1251 (  0%)]  Loss:  2.986366 (2.9864)  Time: 1.102s,  929.24/s  (1.102s,  929.24/s)  LR: 3.561e-05  Data: 0.020 (0.020)
Train: 252 [  50/1251 (  4%)]  Loss:  2.838015 (2.9122)  Time: 1.102s,  929.02/s  (1.108s,  923.86/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [ 100/1251 (  8%)]  Loss:  2.705937 (2.8434)  Time: 1.098s,  932.76/s  (1.106s,  925.67/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 150/1251 ( 12%)]  Loss:  2.592562 (2.7807)  Time: 1.094s,  936.07/s  (1.105s,  926.40/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 200/1251 ( 16%)]  Loss:  2.999497 (2.8245)  Time: 1.102s,  928.81/s  (1.106s,  925.90/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [ 250/1251 ( 20%)]  Loss:  2.999374 (2.8536)  Time: 1.102s,  929.42/s  (1.107s,  925.37/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 300/1251 ( 24%)]  Loss:  2.849852 (2.8531)  Time: 1.103s,  928.40/s  (1.106s,  926.07/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 350/1251 ( 28%)]  Loss:  2.996763 (2.8710)  Time: 1.127s,  908.62/s  (1.106s,  925.96/s)  LR: 3.561e-05  Data: 0.009 (0.012)
Train: 252 [ 400/1251 ( 32%)]  Loss:  2.915257 (2.8760)  Time: 1.098s,  932.51/s  (1.106s,  925.96/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [ 450/1251 ( 36%)]  Loss:  2.740053 (2.8624)  Time: 1.097s,  933.50/s  (1.106s,  925.95/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 500/1251 ( 40%)]  Loss:  3.123699 (2.8861)  Time: 1.098s,  932.67/s  (1.106s,  925.94/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 550/1251 ( 44%)]  Loss:  2.510472 (2.8548)  Time: 1.096s,  934.59/s  (1.106s,  925.77/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [ 600/1251 ( 48%)]  Loss:  2.684539 (2.8417)  Time: 1.100s,  930.65/s  (1.107s,  925.31/s)  LR: 3.561e-05  Data: 0.010 (0.012)
Train: 252 [ 650/1251 ( 52%)]  Loss:  2.943113 (2.8490)  Time: 1.098s,  932.31/s  (1.107s,  925.26/s)  LR: 3.561e-05  Data: 0.015 (0.012)
Train: 252 [ 700/1251 ( 56%)]  Loss:  2.978868 (2.8576)  Time: 1.102s,  929.64/s  (1.106s,  925.46/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 750/1251 ( 60%)]  Loss:  3.016598 (2.8676)  Time: 1.094s,  935.71/s  (1.107s,  925.35/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [ 800/1251 ( 64%)]  Loss:  2.845127 (2.8662)  Time: 1.102s,  929.05/s  (1.106s,  925.49/s)  LR: 3.561e-05  Data: 0.013 (0.012)
Train: 252 [ 850/1251 ( 68%)]  Loss:  2.822541 (2.8638)  Time: 1.098s,  932.46/s  (1.107s,  925.42/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 900/1251 ( 72%)]  Loss:  2.933237 (2.8675)  Time: 1.097s,  933.38/s  (1.107s,  925.01/s)  LR: 3.561e-05  Data: 0.012 (0.012)
Train: 252 [ 950/1251 ( 76%)]  Loss:  3.001773 (2.8742)  Time: 1.102s,  929.38/s  (1.107s,  924.97/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 252 [1000/1251 ( 80%)]  Loss:  3.017148 (2.8810)  Time: 1.098s,  932.95/s  (1.107s,  925.00/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [1050/1251 ( 84%)]  Loss:  2.848419 (2.8795)  Time: 1.097s,  933.11/s  (1.107s,  925.11/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [1100/1251 ( 88%)]  Loss:  2.831276 (2.8774)  Time: 1.098s,  932.88/s  (1.107s,  925.11/s)  LR: 3.561e-05  Data: 0.013 (0.012)
Train: 252 [1150/1251 ( 92%)]  Loss:  2.648151 (2.8679)  Time: 1.096s,  934.46/s  (1.107s,  925.07/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [1200/1251 ( 96%)]  Loss:  2.548383 (2.8551)  Time: 1.098s,  932.59/s  (1.107s,  925.21/s)  LR: 3.561e-05  Data: 0.011 (0.012)
Train: 252 [1250/1251 (100%)]  Loss:  2.784657 (2.8524)  Time: 1.103s,  928.76/s  (1.107s,  924.69/s)  LR: 3.561e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.255 (3.255)  Loss:  0.4151 (0.4151)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.412)  Loss:  0.5588 (0.8588)  Acc@1: 87.1462 (80.7740)  Acc@5: 97.5236 (95.2960)
Test (EMA): [   0/48]  Time: 3.047 (3.047)  Loss:  0.3985 (0.3985)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.406)  Loss:  0.5478 (0.8345)  Acc@1: 87.5000 (81.2340)  Acc@5: 97.7594 (95.5240)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 81.267999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 81.23999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 81.23399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 81.227999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 81.19199994873047)

Train: 253 [   0/1251 (  0%)]  Loss:  3.142176 (3.1422)  Time: 1.110s,  922.46/s  (1.110s,  922.46/s)  LR: 3.438e-05  Data: 0.027 (0.027)
Train: 253 [  50/1251 (  4%)]  Loss:  2.694807 (2.9185)  Time: 1.133s,  903.47/s  (1.111s,  921.41/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 100/1251 (  8%)]  Loss:  3.054925 (2.9640)  Time: 1.102s,  928.99/s  (1.114s,  919.08/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 150/1251 ( 12%)]  Loss:  2.723408 (2.9038)  Time: 1.095s,  935.37/s  (1.112s,  920.87/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 200/1251 ( 16%)]  Loss:  3.073372 (2.9377)  Time: 1.096s,  934.17/s  (1.111s,  922.00/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [ 250/1251 ( 20%)]  Loss:  2.703003 (2.8986)  Time: 1.197s,  855.33/s  (1.112s,  921.10/s)  LR: 3.438e-05  Data: 0.013 (0.012)
Train: 253 [ 300/1251 ( 24%)]  Loss:  3.212898 (2.9435)  Time: 1.095s,  934.76/s  (1.111s,  921.63/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 350/1251 ( 28%)]  Loss:  2.772531 (2.9221)  Time: 1.102s,  929.16/s  (1.110s,  922.63/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 400/1251 ( 32%)]  Loss:  2.622627 (2.8889)  Time: 1.098s,  932.87/s  (1.109s,  923.15/s)  LR: 3.438e-05  Data: 0.013 (0.012)
Train: 253 [ 450/1251 ( 36%)]  Loss:  3.077690 (2.9077)  Time: 1.098s,  932.61/s  (1.110s,  922.91/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [ 500/1251 ( 40%)]  Loss:  2.914712 (2.9084)  Time: 1.120s,  914.10/s  (1.109s,  923.58/s)  LR: 3.438e-05  Data: 0.010 (0.012)
Train: 253 [ 550/1251 ( 44%)]  Loss:  2.993150 (2.9154)  Time: 1.127s,  908.83/s  (1.109s,  923.07/s)  LR: 3.438e-05  Data: 0.014 (0.012)
Train: 253 [ 600/1251 ( 48%)]  Loss:  2.963900 (2.9192)  Time: 1.098s,  932.52/s  (1.109s,  923.30/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [ 650/1251 ( 52%)]  Loss:  3.196476 (2.9390)  Time: 1.093s,  936.68/s  (1.109s,  923.46/s)  LR: 3.438e-05  Data: 0.010 (0.012)
Train: 253 [ 700/1251 ( 56%)]  Loss:  2.942167 (2.9392)  Time: 1.098s,  932.47/s  (1.109s,  923.31/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 750/1251 ( 60%)]  Loss:  2.912071 (2.9375)  Time: 1.119s,  915.39/s  (1.109s,  923.33/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [ 800/1251 ( 64%)]  Loss:  2.965231 (2.9391)  Time: 1.097s,  933.70/s  (1.109s,  923.45/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [ 850/1251 ( 68%)]  Loss:  2.696682 (2.9257)  Time: 1.101s,  929.91/s  (1.109s,  923.60/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [ 900/1251 ( 72%)]  Loss:  3.161342 (2.9381)  Time: 1.100s,  931.24/s  (1.109s,  923.58/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [ 950/1251 ( 76%)]  Loss:  2.790854 (2.9307)  Time: 1.099s,  931.98/s  (1.109s,  923.60/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [1000/1251 ( 80%)]  Loss:  2.861091 (2.9274)  Time: 1.100s,  931.06/s  (1.108s,  923.80/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [1050/1251 ( 84%)]  Loss:  3.055455 (2.9332)  Time: 1.095s,  935.55/s  (1.108s,  923.86/s)  LR: 3.438e-05  Data: 0.010 (0.012)
Train: 253 [1100/1251 ( 88%)]  Loss:  2.867422 (2.9303)  Time: 1.095s,  935.24/s  (1.108s,  924.01/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [1150/1251 ( 92%)]  Loss:  2.839345 (2.9266)  Time: 1.119s,  915.40/s  (1.108s,  924.30/s)  LR: 3.438e-05  Data: 0.011 (0.012)
Train: 253 [1200/1251 ( 96%)]  Loss:  2.814937 (2.9221)  Time: 1.098s,  932.37/s  (1.108s,  924.20/s)  LR: 3.438e-05  Data: 0.012 (0.012)
Train: 253 [1250/1251 (100%)]  Loss:  2.812442 (2.9179)  Time: 1.083s,  945.19/s  (1.108s,  924.30/s)  LR: 3.438e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.235 (3.235)  Loss:  0.4069 (0.4069)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.401)  Loss:  0.5511 (0.8625)  Acc@1: 86.9104 (80.7380)  Acc@5: 98.1132 (95.2200)
Test (EMA): [   0/48]  Time: 3.150 (3.150)  Loss:  0.3983 (0.3983)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.402)  Loss:  0.5484 (0.8350)  Acc@1: 87.2642 (81.2140)  Acc@5: 97.7594 (95.4960)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 81.267999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 81.23999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 81.23399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 81.227999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 81.21400002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 81.194)

Train: 254 [   0/1251 (  0%)]  Loss:  2.839422 (2.8394)  Time: 1.105s,  927.04/s  (1.105s,  927.04/s)  LR: 3.316e-05  Data: 0.023 (0.023)
Train: 254 [  50/1251 (  4%)]  Loss:  2.666389 (2.7529)  Time: 1.098s,  932.25/s  (1.105s,  926.74/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 100/1251 (  8%)]  Loss:  2.981435 (2.8291)  Time: 1.129s,  907.11/s  (1.105s,  926.73/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 150/1251 ( 12%)]  Loss:  2.560140 (2.7618)  Time: 1.121s,  913.22/s  (1.108s,  923.98/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 200/1251 ( 16%)]  Loss:  2.936824 (2.7968)  Time: 1.095s,  935.15/s  (1.108s,  924.12/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 250/1251 ( 20%)]  Loss:  3.012714 (2.8328)  Time: 1.098s,  932.38/s  (1.109s,  923.20/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 300/1251 ( 24%)]  Loss:  3.047827 (2.8635)  Time: 1.098s,  932.49/s  (1.109s,  923.63/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 350/1251 ( 28%)]  Loss:  2.516497 (2.8202)  Time: 1.099s,  931.77/s  (1.110s,  922.64/s)  LR: 3.316e-05  Data: 0.013 (0.011)
Train: 254 [ 400/1251 ( 32%)]  Loss:  2.882698 (2.8271)  Time: 1.119s,  915.18/s  (1.111s,  922.10/s)  LR: 3.316e-05  Data: 0.010 (0.011)
Train: 254 [ 450/1251 ( 36%)]  Loss:  2.793677 (2.8238)  Time: 1.122s,  912.55/s  (1.110s,  922.65/s)  LR: 3.316e-05  Data: 0.013 (0.011)
Train: 254 [ 500/1251 ( 40%)]  Loss:  2.971916 (2.8372)  Time: 1.130s,  906.52/s  (1.110s,  922.51/s)  LR: 3.316e-05  Data: 0.013 (0.011)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 254 [ 550/1251 ( 44%)]  Loss:  2.811836 (2.8351)  Time: 1.101s,  930.01/s  (1.110s,  922.46/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 600/1251 ( 48%)]  Loss:  3.080894 (2.8540)  Time: 1.094s,  935.70/s  (1.111s,  922.08/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 650/1251 ( 52%)]  Loss:  2.821801 (2.8517)  Time: 1.099s,  932.18/s  (1.110s,  922.31/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 700/1251 ( 56%)]  Loss:  2.831250 (2.8504)  Time: 1.098s,  932.62/s  (1.110s,  922.13/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 750/1251 ( 60%)]  Loss:  2.887184 (2.8527)  Time: 1.122s,  912.85/s  (1.111s,  922.04/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 800/1251 ( 64%)]  Loss:  2.862664 (2.8532)  Time: 1.096s,  934.50/s  (1.110s,  922.17/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [ 850/1251 ( 68%)]  Loss:  2.797322 (2.8501)  Time: 1.098s,  932.34/s  (1.110s,  922.54/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 900/1251 ( 72%)]  Loss:  2.664487 (2.8404)  Time: 1.122s,  912.27/s  (1.110s,  922.51/s)  LR: 3.316e-05  Data: 0.012 (0.012)
Train: 254 [ 950/1251 ( 76%)]  Loss:  2.621460 (2.8294)  Time: 1.131s,  905.69/s  (1.110s,  922.45/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [1000/1251 ( 80%)]  Loss:  2.896861 (2.8326)  Time: 1.096s,  934.52/s  (1.110s,  922.67/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [1050/1251 ( 84%)]  Loss:  3.251066 (2.8517)  Time: 1.098s,  932.44/s  (1.110s,  922.87/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [1100/1251 ( 88%)]  Loss:  2.907350 (2.8541)  Time: 1.097s,  933.46/s  (1.110s,  922.89/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [1150/1251 ( 92%)]  Loss:  2.878298 (2.8551)  Time: 1.100s,  930.75/s  (1.110s,  922.76/s)  LR: 3.316e-05  Data: 0.011 (0.012)
Train: 254 [1200/1251 ( 96%)]  Loss:  2.788199 (2.8524)  Time: 1.119s,  915.00/s  (1.110s,  922.58/s)  LR: 3.316e-05  Data: 0.010 (0.012)
Train: 254 [1250/1251 (100%)]  Loss:  2.985817 (2.8575)  Time: 1.081s,  947.27/s  (1.110s,  922.73/s)  LR: 3.316e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.344 (3.344)  Loss:  0.4073 (0.4073)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.405)  Loss:  0.5879 (0.8678)  Acc@1: 87.2641 (80.7800)  Acc@5: 97.8774 (95.3300)
Test (EMA): [   0/48]  Time: 3.273 (3.273)  Loss:  0.3985 (0.3985)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5491 (0.8355)  Acc@1: 87.1462 (81.2240)  Acc@5: 97.7594 (95.5080)
Current checkpoints:
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 81.267999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 81.23999994873047)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 81.23399987060547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 81.227999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 81.22399997558594)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 81.221999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 81.219999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 81.21400002685547)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 81.203999921875)
 ('./output/train/20220303-190328-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 81.20000002685546)

Train: 255 [   0/1251 (  0%)]  Loss:  3.112189 (3.1122)  Time: 1.130s,  906.39/s  (1.130s,  906.39/s)  LR: 3.198e-05  Data: 0.023 (0.023)
Train: 255 [  50/1251 (  4%)]  Loss:  2.966478 (3.0393)  Time: 1.101s,  930.10/s  (1.106s,  925.45/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [ 100/1251 (  8%)]  Loss:  2.915447 (2.9980)  Time: 1.115s,  918.08/s  (1.107s,  924.89/s)  LR: 3.198e-05  Data: 0.010 (0.012)
Train: 255 [ 150/1251 ( 12%)]  Loss:  3.037677 (3.0079)  Time: 1.096s,  934.27/s  (1.107s,  924.86/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [ 200/1251 ( 16%)]  Loss:  2.873341 (2.9810)  Time: 1.100s,  930.58/s  (1.109s,  923.64/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [ 250/1251 ( 20%)]  Loss:  2.680367 (2.9309)  Time: 1.097s,  933.47/s  (1.110s,  922.55/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 300/1251 ( 24%)]  Loss:  2.616730 (2.8860)  Time: 1.100s,  930.71/s  (1.109s,  923.25/s)  LR: 3.198e-05  Data: 0.013 (0.012)
Train: 255 [ 350/1251 ( 28%)]  Loss:  3.096281 (2.9123)  Time: 1.149s,  891.46/s  (1.111s,  921.80/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [ 400/1251 ( 32%)]  Loss:  2.984416 (2.9203)  Time: 1.098s,  932.41/s  (1.110s,  922.23/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [ 450/1251 ( 36%)]  Loss:  2.992556 (2.9275)  Time: 1.097s,  933.70/s  (1.110s,  922.26/s)  LR: 3.198e-05  Data: 0.014 (0.012)
Train: 255 [ 500/1251 ( 40%)]  Loss:  2.792811 (2.9153)  Time: 1.100s,  931.17/s  (1.110s,  922.29/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 550/1251 ( 44%)]  Loss:  2.924936 (2.9161)  Time: 1.121s,  913.30/s  (1.110s,  922.13/s)  LR: 3.198e-05  Data: 0.010 (0.012)
Train: 255 [ 600/1251 ( 48%)]  Loss:  2.711722 (2.9004)  Time: 1.095s,  935.07/s  (1.110s,  922.15/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 650/1251 ( 52%)]  Loss:  3.159356 (2.9189)  Time: 1.103s,  928.76/s  (1.110s,  922.33/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [ 700/1251 ( 56%)]  Loss:  3.131526 (2.9331)  Time: 1.098s,  933.01/s  (1.110s,  922.22/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 750/1251 ( 60%)]  Loss:  3.154629 (2.9469)  Time: 1.097s,  933.68/s  (1.110s,  922.36/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 800/1251 ( 64%)]  Loss:  2.698260 (2.9323)  Time: 1.098s,  932.45/s  (1.110s,  922.90/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 850/1251 ( 68%)]  Loss:  2.977924 (2.9348)  Time: 1.099s,  931.50/s  (1.110s,  922.86/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 900/1251 ( 72%)]  Loss:  2.788737 (2.9271)  Time: 1.099s,  931.63/s  (1.110s,  922.83/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [ 950/1251 ( 76%)]  Loss:  2.715557 (2.9165)  Time: 1.095s,  935.57/s  (1.109s,  923.13/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [1000/1251 ( 80%)]  Loss:  2.817406 (2.9118)  Time: 1.105s,  926.81/s  (1.109s,  923.23/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [1050/1251 ( 84%)]  Loss:  2.805528 (2.9070)  Time: 1.097s,  933.17/s  (1.109s,  923.48/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [1100/1251 ( 88%)]  Loss:  2.931321 (2.9081)  Time: 1.097s,  933.76/s  (1.109s,  923.74/s)  LR: 3.198e-05  Data: 0.012 (0.012)
Train: 255 [1150/1251 ( 92%)]  Loss:  2.883657 (2.9070)  Time: 1.096s,  934.62/s  (1.109s,  923.64/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [1200/1251 ( 96%)]  Loss:  2.735375 (2.9002)  Time: 1.103s,  928.57/s  (1.109s,  923.71/s)  LR: 3.198e-05  Data: 0.011 (0.012)
Train: 255 [1250/1251 (100%)]  Loss:  3.025791 (2.9050)  Time: 1.169s,  875.94/s  (1.108s,  923.78/s)  LR: 3.198e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.272 (3.272)  Loss:  0.4089 (0.4089)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.411)  Loss:  0.5666 (0.8580)  Acc@1: 87.3821 (80.7380)  Acc@5: 97.9953 (95.3060)
Test (EMA): [   0/48]  Time: 3.059 (3.059)  Loss:  0.3982 (0.3982)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.230 (0.405)  Loss:  0.5499 (0.8360)  Acc@1: 87.1462 (81.1840)  Acc@5: 97.7594 (95.5160)
Train: 256 [   0/1251 (  0%)]  Loss:  3.056898 (3.0569)  Time: 1.107s,  924.76/s  (1.107s,  924.76/s)  LR: 3.081e-05  Data: 0.023 (0.023)
Train: 256 [  50/1251 (  4%)]  Loss:  2.825417 (2.9412)  Time: 1.096s,  934.25/s  (1.110s,  922.68/s)  LR: 3.081e-05  Data: 0.013 (0.012)
Train: 256 [ 100/1251 (  8%)]  Loss:  2.762565 (2.8816)  Time: 1.100s,  930.73/s  (1.107s,  925.24/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 256 [ 150/1251 ( 12%)]  Loss:  2.922608 (2.8919)  Time: 1.098s,  932.21/s  (1.106s,  925.56/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 200/1251 ( 16%)]  Loss:  2.546502 (2.8228)  Time: 1.097s,  933.21/s  (1.105s,  926.51/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 250/1251 ( 20%)]  Loss:  2.863802 (2.8296)  Time: 1.096s,  934.68/s  (1.107s,  925.27/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 300/1251 ( 24%)]  Loss:  3.092875 (2.8672)  Time: 1.100s,  930.53/s  (1.108s,  924.55/s)  LR: 3.081e-05  Data: 0.013 (0.012)
Train: 256 [ 350/1251 ( 28%)]  Loss:  2.868337 (2.8674)  Time: 1.121s,  913.81/s  (1.108s,  924.54/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [ 400/1251 ( 32%)]  Loss:  2.783574 (2.8581)  Time: 1.136s,  901.41/s  (1.109s,  923.34/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [ 450/1251 ( 36%)]  Loss:  2.761352 (2.8484)  Time: 1.098s,  932.85/s  (1.109s,  923.03/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 500/1251 ( 40%)]  Loss:  2.601726 (2.8260)  Time: 1.099s,  931.48/s  (1.109s,  923.32/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 550/1251 ( 44%)]  Loss:  2.934774 (2.8350)  Time: 1.101s,  929.97/s  (1.108s,  923.97/s)  LR: 3.081e-05  Data: 0.014 (0.012)
Train: 256 [ 600/1251 ( 48%)]  Loss:  2.629709 (2.8192)  Time: 1.105s,  926.95/s  (1.108s,  923.80/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [ 650/1251 ( 52%)]  Loss:  2.862826 (2.8224)  Time: 1.192s,  859.19/s  (1.109s,  923.66/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 700/1251 ( 56%)]  Loss:  2.613667 (2.8084)  Time: 1.098s,  932.20/s  (1.108s,  923.85/s)  LR: 3.081e-05  Data: 0.011 (0.012)
Train: 256 [ 750/1251 ( 60%)]  Loss:  2.790555 (2.8073)  Time: 1.096s,  934.40/s  (1.108s,  924.07/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [ 800/1251 ( 64%)]  Loss:  2.862562 (2.8106)  Time: 1.102s,  929.51/s  (1.108s,  924.03/s)  LR: 3.081e-05  Data: 0.010 (0.012)
Train: 256 [ 850/1251 ( 68%)]  Loss:  2.973768 (2.8196)  Time: 1.123s,  911.51/s  (1.108s,  924.02/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [ 900/1251 ( 72%)]  Loss:  2.692952 (2.8130)  Time: 1.099s,  931.59/s  (1.109s,  923.77/s)  LR: 3.081e-05  Data: 0.010 (0.012)
Train: 256 [ 950/1251 ( 76%)]  Loss:  2.811705 (2.8129)  Time: 1.097s,  933.24/s  (1.109s,  923.71/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [1000/1251 ( 80%)]  Loss:  2.833897 (2.8139)  Time: 1.097s,  933.40/s  (1.108s,  923.89/s)  LR: 3.081e-05  Data: 0.013 (0.012)
Train: 256 [1050/1251 ( 84%)]  Loss:  2.915206 (2.8185)  Time: 1.099s,  931.36/s  (1.108s,  924.02/s)  LR: 3.081e-05  Data: 0.013 (0.012)
Train: 256 [1100/1251 ( 88%)]  Loss:  2.963214 (2.8248)  Time: 1.095s,  934.82/s  (1.108s,  924.11/s)  LR: 3.081e-05  Data: 0.015 (0.012)
Train: 256 [1150/1251 ( 92%)]  Loss:  2.947623 (2.8299)  Time: 1.100s,  931.33/s  (1.108s,  923.92/s)  LR: 3.081e-05  Data: 0.016 (0.012)
Train: 256 [1200/1251 ( 96%)]  Loss:  2.957488 (2.8350)  Time: 1.098s,  932.90/s  (1.108s,  924.06/s)  LR: 3.081e-05  Data: 0.012 (0.012)
Train: 256 [1250/1251 (100%)]  Loss:  2.774696 (2.8327)  Time: 1.165s,  878.61/s  (1.109s,  923.58/s)  LR: 3.081e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.294 (3.294)  Loss:  0.4032 (0.4032)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.409)  Loss:  0.5544 (0.8610)  Acc@1: 87.6179 (80.7620)  Acc@5: 97.8774 (95.3480)
Test (EMA): [   0/48]  Time: 3.052 (3.052)  Loss:  0.3982 (0.3982)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.409)  Loss:  0.5505 (0.8365)  Acc@1: 87.1462 (81.1840)  Acc@5: 97.7594 (95.5280)
Train: 257 [   0/1251 (  0%)]  Loss:  2.836790 (2.8368)  Time: 1.102s,  928.90/s  (1.102s,  928.90/s)  LR: 2.967e-05  Data: 0.020 (0.020)
Train: 257 [  50/1251 (  4%)]  Loss:  2.722592 (2.7797)  Time: 1.096s,  934.19/s  (1.104s,  927.92/s)  LR: 2.967e-05  Data: 0.011 (0.012)
Train: 257 [ 100/1251 (  8%)]  Loss:  2.700069 (2.7532)  Time: 1.103s,  928.02/s  (1.106s,  925.72/s)  LR: 2.967e-05  Data: 0.011 (0.012)
Train: 257 [ 150/1251 ( 12%)]  Loss:  2.656012 (2.7289)  Time: 1.097s,  933.81/s  (1.105s,  926.34/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [ 200/1251 ( 16%)]  Loss:  3.062039 (2.7955)  Time: 1.096s,  933.89/s  (1.106s,  925.62/s)  LR: 2.967e-05  Data: 0.011 (0.012)
Train: 257 [ 250/1251 ( 20%)]  Loss:  3.062505 (2.8400)  Time: 1.105s,  926.79/s  (1.106s,  925.67/s)  LR: 2.967e-05  Data: 0.010 (0.012)
Train: 257 [ 300/1251 ( 24%)]  Loss:  3.022375 (2.8661)  Time: 1.097s,  933.50/s  (1.107s,  924.84/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [ 350/1251 ( 28%)]  Loss:  3.021081 (2.8854)  Time: 1.091s,  938.28/s  (1.108s,  924.23/s)  LR: 2.967e-05  Data: 0.011 (0.012)
Train: 257 [ 400/1251 ( 32%)]  Loss:  2.624337 (2.8564)  Time: 1.096s,  934.30/s  (1.108s,  924.31/s)  LR: 2.967e-05  Data: 0.011 (0.012)
Train: 257 [ 450/1251 ( 36%)]  Loss:  2.641944 (2.8350)  Time: 1.098s,  932.61/s  (1.107s,  924.69/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [ 500/1251 ( 40%)]  Loss:  2.946283 (2.8451)  Time: 1.123s,  912.02/s  (1.108s,  923.79/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [ 550/1251 ( 44%)]  Loss:  2.960383 (2.8547)  Time: 1.102s,  929.22/s  (1.109s,  923.17/s)  LR: 2.967e-05  Data: 0.016 (0.011)
Train: 257 [ 600/1251 ( 48%)]  Loss:  2.951959 (2.8622)  Time: 1.229s,  833.14/s  (1.110s,  922.78/s)  LR: 2.967e-05  Data: 0.010 (0.011)
Train: 257 [ 650/1251 ( 52%)]  Loss:  2.810530 (2.8585)  Time: 1.182s,  866.60/s  (1.110s,  922.48/s)  LR: 2.967e-05  Data: 0.011 (0.011)
Train: 257 [ 700/1251 ( 56%)]  Loss:  3.016169 (2.8690)  Time: 1.092s,  937.49/s  (1.110s,  922.60/s)  LR: 2.967e-05  Data: 0.010 (0.012)
Train: 257 [ 750/1251 ( 60%)]  Loss:  2.826837 (2.8664)  Time: 1.127s,  908.65/s  (1.110s,  922.82/s)  LR: 2.967e-05  Data: 0.017 (0.012)
Train: 257 [ 800/1251 ( 64%)]  Loss:  2.541735 (2.8473)  Time: 1.101s,  930.27/s  (1.110s,  922.44/s)  LR: 2.967e-05  Data: 0.011 (0.012)
Train: 257 [ 850/1251 ( 68%)]  Loss:  2.860643 (2.8480)  Time: 1.099s,  931.71/s  (1.110s,  922.47/s)  LR: 2.967e-05  Data: 0.013 (0.012)
Train: 257 [ 900/1251 ( 72%)]  Loss:  2.827392 (2.8469)  Time: 1.100s,  930.75/s  (1.110s,  922.28/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [ 950/1251 ( 76%)]  Loss:  2.937611 (2.8515)  Time: 1.098s,  932.70/s  (1.110s,  922.72/s)  LR: 2.967e-05  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 257 [1000/1251 ( 80%)]  Loss:  3.069534 (2.8618)  Time: 1.128s,  907.49/s  (1.110s,  922.85/s)  LR: 2.967e-05  Data: 0.010 (0.012)
Train: 257 [1050/1251 ( 84%)]  Loss:  2.887896 (2.8630)  Time: 1.105s,  926.92/s  (1.110s,  922.90/s)  LR: 2.967e-05  Data: 0.013 (0.012)
Train: 257 [1100/1251 ( 88%)]  Loss:  2.896912 (2.8645)  Time: 1.097s,  933.70/s  (1.109s,  923.18/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [1150/1251 ( 92%)]  Loss:  2.775072 (2.8608)  Time: 1.099s,  932.09/s  (1.109s,  923.09/s)  LR: 2.967e-05  Data: 0.012 (0.012)
Train: 257 [1200/1251 ( 96%)]  Loss:  2.752228 (2.8564)  Time: 1.124s,  910.67/s  (1.109s,  923.19/s)  LR: 2.967e-05  Data: 0.010 (0.012)
Train: 257 [1250/1251 (100%)]  Loss:  2.837928 (2.8557)  Time: 1.090s,  939.30/s  (1.109s,  923.12/s)  LR: 2.967e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.271 (3.271)  Loss:  0.4129 (0.4129)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.400)  Loss:  0.5597 (0.8623)  Acc@1: 87.3821 (80.8240)  Acc@5: 97.8774 (95.3060)
Test (EMA): [   0/48]  Time: 3.273 (3.273)  Loss:  0.3983 (0.3983)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5511 (0.8370)  Acc@1: 87.2642 (81.1860)  Acc@5: 97.7594 (95.5000)
Train: 258 [   0/1251 (  0%)]  Loss:  3.036252 (3.0363)  Time: 1.101s,  930.28/s  (1.101s,  930.28/s)  LR: 2.856e-05  Data: 0.020 (0.020)
Train: 258 [  50/1251 (  4%)]  Loss:  2.738150 (2.8872)  Time: 1.210s,  846.04/s  (1.103s,  928.58/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 100/1251 (  8%)]  Loss:  2.745462 (2.8400)  Time: 1.098s,  932.49/s  (1.111s,  921.54/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 150/1251 ( 12%)]  Loss:  2.787540 (2.8269)  Time: 1.106s,  925.79/s  (1.113s,  920.23/s)  LR: 2.856e-05  Data: 0.013 (0.012)
Train: 258 [ 200/1251 ( 16%)]  Loss:  3.049181 (2.8713)  Time: 1.101s,  930.04/s  (1.116s,  917.81/s)  LR: 2.856e-05  Data: 0.017 (0.012)
Train: 258 [ 250/1251 ( 20%)]  Loss:  3.013820 (2.8951)  Time: 1.097s,  933.21/s  (1.114s,  919.35/s)  LR: 2.856e-05  Data: 0.014 (0.012)
Train: 258 [ 300/1251 ( 24%)]  Loss:  2.773649 (2.8777)  Time: 1.096s,  934.38/s  (1.114s,  919.31/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 350/1251 ( 28%)]  Loss:  2.810546 (2.8693)  Time: 1.102s,  929.46/s  (1.112s,  920.75/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 400/1251 ( 32%)]  Loss:  3.034740 (2.8877)  Time: 1.094s,  935.92/s  (1.113s,  920.36/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 450/1251 ( 36%)]  Loss:  3.002558 (2.8992)  Time: 1.095s,  934.83/s  (1.112s,  921.16/s)  LR: 2.856e-05  Data: 0.012 (0.012)
Train: 258 [ 500/1251 ( 40%)]  Loss:  2.783303 (2.8887)  Time: 1.096s,  933.89/s  (1.111s,  922.02/s)  LR: 2.856e-05  Data: 0.012 (0.012)
Train: 258 [ 550/1251 ( 44%)]  Loss:  2.924237 (2.8916)  Time: 1.182s,  866.30/s  (1.111s,  921.61/s)  LR: 2.856e-05  Data: 0.010 (0.012)
Train: 258 [ 600/1251 ( 48%)]  Loss:  2.920960 (2.8939)  Time: 1.099s,  931.64/s  (1.111s,  921.35/s)  LR: 2.856e-05  Data: 0.012 (0.012)
Train: 258 [ 650/1251 ( 52%)]  Loss:  2.794860 (2.8868)  Time: 1.104s,  927.67/s  (1.111s,  921.61/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 700/1251 ( 56%)]  Loss:  2.813726 (2.8819)  Time: 1.100s,  931.04/s  (1.110s,  922.23/s)  LR: 2.856e-05  Data: 0.017 (0.012)
Train: 258 [ 750/1251 ( 60%)]  Loss:  3.004551 (2.8896)  Time: 1.098s,  932.78/s  (1.110s,  922.52/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 800/1251 ( 64%)]  Loss:  3.125162 (2.9035)  Time: 1.097s,  933.56/s  (1.110s,  922.66/s)  LR: 2.856e-05  Data: 0.013 (0.012)
Train: 258 [ 850/1251 ( 68%)]  Loss:  2.686301 (2.8914)  Time: 1.100s,  930.66/s  (1.110s,  922.86/s)  LR: 2.856e-05  Data: 0.012 (0.012)
Train: 258 [ 900/1251 ( 72%)]  Loss:  2.989921 (2.8966)  Time: 1.100s,  931.26/s  (1.109s,  923.00/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [ 950/1251 ( 76%)]  Loss:  3.180859 (2.9108)  Time: 1.122s,  912.71/s  (1.109s,  923.12/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [1000/1251 ( 80%)]  Loss:  3.212327 (2.9251)  Time: 1.097s,  933.50/s  (1.109s,  923.30/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [1050/1251 ( 84%)]  Loss:  2.748170 (2.9171)  Time: 1.097s,  933.42/s  (1.109s,  923.16/s)  LR: 2.856e-05  Data: 0.013 (0.012)
Train: 258 [1100/1251 ( 88%)]  Loss:  2.940934 (2.9181)  Time: 1.091s,  938.87/s  (1.109s,  923.11/s)  LR: 2.856e-05  Data: 0.010 (0.012)
Train: 258 [1150/1251 ( 92%)]  Loss:  3.098686 (2.9257)  Time: 1.105s,  927.10/s  (1.109s,  923.29/s)  LR: 2.856e-05  Data: 0.011 (0.012)
Train: 258 [1200/1251 ( 96%)]  Loss:  2.699340 (2.9166)  Time: 1.098s,  932.78/s  (1.109s,  923.52/s)  LR: 2.856e-05  Data: 0.014 (0.012)
Train: 258 [1250/1251 (100%)]  Loss:  2.938472 (2.9175)  Time: 1.080s,  948.07/s  (1.109s,  923.49/s)  LR: 2.856e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 3.324 (3.324)  Loss:  0.4193 (0.4193)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.408)  Loss:  0.5725 (0.8739)  Acc@1: 86.9104 (80.8960)  Acc@5: 97.8774 (95.3040)
Test (EMA): [   0/48]  Time: 3.237 (3.237)  Loss:  0.3989 (0.3989)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.5352 (98.5352)
Test (EMA): [  48/48]  Time: 0.229 (0.408)  Loss:  0.5516 (0.8376)  Acc@1: 87.2642 (81.1880)  Acc@5: 97.7594 (95.5000)
Train: 259 [   0/1251 (  0%)]  Loss:  2.634498 (2.6345)  Time: 1.108s,  924.08/s  (1.108s,  924.08/s)  LR: 2.746e-05  Data: 0.024 (0.024)
Train: 259 [  50/1251 (  4%)]  Loss:  2.953938 (2.7942)  Time: 1.096s,  934.66/s  (1.109s,  923.35/s)  LR: 2.746e-05  Data: 0.011 (0.012)
Train: 259 [ 100/1251 (  8%)]  Loss:  2.779468 (2.7893)  Time: 1.098s,  932.70/s  (1.108s,  924.27/s)  LR: 2.746e-05  Data: 0.011 (0.012)
Train: 259 [ 150/1251 ( 12%)]  Loss:  2.776900 (2.7862)  Time: 1.103s,  928.73/s  (1.107s,  924.87/s)  LR: 2.746e-05  Data: 0.011 (0.012)
Train: 259 [ 200/1251 ( 16%)]  Loss:  2.915866 (2.8121)  Time: 1.099s,  931.58/s  (1.106s,  925.58/s)  LR: 2.746e-05  Data: 0.011 (0.012)
Train: 259 [ 250/1251 ( 20%)]  Loss:  2.623354 (2.7807)  Time: 1.101s,  930.41/s  (1.108s,  924.00/s)  LR: 2.746e-05  Data: 0.011 (0.012)
Train: 259 [ 300/1251 ( 24%)]  Loss:  2.838622 (2.7889)  Time: 1.096s,  934.60/s  (1.107s,  924.77/s)  LR: 2.746e-05  Data: 0.012 (0.012)
Train: 259 [ 350/1251 ( 28%)]  Loss:  2.925794 (2.8061)  Time: 1.097s,  933.72/s  (1.107s,  925.35/s)  LR: 2.746e-05  Data: 0.012 (0.012)
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16821 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16822 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16823 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16824 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16826 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16828 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16829 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -15) local_rank: 0 (pid: 16820) of binary: /home/kkahatapitiy/anaconda3/envs/kkenv3/bin/python3
Traceback (most recent call last):
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
************************************************
                train.py FAILED                 
================================================
Root Cause:
[0]:
  time: 2022-03-07_21:57:13
  rank: 0 (local_rank: 0)
  exitcode: -15 (pid: 16820)
  error_file: <N/A>
  msg: Signal 15 (SIGTERM) received by PID 16820
================================================
Other Failures:
  <NO_OTHER_FAILURES>
************************************************

