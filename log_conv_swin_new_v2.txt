/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 2
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Model swin_tiny_patch4_window7_224 created, param count:27110704
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Using NVIDIA APEX AMP. Training in mixed precision.
Using NVIDIA APEX DistributedDataParallel.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Scheduled epochs: 310
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Train: 0 [   0/1251 (  0%)]  Loss:  7.009159 (7.0092)  Time: 14.335s,   71.43/s  (14.335s,   71.43/s)  LR: 1.000e-06  Data: 4.824 (4.824)
Train: 0 [  50/1251 (  4%)]  Loss:  6.970273 (6.9897)  Time: 1.080s,  948.37/s  (1.359s,  753.70/s)  LR: 1.000e-06  Data: 0.014 (0.108)
Train: 0 [ 100/1251 (  8%)]  Loss:  6.951767 (6.9771)  Time: 1.099s,  931.44/s  (1.225s,  835.63/s)  LR: 1.000e-06  Data: 0.012 (0.061)
Train: 0 [ 150/1251 ( 12%)]  Loss:  6.954562 (6.9714)  Time: 1.104s,  927.31/s  (1.180s,  867.83/s)  LR: 1.000e-06  Data: 0.011 (0.045)
Train: 0 [ 200/1251 ( 16%)]  Loss:  6.954917 (6.9681)  Time: 1.077s,  950.71/s  (1.157s,  885.25/s)  LR: 1.000e-06  Data: 0.013 (0.037)
Train: 0 [ 250/1251 ( 20%)]  Loss:  6.951443 (6.9654)  Time: 1.078s,  949.51/s  (1.143s,  895.82/s)  LR: 1.000e-06  Data: 0.013 (0.032)
Train: 0 [ 300/1251 ( 24%)]  Loss:  6.948601 (6.9630)  Time: 1.084s,  944.47/s  (1.134s,  902.97/s)  LR: 1.000e-06  Data: 0.012 (0.029)
Train: 0 [ 350/1251 ( 28%)]  Loss:  6.935634 (6.9595)  Time: 1.096s,  933.95/s  (1.128s,  908.04/s)  LR: 1.000e-06  Data: 0.013 (0.027)
Train: 0 [ 400/1251 ( 32%)]  Loss:  6.931929 (6.9565)  Time: 1.106s,  926.19/s  (1.123s,  911.58/s)  LR: 1.000e-06  Data: 0.018 (0.025)
Train: 0 [ 450/1251 ( 36%)]  Loss:  6.932549 (6.9541)  Time: 1.080s,  947.77/s  (1.119s,  914.93/s)  LR: 1.000e-06  Data: 0.012 (0.024)
Train: 0 [ 500/1251 ( 40%)]  Loss:  6.923974 (6.9513)  Time: 1.078s,  950.22/s  (1.116s,  917.67/s)  LR: 1.000e-06  Data: 0.013 (0.023)
Train: 0 [ 550/1251 ( 44%)]  Loss:  6.929062 (6.9495)  Time: 1.080s,  948.02/s  (1.114s,  919.58/s)  LR: 1.000e-06  Data: 0.012 (0.022)
Train: 0 [ 600/1251 ( 48%)]  Loss:  6.927045 (6.9478)  Time: 1.076s,  952.09/s  (1.111s,  921.50/s)  LR: 1.000e-06  Data: 0.012 (0.021)
Train: 0 [ 650/1251 ( 52%)]  Loss:  6.924636 (6.9461)  Time: 1.101s,  929.82/s  (1.109s,  923.08/s)  LR: 1.000e-06  Data: 0.012 (0.021)
Train: 0 [ 700/1251 ( 56%)]  Loss:  6.921068 (6.9444)  Time: 1.080s,  948.45/s  (1.108s,  924.02/s)  LR: 1.000e-06  Data: 0.012 (0.020)
Train: 0 [ 750/1251 ( 60%)]  Loss:  6.918362 (6.9428)  Time: 1.103s,  928.69/s  (1.107s,  925.11/s)  LR: 1.000e-06  Data: 0.015 (0.020)
Train: 0 [ 800/1251 ( 64%)]  Loss:  6.915516 (6.9412)  Time: 1.089s,  940.45/s  (1.105s,  926.28/s)  LR: 1.000e-06  Data: 0.014 (0.019)
Train: 0 [ 850/1251 ( 68%)]  Loss:  6.914868 (6.9397)  Time: 1.083s,  945.80/s  (1.104s,  927.37/s)  LR: 1.000e-06  Data: 0.012 (0.019)
Train: 0 [ 900/1251 ( 72%)]  Loss:  6.923414 (6.9389)  Time: 1.078s,  949.63/s  (1.103s,  928.14/s)  LR: 1.000e-06  Data: 0.013 (0.018)
Train: 0 [ 950/1251 ( 76%)]  Loss:  6.924970 (6.9382)  Time: 1.078s,  950.23/s  (1.102s,  929.08/s)  LR: 1.000e-06  Data: 0.013 (0.018)
Train: 0 [1000/1251 ( 80%)]  Loss:  6.922482 (6.9374)  Time: 1.081s,  947.28/s  (1.101s,  929.86/s)  LR: 1.000e-06  Data: 0.015 (0.018)
Train: 0 [1050/1251 ( 84%)]  Loss:  6.917595 (6.9365)  Time: 1.081s,  947.57/s  (1.100s,  930.49/s)  LR: 1.000e-06  Data: 0.012 (0.018)
Train: 0 [1100/1251 ( 88%)]  Loss:  6.910913 (6.9354)  Time: 1.100s,  930.51/s  (1.100s,  930.74/s)  LR: 1.000e-06  Data: 0.012 (0.017)
Train: 0 [1150/1251 ( 92%)]  Loss:  6.909927 (6.9344)  Time: 1.078s,  949.82/s  (1.100s,  930.76/s)  LR: 1.000e-06  Data: 0.013 (0.017)
Train: 0 [1200/1251 ( 96%)]  Loss:  6.908263 (6.9333)  Time: 1.101s,  930.07/s  (1.100s,  931.16/s)  LR: 1.000e-06  Data: 0.014 (0.017)
Train: 0 [1250/1251 (100%)]  Loss:  6.919731 (6.9328)  Time: 1.078s,  950.34/s  (1.099s,  931.45/s)  LR: 1.000e-06  Data: 0.000 (0.017)
Test: [   0/48]  Time: 8.252 (8.252)  Loss:  6.8268 (6.8268)  Acc@1:  0.0977 ( 0.0977)  Acc@5:  1.6602 ( 1.6602)
Test: [  48/48]  Time: 2.336 (0.531)  Loss:  6.8202 (6.8805)  Acc@1:  0.0000 ( 0.2200)  Acc@5:  0.8255 ( 0.9960)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 1 [   0/1251 (  0%)]  Loss:  6.907842 (6.9078)  Time: 1.132s,  904.36/s  (1.132s,  904.36/s)  LR: 5.095e-05  Data: 0.041 (0.041)
Train: 1 [  50/1251 (  4%)]  Loss:  6.896815 (6.9023)  Time: 1.095s,  935.45/s  (1.094s,  935.96/s)  LR: 5.095e-05  Data: 0.012 (0.014)
Train: 1 [ 100/1251 (  8%)]  Loss:  6.891252 (6.8986)  Time: 1.076s,  951.30/s  (1.093s,  937.19/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [ 150/1251 ( 12%)]  Loss:  6.862867 (6.8897)  Time: 1.083s,  945.45/s  (1.091s,  938.61/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [ 200/1251 ( 16%)]  Loss:  6.828759 (6.8775)  Time: 1.094s,  935.91/s  (1.090s,  939.65/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [ 250/1251 ( 20%)]  Loss:  6.830787 (6.8697)  Time: 1.166s,  878.28/s  (1.092s,  937.83/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 1 [ 300/1251 ( 24%)]  Loss:  6.832535 (6.8644)  Time: 1.082s,  946.59/s  (1.091s,  938.68/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [ 350/1251 ( 28%)]  Loss:  6.804867 (6.8570)  Time: 1.076s,  951.29/s  (1.090s,  939.71/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [ 400/1251 ( 32%)]  Loss:  6.796515 (6.8502)  Time: 1.077s,  950.92/s  (1.090s,  939.63/s)  LR: 5.095e-05  Data: 0.011 (0.013)
Train: 1 [ 450/1251 ( 36%)]  Loss:  6.746512 (6.8399)  Time: 1.079s,  949.16/s  (1.090s,  939.35/s)  LR: 5.095e-05  Data: 0.015 (0.013)
Train: 1 [ 500/1251 ( 40%)]  Loss:  6.756723 (6.8323)  Time: 1.098s,  932.57/s  (1.091s,  938.99/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [ 550/1251 ( 44%)]  Loss:  6.726631 (6.8235)  Time: 1.097s,  933.25/s  (1.090s,  939.14/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [ 600/1251 ( 48%)]  Loss:  6.725993 (6.8160)  Time: 1.075s,  952.70/s  (1.090s,  939.26/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [ 650/1251 ( 52%)]  Loss:  6.738335 (6.8105)  Time: 1.077s,  950.58/s  (1.090s,  939.18/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [ 700/1251 ( 56%)]  Loss:  6.716228 (6.8042)  Time: 1.087s,  942.07/s  (1.090s,  939.06/s)  LR: 5.095e-05  Data: 0.015 (0.013)
Train: 1 [ 750/1251 ( 60%)]  Loss:  6.674165 (6.7961)  Time: 1.075s,  952.52/s  (1.090s,  939.37/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [ 800/1251 ( 64%)]  Loss:  6.659480 (6.7880)  Time: 1.083s,  945.86/s  (1.090s,  939.37/s)  LR: 5.095e-05  Data: 0.016 (0.013)
Train: 1 [ 850/1251 ( 68%)]  Loss:  6.662674 (6.7811)  Time: 1.097s,  933.74/s  (1.090s,  939.27/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [ 900/1251 ( 72%)]  Loss:  6.661761 (6.7748)  Time: 1.076s,  951.61/s  (1.090s,  939.54/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [ 950/1251 ( 76%)]  Loss:  6.713840 (6.7717)  Time: 1.180s,  867.88/s  (1.090s,  939.43/s)  LR: 5.095e-05  Data: 0.015 (0.013)
Train: 1 [1000/1251 ( 80%)]  Loss:  6.643600 (6.7656)  Time: 1.114s,  919.43/s  (1.090s,  939.47/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [1050/1251 ( 84%)]  Loss:  6.675099 (6.7615)  Time: 1.085s,  943.96/s  (1.090s,  939.49/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [1100/1251 ( 88%)]  Loss:  6.649285 (6.7566)  Time: 1.085s,  943.49/s  (1.090s,  939.62/s)  LR: 5.095e-05  Data: 0.013 (0.013)
Train: 1 [1150/1251 ( 92%)]  Loss:  6.617641 (6.7508)  Time: 1.096s,  934.21/s  (1.090s,  939.60/s)  LR: 5.095e-05  Data: 0.012 (0.013)
Train: 1 [1200/1251 ( 96%)]  Loss:  6.620249 (6.7456)  Time: 1.104s,  927.37/s  (1.090s,  939.39/s)  LR: 5.095e-05  Data: 0.014 (0.013)
Train: 1 [1250/1251 (100%)]  Loss:  6.664131 (6.7425)  Time: 1.077s,  950.52/s  (1.090s,  939.29/s)  LR: 5.095e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.738 (5.738)  Loss:  5.8954 (5.8954)  Acc@1:  1.9531 ( 1.9531)  Acc@5: 12.8906 (12.8906)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  5.4521 (6.0914)  Acc@1: 14.5047 ( 2.9900)  Acc@5: 22.1698 ( 9.6900)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 2 [   0/1251 (  0%)]  Loss:  6.587912 (6.5879)  Time: 1.098s,  932.51/s  (1.098s,  932.51/s)  LR: 1.009e-04  Data: 0.035 (0.035)
Train: 2 [  50/1251 (  4%)]  Loss:  6.624573 (6.6062)  Time: 1.099s,  931.87/s  (1.092s,  937.38/s)  LR: 1.009e-04  Data: 0.015 (0.014)
Train: 2 [ 100/1251 (  8%)]  Loss:  6.537221 (6.5832)  Time: 1.094s,  935.94/s  (1.094s,  936.35/s)  LR: 1.009e-04  Data: 0.012 (0.014)
Train: 2 [ 150/1251 ( 12%)]  Loss:  6.596945 (6.5867)  Time: 1.075s,  952.60/s  (1.093s,  937.09/s)  LR: 1.009e-04  Data: 0.011 (0.014)
Train: 2 [ 200/1251 ( 16%)]  Loss:  6.535676 (6.5765)  Time: 1.082s,  946.16/s  (1.092s,  937.95/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 250/1251 ( 20%)]  Loss:  6.621840 (6.5840)  Time: 1.100s,  930.61/s  (1.092s,  937.65/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 300/1251 ( 24%)]  Loss:  6.569441 (6.5819)  Time: 1.104s,  927.28/s  (1.092s,  937.72/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 350/1251 ( 28%)]  Loss:  6.575356 (6.5811)  Time: 1.076s,  951.44/s  (1.092s,  937.70/s)  LR: 1.009e-04  Data: 0.014 (0.013)
Train: 2 [ 400/1251 ( 32%)]  Loss:  6.499982 (6.5721)  Time: 1.096s,  934.46/s  (1.093s,  937.06/s)  LR: 1.009e-04  Data: 0.014 (0.013)
Train: 2 [ 450/1251 ( 36%)]  Loss:  6.538085 (6.5687)  Time: 1.094s,  936.17/s  (1.093s,  936.57/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 500/1251 ( 40%)]  Loss:  6.412903 (6.5545)  Time: 1.082s,  946.47/s  (1.093s,  937.07/s)  LR: 1.009e-04  Data: 0.013 (0.013)
Train: 2 [ 550/1251 ( 44%)]  Loss:  6.461619 (6.5468)  Time: 1.098s,  932.55/s  (1.092s,  937.42/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 600/1251 ( 48%)]  Loss:  6.564677 (6.5482)  Time: 1.078s,  949.78/s  (1.092s,  937.47/s)  LR: 1.009e-04  Data: 0.015 (0.013)
Train: 2 [ 650/1251 ( 52%)]  Loss:  6.555504 (6.5487)  Time: 1.115s,  918.56/s  (1.092s,  937.78/s)  LR: 1.009e-04  Data: 0.011 (0.013)
Train: 2 [ 700/1251 ( 56%)]  Loss:  6.406674 (6.5392)  Time: 1.091s,  938.94/s  (1.092s,  937.96/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 750/1251 ( 60%)]  Loss:  6.398449 (6.5304)  Time: 1.094s,  935.74/s  (1.092s,  937.89/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 800/1251 ( 64%)]  Loss:  6.482837 (6.5276)  Time: 1.075s,  952.98/s  (1.092s,  937.83/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 850/1251 ( 68%)]  Loss:  6.468030 (6.5243)  Time: 1.082s,  946.37/s  (1.092s,  937.96/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [ 900/1251 ( 72%)]  Loss:  6.491298 (6.5226)  Time: 1.101s,  929.99/s  (1.092s,  937.87/s)  LR: 1.009e-04  Data: 0.015 (0.013)
Train: 2 [ 950/1251 ( 76%)]  Loss:  6.537653 (6.5233)  Time: 1.094s,  935.76/s  (1.092s,  938.01/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [1000/1251 ( 80%)]  Loss:  6.485712 (6.5215)  Time: 1.078s,  950.03/s  (1.091s,  938.28/s)  LR: 1.009e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 2 [1050/1251 ( 84%)]  Loss:  6.432339 (6.5175)  Time: 1.084s,  944.97/s  (1.091s,  938.57/s)  LR: 1.009e-04  Data: 0.016 (0.013)
Train: 2 [1100/1251 ( 88%)]  Loss:  6.419532 (6.5132)  Time: 1.093s,  937.03/s  (1.091s,  938.84/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [1150/1251 ( 92%)]  Loss:  6.313908 (6.5049)  Time: 1.080s,  948.21/s  (1.090s,  939.08/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [1200/1251 ( 96%)]  Loss:  6.264334 (6.4953)  Time: 1.076s,  951.50/s  (1.090s,  939.02/s)  LR: 1.009e-04  Data: 0.012 (0.013)
Train: 2 [1250/1251 (100%)]  Loss:  6.412767 (6.4921)  Time: 1.080s,  948.31/s  (1.091s,  938.93/s)  LR: 1.009e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.710 (5.710)  Loss:  5.1248 (5.1248)  Acc@1:  9.4727 ( 9.4727)  Acc@5: 29.9805 (29.9805)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  4.6721 (5.3477)  Acc@1: 24.0566 ( 8.1740)  Acc@5: 39.1509 (21.6420)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 3 [   0/1251 (  0%)]  Loss:  6.362627 (6.3626)  Time: 1.095s,  935.28/s  (1.095s,  935.28/s)  LR: 1.509e-04  Data: 0.031 (0.031)
Train: 3 [  50/1251 (  4%)]  Loss:  6.347967 (6.3553)  Time: 1.078s,  949.90/s  (1.091s,  938.94/s)  LR: 1.509e-04  Data: 0.014 (0.013)
Train: 3 [ 100/1251 (  8%)]  Loss:  6.385546 (6.3654)  Time: 1.083s,  945.92/s  (1.091s,  938.25/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [ 150/1251 ( 12%)]  Loss:  6.436424 (6.3831)  Time: 1.075s,  952.27/s  (1.091s,  938.90/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [ 200/1251 ( 16%)]  Loss:  6.247054 (6.3559)  Time: 1.086s,  942.66/s  (1.089s,  940.57/s)  LR: 1.509e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 3 [ 250/1251 ( 20%)]  Loss:  6.328655 (6.3514)  Time: 1.095s,  934.90/s  (1.089s,  940.56/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [ 300/1251 ( 24%)]  Loss:  6.299104 (6.3439)  Time: 1.094s,  936.35/s  (1.089s,  940.08/s)  LR: 1.509e-04  Data: 0.011 (0.013)
Train: 3 [ 350/1251 ( 28%)]  Loss:  6.261255 (6.3336)  Time: 1.095s,  935.05/s  (1.089s,  940.31/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [ 400/1251 ( 32%)]  Loss:  6.462595 (6.3479)  Time: 1.079s,  948.85/s  (1.089s,  940.38/s)  LR: 1.509e-04  Data: 0.014 (0.013)
Train: 3 [ 450/1251 ( 36%)]  Loss:  6.258461 (6.3390)  Time: 1.079s,  949.36/s  (1.089s,  940.67/s)  LR: 1.509e-04  Data: 0.015 (0.013)
Train: 3 [ 500/1251 ( 40%)]  Loss:  6.088877 (6.3162)  Time: 1.076s,  951.45/s  (1.089s,  940.31/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [ 550/1251 ( 44%)]  Loss:  6.209082 (6.3073)  Time: 1.078s,  949.72/s  (1.089s,  940.24/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [ 600/1251 ( 48%)]  Loss:  6.285301 (6.3056)  Time: 1.079s,  948.92/s  (1.089s,  939.97/s)  LR: 1.509e-04  Data: 0.018 (0.013)
Train: 3 [ 650/1251 ( 52%)]  Loss:  6.266402 (6.3028)  Time: 1.103s,  928.69/s  (1.089s,  940.20/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [ 700/1251 ( 56%)]  Loss:  6.039896 (6.2853)  Time: 1.097s,  933.77/s  (1.089s,  940.11/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [ 750/1251 ( 60%)]  Loss:  6.138728 (6.2761)  Time: 1.078s,  949.59/s  (1.089s,  940.11/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [ 800/1251 ( 64%)]  Loss:  6.067909 (6.2639)  Time: 1.091s,  938.49/s  (1.089s,  939.98/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [ 850/1251 ( 68%)]  Loss:  6.112192 (6.2554)  Time: 1.077s,  950.39/s  (1.090s,  939.68/s)  LR: 1.509e-04  Data: 0.014 (0.013)
Train: 3 [ 900/1251 ( 72%)]  Loss:  6.018929 (6.2430)  Time: 1.098s,  932.66/s  (1.090s,  939.61/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [ 950/1251 ( 76%)]  Loss:  6.244358 (6.2431)  Time: 1.089s,  940.32/s  (1.090s,  939.54/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [1000/1251 ( 80%)]  Loss:  6.187922 (6.2404)  Time: 1.082s,  946.27/s  (1.090s,  939.60/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [1050/1251 ( 84%)]  Loss:  6.139189 (6.2358)  Time: 1.083s,  945.15/s  (1.090s,  939.61/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [1100/1251 ( 88%)]  Loss:  6.018342 (6.2264)  Time: 1.097s,  933.05/s  (1.090s,  939.67/s)  LR: 1.509e-04  Data: 0.014 (0.013)
Train: 3 [1150/1251 ( 92%)]  Loss:  6.022945 (6.2179)  Time: 1.100s,  930.55/s  (1.090s,  939.60/s)  LR: 1.509e-04  Data: 0.012 (0.013)
Train: 3 [1200/1251 ( 96%)]  Loss:  5.934279 (6.2066)  Time: 1.096s,  934.14/s  (1.090s,  939.71/s)  LR: 1.509e-04  Data: 0.013 (0.013)
Train: 3 [1250/1251 (100%)]  Loss:  6.043305 (6.2003)  Time: 1.065s,  961.49/s  (1.090s,  939.63/s)  LR: 1.509e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 6.020 (6.020)  Loss:  3.9734 (3.9734)  Acc@1: 23.9258 (23.9258)  Acc@5: 51.0742 (51.0742)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  3.5235 (4.5770)  Acc@1: 38.2075 (15.8600)  Acc@5: 56.9575 (35.3560)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 4 [   0/1251 (  0%)]  Loss:  5.887882 (5.8879)  Time: 1.108s,  923.89/s  (1.108s,  923.89/s)  LR: 2.008e-04  Data: 0.029 (0.029)
Train: 4 [  50/1251 (  4%)]  Loss:  6.027372 (5.9576)  Time: 1.109s,  923.37/s  (1.093s,  937.01/s)  LR: 2.008e-04  Data: 0.013 (0.014)
Train: 4 [ 100/1251 (  8%)]  Loss:  6.000010 (5.9718)  Time: 1.175s,  871.56/s  (1.096s,  934.24/s)  LR: 2.008e-04  Data: 0.013 (0.013)
Train: 4 [ 150/1251 ( 12%)]  Loss:  6.054413 (5.9924)  Time: 1.081s,  947.57/s  (1.094s,  936.42/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 200/1251 ( 16%)]  Loss:  6.062151 (6.0064)  Time: 1.110s,  922.35/s  (1.094s,  936.25/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 250/1251 ( 20%)]  Loss:  6.015501 (6.0079)  Time: 1.093s,  937.09/s  (1.094s,  936.33/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 300/1251 ( 24%)]  Loss:  6.027053 (6.0106)  Time: 1.081s,  946.86/s  (1.093s,  936.93/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 350/1251 ( 28%)]  Loss:  6.118710 (6.0241)  Time: 1.090s,  939.32/s  (1.093s,  936.73/s)  LR: 2.008e-04  Data: 0.013 (0.013)
Train: 4 [ 400/1251 ( 32%)]  Loss:  6.042348 (6.0262)  Time: 1.097s,  933.59/s  (1.093s,  936.99/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 450/1251 ( 36%)]  Loss:  5.973151 (6.0209)  Time: 1.084s,  944.60/s  (1.092s,  937.66/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 500/1251 ( 40%)]  Loss:  5.864012 (6.0066)  Time: 1.083s,  945.73/s  (1.092s,  938.01/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 550/1251 ( 44%)]  Loss:  5.918140 (5.9992)  Time: 1.152s,  888.92/s  (1.091s,  938.20/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 600/1251 ( 48%)]  Loss:  6.095662 (6.0066)  Time: 1.123s,  912.11/s  (1.092s,  938.01/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 650/1251 ( 52%)]  Loss:  5.795896 (5.9916)  Time: 1.094s,  935.65/s  (1.092s,  938.08/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 700/1251 ( 56%)]  Loss:  5.825025 (5.9805)  Time: 1.077s,  950.95/s  (1.091s,  938.66/s)  LR: 2.008e-04  Data: 0.013 (0.013)
Train: 4 [ 750/1251 ( 60%)]  Loss:  5.895231 (5.9752)  Time: 1.079s,  949.02/s  (1.091s,  938.43/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 800/1251 ( 64%)]  Loss:  5.812731 (5.9656)  Time: 1.098s,  932.53/s  (1.091s,  938.65/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 850/1251 ( 68%)]  Loss:  5.877513 (5.9607)  Time: 1.078s,  950.31/s  (1.090s,  939.03/s)  LR: 2.008e-04  Data: 0.013 (0.013)
Train: 4 [ 900/1251 ( 72%)]  Loss:  5.844996 (5.9546)  Time: 1.093s,  936.80/s  (1.090s,  939.16/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [ 950/1251 ( 76%)]  Loss:  5.669728 (5.9404)  Time: 1.084s,  944.64/s  (1.090s,  939.43/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [1000/1251 ( 80%)]  Loss:  5.808860 (5.9341)  Time: 1.084s,  944.26/s  (1.090s,  939.66/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 4 [1050/1251 ( 84%)]  Loss:  5.944741 (5.9346)  Time: 1.075s,  952.65/s  (1.090s,  939.67/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [1100/1251 ( 88%)]  Loss:  6.040764 (5.9392)  Time: 1.078s,  950.08/s  (1.090s,  939.72/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [1150/1251 ( 92%)]  Loss:  5.833166 (5.9348)  Time: 1.169s,  875.95/s  (1.090s,  939.77/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [1200/1251 ( 96%)]  Loss:  5.832987 (5.9307)  Time: 1.079s,  948.86/s  (1.090s,  939.85/s)  LR: 2.008e-04  Data: 0.012 (0.013)
Train: 4 [1250/1251 (100%)]  Loss:  5.670841 (5.9207)  Time: 1.150s,  890.53/s  (1.090s,  939.71/s)  LR: 2.008e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.751 (5.751)  Loss:  3.0175 (3.0175)  Acc@1: 37.7930 (37.7930)  Acc@5: 68.8477 (68.8477)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  2.5077 (3.8983)  Acc@1: 53.0660 (24.5660)  Acc@5: 71.8160 (47.9860)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 5 [   0/1251 (  0%)]  Loss:  5.636627 (5.6366)  Time: 1.089s,  940.44/s  (1.089s,  940.44/s)  LR: 2.508e-04  Data: 0.029 (0.029)
Train: 5 [  50/1251 (  4%)]  Loss:  5.740282 (5.6885)  Time: 1.078s,  949.89/s  (1.090s,  939.13/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [ 100/1251 (  8%)]  Loss:  5.800159 (5.7257)  Time: 1.078s,  949.83/s  (1.089s,  940.64/s)  LR: 2.508e-04  Data: 0.015 (0.013)
Train: 5 [ 150/1251 ( 12%)]  Loss:  5.854292 (5.7578)  Time: 1.076s,  951.68/s  (1.087s,  942.24/s)  LR: 2.508e-04  Data: 0.015 (0.013)
Train: 5 [ 200/1251 ( 16%)]  Loss:  5.628691 (5.7320)  Time: 1.076s,  951.30/s  (1.087s,  942.33/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [ 250/1251 ( 20%)]  Loss:  5.758487 (5.7364)  Time: 1.082s,  946.82/s  (1.087s,  941.69/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [ 300/1251 ( 24%)]  Loss:  5.744153 (5.7375)  Time: 1.080s,  948.35/s  (1.088s,  941.12/s)  LR: 2.508e-04  Data: 0.016 (0.013)
Train: 5 [ 350/1251 ( 28%)]  Loss:  5.977757 (5.7676)  Time: 1.094s,  936.05/s  (1.088s,  941.22/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [ 400/1251 ( 32%)]  Loss:  6.083072 (5.8026)  Time: 1.093s,  936.66/s  (1.088s,  941.39/s)  LR: 2.508e-04  Data: 0.011 (0.013)
Train: 5 [ 450/1251 ( 36%)]  Loss:  5.892492 (5.8116)  Time: 1.143s,  895.86/s  (1.088s,  941.31/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [ 500/1251 ( 40%)]  Loss:  5.437218 (5.7776)  Time: 1.078s,  950.17/s  (1.088s,  941.50/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [ 550/1251 ( 44%)]  Loss:  5.650434 (5.7670)  Time: 1.082s,  946.42/s  (1.088s,  941.36/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [ 600/1251 ( 48%)]  Loss:  5.558344 (5.7509)  Time: 1.078s,  950.03/s  (1.089s,  940.57/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [ 650/1251 ( 52%)]  Loss:  5.864930 (5.7591)  Time: 1.098s,  932.96/s  (1.089s,  940.16/s)  LR: 2.508e-04  Data: 0.014 (0.013)
Train: 5 [ 700/1251 ( 56%)]  Loss:  5.693505 (5.7547)  Time: 1.081s,  947.67/s  (1.089s,  940.09/s)  LR: 2.508e-04  Data: 0.015 (0.013)
Train: 5 [ 750/1251 ( 60%)]  Loss:  5.697273 (5.7511)  Time: 1.078s,  949.73/s  (1.090s,  939.87/s)  LR: 2.508e-04  Data: 0.015 (0.013)
Train: 5 [ 800/1251 ( 64%)]  Loss:  5.770095 (5.7522)  Time: 1.084s,  944.94/s  (1.090s,  939.63/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [ 850/1251 ( 68%)]  Loss:  5.677014 (5.7480)  Time: 1.078s,  950.31/s  (1.090s,  939.34/s)  LR: 2.508e-04  Data: 0.015 (0.013)
Train: 5 [ 900/1251 ( 72%)]  Loss:  5.612917 (5.7409)  Time: 1.171s,  874.56/s  (1.090s,  939.34/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [ 950/1251 ( 76%)]  Loss:  5.523872 (5.7301)  Time: 1.077s,  950.62/s  (1.090s,  939.53/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [1000/1251 ( 80%)]  Loss:  5.559507 (5.7220)  Time: 1.082s,  946.32/s  (1.090s,  939.77/s)  LR: 2.508e-04  Data: 0.012 (0.013)
Train: 5 [1050/1251 ( 84%)]  Loss:  5.532313 (5.7133)  Time: 1.076s,  951.62/s  (1.090s,  939.71/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [1100/1251 ( 88%)]  Loss:  5.427456 (5.7009)  Time: 1.088s,  941.25/s  (1.090s,  939.48/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [1150/1251 ( 92%)]  Loss:  5.642441 (5.6985)  Time: 1.081s,  947.45/s  (1.090s,  939.61/s)  LR: 2.508e-04  Data: 0.013 (0.013)
Train: 5 [1200/1251 ( 96%)]  Loss:  5.443226 (5.6883)  Time: 1.090s,  939.65/s  (1.090s,  939.50/s)  LR: 2.508e-04  Data: 0.026 (0.013)
Train: 5 [1250/1251 (100%)]  Loss:  5.512551 (5.6815)  Time: 1.079s,  948.60/s  (1.090s,  939.50/s)  LR: 2.508e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.839 (5.839)  Loss:  2.3831 (2.3831)  Acc@1: 50.0977 (50.0977)  Acc@5: 77.2461 (77.2461)
Test: [  48/48]  Time: 0.229 (0.448)  Loss:  2.1200 (3.3836)  Acc@1: 57.7830 (31.8920)  Acc@5: 76.4151 (56.9860)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 6 [   0/1251 (  0%)]  Loss:  5.340343 (5.3403)  Time: 1.090s,  939.81/s  (1.090s,  939.81/s)  LR: 3.007e-04  Data: 0.028 (0.028)
Train: 6 [  50/1251 (  4%)]  Loss:  5.532044 (5.4362)  Time: 1.082s,  946.01/s  (1.084s,  944.50/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [ 100/1251 (  8%)]  Loss:  5.492177 (5.4549)  Time: 1.077s,  950.52/s  (1.084s,  944.39/s)  LR: 3.007e-04  Data: 0.015 (0.013)
Train: 6 [ 150/1251 ( 12%)]  Loss:  5.379136 (5.4359)  Time: 1.077s,  951.14/s  (1.085s,  943.72/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [ 200/1251 ( 16%)]  Loss:  5.824221 (5.5136)  Time: 1.116s,  917.35/s  (1.086s,  942.50/s)  LR: 3.007e-04  Data: 0.015 (0.013)
Train: 6 [ 250/1251 ( 20%)]  Loss:  5.393049 (5.4935)  Time: 1.095s,  935.01/s  (1.087s,  942.41/s)  LR: 3.007e-04  Data: 0.011 (0.013)
Train: 6 [ 300/1251 ( 24%)]  Loss:  5.617054 (5.5111)  Time: 1.093s,  936.60/s  (1.088s,  940.89/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [ 350/1251 ( 28%)]  Loss:  5.401462 (5.4974)  Time: 1.074s,  953.81/s  (1.089s,  940.35/s)  LR: 3.007e-04  Data: 0.011 (0.013)
Train: 6 [ 400/1251 ( 32%)]  Loss:  5.219951 (5.4666)  Time: 1.079s,  949.41/s  (1.088s,  940.76/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [ 450/1251 ( 36%)]  Loss:  5.565681 (5.4765)  Time: 1.080s,  948.53/s  (1.089s,  940.61/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [ 500/1251 ( 40%)]  Loss:  5.660238 (5.4932)  Time: 1.096s,  934.51/s  (1.089s,  940.58/s)  LR: 3.007e-04  Data: 0.014 (0.013)
Train: 6 [ 550/1251 ( 44%)]  Loss:  5.541788 (5.4973)  Time: 1.072s,  954.84/s  (1.090s,  939.84/s)  LR: 3.007e-04  Data: 0.011 (0.013)
Train: 6 [ 600/1251 ( 48%)]  Loss:  5.546081 (5.5010)  Time: 1.078s,  949.94/s  (1.089s,  939.95/s)  LR: 3.007e-04  Data: 0.013 (0.013)
Train: 6 [ 650/1251 ( 52%)]  Loss:  5.456954 (5.4979)  Time: 1.074s,  953.18/s  (1.090s,  939.67/s)  LR: 3.007e-04  Data: 0.011 (0.013)
Train: 6 [ 700/1251 ( 56%)]  Loss:  5.289091 (5.4840)  Time: 1.104s,  927.22/s  (1.090s,  939.78/s)  LR: 3.007e-04  Data: 0.011 (0.013)
Train: 6 [ 750/1251 ( 60%)]  Loss:  5.649941 (5.4943)  Time: 1.081s,  947.65/s  (1.090s,  939.52/s)  LR: 3.007e-04  Data: 0.013 (0.013)
Train: 6 [ 800/1251 ( 64%)]  Loss:  5.335042 (5.4850)  Time: 1.188s,  861.96/s  (1.090s,  939.32/s)  LR: 3.007e-04  Data: 0.011 (0.013)
Train: 6 [ 850/1251 ( 68%)]  Loss:  5.395030 (5.4800)  Time: 1.077s,  950.43/s  (1.090s,  939.38/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [ 900/1251 ( 72%)]  Loss:  5.509540 (5.4815)  Time: 1.100s,  930.95/s  (1.090s,  939.10/s)  LR: 3.007e-04  Data: 0.018 (0.013)
Train: 6 [ 950/1251 ( 76%)]  Loss:  5.823981 (5.4986)  Time: 1.082s,  946.44/s  (1.090s,  939.32/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [1000/1251 ( 80%)]  Loss:  5.483721 (5.4979)  Time: 1.074s,  953.37/s  (1.090s,  939.40/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [1050/1251 ( 84%)]  Loss:  5.461071 (5.4963)  Time: 1.078s,  949.65/s  (1.090s,  939.36/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [1100/1251 ( 88%)]  Loss:  5.314307 (5.4883)  Time: 1.097s,  933.40/s  (1.090s,  939.36/s)  LR: 3.007e-04  Data: 0.013 (0.013)
Train: 6 [1150/1251 ( 92%)]  Loss:  5.291927 (5.4802)  Time: 1.099s,  932.17/s  (1.090s,  939.35/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [1200/1251 ( 96%)]  Loss:  5.437864 (5.4785)  Time: 1.103s,  928.51/s  (1.090s,  939.40/s)  LR: 3.007e-04  Data: 0.012 (0.013)
Train: 6 [1250/1251 (100%)]  Loss:  5.364462 (5.4741)  Time: 1.060s,  966.24/s  (1.090s,  939.21/s)  LR: 3.007e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.806 (5.806)  Loss:  1.9692 (1.9692)  Acc@1: 57.2266 (57.2266)  Acc@5: 83.1055 (83.1055)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  2.0303 (2.9958)  Acc@1: 58.9623 (37.9560)  Acc@5: 78.3019 (64.1640)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 7 [   0/1251 (  0%)]  Loss:  5.442340 (5.4423)  Time: 1.097s,  933.03/s  (1.097s,  933.03/s)  LR: 3.507e-04  Data: 0.034 (0.034)
Train: 7 [  50/1251 (  4%)]  Loss:  5.186205 (5.3143)  Time: 1.094s,  935.96/s  (1.090s,  939.47/s)  LR: 3.507e-04  Data: 0.011 (0.014)
Train: 7 [ 100/1251 (  8%)]  Loss:  5.283014 (5.3039)  Time: 1.078s,  949.72/s  (1.091s,  938.38/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 150/1251 ( 12%)]  Loss:  5.163101 (5.2687)  Time: 1.098s,  932.54/s  (1.091s,  938.42/s)  LR: 3.507e-04  Data: 0.015 (0.013)
Train: 7 [ 200/1251 ( 16%)]  Loss:  5.477834 (5.3105)  Time: 1.106s,  925.56/s  (1.089s,  940.38/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 250/1251 ( 20%)]  Loss:  5.268374 (5.3035)  Time: 1.111s,  921.62/s  (1.089s,  940.18/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 300/1251 ( 24%)]  Loss:  5.130802 (5.2788)  Time: 1.104s,  927.89/s  (1.090s,  939.33/s)  LR: 3.507e-04  Data: 0.013 (0.013)
Train: 7 [ 350/1251 ( 28%)]  Loss:  5.341268 (5.2866)  Time: 1.088s,  941.08/s  (1.090s,  939.50/s)  LR: 3.507e-04  Data: 0.018 (0.013)
Train: 7 [ 400/1251 ( 32%)]  Loss:  5.134104 (5.2697)  Time: 1.092s,  937.66/s  (1.090s,  939.17/s)  LR: 3.507e-04  Data: 0.013 (0.013)
Train: 7 [ 450/1251 ( 36%)]  Loss:  5.040517 (5.2468)  Time: 1.094s,  935.67/s  (1.090s,  939.25/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 500/1251 ( 40%)]  Loss:  5.533106 (5.2728)  Time: 1.082s,  946.21/s  (1.090s,  939.29/s)  LR: 3.507e-04  Data: 0.013 (0.013)
Train: 7 [ 550/1251 ( 44%)]  Loss:  5.258149 (5.2716)  Time: 1.103s,  928.06/s  (1.091s,  938.95/s)  LR: 3.507e-04  Data: 0.013 (0.013)
Train: 7 [ 600/1251 ( 48%)]  Loss:  5.362429 (5.2786)  Time: 1.073s,  953.99/s  (1.091s,  938.91/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 650/1251 ( 52%)]  Loss:  5.335442 (5.2826)  Time: 1.094s,  936.33/s  (1.091s,  939.00/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 700/1251 ( 56%)]  Loss:  5.216462 (5.2782)  Time: 1.102s,  929.23/s  (1.091s,  938.55/s)  LR: 3.507e-04  Data: 0.016 (0.013)
Train: 7 [ 750/1251 ( 60%)]  Loss:  5.409226 (5.2864)  Time: 1.095s,  935.51/s  (1.091s,  938.75/s)  LR: 3.507e-04  Data: 0.011 (0.013)
Train: 7 [ 800/1251 ( 64%)]  Loss:  5.617383 (5.3059)  Time: 1.078s,  949.76/s  (1.091s,  938.90/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 850/1251 ( 68%)]  Loss:  5.261625 (5.3034)  Time: 1.074s,  953.72/s  (1.090s,  939.14/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 900/1251 ( 72%)]  Loss:  5.404500 (5.3087)  Time: 1.095s,  934.82/s  (1.091s,  938.97/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [ 950/1251 ( 76%)]  Loss:  5.245871 (5.3056)  Time: 1.095s,  935.15/s  (1.091s,  938.51/s)  LR: 3.507e-04  Data: 0.011 (0.013)
Train: 7 [1000/1251 ( 80%)]  Loss:  5.429661 (5.3115)  Time: 1.073s,  954.21/s  (1.091s,  938.35/s)  LR: 3.507e-04  Data: 0.011 (0.013)
Train: 7 [1050/1251 ( 84%)]  Loss:  4.953112 (5.2952)  Time: 1.108s,  924.50/s  (1.091s,  938.25/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [1100/1251 ( 88%)]  Loss:  5.246304 (5.2931)  Time: 1.076s,  951.46/s  (1.092s,  938.13/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [1150/1251 ( 92%)]  Loss:  5.580977 (5.3051)  Time: 1.080s,  948.39/s  (1.091s,  938.35/s)  LR: 3.507e-04  Data: 0.013 (0.013)
Train: 7 [1200/1251 ( 96%)]  Loss:  5.410271 (5.3093)  Time: 1.076s,  951.26/s  (1.091s,  938.56/s)  LR: 3.507e-04  Data: 0.012 (0.013)
Train: 7 [1250/1251 (100%)]  Loss:  4.885231 (5.2930)  Time: 1.069s,  957.75/s  (1.091s,  938.37/s)  LR: 3.507e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.879 (5.879)  Loss:  1.7291 (1.7291)  Acc@1: 65.5273 (65.5273)  Acc@5: 85.4492 (85.4492)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.7402 (2.7842)  Acc@1: 66.3915 (42.3480)  Acc@5: 81.9576 (68.1100)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 8 [   0/1251 (  0%)]  Loss:  5.124583 (5.1246)  Time: 1.090s,  939.45/s  (1.090s,  939.45/s)  LR: 4.006e-04  Data: 0.027 (0.027)
Train: 8 [  50/1251 (  4%)]  Loss:  5.034089 (5.0793)  Time: 1.076s,  951.30/s  (1.090s,  939.14/s)  LR: 4.006e-04  Data: 0.013 (0.014)
Train: 8 [ 100/1251 (  8%)]  Loss:  5.199523 (5.1194)  Time: 1.097s,  933.26/s  (1.088s,  941.10/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [ 150/1251 ( 12%)]  Loss:  5.312034 (5.1676)  Time: 1.085s,  943.59/s  (1.089s,  940.28/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [ 200/1251 ( 16%)]  Loss:  4.817538 (5.0976)  Time: 1.078s,  949.99/s  (1.089s,  940.16/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [ 250/1251 ( 20%)]  Loss:  5.128790 (5.1028)  Time: 1.095s,  934.76/s  (1.088s,  940.75/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 8 [ 300/1251 ( 24%)]  Loss:  5.134971 (5.1074)  Time: 1.101s,  930.36/s  (1.089s,  940.69/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [ 350/1251 ( 28%)]  Loss:  5.620487 (5.1715)  Time: 1.088s,  940.85/s  (1.088s,  941.07/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [ 400/1251 ( 32%)]  Loss:  4.984502 (5.1507)  Time: 1.080s,  948.29/s  (1.088s,  940.85/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [ 450/1251 ( 36%)]  Loss:  4.739941 (5.1096)  Time: 1.089s,  940.55/s  (1.088s,  940.83/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [ 500/1251 ( 40%)]  Loss:  4.795107 (5.0811)  Time: 1.094s,  935.65/s  (1.089s,  940.45/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 8 [ 550/1251 ( 44%)]  Loss:  5.361143 (5.1044)  Time: 1.085s,  943.56/s  (1.089s,  940.41/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [ 600/1251 ( 48%)]  Loss:  5.098297 (5.1039)  Time: 1.082s,  946.76/s  (1.089s,  940.62/s)  LR: 4.006e-04  Data: 0.017 (0.013)
Train: 8 [ 650/1251 ( 52%)]  Loss:  5.129598 (5.1058)  Time: 1.076s,  951.65/s  (1.088s,  940.78/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [ 700/1251 ( 56%)]  Loss:  5.058199 (5.1026)  Time: 1.079s,  949.45/s  (1.089s,  940.55/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 8 [ 750/1251 ( 60%)]  Loss:  4.949637 (5.0930)  Time: 1.076s,  951.55/s  (1.088s,  940.75/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [ 800/1251 ( 64%)]  Loss:  5.125904 (5.0950)  Time: 1.096s,  933.88/s  (1.088s,  940.94/s)  LR: 4.006e-04  Data: 0.011 (0.013)
Train: 8 [ 850/1251 ( 68%)]  Loss:  4.984975 (5.0889)  Time: 1.104s,  927.80/s  (1.088s,  941.16/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [ 900/1251 ( 72%)]  Loss:  4.803314 (5.0738)  Time: 1.079s,  949.16/s  (1.088s,  940.85/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [ 950/1251 ( 76%)]  Loss:  4.983971 (5.0693)  Time: 1.085s,  944.18/s  (1.088s,  940.80/s)  LR: 4.006e-04  Data: 0.013 (0.013)
Train: 8 [1000/1251 ( 80%)]  Loss:  5.216939 (5.0764)  Time: 1.093s,  936.80/s  (1.089s,  940.55/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [1050/1251 ( 84%)]  Loss:  5.248616 (5.0842)  Time: 1.076s,  951.54/s  (1.089s,  940.13/s)  LR: 4.006e-04  Data: 0.014 (0.013)
Train: 8 [1100/1251 ( 88%)]  Loss:  4.966400 (5.0791)  Time: 1.079s,  948.74/s  (1.089s,  940.33/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [1150/1251 ( 92%)]  Loss:  5.296624 (5.0881)  Time: 1.101s,  930.48/s  (1.089s,  940.42/s)  LR: 4.006e-04  Data: 0.018 (0.013)
Train: 8 [1200/1251 ( 96%)]  Loss:  5.163787 (5.0912)  Time: 1.076s,  951.28/s  (1.089s,  940.28/s)  LR: 4.006e-04  Data: 0.012 (0.013)
Train: 8 [1250/1251 (100%)]  Loss:  5.337631 (5.1006)  Time: 1.080s,  948.38/s  (1.089s,  939.97/s)  LR: 4.006e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.865 (5.865)  Loss:  1.6851 (1.6851)  Acc@1: 66.0156 (66.0156)  Acc@5: 85.3516 (85.3516)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.5681 (2.5420)  Acc@1: 69.5755 (46.2720)  Acc@5: 84.1981 (72.0740)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 9 [   0/1251 (  0%)]  Loss:  5.141623 (5.1416)  Time: 1.109s,  923.62/s  (1.109s,  923.62/s)  LR: 4.506e-04  Data: 0.036 (0.036)
Train: 9 [  50/1251 (  4%)]  Loss:  5.028909 (5.0853)  Time: 1.081s,  946.95/s  (1.085s,  944.02/s)  LR: 4.506e-04  Data: 0.012 (0.014)
Train: 9 [ 100/1251 (  8%)]  Loss:  5.055824 (5.0755)  Time: 1.095s,  935.34/s  (1.087s,  942.11/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [ 150/1251 ( 12%)]  Loss:  4.836260 (5.0157)  Time: 1.075s,  952.39/s  (1.090s,  939.33/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [ 200/1251 ( 16%)]  Loss:  4.955376 (5.0036)  Time: 1.104s,  927.15/s  (1.089s,  939.95/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [ 250/1251 ( 20%)]  Loss:  4.875957 (4.9823)  Time: 1.092s,  938.01/s  (1.090s,  939.60/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 300/1251 ( 24%)]  Loss:  5.276947 (5.0244)  Time: 1.172s,  873.89/s  (1.091s,  938.71/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 350/1251 ( 28%)]  Loss:  5.304518 (5.0594)  Time: 1.095s,  935.34/s  (1.091s,  938.81/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 400/1251 ( 32%)]  Loss:  5.168639 (5.0716)  Time: 1.082s,  946.52/s  (1.090s,  939.41/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 450/1251 ( 36%)]  Loss:  4.992225 (5.0636)  Time: 1.093s,  936.77/s  (1.091s,  938.91/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 500/1251 ( 40%)]  Loss:  5.352978 (5.0899)  Time: 1.084s,  945.02/s  (1.091s,  938.84/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 550/1251 ( 44%)]  Loss:  5.308949 (5.1082)  Time: 1.092s,  937.64/s  (1.090s,  939.08/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 600/1251 ( 48%)]  Loss:  5.172963 (5.1132)  Time: 1.094s,  936.35/s  (1.090s,  939.34/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [ 650/1251 ( 52%)]  Loss:  4.947437 (5.1013)  Time: 1.083s,  945.73/s  (1.090s,  939.22/s)  LR: 4.506e-04  Data: 0.010 (0.013)
Train: 9 [ 700/1251 ( 56%)]  Loss:  4.568141 (5.0658)  Time: 1.075s,  952.37/s  (1.090s,  939.02/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 750/1251 ( 60%)]  Loss:  5.059350 (5.0654)  Time: 1.083s,  945.94/s  (1.090s,  939.55/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [ 800/1251 ( 64%)]  Loss:  5.122107 (5.0687)  Time: 1.100s,  930.54/s  (1.090s,  939.50/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [ 850/1251 ( 68%)]  Loss:  4.930264 (5.0610)  Time: 1.098s,  932.87/s  (1.090s,  939.49/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 900/1251 ( 72%)]  Loss:  5.392826 (5.0785)  Time: 1.083s,  945.49/s  (1.090s,  939.53/s)  LR: 4.506e-04  Data: 0.012 (0.013)
Train: 9 [ 950/1251 ( 76%)]  Loss:  4.987816 (5.0740)  Time: 1.098s,  932.26/s  (1.090s,  939.56/s)  LR: 4.506e-04  Data: 0.011 (0.013)
Train: 9 [1000/1251 ( 80%)]  Loss:  4.680897 (5.0552)  Time: 1.093s,  936.57/s  (1.090s,  939.46/s)  LR: 4.506e-04  Data: 0.015 (0.013)
Train: 9 [1050/1251 ( 84%)]  Loss:  4.779020 (5.0427)  Time: 1.099s,  931.98/s  (1.090s,  939.48/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [1100/1251 ( 88%)]  Loss:  5.149116 (5.0473)  Time: 1.078s,  950.26/s  (1.090s,  939.78/s)  LR: 4.506e-04  Data: 0.015 (0.013)
Train: 9 [1150/1251 ( 92%)]  Loss:  4.598444 (5.0286)  Time: 1.082s,  946.58/s  (1.090s,  939.77/s)  LR: 4.506e-04  Data: 0.018 (0.013)
Train: 9 [1200/1251 ( 96%)]  Loss:  4.962499 (5.0260)  Time: 1.079s,  949.41/s  (1.090s,  939.87/s)  LR: 4.506e-04  Data: 0.013 (0.013)
Train: 9 [1250/1251 (100%)]  Loss:  4.858315 (5.0195)  Time: 1.070s,  957.45/s  (1.090s,  939.81/s)  LR: 4.506e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.845 (5.845)  Loss:  1.5536 (1.5536)  Acc@1: 68.6523 (68.6523)  Acc@5: 88.5742 (88.5742)
Test: [  48/48]  Time: 0.230 (0.448)  Loss:  1.4157 (2.3573)  Acc@1: 71.4623 (49.5600)  Acc@5: 87.2642 (75.1640)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-0.pth.tar', 0.22)

Train: 10 [   0/1251 (  0%)]  Loss:  4.878625 (4.8786)  Time: 1.098s,  932.64/s  (1.098s,  932.64/s)  LR: 5.005e-04  Data: 0.036 (0.036)
Train: 10 [  50/1251 (  4%)]  Loss:  4.826306 (4.8525)  Time: 1.086s,  942.78/s  (1.096s,  934.28/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 100/1251 (  8%)]  Loss:  5.117486 (4.9408)  Time: 1.077s,  950.81/s  (1.092s,  937.80/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 150/1251 ( 12%)]  Loss:  4.726930 (4.8873)  Time: 1.073s,  954.62/s  (1.091s,  938.92/s)  LR: 5.005e-04  Data: 0.011 (0.013)
Train: 10 [ 200/1251 ( 16%)]  Loss:  4.630484 (4.8360)  Time: 1.097s,  933.59/s  (1.090s,  939.12/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [ 250/1251 ( 20%)]  Loss:  5.154669 (4.8891)  Time: 1.094s,  935.97/s  (1.090s,  939.13/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 300/1251 ( 24%)]  Loss:  5.048499 (4.9119)  Time: 1.105s,  926.95/s  (1.089s,  939.97/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [ 350/1251 ( 28%)]  Loss:  4.877942 (4.9076)  Time: 1.080s,  948.28/s  (1.089s,  940.29/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 400/1251 ( 32%)]  Loss:  5.236537 (4.9442)  Time: 1.084s,  944.88/s  (1.090s,  939.88/s)  LR: 5.005e-04  Data: 0.015 (0.013)
Train: 10 [ 450/1251 ( 36%)]  Loss:  4.875666 (4.9373)  Time: 1.088s,  940.95/s  (1.089s,  940.26/s)  LR: 5.005e-04  Data: 0.019 (0.013)
Train: 10 [ 500/1251 ( 40%)]  Loss:  4.778345 (4.9229)  Time: 1.077s,  950.68/s  (1.089s,  940.56/s)  LR: 5.005e-04  Data: 0.016 (0.013)
Train: 10 [ 550/1251 ( 44%)]  Loss:  4.998244 (4.9291)  Time: 1.106s,  925.68/s  (1.088s,  940.81/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 600/1251 ( 48%)]  Loss:  4.740808 (4.9147)  Time: 1.081s,  947.03/s  (1.088s,  940.92/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 650/1251 ( 52%)]  Loss:  4.722439 (4.9009)  Time: 1.077s,  951.20/s  (1.088s,  941.18/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 700/1251 ( 56%)]  Loss:  5.067511 (4.9120)  Time: 1.095s,  934.83/s  (1.088s,  940.90/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [ 750/1251 ( 60%)]  Loss:  4.930111 (4.9132)  Time: 1.080s,  948.48/s  (1.089s,  940.71/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [ 800/1251 ( 64%)]  Loss:  4.962787 (4.9161)  Time: 1.080s,  948.26/s  (1.089s,  940.59/s)  LR: 5.005e-04  Data: 0.015 (0.013)
Train: 10 [ 850/1251 ( 68%)]  Loss:  4.739964 (4.9063)  Time: 1.084s,  944.61/s  (1.089s,  940.49/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 900/1251 ( 72%)]  Loss:  5.085495 (4.9157)  Time: 1.078s,  950.04/s  (1.089s,  940.29/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [ 950/1251 ( 76%)]  Loss:  4.988576 (4.9194)  Time: 1.095s,  935.31/s  (1.089s,  940.44/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [1000/1251 ( 80%)]  Loss:  4.911363 (4.9190)  Time: 1.076s,  951.61/s  (1.089s,  940.67/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [1050/1251 ( 84%)]  Loss:  4.995265 (4.9225)  Time: 1.077s,  950.65/s  (1.089s,  940.74/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [1100/1251 ( 88%)]  Loss:  5.198838 (4.9345)  Time: 1.077s,  950.43/s  (1.088s,  940.75/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [1150/1251 ( 92%)]  Loss:  4.648418 (4.9226)  Time: 1.077s,  950.60/s  (1.089s,  940.53/s)  LR: 5.005e-04  Data: 0.013 (0.013)
Train: 10 [1200/1251 ( 96%)]  Loss:  4.611078 (4.9101)  Time: 1.093s,  937.18/s  (1.089s,  940.44/s)  LR: 5.005e-04  Data: 0.012 (0.013)
Train: 10 [1250/1251 (100%)]  Loss:  4.701612 (4.9021)  Time: 1.062s,  964.26/s  (1.089s,  940.25/s)  LR: 5.005e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.832 (5.832)  Loss:  1.3259 (1.3259)  Acc@1: 72.4609 (72.4609)  Acc@5: 88.8672 (88.8672)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  1.3177 (2.2061)  Acc@1: 74.6462 (52.3920)  Acc@5: 88.6793 (77.2060)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-1.pth.tar', 2.9899999981689453)

Train: 11 [   0/1251 (  0%)]  Loss:  5.163011 (5.1630)  Time: 1.089s,  940.00/s  (1.089s,  940.00/s)  LR: 5.505e-04  Data: 0.026 (0.026)
Train: 11 [  50/1251 (  4%)]  Loss:  4.596564 (4.8798)  Time: 1.097s,  933.26/s  (1.093s,  936.88/s)  LR: 5.505e-04  Data: 0.014 (0.013)
Train: 11 [ 100/1251 (  8%)]  Loss:  5.051434 (4.9370)  Time: 1.077s,  950.48/s  (1.092s,  937.72/s)  LR: 5.505e-04  Data: 0.012 (0.013)
Train: 11 [ 150/1251 ( 12%)]  Loss:  5.018306 (4.9573)  Time: 1.082s,  946.49/s  (1.090s,  939.28/s)  LR: 5.505e-04  Data: 0.012 (0.013)
Train: 11 [ 200/1251 ( 16%)]  Loss:  4.611919 (4.8882)  Time: 1.079s,  948.91/s  (1.090s,  939.49/s)  LR: 5.505e-04  Data: 0.015 (0.013)
Train: 11 [ 250/1251 ( 20%)]  Loss:  4.643106 (4.8474)  Time: 1.097s,  933.27/s  (1.091s,  939.01/s)  LR: 5.505e-04  Data: 0.014 (0.013)
Train: 11 [ 300/1251 ( 24%)]  Loss:  4.909371 (4.8562)  Time: 1.093s,  937.22/s  (1.091s,  939.02/s)  LR: 5.505e-04  Data: 0.012 (0.013)
Train: 11 [ 350/1251 ( 28%)]  Loss:  4.902256 (4.8620)  Time: 1.074s,  953.07/s  (1.091s,  938.28/s)  LR: 5.505e-04  Data: 0.011 (0.013)
Train: 11 [ 400/1251 ( 32%)]  Loss:  4.930262 (4.8696)  Time: 1.074s,  953.30/s  (1.091s,  938.44/s)  LR: 5.505e-04  Data: 0.011 (0.013)
Train: 11 [ 450/1251 ( 36%)]  Loss:  5.002464 (4.8829)  Time: 1.089s,  940.45/s  (1.091s,  938.72/s)  LR: 5.505e-04  Data: 0.015 (0.013)
Train: 11 [ 500/1251 ( 40%)]  Loss:  4.875107 (4.8822)  Time: 1.117s,  916.72/s  (1.091s,  938.35/s)  LR: 5.505e-04  Data: 0.012 (0.013)
Train: 11 [ 550/1251 ( 44%)]  Loss:  5.021179 (4.8937)  Time: 1.094s,  936.00/s  (1.092s,  937.82/s)  LR: 5.505e-04  Data: 0.012 (0.013)
Train: 11 [ 600/1251 ( 48%)]  Loss:  4.977869 (4.9002)  Time: 1.105s,  926.77/s  (1.092s,  937.94/s)  LR: 5.505e-04  Data: 0.013 (0.013)
Train: 11 [ 650/1251 ( 52%)]  Loss:  4.404914 (4.8648)  Time: 1.080s,  948.12/s  (1.091s,  938.56/s)  LR: 5.505e-04  Data: 0.018 (0.013)
Train: 11 [ 700/1251 ( 56%)]  Loss:  4.743920 (4.8568)  Time: 1.092s,  937.89/s  (1.092s,  938.08/s)  LR: 5.505e-04  Data: 0.014 (0.013)
Train: 11 [ 750/1251 ( 60%)]  Loss:  4.627936 (4.8425)  Time: 1.098s,  932.30/s  (1.091s,  938.27/s)  LR: 5.505e-04  Data: 0.011 (0.013)
Train: 11 [ 800/1251 ( 64%)]  Loss:  4.793293 (4.8396)  Time: 1.139s,  898.96/s  (1.091s,  938.56/s)  LR: 5.505e-04  Data: 0.011 (0.013)
Train: 11 [ 850/1251 ( 68%)]  Loss:  4.644924 (4.8288)  Time: 1.077s,  950.38/s  (1.091s,  938.56/s)  LR: 5.505e-04  Data: 0.015 (0.013)
Train: 11 [ 900/1251 ( 72%)]  Loss:  4.622476 (4.8179)  Time: 1.074s,  953.62/s  (1.091s,  938.24/s)  LR: 5.505e-04  Data: 0.011 (0.013)
Train: 11 [ 950/1251 ( 76%)]  Loss:  4.850265 (4.8195)  Time: 1.092s,  937.74/s  (1.091s,  938.45/s)  LR: 5.505e-04  Data: 0.016 (0.013)
Train: 11 [1000/1251 ( 80%)]  Loss:  4.422113 (4.8006)  Time: 1.083s,  945.85/s  (1.091s,  938.78/s)  LR: 5.505e-04  Data: 0.013 (0.013)
Train: 11 [1050/1251 ( 84%)]  Loss:  4.575469 (4.7904)  Time: 1.102s,  929.15/s  (1.091s,  938.68/s)  LR: 5.505e-04  Data: 0.013 (0.013)
Train: 11 [1100/1251 ( 88%)]  Loss:  4.815544 (4.7915)  Time: 1.096s,  933.89/s  (1.091s,  938.56/s)  LR: 5.505e-04  Data: 0.013 (0.013)
Train: 11 [1150/1251 ( 92%)]  Loss:  4.219391 (4.7676)  Time: 1.079s,  948.90/s  (1.091s,  938.78/s)  LR: 5.505e-04  Data: 0.013 (0.013)
Train: 11 [1200/1251 ( 96%)]  Loss:  4.360569 (4.7513)  Time: 1.094s,  935.99/s  (1.091s,  938.88/s)  LR: 5.505e-04  Data: 0.012 (0.013)
Train: 11 [1250/1251 (100%)]  Loss:  5.193664 (4.7684)  Time: 1.081s,  946.93/s  (1.091s,  938.78/s)  LR: 5.505e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.868 (5.868)  Loss:  1.2701 (1.2701)  Acc@1: 72.9492 (72.9492)  Acc@5: 91.0156 (91.0156)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.3227 (2.1163)  Acc@1: 72.1698 (54.3880)  Acc@5: 88.3255 (78.9260)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-2.pth.tar', 8.174000010375977)

Train: 12 [   0/1251 (  0%)]  Loss:  5.043653 (5.0437)  Time: 1.100s,  931.11/s  (1.100s,  931.11/s)  LR: 6.004e-04  Data: 0.033 (0.033)
Train: 12 [  50/1251 (  4%)]  Loss:  4.458561 (4.7511)  Time: 1.076s,  951.65/s  (1.090s,  939.63/s)  LR: 6.004e-04  Data: 0.013 (0.014)
Train: 12 [ 100/1251 (  8%)]  Loss:  4.625267 (4.7092)  Time: 1.095s,  935.19/s  (1.093s,  936.91/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [ 150/1251 ( 12%)]  Loss:  5.060263 (4.7969)  Time: 1.078s,  949.66/s  (1.092s,  937.73/s)  LR: 6.004e-04  Data: 0.015 (0.013)
Train: 12 [ 200/1251 ( 16%)]  Loss:  4.666142 (4.7708)  Time: 1.079s,  949.38/s  (1.090s,  939.23/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [ 250/1251 ( 20%)]  Loss:  4.642178 (4.7493)  Time: 1.094s,  935.61/s  (1.091s,  938.79/s)  LR: 6.004e-04  Data: 0.014 (0.013)
Train: 12 [ 300/1251 ( 24%)]  Loss:  4.986790 (4.7833)  Time: 1.085s,  943.88/s  (1.091s,  938.82/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [ 350/1251 ( 28%)]  Loss:  4.558191 (4.7551)  Time: 1.077s,  951.12/s  (1.090s,  939.33/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [ 400/1251 ( 32%)]  Loss:  4.945363 (4.7763)  Time: 1.084s,  944.48/s  (1.090s,  939.33/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 12 [ 450/1251 ( 36%)]  Loss:  4.684514 (4.7671)  Time: 1.086s,  942.65/s  (1.090s,  939.35/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [ 500/1251 ( 40%)]  Loss:  4.627512 (4.7544)  Time: 1.080s,  947.82/s  (1.090s,  939.61/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [ 550/1251 ( 44%)]  Loss:  4.947490 (4.7705)  Time: 1.079s,  949.18/s  (1.090s,  939.59/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [ 600/1251 ( 48%)]  Loss:  4.695480 (4.7647)  Time: 1.076s,  951.36/s  (1.090s,  939.21/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [ 650/1251 ( 52%)]  Loss:  4.775547 (4.7655)  Time: 1.080s,  948.20/s  (1.091s,  938.92/s)  LR: 6.004e-04  Data: 0.014 (0.013)
Train: 12 [ 700/1251 ( 56%)]  Loss:  4.563281 (4.7520)  Time: 1.099s,  932.14/s  (1.091s,  938.97/s)  LR: 6.004e-04  Data: 0.015 (0.013)
Train: 12 [ 750/1251 ( 60%)]  Loss:  4.532618 (4.7383)  Time: 1.105s,  926.28/s  (1.090s,  939.18/s)  LR: 6.004e-04  Data: 0.011 (0.013)
Train: 12 [ 800/1251 ( 64%)]  Loss:  4.667516 (4.7341)  Time: 1.093s,  936.52/s  (1.090s,  939.15/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [ 850/1251 ( 68%)]  Loss:  4.668218 (4.7305)  Time: 1.095s,  934.74/s  (1.091s,  938.76/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [ 900/1251 ( 72%)]  Loss:  4.566465 (4.7218)  Time: 1.103s,  928.41/s  (1.091s,  938.96/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [ 950/1251 ( 76%)]  Loss:  4.431077 (4.7073)  Time: 1.081s,  947.43/s  (1.090s,  939.02/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [1000/1251 ( 80%)]  Loss:  4.843536 (4.7138)  Time: 1.089s,  940.11/s  (1.091s,  938.79/s)  LR: 6.004e-04  Data: 0.012 (0.013)
Train: 12 [1050/1251 ( 84%)]  Loss:  4.591936 (4.7083)  Time: 1.099s,  931.83/s  (1.091s,  939.02/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [1100/1251 ( 88%)]  Loss:  4.635544 (4.7051)  Time: 1.094s,  936.30/s  (1.091s,  939.02/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [1150/1251 ( 92%)]  Loss:  4.782402 (4.7083)  Time: 1.080s,  948.08/s  (1.091s,  938.93/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [1200/1251 ( 96%)]  Loss:  5.056782 (4.7223)  Time: 1.076s,  951.53/s  (1.090s,  939.10/s)  LR: 6.004e-04  Data: 0.013 (0.013)
Train: 12 [1250/1251 (100%)]  Loss:  4.885218 (4.7285)  Time: 1.061s,  965.04/s  (1.090s,  939.20/s)  LR: 6.004e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.859 (5.859)  Loss:  1.1538 (1.1538)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.230 (0.447)  Loss:  1.2884 (2.0369)  Acc@1: 72.4057 (55.5120)  Acc@5: 88.4434 (80.0480)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-3.pth.tar', 15.859999984130859)

Train: 13 [   0/1251 (  0%)]  Loss:  4.737707 (4.7377)  Time: 1.087s,  941.71/s  (1.087s,  941.71/s)  LR: 6.504e-04  Data: 0.026 (0.026)
Train: 13 [  50/1251 (  4%)]  Loss:  5.155601 (4.9467)  Time: 1.080s,  948.46/s  (1.087s,  942.27/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [ 100/1251 (  8%)]  Loss:  4.172279 (4.6885)  Time: 1.105s,  926.71/s  (1.088s,  941.56/s)  LR: 6.504e-04  Data: 0.019 (0.013)
Train: 13 [ 150/1251 ( 12%)]  Loss:  4.613494 (4.6698)  Time: 1.088s,  941.20/s  (1.089s,  940.74/s)  LR: 6.504e-04  Data: 0.015 (0.013)
Train: 13 [ 200/1251 ( 16%)]  Loss:  4.407815 (4.6174)  Time: 1.080s,  947.95/s  (1.090s,  939.77/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [ 250/1251 ( 20%)]  Loss:  4.487739 (4.5958)  Time: 1.079s,  948.83/s  (1.090s,  939.82/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [ 300/1251 ( 24%)]  Loss:  4.626367 (4.6001)  Time: 1.080s,  948.31/s  (1.090s,  939.58/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [ 350/1251 ( 28%)]  Loss:  4.880960 (4.6352)  Time: 1.081s,  947.63/s  (1.090s,  939.56/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [ 400/1251 ( 32%)]  Loss:  4.608390 (4.6323)  Time: 1.101s,  929.98/s  (1.090s,  939.37/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [ 450/1251 ( 36%)]  Loss:  4.389431 (4.6080)  Time: 1.081s,  946.96/s  (1.090s,  939.16/s)  LR: 6.504e-04  Data: 0.014 (0.013)
Train: 13 [ 500/1251 ( 40%)]  Loss:  4.842062 (4.6293)  Time: 1.078s,  950.25/s  (1.090s,  939.62/s)  LR: 6.504e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 13 [ 550/1251 ( 44%)]  Loss:  4.372828 (4.6079)  Time: 1.086s,  943.32/s  (1.090s,  939.47/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [ 600/1251 ( 48%)]  Loss:  4.715899 (4.6162)  Time: 1.075s,  952.92/s  (1.090s,  939.51/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [ 650/1251 ( 52%)]  Loss:  4.759578 (4.6264)  Time: 1.094s,  935.96/s  (1.090s,  939.43/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [ 700/1251 ( 56%)]  Loss:  4.431561 (4.6134)  Time: 1.092s,  937.70/s  (1.090s,  939.07/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [ 750/1251 ( 60%)]  Loss:  4.374815 (4.5985)  Time: 1.095s,  934.94/s  (1.091s,  938.85/s)  LR: 6.504e-04  Data: 0.016 (0.013)
Train: 13 [ 800/1251 ( 64%)]  Loss:  4.489150 (4.5921)  Time: 1.094s,  936.00/s  (1.090s,  939.07/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [ 850/1251 ( 68%)]  Loss:  4.660707 (4.5959)  Time: 1.079s,  948.88/s  (1.090s,  939.22/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [ 900/1251 ( 72%)]  Loss:  4.321665 (4.5815)  Time: 1.106s,  926.27/s  (1.091s,  938.85/s)  LR: 6.504e-04  Data: 0.017 (0.013)
Train: 13 [ 950/1251 ( 76%)]  Loss:  4.955846 (4.6002)  Time: 1.093s,  936.53/s  (1.091s,  938.77/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [1000/1251 ( 80%)]  Loss:  4.118797 (4.5773)  Time: 1.082s,  946.72/s  (1.091s,  938.92/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [1050/1251 ( 84%)]  Loss:  4.970495 (4.5951)  Time: 1.094s,  935.78/s  (1.091s,  938.76/s)  LR: 6.504e-04  Data: 0.012 (0.013)
Train: 13 [1100/1251 ( 88%)]  Loss:  4.786228 (4.6035)  Time: 1.094s,  935.73/s  (1.091s,  938.73/s)  LR: 6.504e-04  Data: 0.016 (0.013)
Train: 13 [1150/1251 ( 92%)]  Loss:  4.749939 (4.6096)  Time: 1.093s,  936.60/s  (1.090s,  939.06/s)  LR: 6.504e-04  Data: 0.014 (0.013)
Train: 13 [1200/1251 ( 96%)]  Loss:  4.993854 (4.6249)  Time: 1.075s,  952.46/s  (1.090s,  939.26/s)  LR: 6.504e-04  Data: 0.013 (0.013)
Train: 13 [1250/1251 (100%)]  Loss:  4.494148 (4.6199)  Time: 1.062s,  964.58/s  (1.091s,  938.92/s)  LR: 6.504e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.857 (5.857)  Loss:  1.1664 (1.1664)  Acc@1: 75.1953 (75.1953)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.1686 (1.9436)  Acc@1: 74.0566 (57.5200)  Acc@5: 90.6840 (81.7980)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-4.pth.tar', 24.565999974365234)

Train: 14 [   0/1251 (  0%)]  Loss:  4.669551 (4.6696)  Time: 1.096s,  934.36/s  (1.096s,  934.36/s)  LR: 7.003e-04  Data: 0.028 (0.028)
Train: 14 [  50/1251 (  4%)]  Loss:  4.631266 (4.6504)  Time: 1.076s,  951.72/s  (1.089s,  940.19/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 100/1251 (  8%)]  Loss:  4.550382 (4.6171)  Time: 1.077s,  950.94/s  (1.089s,  940.28/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 150/1251 ( 12%)]  Loss:  4.404511 (4.5639)  Time: 1.120s,  914.67/s  (1.091s,  938.91/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 200/1251 ( 16%)]  Loss:  4.515799 (4.5543)  Time: 1.083s,  945.63/s  (1.092s,  938.00/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 250/1251 ( 20%)]  Loss:  4.828318 (4.6000)  Time: 1.083s,  945.61/s  (1.091s,  938.31/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 300/1251 ( 24%)]  Loss:  4.860055 (4.6371)  Time: 1.094s,  935.73/s  (1.092s,  937.51/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 350/1251 ( 28%)]  Loss:  4.330937 (4.5989)  Time: 1.077s,  950.68/s  (1.092s,  937.74/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 400/1251 ( 32%)]  Loss:  4.660754 (4.6057)  Time: 1.081s,  947.21/s  (1.091s,  938.41/s)  LR: 7.003e-04  Data: 0.015 (0.013)
Train: 14 [ 450/1251 ( 36%)]  Loss:  4.720132 (4.6172)  Time: 1.080s,  948.53/s  (1.091s,  938.69/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 500/1251 ( 40%)]  Loss:  4.664896 (4.6215)  Time: 1.082s,  946.72/s  (1.091s,  938.92/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 550/1251 ( 44%)]  Loss:  4.831224 (4.6390)  Time: 1.102s,  928.94/s  (1.091s,  938.55/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 600/1251 ( 48%)]  Loss:  4.826859 (4.6534)  Time: 1.082s,  946.39/s  (1.091s,  938.60/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 650/1251 ( 52%)]  Loss:  4.567630 (4.6473)  Time: 1.077s,  951.06/s  (1.091s,  938.60/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 700/1251 ( 56%)]  Loss:  4.743567 (4.6537)  Time: 1.095s,  935.55/s  (1.091s,  938.89/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 750/1251 ( 60%)]  Loss:  4.378110 (4.6365)  Time: 1.090s,  939.24/s  (1.091s,  938.99/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 800/1251 ( 64%)]  Loss:  4.736035 (4.6424)  Time: 1.076s,  951.26/s  (1.090s,  939.09/s)  LR: 7.003e-04  Data: 0.013 (0.013)
Train: 14 [ 850/1251 ( 68%)]  Loss:  4.701377 (4.6456)  Time: 1.080s,  948.58/s  (1.090s,  939.19/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 900/1251 ( 72%)]  Loss:  4.330709 (4.6291)  Time: 1.076s,  951.41/s  (1.090s,  939.12/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [ 950/1251 ( 76%)]  Loss:  4.744244 (4.6348)  Time: 1.075s,  952.73/s  (1.090s,  939.31/s)  LR: 7.003e-04  Data: 0.011 (0.013)
Train: 14 [1000/1251 ( 80%)]  Loss:  4.506428 (4.6287)  Time: 1.095s,  935.44/s  (1.090s,  939.08/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [1050/1251 ( 84%)]  Loss:  4.740934 (4.6338)  Time: 1.078s,  950.26/s  (1.090s,  939.18/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [1100/1251 ( 88%)]  Loss:  4.630679 (4.6337)  Time: 1.096s,  934.24/s  (1.090s,  939.20/s)  LR: 7.003e-04  Data: 0.017 (0.013)
Train: 14 [1150/1251 ( 92%)]  Loss:  4.735670 (4.6379)  Time: 1.081s,  947.41/s  (1.090s,  939.18/s)  LR: 7.003e-04  Data: 0.012 (0.013)
Train: 14 [1200/1251 ( 96%)]  Loss:  4.333224 (4.6257)  Time: 1.120s,  914.29/s  (1.090s,  939.08/s)  LR: 7.003e-04  Data: 0.014 (0.013)
Train: 14 [1250/1251 (100%)]  Loss:  4.616542 (4.6254)  Time: 1.062s,  964.59/s  (1.091s,  939.01/s)  LR: 7.003e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.797 (5.797)  Loss:  1.0725 (1.0725)  Acc@1: 76.2695 (76.2695)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  1.1607 (1.8614)  Acc@1: 75.5896 (58.4860)  Acc@5: 91.3915 (82.3860)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-5.pth.tar', 31.89200001953125)

Train: 15 [   0/1251 (  0%)]  Loss:  4.756062 (4.7561)  Time: 1.117s,  916.85/s  (1.117s,  916.85/s)  LR: 7.503e-04  Data: 0.035 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 15 [  50/1251 (  4%)]  Loss:  4.463936 (4.6100)  Time: 1.078s,  950.11/s  (1.089s,  940.02/s)  LR: 7.503e-04  Data: 0.014 (0.013)
Train: 15 [ 100/1251 (  8%)]  Loss:  4.663136 (4.6277)  Time: 1.085s,  944.16/s  (1.092s,  937.55/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 150/1251 ( 12%)]  Loss:  4.167147 (4.5126)  Time: 1.079s,  949.31/s  (1.092s,  937.38/s)  LR: 7.503e-04  Data: 0.014 (0.013)
Train: 15 [ 200/1251 ( 16%)]  Loss:  4.178787 (4.4458)  Time: 1.083s,  945.68/s  (1.092s,  938.14/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 250/1251 ( 20%)]  Loss:  4.657281 (4.4811)  Time: 1.078s,  950.23/s  (1.092s,  937.76/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 300/1251 ( 24%)]  Loss:  4.628835 (4.5022)  Time: 1.078s,  950.31/s  (1.091s,  938.46/s)  LR: 7.503e-04  Data: 0.015 (0.013)
Train: 15 [ 350/1251 ( 28%)]  Loss:  4.397685 (4.4891)  Time: 1.100s,  931.13/s  (1.091s,  938.19/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 400/1251 ( 32%)]  Loss:  4.752498 (4.5184)  Time: 1.078s,  949.70/s  (1.091s,  938.96/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 450/1251 ( 36%)]  Loss:  4.550755 (4.5216)  Time: 1.080s,  947.84/s  (1.091s,  938.82/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [ 500/1251 ( 40%)]  Loss:  4.206351 (4.4930)  Time: 1.096s,  933.99/s  (1.090s,  939.29/s)  LR: 7.503e-04  Data: 0.011 (0.013)
Train: 15 [ 550/1251 ( 44%)]  Loss:  4.932803 (4.5296)  Time: 1.077s,  951.02/s  (1.090s,  939.10/s)  LR: 7.503e-04  Data: 0.014 (0.013)
Train: 15 [ 600/1251 ( 48%)]  Loss:  4.641659 (4.5382)  Time: 1.093s,  936.47/s  (1.090s,  939.04/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 650/1251 ( 52%)]  Loss:  4.487943 (4.5346)  Time: 1.077s,  950.96/s  (1.091s,  938.91/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 700/1251 ( 56%)]  Loss:  4.229990 (4.5143)  Time: 1.079s,  948.79/s  (1.090s,  939.21/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [ 750/1251 ( 60%)]  Loss:  3.847741 (4.4727)  Time: 1.110s,  922.76/s  (1.090s,  939.08/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [ 800/1251 ( 64%)]  Loss:  4.712641 (4.4868)  Time: 1.094s,  935.73/s  (1.091s,  938.72/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [ 850/1251 ( 68%)]  Loss:  4.769876 (4.5025)  Time: 1.076s,  952.03/s  (1.091s,  938.99/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [ 900/1251 ( 72%)]  Loss:  4.764579 (4.5163)  Time: 1.094s,  936.36/s  (1.091s,  939.01/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [ 950/1251 ( 76%)]  Loss:  4.793910 (4.5302)  Time: 1.094s,  936.08/s  (1.091s,  938.72/s)  LR: 7.503e-04  Data: 0.011 (0.013)
Train: 15 [1000/1251 ( 80%)]  Loss:  4.233221 (4.5160)  Time: 1.078s,  949.87/s  (1.091s,  938.49/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [1050/1251 ( 84%)]  Loss:  4.633867 (4.5214)  Time: 1.078s,  949.62/s  (1.091s,  938.62/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [1100/1251 ( 88%)]  Loss:  4.523412 (4.5215)  Time: 1.078s,  950.27/s  (1.091s,  938.47/s)  LR: 7.503e-04  Data: 0.013 (0.013)
Train: 15 [1150/1251 ( 92%)]  Loss:  4.292602 (4.5119)  Time: 1.074s,  953.40/s  (1.091s,  938.50/s)  LR: 7.503e-04  Data: 0.010 (0.013)
Train: 15 [1200/1251 ( 96%)]  Loss:  4.319446 (4.5042)  Time: 1.103s,  928.33/s  (1.091s,  938.69/s)  LR: 7.503e-04  Data: 0.012 (0.013)
Train: 15 [1250/1251 (100%)]  Loss:  4.684694 (4.5112)  Time: 1.062s,  964.26/s  (1.091s,  938.73/s)  LR: 7.503e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.875 (5.875)  Loss:  1.0370 (1.0370)  Acc@1: 78.3203 (78.3203)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.229 (0.448)  Loss:  1.1949 (1.8357)  Acc@1: 74.1745 (59.2680)  Acc@5: 90.0943 (82.9600)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-6.pth.tar', 37.956000079345706)

Train: 16 [   0/1251 (  0%)]  Loss:  4.432331 (4.4323)  Time: 1.093s,  936.47/s  (1.093s,  936.47/s)  LR: 8.002e-04  Data: 0.031 (0.031)
Train: 16 [  50/1251 (  4%)]  Loss:  4.507825 (4.4701)  Time: 1.078s,  950.10/s  (1.119s,  915.36/s)  LR: 8.002e-04  Data: 0.013 (0.038)
Train: 16 [ 100/1251 (  8%)]  Loss:  4.837206 (4.5925)  Time: 1.082s,  946.82/s  (1.107s,  924.92/s)  LR: 8.002e-04  Data: 0.014 (0.026)
Train: 16 [ 150/1251 ( 12%)]  Loss:  4.282365 (4.5149)  Time: 1.082s,  946.76/s  (1.101s,  930.42/s)  LR: 8.002e-04  Data: 0.012 (0.021)
Train: 16 [ 200/1251 ( 16%)]  Loss:  4.296079 (4.4712)  Time: 1.083s,  945.48/s  (1.098s,  932.74/s)  LR: 8.002e-04  Data: 0.013 (0.019)
Train: 16 [ 250/1251 ( 20%)]  Loss:  4.384286 (4.4567)  Time: 1.095s,  934.85/s  (1.096s,  934.54/s)  LR: 8.002e-04  Data: 0.012 (0.018)
Train: 16 [ 300/1251 ( 24%)]  Loss:  4.370257 (4.4443)  Time: 1.114s,  919.55/s  (1.095s,  935.07/s)  LR: 8.002e-04  Data: 0.012 (0.017)
Train: 16 [ 350/1251 ( 28%)]  Loss:  4.465611 (4.4470)  Time: 1.080s,  948.16/s  (1.096s,  934.59/s)  LR: 8.002e-04  Data: 0.016 (0.017)
Train: 16 [ 400/1251 ( 32%)]  Loss:  4.291727 (4.4297)  Time: 1.077s,  950.36/s  (1.095s,  935.48/s)  LR: 8.002e-04  Data: 0.013 (0.016)
Train: 16 [ 450/1251 ( 36%)]  Loss:  4.748392 (4.4616)  Time: 1.076s,  951.54/s  (1.094s,  936.14/s)  LR: 8.002e-04  Data: 0.012 (0.016)
Train: 16 [ 500/1251 ( 40%)]  Loss:  4.450447 (4.4606)  Time: 1.088s,  941.13/s  (1.094s,  936.01/s)  LR: 8.002e-04  Data: 0.013 (0.016)
Train: 16 [ 550/1251 ( 44%)]  Loss:  4.290186 (4.4464)  Time: 1.078s,  949.88/s  (1.094s,  936.22/s)  LR: 8.002e-04  Data: 0.013 (0.015)
Train: 16 [ 600/1251 ( 48%)]  Loss:  3.882223 (4.4030)  Time: 1.097s,  933.68/s  (1.094s,  936.13/s)  LR: 8.002e-04  Data: 0.012 (0.015)
Train: 16 [ 650/1251 ( 52%)]  Loss:  4.667862 (4.4219)  Time: 1.104s,  927.59/s  (1.094s,  936.23/s)  LR: 8.002e-04  Data: 0.020 (0.015)
Train: 16 [ 700/1251 ( 56%)]  Loss:  4.554753 (4.4308)  Time: 1.098s,  932.74/s  (1.094s,  936.10/s)  LR: 8.002e-04  Data: 0.012 (0.015)
Train: 16 [ 750/1251 ( 60%)]  Loss:  4.485162 (4.4342)  Time: 1.081s,  947.15/s  (1.094s,  936.30/s)  LR: 8.002e-04  Data: 0.018 (0.015)
Train: 16 [ 800/1251 ( 64%)]  Loss:  4.529210 (4.4398)  Time: 1.096s,  934.09/s  (1.094s,  935.95/s)  LR: 8.002e-04  Data: 0.012 (0.015)
Train: 16 [ 850/1251 ( 68%)]  Loss:  4.112226 (4.4216)  Time: 1.094s,  936.15/s  (1.094s,  936.12/s)  LR: 8.002e-04  Data: 0.012 (0.015)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 16 [ 900/1251 ( 72%)]  Loss:  4.722769 (4.4374)  Time: 1.081s,  947.23/s  (1.093s,  936.56/s)  LR: 8.002e-04  Data: 0.014 (0.015)
Train: 16 [ 950/1251 ( 76%)]  Loss:  4.315814 (4.4313)  Time: 1.096s,  934.06/s  (1.093s,  936.56/s)  LR: 8.002e-04  Data: 0.012 (0.014)
Train: 16 [1000/1251 ( 80%)]  Loss:  4.354568 (4.4277)  Time: 1.097s,  933.55/s  (1.093s,  936.91/s)  LR: 8.002e-04  Data: 0.013 (0.014)
Train: 16 [1050/1251 ( 84%)]  Loss:  4.365290 (4.4248)  Time: 1.077s,  950.78/s  (1.092s,  937.44/s)  LR: 8.002e-04  Data: 0.013 (0.014)
Train: 16 [1100/1251 ( 88%)]  Loss:  4.424200 (4.4248)  Time: 1.074s,  953.16/s  (1.092s,  937.58/s)  LR: 8.002e-04  Data: 0.011 (0.014)
Train: 16 [1150/1251 ( 92%)]  Loss:  4.446299 (4.4257)  Time: 1.080s,  948.15/s  (1.092s,  937.36/s)  LR: 8.002e-04  Data: 0.014 (0.014)
Train: 16 [1200/1251 ( 96%)]  Loss:  4.566796 (4.4314)  Time: 1.105s,  926.91/s  (1.092s,  937.56/s)  LR: 8.002e-04  Data: 0.012 (0.014)
Train: 16 [1250/1251 (100%)]  Loss:  4.889284 (4.4490)  Time: 1.062s,  964.52/s  (1.092s,  937.65/s)  LR: 8.002e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 5.914 (5.914)  Loss:  1.0755 (1.0755)  Acc@1: 78.1250 (78.1250)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.0665 (1.7857)  Acc@1: 78.1840 (60.4780)  Acc@5: 93.9859 (83.7480)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-7.pth.tar', 42.34800000976563)

Train: 17 [   0/1251 (  0%)]  Loss:  4.532381 (4.5324)  Time: 1.111s,  922.10/s  (1.111s,  922.10/s)  LR: 8.502e-04  Data: 0.028 (0.028)
Train: 17 [  50/1251 (  4%)]  Loss:  4.894212 (4.7133)  Time: 1.095s,  935.16/s  (1.095s,  934.90/s)  LR: 8.502e-04  Data: 0.015 (0.013)
Train: 17 [ 100/1251 (  8%)]  Loss:  4.310593 (4.5791)  Time: 1.094s,  936.23/s  (1.091s,  938.44/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [ 150/1251 ( 12%)]  Loss:  4.648152 (4.5963)  Time: 1.081s,  947.56/s  (1.089s,  940.20/s)  LR: 8.502e-04  Data: 0.014 (0.013)
Train: 17 [ 200/1251 ( 16%)]  Loss:  4.251336 (4.5273)  Time: 1.078s,  949.73/s  (1.091s,  938.77/s)  LR: 8.502e-04  Data: 0.014 (0.013)
Train: 17 [ 250/1251 ( 20%)]  Loss:  4.387890 (4.5041)  Time: 1.095s,  934.94/s  (1.090s,  939.12/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [ 300/1251 ( 24%)]  Loss:  4.467711 (4.4989)  Time: 1.082s,  946.34/s  (1.090s,  939.27/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [ 350/1251 ( 28%)]  Loss:  4.227014 (4.4649)  Time: 1.080s,  948.55/s  (1.089s,  940.26/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [ 400/1251 ( 32%)]  Loss:  4.308156 (4.4475)  Time: 1.078s,  950.06/s  (1.089s,  940.11/s)  LR: 8.502e-04  Data: 0.015 (0.013)
Train: 17 [ 450/1251 ( 36%)]  Loss:  4.435936 (4.4463)  Time: 1.097s,  933.18/s  (1.090s,  939.36/s)  LR: 8.502e-04  Data: 0.018 (0.013)
Train: 17 [ 500/1251 ( 40%)]  Loss:  4.325201 (4.4353)  Time: 1.076s,  951.50/s  (1.090s,  939.25/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [ 550/1251 ( 44%)]  Loss:  4.337837 (4.4272)  Time: 1.104s,  927.67/s  (1.091s,  938.96/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [ 600/1251 ( 48%)]  Loss:  4.627855 (4.4426)  Time: 1.080s,  948.51/s  (1.090s,  939.02/s)  LR: 8.502e-04  Data: 0.016 (0.013)
Train: 17 [ 650/1251 ( 52%)]  Loss:  4.390762 (4.4389)  Time: 1.096s,  934.52/s  (1.091s,  938.67/s)  LR: 8.502e-04  Data: 0.011 (0.013)
Train: 17 [ 700/1251 ( 56%)]  Loss:  4.253036 (4.4265)  Time: 1.079s,  949.34/s  (1.091s,  938.43/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [ 750/1251 ( 60%)]  Loss:  4.244522 (4.4152)  Time: 1.077s,  950.82/s  (1.091s,  938.77/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [ 800/1251 ( 64%)]  Loss:  4.405449 (4.4146)  Time: 1.096s,  934.62/s  (1.091s,  938.93/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [ 850/1251 ( 68%)]  Loss:  4.333283 (4.4101)  Time: 1.122s,  912.61/s  (1.091s,  938.65/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [ 900/1251 ( 72%)]  Loss:  4.307266 (4.4047)  Time: 1.096s,  934.50/s  (1.092s,  938.06/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [ 950/1251 ( 76%)]  Loss:  4.351225 (4.4020)  Time: 1.080s,  948.36/s  (1.091s,  938.18/s)  LR: 8.502e-04  Data: 0.016 (0.013)
Train: 17 [1000/1251 ( 80%)]  Loss:  4.365255 (4.4002)  Time: 1.081s,  947.10/s  (1.091s,  938.16/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [1050/1251 ( 84%)]  Loss:  4.613552 (4.4099)  Time: 1.097s,  933.62/s  (1.091s,  938.19/s)  LR: 8.502e-04  Data: 0.012 (0.013)
Train: 17 [1100/1251 ( 88%)]  Loss:  4.717206 (4.4233)  Time: 1.084s,  944.72/s  (1.091s,  938.40/s)  LR: 8.502e-04  Data: 0.020 (0.013)
Train: 17 [1150/1251 ( 92%)]  Loss:  4.301124 (4.4182)  Time: 1.096s,  934.57/s  (1.091s,  938.44/s)  LR: 8.502e-04  Data: 0.015 (0.013)
Train: 17 [1200/1251 ( 96%)]  Loss:  4.241959 (4.4112)  Time: 1.081s,  947.14/s  (1.091s,  938.42/s)  LR: 8.502e-04  Data: 0.013 (0.013)
Train: 17 [1250/1251 (100%)]  Loss:  4.736401 (4.4237)  Time: 1.108s,  923.89/s  (1.091s,  938.50/s)  LR: 8.502e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.860 (5.860)  Loss:  1.1053 (1.1053)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.0569 (1.7708)  Acc@1: 77.5943 (61.0380)  Acc@5: 93.1604 (84.3560)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-8.pth.tar', 46.27199997070313)

Train: 18 [   0/1251 (  0%)]  Loss:  4.539225 (4.5392)  Time: 1.088s,  941.44/s  (1.088s,  941.44/s)  LR: 9.001e-04  Data: 0.026 (0.026)
Train: 18 [  50/1251 (  4%)]  Loss:  4.416144 (4.4777)  Time: 1.080s,  948.30/s  (1.093s,  937.01/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [ 100/1251 (  8%)]  Loss:  3.715986 (4.2238)  Time: 1.077s,  950.68/s  (1.090s,  939.16/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [ 150/1251 ( 12%)]  Loss:  4.413705 (4.2713)  Time: 1.104s,  927.81/s  (1.089s,  940.38/s)  LR: 9.001e-04  Data: 0.011 (0.013)
Train: 18 [ 200/1251 ( 16%)]  Loss:  4.263716 (4.2698)  Time: 1.105s,  926.44/s  (1.089s,  940.28/s)  LR: 9.001e-04  Data: 0.014 (0.013)
Train: 18 [ 250/1251 ( 20%)]  Loss:  4.242702 (4.2652)  Time: 1.079s,  949.44/s  (1.090s,  939.78/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [ 300/1251 ( 24%)]  Loss:  4.382641 (4.2820)  Time: 1.089s,  940.71/s  (1.090s,  939.63/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [ 350/1251 ( 28%)]  Loss:  4.081728 (4.2570)  Time: 1.077s,  951.10/s  (1.090s,  939.77/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 400/1251 ( 32%)]  Loss:  4.452640 (4.2787)  Time: 1.078s,  950.13/s  (1.089s,  940.01/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [ 450/1251 ( 36%)]  Loss:  4.153254 (4.2662)  Time: 1.093s,  936.67/s  (1.090s,  939.84/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 18 [ 500/1251 ( 40%)]  Loss:  4.551943 (4.2922)  Time: 1.076s,  951.80/s  (1.089s,  939.89/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 550/1251 ( 44%)]  Loss:  4.513397 (4.3106)  Time: 1.081s,  947.53/s  (1.090s,  939.84/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 600/1251 ( 48%)]  Loss:  4.629510 (4.3351)  Time: 1.078s,  949.86/s  (1.089s,  939.97/s)  LR: 9.001e-04  Data: 0.014 (0.013)
Train: 18 [ 650/1251 ( 52%)]  Loss:  4.311370 (4.3334)  Time: 1.096s,  934.42/s  (1.090s,  939.76/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 700/1251 ( 56%)]  Loss:  4.730617 (4.3599)  Time: 1.102s,  929.32/s  (1.090s,  939.56/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 750/1251 ( 60%)]  Loss:  4.210368 (4.3506)  Time: 1.084s,  944.87/s  (1.090s,  939.13/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 800/1251 ( 64%)]  Loss:  4.233966 (4.3437)  Time: 1.077s,  950.45/s  (1.091s,  938.80/s)  LR: 9.001e-04  Data: 0.014 (0.013)
Train: 18 [ 850/1251 ( 68%)]  Loss:  4.338030 (4.3434)  Time: 1.098s,  932.98/s  (1.090s,  939.09/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [ 900/1251 ( 72%)]  Loss:  4.812351 (4.3681)  Time: 1.076s,  951.25/s  (1.090s,  939.18/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [ 950/1251 ( 76%)]  Loss:  4.492571 (4.3743)  Time: 1.081s,  947.55/s  (1.090s,  939.20/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [1000/1251 ( 80%)]  Loss:  4.406294 (4.3758)  Time: 1.091s,  938.24/s  (1.090s,  939.28/s)  LR: 9.001e-04  Data: 0.014 (0.013)
Train: 18 [1050/1251 ( 84%)]  Loss:  4.167044 (4.3663)  Time: 1.082s,  946.07/s  (1.090s,  939.26/s)  LR: 9.001e-04  Data: 0.013 (0.013)
Train: 18 [1100/1251 ( 88%)]  Loss:  4.794882 (4.3850)  Time: 1.100s,  930.82/s  (1.090s,  939.15/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [1150/1251 ( 92%)]  Loss:  4.409766 (4.3860)  Time: 1.079s,  949.17/s  (1.090s,  939.26/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [1200/1251 ( 96%)]  Loss:  4.572436 (4.3935)  Time: 1.093s,  936.84/s  (1.090s,  939.34/s)  LR: 9.001e-04  Data: 0.012 (0.013)
Train: 18 [1250/1251 (100%)]  Loss:  4.837691 (4.4105)  Time: 1.063s,  963.75/s  (1.090s,  939.23/s)  LR: 9.001e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.917 (5.917)  Loss:  1.1654 (1.1654)  Acc@1: 77.0508 (77.0508)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.230 (0.448)  Loss:  1.0782 (1.7541)  Acc@1: 78.1840 (62.0340)  Acc@5: 91.9811 (84.7880)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-9.pth.tar', 49.56000014404297)

Train: 19 [   0/1251 (  0%)]  Loss:  4.538532 (4.5385)  Time: 1.094s,  936.25/s  (1.094s,  936.25/s)  LR: 9.501e-04  Data: 0.031 (0.031)
Train: 19 [  50/1251 (  4%)]  Loss:  4.766733 (4.6526)  Time: 1.079s,  949.07/s  (1.088s,  941.48/s)  LR: 9.501e-04  Data: 0.015 (0.015)
Train: 19 [ 100/1251 (  8%)]  Loss:  4.530068 (4.6118)  Time: 1.085s,  943.63/s  (1.090s,  939.08/s)  LR: 9.501e-04  Data: 0.013 (0.014)
Train: 19 [ 150/1251 ( 12%)]  Loss:  4.478683 (4.5785)  Time: 1.173s,  872.75/s  (1.092s,  938.02/s)  LR: 9.501e-04  Data: 0.012 (0.014)
Train: 19 [ 200/1251 ( 16%)]  Loss:  4.410478 (4.5449)  Time: 1.083s,  945.94/s  (1.091s,  938.42/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [ 250/1251 ( 20%)]  Loss:  4.255155 (4.4966)  Time: 1.120s,  914.17/s  (1.092s,  938.07/s)  LR: 9.501e-04  Data: 0.015 (0.013)
Train: 19 [ 300/1251 ( 24%)]  Loss:  4.228032 (4.4582)  Time: 1.094s,  936.38/s  (1.091s,  938.88/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [ 350/1251 ( 28%)]  Loss:  4.259874 (4.4334)  Time: 1.079s,  949.39/s  (1.091s,  938.68/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [ 400/1251 ( 32%)]  Loss:  4.756957 (4.4694)  Time: 1.077s,  950.73/s  (1.090s,  939.24/s)  LR: 9.501e-04  Data: 0.014 (0.013)
Train: 19 [ 450/1251 ( 36%)]  Loss:  4.192142 (4.4417)  Time: 1.078s,  950.15/s  (1.090s,  939.76/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [ 500/1251 ( 40%)]  Loss:  4.208240 (4.4204)  Time: 1.098s,  932.50/s  (1.090s,  939.67/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [ 550/1251 ( 44%)]  Loss:  4.644393 (4.4391)  Time: 1.078s,  949.74/s  (1.090s,  939.41/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [ 600/1251 ( 48%)]  Loss:  4.008868 (4.4060)  Time: 1.090s,  939.67/s  (1.090s,  939.36/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [ 650/1251 ( 52%)]  Loss:  4.238549 (4.3941)  Time: 1.176s,  870.94/s  (1.090s,  939.09/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [ 700/1251 ( 56%)]  Loss:  4.475832 (4.3995)  Time: 1.078s,  949.72/s  (1.091s,  938.93/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [ 750/1251 ( 60%)]  Loss:  4.641134 (4.4146)  Time: 1.077s,  950.45/s  (1.090s,  939.19/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [ 800/1251 ( 64%)]  Loss:  4.423169 (4.4151)  Time: 1.123s,  911.49/s  (1.090s,  939.24/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [ 850/1251 ( 68%)]  Loss:  4.594117 (4.4251)  Time: 1.075s,  952.60/s  (1.090s,  939.15/s)  LR: 9.501e-04  Data: 0.011 (0.013)
Train: 19 [ 900/1251 ( 72%)]  Loss:  4.394152 (4.4234)  Time: 1.085s,  944.10/s  (1.090s,  939.45/s)  LR: 9.501e-04  Data: 0.015 (0.013)
Train: 19 [ 950/1251 ( 76%)]  Loss:  4.358249 (4.4202)  Time: 1.076s,  951.54/s  (1.090s,  939.48/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [1000/1251 ( 80%)]  Loss:  4.430165 (4.4206)  Time: 1.106s,  926.17/s  (1.090s,  939.45/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [1050/1251 ( 84%)]  Loss:  4.374671 (4.4186)  Time: 1.087s,  942.28/s  (1.090s,  939.47/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [1100/1251 ( 88%)]  Loss:  4.539144 (4.4238)  Time: 1.090s,  939.80/s  (1.090s,  939.68/s)  LR: 9.501e-04  Data: 0.012 (0.013)
Train: 19 [1150/1251 ( 92%)]  Loss:  4.346544 (4.4206)  Time: 1.079s,  949.25/s  (1.090s,  939.79/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [1200/1251 ( 96%)]  Loss:  4.543671 (4.4255)  Time: 1.115s,  918.27/s  (1.090s,  939.60/s)  LR: 9.501e-04  Data: 0.013 (0.013)
Train: 19 [1250/1251 (100%)]  Loss:  4.084938 (4.4124)  Time: 1.079s,  948.75/s  (1.090s,  939.67/s)  LR: 9.501e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.859 (5.859)  Loss:  0.8705 (0.8705)  Acc@1: 80.0781 (80.0781)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  1.0365 (1.6534)  Acc@1: 78.1840 (62.7580)  Acc@5: 92.0991 (85.2520)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-10.pth.tar', 52.39200010498047)

Train: 20 [   0/1251 (  0%)]  Loss:  4.507771 (4.5078)  Time: 1.092s,  937.91/s  (1.092s,  937.91/s)  LR: 9.892e-04  Data: 0.027 (0.027)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 20 [  50/1251 (  4%)]  Loss:  4.439438 (4.4736)  Time: 1.096s,  934.63/s  (1.085s,  943.46/s)  LR: 9.892e-04  Data: 0.013 (0.014)
Train: 20 [ 100/1251 (  8%)]  Loss:  4.267327 (4.4048)  Time: 1.083s,  945.22/s  (1.089s,  940.71/s)  LR: 9.892e-04  Data: 0.016 (0.014)
Train: 20 [ 150/1251 ( 12%)]  Loss:  4.168324 (4.3457)  Time: 1.081s,  947.29/s  (1.089s,  940.67/s)  LR: 9.892e-04  Data: 0.013 (0.014)
Train: 20 [ 200/1251 ( 16%)]  Loss:  4.278059 (4.3322)  Time: 1.093s,  936.65/s  (1.088s,  941.15/s)  LR: 9.892e-04  Data: 0.011 (0.013)
Train: 20 [ 250/1251 ( 20%)]  Loss:  4.392327 (4.3422)  Time: 1.080s,  948.24/s  (1.089s,  940.23/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 300/1251 ( 24%)]  Loss:  4.421578 (4.3535)  Time: 1.172s,  873.39/s  (1.089s,  940.15/s)  LR: 9.892e-04  Data: 0.014 (0.013)
Train: 20 [ 350/1251 ( 28%)]  Loss:  4.305525 (4.3475)  Time: 1.084s,  944.83/s  (1.089s,  940.56/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 400/1251 ( 32%)]  Loss:  4.401315 (4.3535)  Time: 1.097s,  933.47/s  (1.088s,  941.34/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 450/1251 ( 36%)]  Loss:  4.344187 (4.3526)  Time: 1.105s,  927.06/s  (1.089s,  940.61/s)  LR: 9.892e-04  Data: 0.016 (0.013)
Train: 20 [ 500/1251 ( 40%)]  Loss:  4.181210 (4.3370)  Time: 1.096s,  934.40/s  (1.089s,  940.16/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [ 550/1251 ( 44%)]  Loss:  4.102291 (4.3174)  Time: 1.081s,  947.32/s  (1.089s,  940.10/s)  LR: 9.892e-04  Data: 0.014 (0.013)
Train: 20 [ 600/1251 ( 48%)]  Loss:  4.411510 (4.3247)  Time: 1.078s,  950.11/s  (1.089s,  939.92/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [ 650/1251 ( 52%)]  Loss:  4.298140 (4.3228)  Time: 1.084s,  944.72/s  (1.090s,  939.78/s)  LR: 9.892e-04  Data: 0.020 (0.013)
Train: 20 [ 700/1251 ( 56%)]  Loss:  4.445489 (4.3310)  Time: 1.103s,  928.63/s  (1.090s,  939.45/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 750/1251 ( 60%)]  Loss:  4.316843 (4.3301)  Time: 1.075s,  952.31/s  (1.090s,  939.36/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 800/1251 ( 64%)]  Loss:  4.410276 (4.3348)  Time: 1.094s,  936.05/s  (1.091s,  938.66/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 850/1251 ( 68%)]  Loss:  4.264646 (4.3309)  Time: 1.097s,  933.86/s  (1.091s,  938.73/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [ 900/1251 ( 72%)]  Loss:  4.029957 (4.3151)  Time: 1.096s,  934.60/s  (1.091s,  938.75/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [ 950/1251 ( 76%)]  Loss:  4.710895 (4.3349)  Time: 1.095s,  935.27/s  (1.091s,  938.63/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [1000/1251 ( 80%)]  Loss:  3.992634 (4.3186)  Time: 1.078s,  950.13/s  (1.091s,  938.56/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [1050/1251 ( 84%)]  Loss:  4.661488 (4.3341)  Time: 1.083s,  945.73/s  (1.091s,  938.54/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [1100/1251 ( 88%)]  Loss:  4.627417 (4.3469)  Time: 1.077s,  950.88/s  (1.091s,  938.64/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [1150/1251 ( 92%)]  Loss:  4.333590 (4.3463)  Time: 1.085s,  944.01/s  (1.091s,  938.64/s)  LR: 9.892e-04  Data: 0.012 (0.013)
Train: 20 [1200/1251 ( 96%)]  Loss:  4.479518 (4.3517)  Time: 1.096s,  934.26/s  (1.091s,  938.88/s)  LR: 9.892e-04  Data: 0.013 (0.013)
Train: 20 [1250/1251 (100%)]  Loss:  4.488820 (4.3569)  Time: 1.069s,  957.63/s  (1.091s,  938.82/s)  LR: 9.892e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.790 (5.790)  Loss:  0.9410 (0.9410)  Acc@1: 80.2734 (80.2734)  Acc@5: 93.3594 (93.3594)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.9756 (1.6354)  Acc@1: 78.8915 (63.5200)  Acc@5: 92.9245 (85.6100)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-11.pth.tar', 54.38800006347656)

Train: 21 [   0/1251 (  0%)]  Loss:  4.233425 (4.2334)  Time: 1.098s,  932.75/s  (1.098s,  932.75/s)  LR: 9.881e-04  Data: 0.033 (0.033)
Train: 21 [  50/1251 (  4%)]  Loss:  4.576621 (4.4050)  Time: 1.084s,  944.40/s  (1.095s,  934.88/s)  LR: 9.881e-04  Data: 0.013 (0.014)
Train: 21 [ 100/1251 (  8%)]  Loss:  4.256053 (4.3554)  Time: 1.079s,  949.06/s  (1.089s,  940.04/s)  LR: 9.881e-04  Data: 0.014 (0.014)
Train: 21 [ 150/1251 ( 12%)]  Loss:  3.912117 (4.2446)  Time: 1.085s,  943.87/s  (1.089s,  940.13/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 200/1251 ( 16%)]  Loss:  4.351989 (4.2660)  Time: 1.175s,  871.41/s  (1.090s,  939.84/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 250/1251 ( 20%)]  Loss:  4.417167 (4.2912)  Time: 1.099s,  931.73/s  (1.089s,  940.20/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 300/1251 ( 24%)]  Loss:  4.119116 (4.2666)  Time: 1.083s,  945.26/s  (1.089s,  940.25/s)  LR: 9.881e-04  Data: 0.019 (0.013)
Train: 21 [ 350/1251 ( 28%)]  Loss:  4.144926 (4.2514)  Time: 1.096s,  934.02/s  (1.089s,  940.49/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 400/1251 ( 32%)]  Loss:  4.161786 (4.2415)  Time: 1.178s,  869.20/s  (1.088s,  940.81/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 450/1251 ( 36%)]  Loss:  4.421097 (4.2594)  Time: 1.077s,  950.64/s  (1.088s,  940.96/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 21 [ 500/1251 ( 40%)]  Loss:  4.313019 (4.2643)  Time: 1.079s,  949.07/s  (1.088s,  941.35/s)  LR: 9.881e-04  Data: 0.014 (0.013)
Train: 21 [ 550/1251 ( 44%)]  Loss:  4.440225 (4.2790)  Time: 1.094s,  936.15/s  (1.088s,  941.13/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 600/1251 ( 48%)]  Loss:  4.359969 (4.2852)  Time: 1.095s,  935.40/s  (1.089s,  940.47/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [ 650/1251 ( 52%)]  Loss:  4.327817 (4.2882)  Time: 1.080s,  948.57/s  (1.089s,  940.28/s)  LR: 9.881e-04  Data: 0.014 (0.013)
Train: 21 [ 700/1251 ( 56%)]  Loss:  4.589890 (4.3083)  Time: 1.189s,  861.23/s  (1.089s,  939.89/s)  LR: 9.881e-04  Data: 0.015 (0.013)
Train: 21 [ 750/1251 ( 60%)]  Loss:  4.250052 (4.3047)  Time: 1.081s,  947.45/s  (1.089s,  940.06/s)  LR: 9.881e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0
Train: 21 [ 800/1251 ( 64%)]  Loss:  3.871682 (4.2792)  Time: 1.077s,  950.96/s  (1.089s,  940.17/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 21 [ 850/1251 ( 68%)]  Loss:  3.826458 (4.2541)  Time: 1.097s,  933.81/s  (1.089s,  940.26/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 21 [ 900/1251 ( 72%)]  Loss:  4.124349 (4.2473)  Time: 1.081s,  947.43/s  (1.089s,  940.41/s)  LR: 9.881e-04  Data: 0.014 (0.013)
Train: 21 [ 950/1251 ( 76%)]  Loss:  3.867708 (4.2283)  Time: 1.081s,  947.11/s  (1.089s,  940.32/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [1000/1251 ( 80%)]  Loss:  4.160392 (4.2250)  Time: 1.082s,  946.49/s  (1.089s,  940.20/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [1050/1251 ( 84%)]  Loss:  4.258449 (4.2266)  Time: 1.104s,  927.58/s  (1.089s,  940.13/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 21 [1100/1251 ( 88%)]  Loss:  4.216340 (4.2261)  Time: 1.078s,  949.63/s  (1.089s,  940.44/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 21 [1150/1251 ( 92%)]  Loss:  3.909826 (4.2129)  Time: 1.077s,  950.37/s  (1.089s,  940.53/s)  LR: 9.881e-04  Data: 0.012 (0.013)
Train: 21 [1200/1251 ( 96%)]  Loss:  4.184689 (4.2118)  Time: 1.077s,  950.61/s  (1.089s,  940.59/s)  LR: 9.881e-04  Data: 0.013 (0.013)
Train: 21 [1250/1251 (100%)]  Loss:  4.140378 (4.2091)  Time: 1.086s,  942.86/s  (1.089s,  940.37/s)  LR: 9.881e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.963 (5.963)  Loss:  0.9913 (0.9913)  Acc@1: 79.6875 (79.6875)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  1.0283 (1.6388)  Acc@1: 79.0094 (63.9520)  Acc@5: 93.3962 (86.2760)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-12.pth.tar', 55.51199990722656)

Train: 22 [   0/1251 (  0%)]  Loss:  4.324673 (4.3247)  Time: 1.117s,  916.93/s  (1.117s,  916.93/s)  LR: 9.869e-04  Data: 0.034 (0.034)
Train: 22 [  50/1251 (  4%)]  Loss:  4.170163 (4.2474)  Time: 1.079s,  948.65/s  (1.085s,  943.65/s)  LR: 9.869e-04  Data: 0.017 (0.014)
Train: 22 [ 100/1251 (  8%)]  Loss:  4.390567 (4.2951)  Time: 1.078s,  949.75/s  (1.091s,  938.67/s)  LR: 9.869e-04  Data: 0.013 (0.014)
Train: 22 [ 150/1251 ( 12%)]  Loss:  4.415138 (4.3251)  Time: 1.077s,  950.84/s  (1.091s,  938.83/s)  LR: 9.869e-04  Data: 0.015 (0.013)
Train: 22 [ 200/1251 ( 16%)]  Loss:  4.182106 (4.2965)  Time: 1.093s,  936.82/s  (1.091s,  938.49/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 250/1251 ( 20%)]  Loss:  4.282611 (4.2942)  Time: 1.080s,  948.00/s  (1.092s,  938.09/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 300/1251 ( 24%)]  Loss:  4.157855 (4.2747)  Time: 1.095s,  935.33/s  (1.091s,  938.47/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 350/1251 ( 28%)]  Loss:  4.186897 (4.2638)  Time: 1.076s,  951.27/s  (1.091s,  938.57/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [ 400/1251 ( 32%)]  Loss:  4.265519 (4.2639)  Time: 1.098s,  932.88/s  (1.091s,  938.44/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [ 450/1251 ( 36%)]  Loss:  3.938640 (4.2314)  Time: 1.086s,  942.57/s  (1.092s,  938.03/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 500/1251 ( 40%)]  Loss:  4.268777 (4.2348)  Time: 1.094s,  935.59/s  (1.091s,  938.21/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 550/1251 ( 44%)]  Loss:  4.222334 (4.2338)  Time: 1.083s,  945.85/s  (1.091s,  938.49/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [ 600/1251 ( 48%)]  Loss:  4.171258 (4.2290)  Time: 1.173s,  873.18/s  (1.091s,  938.42/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 22 [ 650/1251 ( 52%)]  Loss:  4.268078 (4.2318)  Time: 1.080s,  948.37/s  (1.091s,  938.91/s)  LR: 9.869e-04  Data: 0.016 (0.013)
Train: 22 [ 700/1251 ( 56%)]  Loss:  4.103297 (4.2232)  Time: 1.078s,  949.74/s  (1.090s,  939.31/s)  LR: 9.869e-04  Data: 0.014 (0.013)
Train: 22 [ 750/1251 ( 60%)]  Loss:  4.004072 (4.2095)  Time: 1.104s,  927.23/s  (1.090s,  939.29/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [ 800/1251 ( 64%)]  Loss:  3.980012 (4.1960)  Time: 1.077s,  950.48/s  (1.091s,  939.01/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [ 850/1251 ( 68%)]  Loss:  4.246238 (4.1988)  Time: 1.097s,  933.43/s  (1.091s,  938.80/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [ 900/1251 ( 72%)]  Loss:  4.335064 (4.2060)  Time: 1.079s,  949.29/s  (1.091s,  938.89/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [ 950/1251 ( 76%)]  Loss:  4.117398 (4.2015)  Time: 1.093s,  936.55/s  (1.091s,  938.72/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1000/1251 ( 80%)]  Loss:  4.284261 (4.2055)  Time: 1.076s,  951.32/s  (1.091s,  938.53/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [1050/1251 ( 84%)]  Loss:  4.426995 (4.2155)  Time: 1.077s,  950.97/s  (1.091s,  938.65/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [1100/1251 ( 88%)]  Loss:  4.221443 (4.2158)  Time: 1.098s,  932.20/s  (1.091s,  938.68/s)  LR: 9.869e-04  Data: 0.012 (0.013)
Train: 22 [1150/1251 ( 92%)]  Loss:  4.229301 (4.2164)  Time: 1.097s,  933.31/s  (1.091s,  938.34/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [1200/1251 ( 96%)]  Loss:  4.488667 (4.2273)  Time: 1.078s,  950.08/s  (1.091s,  938.17/s)  LR: 9.869e-04  Data: 0.013 (0.013)
Train: 22 [1250/1251 (100%)]  Loss:  4.197918 (4.2261)  Time: 1.063s,  963.19/s  (1.091s,  938.16/s)  LR: 9.869e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.859 (5.859)  Loss:  0.8643 (0.8643)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.9111 (1.5577)  Acc@1: 79.9528 (64.6820)  Acc@5: 94.2217 (86.7680)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-13.pth.tar', 57.51999997802734)

Train: 23 [   0/1251 (  0%)]  Loss:  3.988408 (3.9884)  Time: 1.088s,  941.33/s  (1.088s,  941.33/s)  LR: 9.857e-04  Data: 0.027 (0.027)
Train: 23 [  50/1251 (  4%)]  Loss:  4.206249 (4.0973)  Time: 1.091s,  938.20/s  (1.087s,  942.33/s)  LR: 9.857e-04  Data: 0.019 (0.014)
Train: 23 [ 100/1251 (  8%)]  Loss:  3.887141 (4.0273)  Time: 1.078s,  949.86/s  (1.091s,  938.70/s)  LR: 9.857e-04  Data: 0.014 (0.014)
Train: 23 [ 150/1251 ( 12%)]  Loss:  4.252708 (4.0836)  Time: 1.082s,  946.24/s  (1.090s,  939.21/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [ 200/1251 ( 16%)]  Loss:  4.174794 (4.1019)  Time: 1.094s,  935.96/s  (1.090s,  939.33/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 250/1251 ( 20%)]  Loss:  4.103068 (4.1021)  Time: 1.173s,  872.70/s  (1.091s,  938.87/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 23 [ 300/1251 ( 24%)]  Loss:  4.048482 (4.0944)  Time: 1.103s,  928.57/s  (1.091s,  938.98/s)  LR: 9.857e-04  Data: 0.015 (0.013)
Train: 23 [ 350/1251 ( 28%)]  Loss:  4.293179 (4.1193)  Time: 1.105s,  926.37/s  (1.090s,  939.08/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [ 400/1251 ( 32%)]  Loss:  4.158660 (4.1236)  Time: 1.094s,  936.14/s  (1.090s,  939.12/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 450/1251 ( 36%)]  Loss:  4.410928 (4.1524)  Time: 1.078s,  950.13/s  (1.090s,  939.32/s)  LR: 9.857e-04  Data: 0.015 (0.013)
Train: 23 [ 500/1251 ( 40%)]  Loss:  4.159846 (4.1530)  Time: 1.099s,  931.55/s  (1.090s,  939.56/s)  LR: 9.857e-04  Data: 0.015 (0.013)
Train: 23 [ 550/1251 ( 44%)]  Loss:  4.455152 (4.1782)  Time: 1.078s,  949.86/s  (1.089s,  940.20/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [ 600/1251 ( 48%)]  Loss:  4.242671 (4.1832)  Time: 1.081s,  947.26/s  (1.089s,  940.31/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 650/1251 ( 52%)]  Loss:  4.071328 (4.1752)  Time: 1.078s,  950.31/s  (1.089s,  940.50/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [ 700/1251 ( 56%)]  Loss:  4.294131 (4.1831)  Time: 1.094s,  935.81/s  (1.089s,  940.32/s)  LR: 9.857e-04  Data: 0.014 (0.013)
Train: 23 [ 750/1251 ( 60%)]  Loss:  4.464413 (4.2007)  Time: 1.086s,  942.99/s  (1.089s,  940.31/s)  LR: 9.857e-04  Data: 0.014 (0.013)
Train: 23 [ 800/1251 ( 64%)]  Loss:  4.248557 (4.2035)  Time: 1.079s,  949.25/s  (1.089s,  940.46/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 850/1251 ( 68%)]  Loss:  4.571808 (4.2240)  Time: 1.095s,  935.20/s  (1.089s,  940.36/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [ 900/1251 ( 72%)]  Loss:  4.280022 (4.2269)  Time: 1.105s,  927.06/s  (1.089s,  940.33/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [ 950/1251 ( 76%)]  Loss:  4.105419 (4.2208)  Time: 1.108s,  924.29/s  (1.090s,  939.85/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [1000/1251 ( 80%)]  Loss:  3.851618 (4.2033)  Time: 1.083s,  945.37/s  (1.089s,  939.95/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [1050/1251 ( 84%)]  Loss:  4.327434 (4.2089)  Time: 1.076s,  951.73/s  (1.089s,  940.18/s)  LR: 9.857e-04  Data: 0.013 (0.013)
Train: 23 [1100/1251 ( 88%)]  Loss:  4.641666 (4.2277)  Time: 1.109s,  923.60/s  (1.089s,  939.94/s)  LR: 9.857e-04  Data: 0.012 (0.013)
Train: 23 [1150/1251 ( 92%)]  Loss:  4.283094 (4.2300)  Time: 1.081s,  947.05/s  (1.090s,  939.78/s)  LR: 9.857e-04  Data: 0.014 (0.013)
Train: 23 [1200/1251 ( 96%)]  Loss:  4.135336 (4.2262)  Time: 1.081s,  947.39/s  (1.090s,  939.57/s)  LR: 9.857e-04  Data: 0.011 (0.013)
Train: 23 [1250/1251 (100%)]  Loss:  4.322184 (4.2299)  Time: 1.064s,  962.74/s  (1.090s,  939.64/s)  LR: 9.857e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.808 (5.808)  Loss:  0.8616 (0.8616)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.9091 (1.5457)  Acc@1: 79.4811 (65.1680)  Acc@5: 94.2217 (87.1460)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-14.pth.tar', 58.485999868164065)

Train: 24 [   0/1251 (  0%)]  Loss:  4.246258 (4.2463)  Time: 1.090s,  939.54/s  (1.090s,  939.54/s)  LR: 9.844e-04  Data: 0.028 (0.028)
Train: 24 [  50/1251 (  4%)]  Loss:  4.087644 (4.1670)  Time: 1.077s,  950.83/s  (1.090s,  939.61/s)  LR: 9.844e-04  Data: 0.013 (0.013)
Train: 24 [ 100/1251 (  8%)]  Loss:  3.952609 (4.0955)  Time: 1.082s,  946.03/s  (1.090s,  939.87/s)  LR: 9.844e-04  Data: 0.015 (0.013)
Train: 24 [ 150/1251 ( 12%)]  Loss:  3.974429 (4.0652)  Time: 1.078s,  949.70/s  (1.089s,  940.26/s)  LR: 9.844e-04  Data: 0.014 (0.013)
Train: 24 [ 200/1251 ( 16%)]  Loss:  4.003146 (4.0528)  Time: 1.093s,  936.95/s  (1.091s,  938.64/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 250/1251 ( 20%)]  Loss:  3.988758 (4.0421)  Time: 1.094s,  935.77/s  (1.091s,  938.33/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 300/1251 ( 24%)]  Loss:  3.761379 (4.0020)  Time: 1.104s,  927.60/s  (1.090s,  939.07/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 350/1251 ( 28%)]  Loss:  4.258974 (4.0341)  Time: 1.094s,  935.64/s  (1.090s,  939.32/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 400/1251 ( 32%)]  Loss:  4.136811 (4.0456)  Time: 1.088s,  940.96/s  (1.090s,  939.19/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 450/1251 ( 36%)]  Loss:  4.158844 (4.0569)  Time: 1.080s,  947.79/s  (1.090s,  939.81/s)  LR: 9.844e-04  Data: 0.013 (0.013)
Train: 24 [ 500/1251 ( 40%)]  Loss:  3.766326 (4.0305)  Time: 1.081s,  947.56/s  (1.089s,  939.95/s)  LR: 9.844e-04  Data: 0.018 (0.013)
Train: 24 [ 550/1251 ( 44%)]  Loss:  4.196192 (4.0443)  Time: 1.078s,  949.57/s  (1.090s,  939.75/s)  LR: 9.844e-04  Data: 0.014 (0.013)
Train: 24 [ 600/1251 ( 48%)]  Loss:  4.376557 (4.0698)  Time: 1.080s,  948.58/s  (1.089s,  939.94/s)  LR: 9.844e-04  Data: 0.015 (0.013)
Train: 24 [ 650/1251 ( 52%)]  Loss:  4.014465 (4.0659)  Time: 1.084s,  944.76/s  (1.090s,  939.78/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 700/1251 ( 56%)]  Loss:  4.190092 (4.0742)  Time: 1.082s,  946.60/s  (1.090s,  939.85/s)  LR: 9.844e-04  Data: 0.013 (0.013)
Train: 24 [ 750/1251 ( 60%)]  Loss:  3.783876 (4.0560)  Time: 1.087s,  941.95/s  (1.089s,  939.95/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 800/1251 ( 64%)]  Loss:  4.134129 (4.0606)  Time: 1.174s,  872.32/s  (1.090s,  939.72/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 850/1251 ( 68%)]  Loss:  4.165544 (4.0664)  Time: 1.094s,  935.72/s  (1.090s,  939.86/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 900/1251 ( 72%)]  Loss:  4.451436 (4.0867)  Time: 1.077s,  951.13/s  (1.089s,  940.13/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [ 950/1251 ( 76%)]  Loss:  3.949605 (4.0799)  Time: 1.078s,  950.19/s  (1.089s,  940.00/s)  LR: 9.844e-04  Data: 0.014 (0.013)
Train: 24 [1000/1251 ( 80%)]  Loss:  4.421832 (4.0961)  Time: 1.095s,  935.03/s  (1.089s,  939.94/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [1050/1251 ( 84%)]  Loss:  4.202133 (4.1010)  Time: 1.082s,  946.00/s  (1.089s,  940.06/s)  LR: 9.844e-04  Data: 0.013 (0.013)
Train: 24 [1100/1251 ( 88%)]  Loss:  4.087208 (4.1004)  Time: 1.173s,  873.05/s  (1.089s,  939.96/s)  LR: 9.844e-04  Data: 0.015 (0.013)
Train: 24 [1150/1251 ( 92%)]  Loss:  4.028482 (4.0974)  Time: 1.077s,  951.22/s  (1.090s,  939.86/s)  LR: 9.844e-04  Data: 0.013 (0.013)
Train: 24 [1200/1251 ( 96%)]  Loss:  4.136083 (4.0989)  Time: 1.095s,  934.93/s  (1.090s,  939.82/s)  LR: 9.844e-04  Data: 0.012 (0.013)
Train: 24 [1250/1251 (100%)]  Loss:  3.734957 (4.0849)  Time: 1.079s,  949.07/s  (1.090s,  939.88/s)  LR: 9.844e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.816 (5.816)  Loss:  0.7955 (0.7955)  Acc@1: 83.2031 (83.2031)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.229 (0.448)  Loss:  0.8829 (1.4849)  Acc@1: 81.1321 (66.0460)  Acc@5: 94.2217 (87.5440)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-15.pth.tar', 59.26800002929688)

Train: 25 [   0/1251 (  0%)]  Loss:  3.483623 (3.4836)  Time: 1.098s,  932.69/s  (1.098s,  932.69/s)  LR: 9.831e-04  Data: 0.034 (0.034)
Train: 25 [  50/1251 (  4%)]  Loss:  3.963272 (3.7234)  Time: 1.082s,  946.30/s  (1.089s,  939.95/s)  LR: 9.831e-04  Data: 0.012 (0.014)
Train: 25 [ 100/1251 (  8%)]  Loss:  3.898139 (3.7817)  Time: 1.076s,  951.68/s  (1.090s,  939.58/s)  LR: 9.831e-04  Data: 0.013 (0.014)
Train: 25 [ 150/1251 ( 12%)]  Loss:  4.121836 (3.8667)  Time: 1.079s,  948.78/s  (1.089s,  940.72/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [ 200/1251 ( 16%)]  Loss:  4.303154 (3.9540)  Time: 1.078s,  949.84/s  (1.089s,  940.60/s)  LR: 9.831e-04  Data: 0.014 (0.013)
Train: 25 [ 250/1251 ( 20%)]  Loss:  4.167139 (3.9895)  Time: 1.078s,  950.15/s  (1.090s,  939.83/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [ 300/1251 ( 24%)]  Loss:  4.295463 (4.0332)  Time: 1.073s,  954.03/s  (1.089s,  940.33/s)  LR: 9.831e-04  Data: 0.011 (0.013)
Train: 25 [ 350/1251 ( 28%)]  Loss:  4.255861 (4.0611)  Time: 1.077s,  950.36/s  (1.089s,  940.64/s)  LR: 9.831e-04  Data: 0.014 (0.013)
Train: 25 [ 400/1251 ( 32%)]  Loss:  4.265922 (4.0838)  Time: 1.103s,  928.14/s  (1.089s,  940.61/s)  LR: 9.831e-04  Data: 0.021 (0.013)
Train: 25 [ 450/1251 ( 36%)]  Loss:  3.954559 (4.0709)  Time: 1.076s,  951.62/s  (1.089s,  940.67/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [ 500/1251 ( 40%)]  Loss:  4.179770 (4.0808)  Time: 1.092s,  938.03/s  (1.088s,  941.06/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [ 550/1251 ( 44%)]  Loss:  4.431906 (4.1101)  Time: 1.095s,  935.53/s  (1.088s,  940.85/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [ 600/1251 ( 48%)]  Loss:  4.021951 (4.1033)  Time: 1.094s,  935.90/s  (1.088s,  940.92/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [ 650/1251 ( 52%)]  Loss:  3.850238 (4.0852)  Time: 1.078s,  949.78/s  (1.089s,  940.65/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [ 700/1251 ( 56%)]  Loss:  3.966158 (4.0773)  Time: 1.078s,  950.30/s  (1.089s,  940.28/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [ 750/1251 ( 60%)]  Loss:  4.014641 (4.0734)  Time: 1.080s,  948.39/s  (1.089s,  940.38/s)  LR: 9.831e-04  Data: 0.014 (0.013)
Train: 25 [ 800/1251 ( 64%)]  Loss:  4.296992 (4.0865)  Time: 1.095s,  935.13/s  (1.089s,  940.48/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [ 850/1251 ( 68%)]  Loss:  3.727209 (4.0665)  Time: 1.188s,  861.75/s  (1.089s,  940.30/s)  LR: 9.831e-04  Data: 0.016 (0.013)
Train: 25 [ 900/1251 ( 72%)]  Loss:  4.136735 (4.0702)  Time: 1.097s,  933.30/s  (1.090s,  939.84/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [ 950/1251 ( 76%)]  Loss:  3.970716 (4.0653)  Time: 1.089s,  940.66/s  (1.090s,  939.76/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [1000/1251 ( 80%)]  Loss:  4.000360 (4.0622)  Time: 1.079s,  949.38/s  (1.090s,  939.63/s)  LR: 9.831e-04  Data: 0.014 (0.013)
Train: 25 [1050/1251 ( 84%)]  Loss:  3.824946 (4.0514)  Time: 1.076s,  951.31/s  (1.090s,  939.47/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [1100/1251 ( 88%)]  Loss:  4.170474 (4.0566)  Time: 1.091s,  938.61/s  (1.090s,  939.66/s)  LR: 9.831e-04  Data: 0.012 (0.013)
Train: 25 [1150/1251 ( 92%)]  Loss:  4.325961 (4.0678)  Time: 1.099s,  931.89/s  (1.090s,  939.66/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [1200/1251 ( 96%)]  Loss:  4.311361 (4.0775)  Time: 1.078s,  949.63/s  (1.090s,  939.75/s)  LR: 9.831e-04  Data: 0.013 (0.013)
Train: 25 [1250/1251 (100%)]  Loss:  4.160983 (4.0807)  Time: 1.061s,  965.37/s  (1.090s,  939.39/s)  LR: 9.831e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.908 (5.908)  Loss:  0.7851 (0.7851)  Acc@1: 83.2031 (83.2031)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.9335 (1.4776)  Acc@1: 79.7170 (66.3900)  Acc@5: 94.1038 (87.8480)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-16.pth.tar', 60.47800021972656)

Train: 26 [   0/1251 (  0%)]  Loss:  4.074212 (4.0742)  Time: 1.090s,  939.63/s  (1.090s,  939.63/s)  LR: 9.818e-04  Data: 0.027 (0.027)
Train: 26 [  50/1251 (  4%)]  Loss:  4.255097 (4.1647)  Time: 1.086s,  943.07/s  (1.089s,  940.41/s)  LR: 9.818e-04  Data: 0.014 (0.013)
Train: 26 [ 100/1251 (  8%)]  Loss:  4.354567 (4.2280)  Time: 1.105s,  926.63/s  (1.085s,  943.73/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 150/1251 ( 12%)]  Loss:  4.081800 (4.1914)  Time: 1.078s,  949.96/s  (1.087s,  942.28/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 200/1251 ( 16%)]  Loss:  3.795786 (4.1123)  Time: 1.077s,  950.42/s  (1.088s,  941.22/s)  LR: 9.818e-04  Data: 0.015 (0.013)
Train: 26 [ 250/1251 ( 20%)]  Loss:  3.782253 (4.0573)  Time: 1.103s,  927.99/s  (1.088s,  941.02/s)  LR: 9.818e-04  Data: 0.017 (0.013)
Train: 26 [ 300/1251 ( 24%)]  Loss:  3.844844 (4.0269)  Time: 1.098s,  932.87/s  (1.090s,  939.18/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 350/1251 ( 28%)]  Loss:  4.128845 (4.0397)  Time: 1.096s,  934.22/s  (1.090s,  939.54/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 400/1251 ( 32%)]  Loss:  4.013767 (4.0368)  Time: 1.100s,  930.62/s  (1.090s,  939.35/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 450/1251 ( 36%)]  Loss:  4.161518 (4.0493)  Time: 1.193s,  858.28/s  (1.091s,  938.80/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 500/1251 ( 40%)]  Loss:  4.228449 (4.0656)  Time: 1.095s,  935.04/s  (1.090s,  939.20/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 550/1251 ( 44%)]  Loss:  4.250899 (4.0810)  Time: 1.095s,  935.41/s  (1.091s,  938.88/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 600/1251 ( 48%)]  Loss:  3.942941 (4.0704)  Time: 1.103s,  928.61/s  (1.090s,  939.05/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 650/1251 ( 52%)]  Loss:  4.057607 (4.0695)  Time: 1.075s,  952.66/s  (1.091s,  938.92/s)  LR: 9.818e-04  Data: 0.011 (0.013)
Train: 26 [ 700/1251 ( 56%)]  Loss:  4.107847 (4.0720)  Time: 1.077s,  950.52/s  (1.090s,  939.08/s)  LR: 9.818e-04  Data: 0.013 (0.013)
Train: 26 [ 750/1251 ( 60%)]  Loss:  3.824334 (4.0565)  Time: 1.078s,  949.83/s  (1.090s,  939.12/s)  LR: 9.818e-04  Data: 0.013 (0.013)
Train: 26 [ 800/1251 ( 64%)]  Loss:  4.138888 (4.0614)  Time: 1.106s,  926.08/s  (1.090s,  939.40/s)  LR: 9.818e-04  Data: 0.021 (0.013)
Train: 26 [ 850/1251 ( 68%)]  Loss:  4.221014 (4.0703)  Time: 1.088s,  940.85/s  (1.090s,  939.53/s)  LR: 9.818e-04  Data: 0.018 (0.013)
Train: 26 [ 900/1251 ( 72%)]  Loss:  4.272861 (4.0809)  Time: 1.096s,  934.46/s  (1.090s,  939.19/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [ 950/1251 ( 76%)]  Loss:  3.688493 (4.0613)  Time: 1.077s,  951.22/s  (1.091s,  939.00/s)  LR: 9.818e-04  Data: 0.013 (0.013)
Train: 26 [1000/1251 ( 80%)]  Loss:  4.185759 (4.0672)  Time: 1.094s,  936.31/s  (1.090s,  939.32/s)  LR: 9.818e-04  Data: 0.012 (0.013)
Train: 26 [1050/1251 ( 84%)]  Loss:  4.113136 (4.0693)  Time: 1.072s,  955.18/s  (1.090s,  939.31/s)  LR: 9.818e-04  Data: 0.010 (0.013)
Train: 26 [1100/1251 ( 88%)]  Loss:  3.925649 (4.0631)  Time: 1.098s,  932.58/s  (1.090s,  939.52/s)  LR: 9.818e-04  Data: 0.013 (0.013)
Train: 26 [1150/1251 ( 92%)]  Loss:  3.876848 (4.0553)  Time: 1.080s,  948.35/s  (1.090s,  939.52/s)  LR: 9.818e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 26 [1200/1251 ( 96%)]  Loss:  4.120308 (4.0579)  Time: 1.125s,  910.02/s  (1.090s,  939.56/s)  LR: 9.818e-04  Data: 0.013 (0.013)
Train: 26 [1250/1251 (100%)]  Loss:  4.274043 (4.0662)  Time: 1.063s,  963.12/s  (1.090s,  939.34/s)  LR: 9.818e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.854 (5.854)  Loss:  0.8253 (0.8253)  Acc@1: 84.0820 (84.0820)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.8368 (1.4864)  Acc@1: 81.2500 (66.7660)  Acc@5: 94.6934 (88.0660)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-17.pth.tar', 61.03800009277344)

Train: 27 [   0/1251 (  0%)]  Loss:  4.227621 (4.2276)  Time: 1.099s,  931.78/s  (1.099s,  931.78/s)  LR: 9.803e-04  Data: 0.035 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 27 [  50/1251 (  4%)]  Loss:  3.997510 (4.1126)  Time: 1.082s,  946.24/s  (1.087s,  941.81/s)  LR: 9.803e-04  Data: 0.018 (0.014)
Train: 27 [ 100/1251 (  8%)]  Loss:  3.773379 (3.9995)  Time: 1.104s,  927.89/s  (1.088s,  940.90/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 150/1251 ( 12%)]  Loss:  4.185193 (4.0459)  Time: 1.094s,  935.76/s  (1.089s,  940.47/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 200/1251 ( 16%)]  Loss:  4.118878 (4.0605)  Time: 1.079s,  949.37/s  (1.089s,  940.41/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 250/1251 ( 20%)]  Loss:  4.073441 (4.0627)  Time: 1.084s,  944.71/s  (1.089s,  940.56/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 300/1251 ( 24%)]  Loss:  4.190045 (4.0809)  Time: 1.076s,  951.74/s  (1.089s,  940.41/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 350/1251 ( 28%)]  Loss:  3.996074 (4.0703)  Time: 1.079s,  949.30/s  (1.089s,  940.65/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 400/1251 ( 32%)]  Loss:  4.306333 (4.0965)  Time: 1.096s,  934.72/s  (1.089s,  939.97/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 450/1251 ( 36%)]  Loss:  4.338240 (4.1207)  Time: 1.095s,  935.03/s  (1.090s,  939.42/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 500/1251 ( 40%)]  Loss:  3.828448 (4.0941)  Time: 1.079s,  948.86/s  (1.090s,  939.66/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 550/1251 ( 44%)]  Loss:  4.219796 (4.1046)  Time: 1.076s,  951.75/s  (1.090s,  939.64/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 600/1251 ( 48%)]  Loss:  4.228094 (4.1141)  Time: 1.104s,  927.25/s  (1.089s,  939.91/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 650/1251 ( 52%)]  Loss:  4.007563 (4.1065)  Time: 1.084s,  944.71/s  (1.089s,  939.89/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 700/1251 ( 56%)]  Loss:  4.276487 (4.1178)  Time: 1.076s,  951.73/s  (1.090s,  939.86/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 750/1251 ( 60%)]  Loss:  4.062044 (4.1143)  Time: 1.081s,  947.27/s  (1.089s,  940.10/s)  LR: 9.803e-04  Data: 0.012 (0.013)
Train: 27 [ 800/1251 ( 64%)]  Loss:  3.921674 (4.1030)  Time: 1.100s,  931.25/s  (1.089s,  939.88/s)  LR: 9.803e-04  Data: 0.011 (0.013)
Train: 27 [ 850/1251 ( 68%)]  Loss:  3.870102 (4.0901)  Time: 1.079s,  949.07/s  (1.090s,  939.68/s)  LR: 9.803e-04  Data: 0.014 (0.013)
Train: 27 [ 900/1251 ( 72%)]  Loss:  4.196245 (4.0956)  Time: 1.082s,  946.47/s  (1.089s,  940.13/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [ 950/1251 ( 76%)]  Loss:  3.898747 (4.0858)  Time: 1.096s,  933.91/s  (1.089s,  940.29/s)  LR: 9.803e-04  Data: 0.019 (0.013)
Train: 27 [1000/1251 ( 80%)]  Loss:  4.348342 (4.0983)  Time: 1.081s,  947.15/s  (1.089s,  940.24/s)  LR: 9.803e-04  Data: 0.014 (0.013)
Train: 27 [1050/1251 ( 84%)]  Loss:  4.193318 (4.1026)  Time: 1.099s,  931.65/s  (1.089s,  940.00/s)  LR: 9.803e-04  Data: 0.015 (0.013)
Train: 27 [1100/1251 ( 88%)]  Loss:  3.874156 (4.0927)  Time: 1.077s,  950.51/s  (1.089s,  940.26/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [1150/1251 ( 92%)]  Loss:  4.006450 (4.0891)  Time: 1.077s,  950.39/s  (1.089s,  940.34/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [1200/1251 ( 96%)]  Loss:  4.114175 (4.0901)  Time: 1.079s,  949.18/s  (1.089s,  940.20/s)  LR: 9.803e-04  Data: 0.013 (0.013)
Train: 27 [1250/1251 (100%)]  Loss:  4.487254 (4.1054)  Time: 1.081s,  947.10/s  (1.089s,  940.25/s)  LR: 9.803e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.832 (5.832)  Loss:  0.7825 (0.7825)  Acc@1: 83.6914 (83.6914)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.8475 (1.4726)  Acc@1: 81.9575 (67.3320)  Acc@5: 94.9293 (88.2540)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-18.pth.tar', 62.0339999609375)

Train: 28 [   0/1251 (  0%)]  Loss:  3.942616 (3.9426)  Time: 1.100s,  930.74/s  (1.100s,  930.74/s)  LR: 9.789e-04  Data: 0.038 (0.038)
Train: 28 [  50/1251 (  4%)]  Loss:  4.351534 (4.1471)  Time: 1.077s,  950.52/s  (1.092s,  937.34/s)  LR: 9.789e-04  Data: 0.012 (0.015)
Train: 28 [ 100/1251 (  8%)]  Loss:  4.240985 (4.1784)  Time: 1.096s,  934.45/s  (1.090s,  939.14/s)  LR: 9.789e-04  Data: 0.013 (0.014)
Train: 28 [ 150/1251 ( 12%)]  Loss:  3.676147 (4.0528)  Time: 1.076s,  951.64/s  (1.091s,  938.46/s)  LR: 9.789e-04  Data: 0.012 (0.014)
Train: 28 [ 200/1251 ( 16%)]  Loss:  4.371279 (4.1165)  Time: 1.093s,  936.75/s  (1.091s,  938.20/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 250/1251 ( 20%)]  Loss:  3.906389 (4.0815)  Time: 1.096s,  933.98/s  (1.090s,  939.36/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [ 300/1251 ( 24%)]  Loss:  3.805935 (4.0421)  Time: 1.083s,  945.70/s  (1.090s,  939.30/s)  LR: 9.789e-04  Data: 0.017 (0.013)
Train: 28 [ 350/1251 ( 28%)]  Loss:  4.303010 (4.0747)  Time: 1.098s,  932.68/s  (1.090s,  939.27/s)  LR: 9.789e-04  Data: 0.016 (0.013)
Train: 28 [ 400/1251 ( 32%)]  Loss:  4.172436 (4.0856)  Time: 1.076s,  951.32/s  (1.091s,  938.81/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [ 450/1251 ( 36%)]  Loss:  3.910449 (4.0681)  Time: 1.079s,  949.42/s  (1.090s,  939.07/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 500/1251 ( 40%)]  Loss:  3.816357 (4.0452)  Time: 1.103s,  928.39/s  (1.090s,  939.20/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 550/1251 ( 44%)]  Loss:  4.280271 (4.0648)  Time: 1.076s,  951.47/s  (1.090s,  939.10/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 600/1251 ( 48%)]  Loss:  4.066550 (4.0649)  Time: 1.084s,  944.57/s  (1.090s,  939.13/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [ 650/1251 ( 52%)]  Loss:  3.725391 (4.0407)  Time: 1.082s,  946.61/s  (1.090s,  939.46/s)  LR: 9.789e-04  Data: 0.017 (0.013)
Train: 28 [ 700/1251 ( 56%)]  Loss:  3.893975 (4.0309)  Time: 1.096s,  934.61/s  (1.090s,  939.70/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 750/1251 ( 60%)]  Loss:  4.208283 (4.0420)  Time: 1.098s,  932.51/s  (1.090s,  939.68/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 800/1251 ( 64%)]  Loss:  4.073205 (4.0438)  Time: 1.098s,  932.81/s  (1.090s,  939.81/s)  LR: 9.789e-04  Data: 0.012 (0.013)
Train: 28 [ 850/1251 ( 68%)]  Loss:  4.429034 (4.0652)  Time: 1.095s,  935.40/s  (1.090s,  939.66/s)  LR: 9.789e-04  Data: 0.014 (0.013)
Train: 28 [ 900/1251 ( 72%)]  Loss:  4.026003 (4.0632)  Time: 1.078s,  949.58/s  (1.090s,  939.41/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [ 950/1251 ( 76%)]  Loss:  3.648561 (4.0424)  Time: 1.095s,  934.81/s  (1.090s,  939.52/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [1000/1251 ( 80%)]  Loss:  4.074032 (4.0439)  Time: 1.077s,  950.59/s  (1.090s,  939.38/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [1050/1251 ( 84%)]  Loss:  4.031479 (4.0434)  Time: 1.080s,  947.92/s  (1.090s,  939.24/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [1100/1251 ( 88%)]  Loss:  4.247621 (4.0522)  Time: 1.099s,  931.38/s  (1.090s,  939.15/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [1150/1251 ( 92%)]  Loss:  4.078831 (4.0533)  Time: 1.172s,  873.75/s  (1.090s,  939.06/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [1200/1251 ( 96%)]  Loss:  4.289412 (4.0628)  Time: 1.084s,  944.49/s  (1.090s,  939.32/s)  LR: 9.789e-04  Data: 0.013 (0.013)
Train: 28 [1250/1251 (100%)]  Loss:  3.912429 (4.0570)  Time: 1.154s,  887.24/s  (1.090s,  939.31/s)  LR: 9.789e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.805 (5.805)  Loss:  0.7424 (0.7424)  Acc@1: 82.8125 (82.8125)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.8453 (1.4178)  Acc@1: 81.7217 (67.6460)  Acc@5: 94.6934 (88.7280)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-19.pth.tar', 62.75800009033203)

Train: 29 [   0/1251 (  0%)]  Loss:  4.224963 (4.2250)  Time: 1.095s,  934.74/s  (1.095s,  934.74/s)  LR: 9.773e-04  Data: 0.031 (0.031)
Train: 29 [  50/1251 (  4%)]  Loss:  3.952950 (4.0890)  Time: 1.081s,  947.57/s  (1.083s,  945.27/s)  LR: 9.773e-04  Data: 0.016 (0.014)
Train: 29 [ 100/1251 (  8%)]  Loss:  4.178071 (4.1187)  Time: 1.078s,  949.49/s  (1.087s,  942.12/s)  LR: 9.773e-04  Data: 0.016 (0.014)
Train: 29 [ 150/1251 ( 12%)]  Loss:  4.139488 (4.1239)  Time: 1.096s,  934.64/s  (1.086s,  942.60/s)  LR: 9.773e-04  Data: 0.014 (0.013)
Train: 29 [ 200/1251 ( 16%)]  Loss:  4.101092 (4.1193)  Time: 1.096s,  934.39/s  (1.087s,  942.28/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [ 250/1251 ( 20%)]  Loss:  4.143539 (4.1234)  Time: 1.097s,  933.84/s  (1.087s,  941.97/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [ 300/1251 ( 24%)]  Loss:  4.231544 (4.1388)  Time: 1.085s,  943.63/s  (1.088s,  941.47/s)  LR: 9.773e-04  Data: 0.014 (0.013)
Train: 29 [ 350/1251 ( 28%)]  Loss:  4.211421 (4.1479)  Time: 1.079s,  949.17/s  (1.088s,  941.33/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 400/1251 ( 32%)]  Loss:  4.244360 (4.1586)  Time: 1.079s,  948.93/s  (1.088s,  941.20/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [ 450/1251 ( 36%)]  Loss:  3.672291 (4.1100)  Time: 1.087s,  941.71/s  (1.088s,  941.54/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [ 500/1251 ( 40%)]  Loss:  4.153107 (4.1139)  Time: 1.104s,  927.64/s  (1.088s,  940.98/s)  LR: 9.773e-04  Data: 0.016 (0.013)
Train: 29 [ 550/1251 ( 44%)]  Loss:  3.619757 (4.0727)  Time: 1.078s,  949.48/s  (1.089s,  940.44/s)  LR: 9.773e-04  Data: 0.016 (0.013)
Train: 29 [ 600/1251 ( 48%)]  Loss:  3.706692 (4.0446)  Time: 1.087s,  941.65/s  (1.089s,  940.46/s)  LR: 9.773e-04  Data: 0.011 (0.013)
Train: 29 [ 650/1251 ( 52%)]  Loss:  4.106027 (4.0490)  Time: 1.076s,  951.54/s  (1.089s,  940.37/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [ 700/1251 ( 56%)]  Loss:  4.090397 (4.0517)  Time: 1.082s,  946.64/s  (1.089s,  940.09/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [ 750/1251 ( 60%)]  Loss:  3.955231 (4.0457)  Time: 1.096s,  934.14/s  (1.089s,  940.06/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [ 800/1251 ( 64%)]  Loss:  4.125917 (4.0504)  Time: 1.084s,  944.76/s  (1.089s,  940.14/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [ 850/1251 ( 68%)]  Loss:  4.159320 (4.0565)  Time: 1.080s,  948.41/s  (1.089s,  939.92/s)  LR: 9.773e-04  Data: 0.016 (0.013)
Train: 29 [ 900/1251 ( 72%)]  Loss:  4.090064 (4.0582)  Time: 1.081s,  947.21/s  (1.090s,  939.78/s)  LR: 9.773e-04  Data: 0.016 (0.013)
Train: 29 [ 950/1251 ( 76%)]  Loss:  4.034446 (4.0570)  Time: 1.081s,  947.22/s  (1.090s,  939.70/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [1000/1251 ( 80%)]  Loss:  3.999599 (4.0543)  Time: 1.080s,  948.01/s  (1.090s,  939.48/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [1050/1251 ( 84%)]  Loss:  4.017581 (4.0526)  Time: 1.097s,  933.46/s  (1.090s,  939.48/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [1100/1251 ( 88%)]  Loss:  4.309299 (4.0638)  Time: 1.092s,  937.98/s  (1.090s,  939.54/s)  LR: 9.773e-04  Data: 0.012 (0.013)
Train: 29 [1150/1251 ( 92%)]  Loss:  4.248446 (4.0715)  Time: 1.078s,  950.15/s  (1.090s,  939.37/s)  LR: 9.773e-04  Data: 0.013 (0.013)
Train: 29 [1200/1251 ( 96%)]  Loss:  4.025648 (4.0697)  Time: 1.173s,  872.65/s  (1.090s,  939.52/s)  LR: 9.773e-04  Data: 0.015 (0.013)
Train: 29 [1250/1251 (100%)]  Loss:  3.803066 (4.0594)  Time: 1.069s,  957.89/s  (1.090s,  939.49/s)  LR: 9.773e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.899 (5.899)  Loss:  0.7527 (0.7527)  Acc@1: 82.7148 (82.7148)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.8969 (1.4262)  Acc@1: 81.3679 (67.7480)  Acc@5: 94.9292 (88.6560)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-20.pth.tar', 63.520000009765624)

Train: 30 [   0/1251 (  0%)]  Loss:  3.788596 (3.7886)  Time: 1.113s,  919.68/s  (1.113s,  919.68/s)  LR: 9.758e-04  Data: 0.031 (0.031)
Train: 30 [  50/1251 (  4%)]  Loss:  4.016998 (3.9028)  Time: 1.078s,  950.15/s  (1.094s,  935.60/s)  LR: 9.758e-04  Data: 0.013 (0.014)
Train: 30 [ 100/1251 (  8%)]  Loss:  4.002102 (3.9359)  Time: 1.079s,  948.61/s  (1.092s,  937.35/s)  LR: 9.758e-04  Data: 0.016 (0.014)
Train: 30 [ 150/1251 ( 12%)]  Loss:  3.947201 (3.9387)  Time: 1.097s,  933.79/s  (1.095s,  935.02/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 30 [ 200/1251 ( 16%)]  Loss:  3.514115 (3.8538)  Time: 1.079s,  949.29/s  (1.094s,  936.33/s)  LR: 9.758e-04  Data: 0.015 (0.013)
Train: 30 [ 250/1251 ( 20%)]  Loss:  3.974911 (3.8740)  Time: 1.088s,  941.24/s  (1.094s,  936.27/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [ 300/1251 ( 24%)]  Loss:  4.129653 (3.9105)  Time: 1.083s,  945.95/s  (1.093s,  936.82/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [ 350/1251 ( 28%)]  Loss:  4.218436 (3.9490)  Time: 1.077s,  950.79/s  (1.093s,  937.08/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [ 400/1251 ( 32%)]  Loss:  3.846348 (3.9376)  Time: 1.104s,  927.20/s  (1.093s,  936.86/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [ 450/1251 ( 36%)]  Loss:  3.995098 (3.9433)  Time: 1.096s,  934.49/s  (1.093s,  936.45/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [ 500/1251 ( 40%)]  Loss:  3.874704 (3.9371)  Time: 1.097s,  933.30/s  (1.093s,  936.46/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [ 550/1251 ( 44%)]  Loss:  4.348382 (3.9714)  Time: 1.079s,  948.91/s  (1.093s,  936.81/s)  LR: 9.758e-04  Data: 0.017 (0.013)
Train: 30 [ 600/1251 ( 48%)]  Loss:  4.140702 (3.9844)  Time: 1.085s,  943.66/s  (1.093s,  936.96/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [ 650/1251 ( 52%)]  Loss:  4.180385 (3.9984)  Time: 1.083s,  945.37/s  (1.092s,  937.46/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [ 700/1251 ( 56%)]  Loss:  4.092650 (4.0047)  Time: 1.075s,  952.17/s  (1.092s,  937.37/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [ 750/1251 ( 60%)]  Loss:  4.151886 (4.0139)  Time: 1.177s,  870.22/s  (1.092s,  937.45/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [ 800/1251 ( 64%)]  Loss:  4.306264 (4.0311)  Time: 1.077s,  950.83/s  (1.092s,  937.63/s)  LR: 9.758e-04  Data: 0.014 (0.013)
Train: 30 [ 850/1251 ( 68%)]  Loss:  3.956329 (4.0269)  Time: 1.095s,  935.30/s  (1.092s,  937.75/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [ 900/1251 ( 72%)]  Loss:  4.293787 (4.0410)  Time: 1.078s,  949.48/s  (1.092s,  937.89/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [ 950/1251 ( 76%)]  Loss:  4.075954 (4.0427)  Time: 1.082s,  946.44/s  (1.092s,  937.64/s)  LR: 9.758e-04  Data: 0.014 (0.013)
Train: 30 [1000/1251 ( 80%)]  Loss:  4.176109 (4.0491)  Time: 1.097s,  933.51/s  (1.092s,  937.89/s)  LR: 9.758e-04  Data: 0.015 (0.013)
Train: 30 [1050/1251 ( 84%)]  Loss:  3.787746 (4.0372)  Time: 1.079s,  949.25/s  (1.092s,  937.90/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [1100/1251 ( 88%)]  Loss:  3.941917 (4.0331)  Time: 1.083s,  945.50/s  (1.092s,  937.71/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [1150/1251 ( 92%)]  Loss:  3.848240 (4.0254)  Time: 1.078s,  949.77/s  (1.092s,  937.87/s)  LR: 9.758e-04  Data: 0.013 (0.013)
Train: 30 [1200/1251 ( 96%)]  Loss:  4.249255 (4.0343)  Time: 1.096s,  934.72/s  (1.092s,  937.89/s)  LR: 9.758e-04  Data: 0.012 (0.013)
Train: 30 [1250/1251 (100%)]  Loss:  3.744040 (4.0231)  Time: 1.061s,  964.72/s  (1.092s,  937.72/s)  LR: 9.758e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.860 (5.860)  Loss:  0.7697 (0.7697)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.230 (0.447)  Loss:  0.8270 (1.4285)  Acc@1: 83.1368 (68.2660)  Acc@5: 95.5189 (88.9700)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-21.pth.tar', 63.95200006103516)

Train: 31 [   0/1251 (  0%)]  Loss:  3.820952 (3.8210)  Time: 1.098s,  932.82/s  (1.098s,  932.82/s)  LR: 9.741e-04  Data: 0.032 (0.032)
Train: 31 [  50/1251 (  4%)]  Loss:  3.866633 (3.8438)  Time: 1.082s,  946.38/s  (1.089s,  940.72/s)  LR: 9.741e-04  Data: 0.014 (0.014)
Train: 31 [ 100/1251 (  8%)]  Loss:  3.978266 (3.8886)  Time: 1.077s,  950.64/s  (1.088s,  940.89/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 150/1251 ( 12%)]  Loss:  3.944452 (3.9026)  Time: 1.077s,  950.49/s  (1.089s,  940.64/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 200/1251 ( 16%)]  Loss:  3.968452 (3.9158)  Time: 1.094s,  935.87/s  (1.089s,  940.28/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [ 250/1251 ( 20%)]  Loss:  3.916323 (3.9158)  Time: 1.078s,  949.47/s  (1.090s,  939.61/s)  LR: 9.741e-04  Data: 0.014 (0.013)
Train: 31 [ 300/1251 ( 24%)]  Loss:  4.240771 (3.9623)  Time: 1.081s,  946.85/s  (1.089s,  940.05/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 350/1251 ( 28%)]  Loss:  4.010519 (3.9683)  Time: 1.081s,  947.00/s  (1.089s,  939.92/s)  LR: 9.741e-04  Data: 0.014 (0.013)
Train: 31 [ 400/1251 ( 32%)]  Loss:  4.044450 (3.9768)  Time: 1.084s,  945.02/s  (1.090s,  939.49/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 450/1251 ( 36%)]  Loss:  3.704681 (3.9495)  Time: 1.096s,  934.47/s  (1.090s,  939.55/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 500/1251 ( 40%)]  Loss:  4.093406 (3.9626)  Time: 1.083s,  945.30/s  (1.090s,  939.31/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 550/1251 ( 44%)]  Loss:  4.057517 (3.9705)  Time: 1.104s,  927.71/s  (1.090s,  939.05/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 600/1251 ( 48%)]  Loss:  3.749566 (3.9535)  Time: 1.101s,  930.30/s  (1.091s,  938.99/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [ 650/1251 ( 52%)]  Loss:  3.977638 (3.9553)  Time: 1.098s,  932.40/s  (1.090s,  939.15/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 700/1251 ( 56%)]  Loss:  4.452454 (3.9884)  Time: 1.103s,  928.20/s  (1.090s,  939.19/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 750/1251 ( 60%)]  Loss:  4.040843 (3.9917)  Time: 1.214s,  843.48/s  (1.091s,  938.43/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [ 800/1251 ( 64%)]  Loss:  4.243315 (4.0065)  Time: 1.078s,  949.53/s  (1.091s,  938.19/s)  LR: 9.741e-04  Data: 0.016 (0.013)
Train: 31 [ 850/1251 ( 68%)]  Loss:  4.217259 (4.0182)  Time: 1.080s,  948.40/s  (1.091s,  938.49/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [ 900/1251 ( 72%)]  Loss:  4.063705 (4.0206)  Time: 1.077s,  951.03/s  (1.091s,  938.69/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [ 950/1251 ( 76%)]  Loss:  4.141826 (4.0267)  Time: 1.096s,  934.58/s  (1.091s,  938.69/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [1000/1251 ( 80%)]  Loss:  3.923187 (4.0217)  Time: 1.078s,  949.73/s  (1.091s,  938.50/s)  LR: 9.741e-04  Data: 0.013 (0.013)
Train: 31 [1050/1251 ( 84%)]  Loss:  3.958097 (4.0188)  Time: 1.158s,  884.52/s  (1.091s,  938.40/s)  LR: 9.741e-04  Data: 0.012 (0.013)
Train: 31 [1100/1251 ( 88%)]  Loss:  4.130726 (4.0237)  Time: 1.095s,  935.34/s  (1.091s,  938.38/s)  LR: 9.741e-04  Data: 0.014 (0.013)
Train: 31 [1150/1251 ( 92%)]  Loss:  3.826821 (4.0155)  Time: 1.079s,  948.99/s  (1.091s,  938.33/s)  LR: 9.741e-04  Data: 0.017 (0.013)
Train: 31 [1200/1251 ( 96%)]  Loss:  4.119463 (4.0197)  Time: 1.092s,  937.70/s  (1.092s,  938.14/s)  LR: 9.741e-04  Data: 0.015 (0.013)
Train: 31 [1250/1251 (100%)]  Loss:  4.188881 (4.0262)  Time: 1.082s,  946.80/s  (1.091s,  938.26/s)  LR: 9.741e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 7.192 (7.192)  Loss:  0.7965 (0.7965)  Acc@1: 83.8867 (83.8867)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.8226 (1.4027)  Acc@1: 83.0189 (68.9540)  Acc@5: 95.1651 (89.2040)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-22.pth.tar', 64.68200008300781)

Train: 32 [   0/1251 (  0%)]  Loss:  3.766343 (3.7663)  Time: 1.118s,  916.04/s  (1.118s,  916.04/s)  LR: 9.725e-04  Data: 0.039 (0.039)
Train: 32 [  50/1251 (  4%)]  Loss:  4.303660 (4.0350)  Time: 1.076s,  951.65/s  (1.089s,  940.38/s)  LR: 9.725e-04  Data: 0.013 (0.014)
Train: 32 [ 100/1251 (  8%)]  Loss:  3.824131 (3.9647)  Time: 1.082s,  946.30/s  (1.091s,  938.71/s)  LR: 9.725e-04  Data: 0.020 (0.014)
Train: 32 [ 150/1251 ( 12%)]  Loss:  3.819355 (3.9284)  Time: 1.102s,  929.25/s  (1.089s,  939.95/s)  LR: 9.725e-04  Data: 0.014 (0.013)
Train: 32 [ 200/1251 ( 16%)]  Loss:  4.045127 (3.9517)  Time: 1.096s,  934.18/s  (1.090s,  939.14/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 250/1251 ( 20%)]  Loss:  3.800800 (3.9266)  Time: 1.078s,  949.70/s  (1.090s,  939.46/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 32 [ 300/1251 ( 24%)]  Loss:  4.350855 (3.9872)  Time: 1.096s,  934.14/s  (1.090s,  939.54/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 350/1251 ( 28%)]  Loss:  3.749842 (3.9575)  Time: 1.085s,  943.55/s  (1.090s,  939.24/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 400/1251 ( 32%)]  Loss:  4.126626 (3.9763)  Time: 1.104s,  927.30/s  (1.090s,  939.44/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 32 [ 450/1251 ( 36%)]  Loss:  3.862030 (3.9649)  Time: 1.097s,  933.46/s  (1.091s,  938.86/s)  LR: 9.725e-04  Data: 0.011 (0.013)
Train: 32 [ 500/1251 ( 40%)]  Loss:  3.689696 (3.9399)  Time: 1.172s,  873.59/s  (1.091s,  938.28/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 550/1251 ( 44%)]  Loss:  4.386600 (3.9771)  Time: 1.077s,  950.86/s  (1.092s,  937.94/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 32 [ 600/1251 ( 48%)]  Loss:  3.900549 (3.9712)  Time: 1.096s,  934.03/s  (1.092s,  937.83/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 650/1251 ( 52%)]  Loss:  4.328447 (3.9967)  Time: 1.083s,  945.46/s  (1.092s,  937.99/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 700/1251 ( 56%)]  Loss:  4.117031 (4.0047)  Time: 1.095s,  935.01/s  (1.092s,  937.81/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 750/1251 ( 60%)]  Loss:  3.776461 (3.9905)  Time: 1.082s,  945.97/s  (1.092s,  937.75/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 32 [ 800/1251 ( 64%)]  Loss:  3.829289 (3.9810)  Time: 1.099s,  931.67/s  (1.092s,  937.61/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [ 850/1251 ( 68%)]  Loss:  3.961852 (3.9799)  Time: 1.083s,  945.85/s  (1.092s,  937.59/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 32 [ 900/1251 ( 72%)]  Loss:  3.957014 (3.9787)  Time: 1.103s,  928.60/s  (1.092s,  937.56/s)  LR: 9.725e-04  Data: 0.021 (0.013)
Train: 32 [ 950/1251 ( 76%)]  Loss:  4.027176 (3.9811)  Time: 1.078s,  950.13/s  (1.092s,  937.98/s)  LR: 9.725e-04  Data: 0.012 (0.013)
Train: 32 [1000/1251 ( 80%)]  Loss:  3.714438 (3.9684)  Time: 1.084s,  945.01/s  (1.092s,  937.85/s)  LR: 9.725e-04  Data: 0.015 (0.013)
Train: 32 [1050/1251 ( 84%)]  Loss:  3.797054 (3.9607)  Time: 1.094s,  936.08/s  (1.092s,  937.82/s)  LR: 9.725e-04  Data: 0.015 (0.013)
Train: 32 [1100/1251 ( 88%)]  Loss:  3.830014 (3.9550)  Time: 1.083s,  945.13/s  (1.092s,  937.88/s)  LR: 9.725e-04  Data: 0.013 (0.013)
Train: 32 [1150/1251 ( 92%)]  Loss:  3.653526 (3.9424)  Time: 1.091s,  938.93/s  (1.092s,  937.88/s)  LR: 9.725e-04  Data: 0.016 (0.013)
Train: 32 [1200/1251 ( 96%)]  Loss:  4.039566 (3.9463)  Time: 1.080s,  948.29/s  (1.092s,  938.01/s)  LR: 9.725e-04  Data: 0.016 (0.013)
Train: 32 [1250/1251 (100%)]  Loss:  4.071554 (3.9511)  Time: 1.089s,  940.44/s  (1.092s,  938.07/s)  LR: 9.725e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.833 (5.833)  Loss:  0.7929 (0.7929)  Acc@1: 84.5703 (84.5703)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.8299 (1.3914)  Acc@1: 82.1934 (69.0120)  Acc@5: 94.4576 (89.5360)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-23.pth.tar', 65.16799987792969)

Train: 33 [   0/1251 (  0%)]  Loss:  4.164096 (4.1641)  Time: 1.091s,  938.67/s  (1.091s,  938.67/s)  LR: 9.707e-04  Data: 0.028 (0.028)
Train: 33 [  50/1251 (  4%)]  Loss:  3.975212 (4.0697)  Time: 1.170s,  875.35/s  (1.085s,  943.69/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 100/1251 (  8%)]  Loss:  4.189896 (4.1097)  Time: 1.096s,  934.21/s  (1.085s,  943.63/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 150/1251 ( 12%)]  Loss:  3.775041 (4.0261)  Time: 1.078s,  949.88/s  (1.086s,  942.49/s)  LR: 9.707e-04  Data: 0.014 (0.013)
Train: 33 [ 200/1251 ( 16%)]  Loss:  3.508943 (3.9226)  Time: 1.084s,  944.64/s  (1.086s,  942.56/s)  LR: 9.707e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 33 [ 250/1251 ( 20%)]  Loss:  3.931878 (3.9242)  Time: 1.084s,  944.61/s  (1.086s,  942.57/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 300/1251 ( 24%)]  Loss:  3.856917 (3.9146)  Time: 1.102s,  929.48/s  (1.087s,  942.39/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 350/1251 ( 28%)]  Loss:  4.252268 (3.9568)  Time: 1.094s,  935.88/s  (1.087s,  942.21/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 400/1251 ( 32%)]  Loss:  4.108900 (3.9737)  Time: 1.079s,  949.30/s  (1.087s,  942.11/s)  LR: 9.707e-04  Data: 0.016 (0.013)
Train: 33 [ 450/1251 ( 36%)]  Loss:  3.804082 (3.9567)  Time: 1.079s,  948.81/s  (1.087s,  942.21/s)  LR: 9.707e-04  Data: 0.014 (0.013)
Train: 33 [ 500/1251 ( 40%)]  Loss:  4.247978 (3.9832)  Time: 1.079s,  949.07/s  (1.087s,  942.00/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 550/1251 ( 44%)]  Loss:  4.103355 (3.9932)  Time: 1.099s,  931.97/s  (1.087s,  941.84/s)  LR: 9.707e-04  Data: 0.014 (0.013)
Train: 33 [ 600/1251 ( 48%)]  Loss:  4.061913 (3.9985)  Time: 1.105s,  926.43/s  (1.088s,  941.56/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 650/1251 ( 52%)]  Loss:  3.985770 (3.9976)  Time: 1.096s,  934.16/s  (1.088s,  941.11/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 700/1251 ( 56%)]  Loss:  3.825214 (3.9861)  Time: 1.083s,  945.78/s  (1.088s,  940.86/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 750/1251 ( 60%)]  Loss:  3.866100 (3.9786)  Time: 1.077s,  950.39/s  (1.088s,  941.23/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [ 800/1251 ( 64%)]  Loss:  4.319343 (3.9986)  Time: 1.096s,  934.45/s  (1.088s,  941.12/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 850/1251 ( 68%)]  Loss:  3.655307 (3.9796)  Time: 1.083s,  945.51/s  (1.088s,  941.01/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 900/1251 ( 72%)]  Loss:  3.733383 (3.9666)  Time: 1.076s,  952.11/s  (1.088s,  941.24/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [ 950/1251 ( 76%)]  Loss:  3.944050 (3.9655)  Time: 1.097s,  933.08/s  (1.088s,  941.21/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [1000/1251 ( 80%)]  Loss:  3.599975 (3.9481)  Time: 1.083s,  945.31/s  (1.088s,  940.83/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [1050/1251 ( 84%)]  Loss:  3.808090 (3.9417)  Time: 1.083s,  945.41/s  (1.089s,  940.73/s)  LR: 9.707e-04  Data: 0.012 (0.013)
Train: 33 [1100/1251 ( 88%)]  Loss:  4.157237 (3.9511)  Time: 1.077s,  951.11/s  (1.088s,  940.85/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [1150/1251 ( 92%)]  Loss:  3.966218 (3.9517)  Time: 1.098s,  932.76/s  (1.089s,  940.48/s)  LR: 9.707e-04  Data: 0.013 (0.013)
Train: 33 [1200/1251 ( 96%)]  Loss:  4.112276 (3.9581)  Time: 1.078s,  950.20/s  (1.089s,  940.14/s)  LR: 9.707e-04  Data: 0.014 (0.013)
Train: 33 [1250/1251 (100%)]  Loss:  3.477252 (3.9396)  Time: 1.062s,  964.07/s  (1.089s,  940.02/s)  LR: 9.707e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.809 (5.809)  Loss:  0.7583 (0.7583)  Acc@1: 83.1055 (83.1055)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7556 (1.3609)  Acc@1: 83.9623 (69.3660)  Acc@5: 95.7547 (89.7880)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-24.pth.tar', 66.046000078125)

Train: 34 [   0/1251 (  0%)]  Loss:  4.187706 (4.1877)  Time: 1.095s,  935.16/s  (1.095s,  935.16/s)  LR: 9.690e-04  Data: 0.032 (0.032)
Train: 34 [  50/1251 (  4%)]  Loss:  3.955611 (4.0717)  Time: 1.077s,  950.58/s  (1.091s,  938.83/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [ 100/1251 (  8%)]  Loss:  4.065341 (4.0696)  Time: 1.096s,  934.31/s  (1.090s,  939.23/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 150/1251 ( 12%)]  Loss:  3.860803 (4.0174)  Time: 1.096s,  934.44/s  (1.091s,  938.30/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [ 200/1251 ( 16%)]  Loss:  3.938635 (4.0016)  Time: 1.097s,  933.45/s  (1.092s,  938.08/s)  LR: 9.690e-04  Data: 0.014 (0.013)
Train: 34 [ 250/1251 ( 20%)]  Loss:  4.185993 (4.0323)  Time: 1.081s,  947.17/s  (1.092s,  937.86/s)  LR: 9.690e-04  Data: 0.016 (0.013)
Train: 34 [ 300/1251 ( 24%)]  Loss:  3.677713 (3.9817)  Time: 1.096s,  934.44/s  (1.093s,  937.04/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 350/1251 ( 28%)]  Loss:  4.162048 (4.0042)  Time: 1.168s,  877.07/s  (1.092s,  937.59/s)  LR: 9.690e-04  Data: 0.011 (0.013)
Train: 34 [ 400/1251 ( 32%)]  Loss:  4.049020 (4.0092)  Time: 1.077s,  950.75/s  (1.092s,  937.53/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 450/1251 ( 36%)]  Loss:  3.973265 (4.0056)  Time: 1.079s,  948.81/s  (1.092s,  938.10/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 500/1251 ( 40%)]  Loss:  3.621222 (3.9707)  Time: 1.090s,  939.17/s  (1.091s,  938.24/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [ 550/1251 ( 44%)]  Loss:  4.023499 (3.9751)  Time: 1.092s,  937.43/s  (1.092s,  937.98/s)  LR: 9.690e-04  Data: 0.015 (0.013)
Train: 34 [ 600/1251 ( 48%)]  Loss:  4.062519 (3.9818)  Time: 1.195s,  856.99/s  (1.092s,  938.06/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [ 650/1251 ( 52%)]  Loss:  3.652559 (3.9583)  Time: 1.080s,  948.10/s  (1.091s,  938.27/s)  LR: 9.690e-04  Data: 0.014 (0.013)
Train: 34 [ 700/1251 ( 56%)]  Loss:  3.595051 (3.9341)  Time: 1.094s,  935.67/s  (1.092s,  938.11/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 750/1251 ( 60%)]  Loss:  3.706141 (3.9198)  Time: 1.096s,  934.09/s  (1.092s,  937.78/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [ 800/1251 ( 64%)]  Loss:  3.983730 (3.9236)  Time: 1.079s,  948.79/s  (1.092s,  938.12/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 850/1251 ( 68%)]  Loss:  4.044040 (3.9303)  Time: 1.077s,  950.84/s  (1.091s,  938.18/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 900/1251 ( 72%)]  Loss:  3.989052 (3.9334)  Time: 1.098s,  932.92/s  (1.091s,  938.17/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [ 950/1251 ( 76%)]  Loss:  3.883435 (3.9309)  Time: 1.199s,  854.30/s  (1.092s,  938.07/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [1000/1251 ( 80%)]  Loss:  4.215993 (3.9444)  Time: 1.096s,  934.07/s  (1.091s,  938.34/s)  LR: 9.690e-04  Data: 0.013 (0.013)
Train: 34 [1050/1251 ( 84%)]  Loss:  3.892707 (3.9421)  Time: 1.104s,  927.25/s  (1.092s,  938.11/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [1100/1251 ( 88%)]  Loss:  3.899571 (3.9402)  Time: 1.094s,  935.78/s  (1.092s,  938.02/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [1150/1251 ( 92%)]  Loss:  3.674523 (3.9292)  Time: 1.082s,  946.60/s  (1.092s,  938.03/s)  LR: 9.690e-04  Data: 0.012 (0.013)
Train: 34 [1200/1251 ( 96%)]  Loss:  3.928588 (3.9292)  Time: 1.080s,  947.81/s  (1.092s,  938.10/s)  LR: 9.690e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 34 [1250/1251 (100%)]  Loss:  3.842012 (3.9258)  Time: 1.080s,  947.81/s  (1.091s,  938.28/s)  LR: 9.690e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.823 (5.823)  Loss:  0.7297 (0.7297)  Acc@1: 85.3516 (85.3516)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.8321 (1.3570)  Acc@1: 82.0755 (69.4900)  Acc@5: 94.5755 (89.6720)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-25.pth.tar', 66.39000010986328)

Train: 35 [   0/1251 (  0%)]  Loss:  3.823611 (3.8236)  Time: 1.090s,  939.05/s  (1.090s,  939.05/s)  LR: 9.671e-04  Data: 0.027 (0.027)
Train: 35 [  50/1251 (  4%)]  Loss:  4.258968 (4.0413)  Time: 1.105s,  926.29/s  (1.094s,  936.08/s)  LR: 9.671e-04  Data: 0.013 (0.014)
Train: 35 [ 100/1251 (  8%)]  Loss:  4.122958 (4.0685)  Time: 1.103s,  928.47/s  (1.093s,  936.79/s)  LR: 9.671e-04  Data: 0.013 (0.014)
Train: 35 [ 150/1251 ( 12%)]  Loss:  4.079133 (4.0712)  Time: 1.077s,  950.82/s  (1.094s,  936.37/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 200/1251 ( 16%)]  Loss:  4.174952 (4.0919)  Time: 1.079s,  949.00/s  (1.093s,  936.76/s)  LR: 9.671e-04  Data: 0.015 (0.013)
Train: 35 [ 250/1251 ( 20%)]  Loss:  4.112063 (4.0953)  Time: 1.079s,  948.65/s  (1.093s,  936.92/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 300/1251 ( 24%)]  Loss:  4.077363 (4.0927)  Time: 1.075s,  952.45/s  (1.093s,  937.01/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [ 350/1251 ( 28%)]  Loss:  3.766995 (4.0520)  Time: 1.077s,  950.60/s  (1.093s,  936.53/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 400/1251 ( 32%)]  Loss:  4.099887 (4.0573)  Time: 1.080s,  947.83/s  (1.092s,  937.31/s)  LR: 9.671e-04  Data: 0.014 (0.013)
Train: 35 [ 450/1251 ( 36%)]  Loss:  4.187419 (4.0703)  Time: 1.097s,  933.51/s  (1.093s,  937.04/s)  LR: 9.671e-04  Data: 0.014 (0.013)
Train: 35 [ 500/1251 ( 40%)]  Loss:  4.325348 (4.0935)  Time: 1.095s,  934.85/s  (1.093s,  937.23/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 550/1251 ( 44%)]  Loss:  4.044993 (4.0895)  Time: 1.084s,  944.61/s  (1.092s,  937.52/s)  LR: 9.671e-04  Data: 0.021 (0.013)
Train: 35 [ 600/1251 ( 48%)]  Loss:  4.090342 (4.0895)  Time: 1.097s,  933.34/s  (1.092s,  937.48/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [ 650/1251 ( 52%)]  Loss:  3.952483 (4.0798)  Time: 1.086s,  943.03/s  (1.092s,  937.53/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [ 700/1251 ( 56%)]  Loss:  3.906048 (4.0682)  Time: 1.078s,  949.71/s  (1.092s,  938.13/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 750/1251 ( 60%)]  Loss:  3.796468 (4.0512)  Time: 1.078s,  950.25/s  (1.092s,  938.06/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 800/1251 ( 64%)]  Loss:  4.312428 (4.0666)  Time: 1.099s,  931.94/s  (1.091s,  938.31/s)  LR: 9.671e-04  Data: 0.016 (0.013)
Train: 35 [ 850/1251 ( 68%)]  Loss:  3.995080 (4.0626)  Time: 1.079s,  948.94/s  (1.091s,  938.49/s)  LR: 9.671e-04  Data: 0.016 (0.013)
Train: 35 [ 900/1251 ( 72%)]  Loss:  3.921523 (4.0552)  Time: 1.079s,  949.43/s  (1.091s,  938.69/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [ 950/1251 ( 76%)]  Loss:  3.738693 (4.0393)  Time: 1.076s,  952.02/s  (1.091s,  938.41/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [1000/1251 ( 80%)]  Loss:  3.973623 (4.0362)  Time: 1.077s,  950.44/s  (1.091s,  938.32/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [1050/1251 ( 84%)]  Loss:  3.926294 (4.0312)  Time: 1.096s,  934.38/s  (1.091s,  938.22/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [1100/1251 ( 88%)]  Loss:  3.861139 (4.0238)  Time: 1.098s,  932.59/s  (1.091s,  938.29/s)  LR: 9.671e-04  Data: 0.016 (0.013)
Train: 35 [1150/1251 ( 92%)]  Loss:  4.190984 (4.0308)  Time: 1.083s,  945.47/s  (1.091s,  938.20/s)  LR: 9.671e-04  Data: 0.012 (0.013)
Train: 35 [1200/1251 ( 96%)]  Loss:  4.286097 (4.0410)  Time: 1.088s,  941.44/s  (1.091s,  938.33/s)  LR: 9.671e-04  Data: 0.013 (0.013)
Train: 35 [1250/1251 (100%)]  Loss:  3.814568 (4.0323)  Time: 1.062s,  964.08/s  (1.091s,  938.22/s)  LR: 9.671e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.808 (5.808)  Loss:  0.6690 (0.6690)  Acc@1: 86.6211 (86.6211)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.7984 (1.3296)  Acc@1: 80.8962 (69.8760)  Acc@5: 95.2830 (89.8440)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-26.pth.tar', 66.76600012939453)

Train: 36 [   0/1251 (  0%)]  Loss:  3.845525 (3.8455)  Time: 1.091s,  938.92/s  (1.091s,  938.92/s)  LR: 9.652e-04  Data: 0.027 (0.027)
Train: 36 [  50/1251 (  4%)]  Loss:  3.703067 (3.7743)  Time: 1.098s,  932.90/s  (1.090s,  939.31/s)  LR: 9.652e-04  Data: 0.014 (0.013)
Train: 36 [ 100/1251 (  8%)]  Loss:  3.880446 (3.8097)  Time: 1.079s,  948.99/s  (1.093s,  937.07/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [ 150/1251 ( 12%)]  Loss:  3.895861 (3.8312)  Time: 1.095s,  935.23/s  (1.093s,  936.48/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [ 200/1251 ( 16%)]  Loss:  3.955762 (3.8561)  Time: 1.087s,  942.35/s  (1.094s,  936.33/s)  LR: 9.652e-04  Data: 0.019 (0.013)
Train: 36 [ 250/1251 ( 20%)]  Loss:  3.721116 (3.8336)  Time: 1.079s,  949.39/s  (1.093s,  936.77/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [ 300/1251 ( 24%)]  Loss:  4.030384 (3.8617)  Time: 1.077s,  950.61/s  (1.092s,  937.86/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [ 350/1251 ( 28%)]  Loss:  3.830634 (3.8578)  Time: 1.085s,  943.36/s  (1.091s,  938.22/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [ 400/1251 ( 32%)]  Loss:  3.830213 (3.8548)  Time: 1.093s,  936.95/s  (1.091s,  938.30/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 450/1251 ( 36%)]  Loss:  4.079610 (3.8773)  Time: 1.097s,  933.69/s  (1.091s,  938.81/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [ 500/1251 ( 40%)]  Loss:  4.267639 (3.9128)  Time: 1.193s,  858.06/s  (1.090s,  939.03/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [ 550/1251 ( 44%)]  Loss:  4.028380 (3.9224)  Time: 1.073s,  954.45/s  (1.091s,  938.72/s)  LR: 9.652e-04  Data: 0.011 (0.013)
Train: 36 [ 600/1251 ( 48%)]  Loss:  4.235246 (3.9465)  Time: 1.100s,  930.91/s  (1.091s,  938.62/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [ 650/1251 ( 52%)]  Loss:  3.972614 (3.9483)  Time: 1.097s,  933.19/s  (1.091s,  938.79/s)  LR: 9.652e-04  Data: 0.017 (0.013)
Train: 36 [ 700/1251 ( 56%)]  Loss:  3.549626 (3.9217)  Time: 1.095s,  935.14/s  (1.091s,  938.53/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [ 750/1251 ( 60%)]  Loss:  3.784520 (3.9132)  Time: 1.081s,  947.60/s  (1.091s,  938.40/s)  LR: 9.652e-04  Data: 0.014 (0.013)
Train: 36 [ 800/1251 ( 64%)]  Loss:  3.874509 (3.9109)  Time: 1.080s,  948.30/s  (1.091s,  938.71/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [ 850/1251 ( 68%)]  Loss:  3.862166 (3.9082)  Time: 1.096s,  934.26/s  (1.091s,  938.68/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [ 900/1251 ( 72%)]  Loss:  3.828429 (3.9040)  Time: 1.087s,  942.33/s  (1.091s,  938.72/s)  LR: 9.652e-04  Data: 0.016 (0.013)
Train: 36 [ 950/1251 ( 76%)]  Loss:  3.835601 (3.9006)  Time: 1.077s,  950.96/s  (1.091s,  938.55/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [1000/1251 ( 80%)]  Loss:  3.659937 (3.8891)  Time: 1.080s,  947.79/s  (1.091s,  938.56/s)  LR: 9.652e-04  Data: 0.014 (0.013)
Train: 36 [1050/1251 ( 84%)]  Loss:  3.909706 (3.8900)  Time: 1.084s,  944.46/s  (1.091s,  938.48/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [1100/1251 ( 88%)]  Loss:  4.048613 (3.8969)  Time: 1.077s,  950.91/s  (1.091s,  938.35/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [1150/1251 ( 92%)]  Loss:  4.066042 (3.9040)  Time: 1.106s,  925.61/s  (1.091s,  938.61/s)  LR: 9.652e-04  Data: 0.013 (0.013)
Train: 36 [1200/1251 ( 96%)]  Loss:  3.720781 (3.8967)  Time: 1.077s,  950.97/s  (1.091s,  938.48/s)  LR: 9.652e-04  Data: 0.012 (0.013)
Train: 36 [1250/1251 (100%)]  Loss:  3.793726 (3.8927)  Time: 1.059s,  966.53/s  (1.091s,  938.26/s)  LR: 9.652e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.752 (5.752)  Loss:  0.7364 (0.7364)  Acc@1: 85.3516 (85.3516)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.8382 (1.3471)  Acc@1: 82.1934 (70.1180)  Acc@5: 95.0472 (90.0540)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-27.pth.tar', 67.3319999194336)

Train: 37 [   0/1251 (  0%)]  Loss:  4.045795 (4.0458)  Time: 1.105s,  926.88/s  (1.105s,  926.88/s)  LR: 9.633e-04  Data: 0.035 (0.035)
Train: 37 [  50/1251 (  4%)]  Loss:  3.896292 (3.9710)  Time: 1.083s,  945.65/s  (1.090s,  939.57/s)  LR: 9.633e-04  Data: 0.012 (0.014)
Train: 37 [ 100/1251 (  8%)]  Loss:  3.825532 (3.9225)  Time: 1.079s,  948.81/s  (1.089s,  940.38/s)  LR: 9.633e-04  Data: 0.014 (0.014)
Train: 37 [ 150/1251 ( 12%)]  Loss:  4.112585 (3.9701)  Time: 1.088s,  940.78/s  (1.090s,  939.04/s)  LR: 9.633e-04  Data: 0.014 (0.014)
Train: 37 [ 200/1251 ( 16%)]  Loss:  4.170737 (4.0102)  Time: 1.079s,  948.95/s  (1.090s,  939.22/s)  LR: 9.633e-04  Data: 0.015 (0.014)
Train: 37 [ 250/1251 ( 20%)]  Loss:  3.797497 (3.9747)  Time: 1.077s,  950.41/s  (1.090s,  939.53/s)  LR: 9.633e-04  Data: 0.014 (0.014)
Train: 37 [ 300/1251 ( 24%)]  Loss:  3.963619 (3.9732)  Time: 1.097s,  933.45/s  (1.090s,  939.22/s)  LR: 9.633e-04  Data: 0.015 (0.014)
Train: 37 [ 350/1251 ( 28%)]  Loss:  3.906681 (3.9648)  Time: 1.077s,  950.37/s  (1.091s,  938.95/s)  LR: 9.633e-04  Data: 0.013 (0.014)
Train: 37 [ 400/1251 ( 32%)]  Loss:  4.313266 (4.0036)  Time: 1.077s,  950.66/s  (1.090s,  939.38/s)  LR: 9.633e-04  Data: 0.013 (0.014)
Train: 37 [ 450/1251 ( 36%)]  Loss:  3.878239 (3.9910)  Time: 1.175s,  871.32/s  (1.090s,  939.74/s)  LR: 9.633e-04  Data: 0.013 (0.014)
Train: 37 [ 500/1251 ( 40%)]  Loss:  4.109237 (4.0018)  Time: 1.099s,  931.68/s  (1.091s,  938.85/s)  LR: 9.633e-04  Data: 0.015 (0.014)
Train: 37 [ 550/1251 ( 44%)]  Loss:  4.240682 (4.0217)  Time: 1.094s,  936.12/s  (1.091s,  938.84/s)  LR: 9.633e-04  Data: 0.012 (0.013)
Train: 37 [ 600/1251 ( 48%)]  Loss:  3.924639 (4.0142)  Time: 1.076s,  951.63/s  (1.091s,  938.91/s)  LR: 9.633e-04  Data: 0.012 (0.013)
Train: 37 [ 650/1251 ( 52%)]  Loss:  3.667183 (3.9894)  Time: 1.175s,  871.82/s  (1.091s,  938.88/s)  LR: 9.633e-04  Data: 0.014 (0.013)
Train: 37 [ 700/1251 ( 56%)]  Loss:  3.699554 (3.9701)  Time: 1.078s,  950.02/s  (1.090s,  939.03/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [ 750/1251 ( 60%)]  Loss:  3.622937 (3.9484)  Time: 1.079s,  949.38/s  (1.091s,  938.97/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [ 800/1251 ( 64%)]  Loss:  3.968255 (3.9496)  Time: 1.099s,  932.11/s  (1.091s,  938.94/s)  LR: 9.633e-04  Data: 0.015 (0.013)
Train: 37 [ 850/1251 ( 68%)]  Loss:  4.011427 (3.9530)  Time: 1.075s,  952.22/s  (1.090s,  939.09/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [ 900/1251 ( 72%)]  Loss:  3.802647 (3.9451)  Time: 1.121s,  913.53/s  (1.090s,  939.13/s)  LR: 9.633e-04  Data: 0.015 (0.013)
Train: 37 [ 950/1251 ( 76%)]  Loss:  4.204554 (3.9581)  Time: 1.128s,  907.58/s  (1.090s,  939.09/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [1000/1251 ( 80%)]  Loss:  4.117906 (3.9657)  Time: 1.079s,  949.03/s  (1.091s,  938.92/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [1050/1251 ( 84%)]  Loss:  3.764343 (3.9565)  Time: 1.076s,  951.73/s  (1.091s,  938.97/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [1100/1251 ( 88%)]  Loss:  4.256527 (3.9696)  Time: 1.093s,  936.54/s  (1.091s,  938.77/s)  LR: 9.633e-04  Data: 0.012 (0.013)
Train: 37 [1150/1251 ( 92%)]  Loss:  3.938887 (3.9683)  Time: 1.076s,  951.61/s  (1.091s,  938.91/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [1200/1251 ( 96%)]  Loss:  3.848910 (3.9635)  Time: 1.087s,  941.90/s  (1.091s,  938.72/s)  LR: 9.633e-04  Data: 0.013 (0.013)
Train: 37 [1250/1251 (100%)]  Loss:  4.179214 (3.9718)  Time: 1.069s,  958.34/s  (1.091s,  938.71/s)  LR: 9.633e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.804 (5.804)  Loss:  0.7182 (0.7182)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.8729 (1.3439)  Acc@1: 82.0755 (70.1060)  Acc@5: 95.1651 (89.9740)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-28.pth.tar', 67.64600007568359)

Train: 38 [   0/1251 (  0%)]  Loss:  3.925643 (3.9256)  Time: 1.126s,  909.01/s  (1.126s,  909.01/s)  LR: 9.613e-04  Data: 0.033 (0.033)
Train: 38 [  50/1251 (  4%)]  Loss:  3.992877 (3.9593)  Time: 1.107s,  925.30/s  (1.086s,  943.10/s)  LR: 9.613e-04  Data: 0.013 (0.014)
Train: 38 [ 100/1251 (  8%)]  Loss:  3.849636 (3.9227)  Time: 1.077s,  950.85/s  (1.084s,  944.58/s)  LR: 9.613e-04  Data: 0.013 (0.014)
Train: 38 [ 150/1251 ( 12%)]  Loss:  3.614700 (3.8457)  Time: 1.096s,  934.44/s  (1.086s,  942.63/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 200/1251 ( 16%)]  Loss:  4.137191 (3.9040)  Time: 1.104s,  927.15/s  (1.089s,  940.44/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 250/1251 ( 20%)]  Loss:  3.479942 (3.8333)  Time: 1.087s,  941.75/s  (1.089s,  940.51/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [ 300/1251 ( 24%)]  Loss:  3.723586 (3.8177)  Time: 1.098s,  932.63/s  (1.088s,  941.06/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [ 350/1251 ( 28%)]  Loss:  3.890269 (3.8267)  Time: 1.080s,  948.45/s  (1.089s,  940.31/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [ 400/1251 ( 32%)]  Loss:  3.879312 (3.8326)  Time: 1.094s,  936.11/s  (1.089s,  940.54/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 450/1251 ( 36%)]  Loss:  3.866489 (3.8360)  Time: 1.091s,  938.95/s  (1.089s,  940.25/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 500/1251 ( 40%)]  Loss:  3.971897 (3.8483)  Time: 1.081s,  947.05/s  (1.090s,  939.58/s)  LR: 9.613e-04  Data: 0.011 (0.013)
Train: 38 [ 550/1251 ( 44%)]  Loss:  4.014248 (3.8621)  Time: 1.077s,  950.38/s  (1.090s,  939.73/s)  LR: 9.613e-04  Data: 0.015 (0.013)
Train: 38 [ 600/1251 ( 48%)]  Loss:  4.158807 (3.8850)  Time: 1.094s,  935.59/s  (1.090s,  939.72/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 650/1251 ( 52%)]  Loss:  3.983440 (3.8920)  Time: 1.082s,  946.78/s  (1.089s,  939.92/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [ 700/1251 ( 56%)]  Loss:  3.668027 (3.8771)  Time: 1.075s,  952.29/s  (1.090s,  939.59/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 750/1251 ( 60%)]  Loss:  3.917550 (3.8796)  Time: 1.099s,  932.18/s  (1.090s,  939.27/s)  LR: 9.613e-04  Data: 0.015 (0.013)
Train: 38 [ 800/1251 ( 64%)]  Loss:  4.356457 (3.9077)  Time: 1.077s,  950.79/s  (1.090s,  939.56/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [ 850/1251 ( 68%)]  Loss:  3.837639 (3.9038)  Time: 1.096s,  934.57/s  (1.090s,  939.64/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 900/1251 ( 72%)]  Loss:  3.905759 (3.9039)  Time: 1.094s,  935.94/s  (1.090s,  939.06/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [ 950/1251 ( 76%)]  Loss:  3.760787 (3.8967)  Time: 1.095s,  935.48/s  (1.091s,  938.63/s)  LR: 9.613e-04  Data: 0.014 (0.013)
Train: 38 [1000/1251 ( 80%)]  Loss:  3.632503 (3.8841)  Time: 1.170s,  874.89/s  (1.091s,  938.74/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [1050/1251 ( 84%)]  Loss:  3.867522 (3.8834)  Time: 1.096s,  934.25/s  (1.091s,  938.81/s)  LR: 9.613e-04  Data: 0.013 (0.013)
Train: 38 [1100/1251 ( 88%)]  Loss:  4.003819 (3.8886)  Time: 1.097s,  933.63/s  (1.091s,  938.65/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1150/1251 ( 92%)]  Loss:  3.746255 (3.8827)  Time: 1.097s,  933.39/s  (1.091s,  938.65/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1200/1251 ( 96%)]  Loss:  4.115866 (3.8920)  Time: 1.094s,  935.67/s  (1.091s,  938.73/s)  LR: 9.613e-04  Data: 0.012 (0.013)
Train: 38 [1250/1251 (100%)]  Loss:  3.782031 (3.8878)  Time: 1.107s,  925.32/s  (1.091s,  938.61/s)  LR: 9.613e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.859 (5.859)  Loss:  0.7040 (0.7040)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.230 (0.447)  Loss:  0.7356 (1.2899)  Acc@1: 83.0189 (70.4460)  Acc@5: 95.7547 (90.2100)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-29.pth.tar', 67.74800005126953)

Train: 39 [   0/1251 (  0%)]  Loss:  3.800825 (3.8008)  Time: 1.089s,  940.13/s  (1.089s,  940.13/s)  LR: 9.593e-04  Data: 0.026 (0.026)
Train: 39 [  50/1251 (  4%)]  Loss:  3.879297 (3.8401)  Time: 1.078s,  949.55/s  (1.089s,  940.50/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [ 100/1251 (  8%)]  Loss:  3.683038 (3.7877)  Time: 1.079s,  948.99/s  (1.087s,  941.93/s)  LR: 9.593e-04  Data: 0.014 (0.013)
Train: 39 [ 150/1251 ( 12%)]  Loss:  3.838341 (3.8004)  Time: 1.080s,  948.28/s  (1.089s,  940.71/s)  LR: 9.593e-04  Data: 0.014 (0.013)
Train: 39 [ 200/1251 ( 16%)]  Loss:  4.000981 (3.8405)  Time: 1.077s,  951.08/s  (1.088s,  941.21/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [ 250/1251 ( 20%)]  Loss:  3.773443 (3.8293)  Time: 1.085s,  944.07/s  (1.089s,  940.31/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [ 300/1251 ( 24%)]  Loss:  4.060838 (3.8624)  Time: 1.099s,  931.35/s  (1.089s,  940.12/s)  LR: 9.593e-04  Data: 0.016 (0.013)
Train: 39 [ 350/1251 ( 28%)]  Loss:  3.912484 (3.8687)  Time: 1.084s,  944.94/s  (1.090s,  939.87/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [ 400/1251 ( 32%)]  Loss:  4.106396 (3.8951)  Time: 1.079s,  948.94/s  (1.090s,  939.47/s)  LR: 9.593e-04  Data: 0.015 (0.013)
Train: 39 [ 450/1251 ( 36%)]  Loss:  4.181788 (3.9237)  Time: 1.095s,  935.58/s  (1.091s,  938.99/s)  LR: 9.593e-04  Data: 0.012 (0.013)
Train: 39 [ 500/1251 ( 40%)]  Loss:  3.751982 (3.9081)  Time: 1.103s,  928.21/s  (1.090s,  939.14/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [ 550/1251 ( 44%)]  Loss:  3.812875 (3.9002)  Time: 1.085s,  943.89/s  (1.090s,  939.50/s)  LR: 9.593e-04  Data: 0.012 (0.013)
Train: 39 [ 600/1251 ( 48%)]  Loss:  3.924730 (3.9021)  Time: 1.074s,  953.06/s  (1.091s,  939.01/s)  LR: 9.593e-04  Data: 0.012 (0.013)
Train: 39 [ 650/1251 ( 52%)]  Loss:  4.292803 (3.9300)  Time: 1.096s,  934.33/s  (1.091s,  938.66/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [ 700/1251 ( 56%)]  Loss:  3.435835 (3.8970)  Time: 1.079s,  949.17/s  (1.091s,  938.37/s)  LR: 9.593e-04  Data: 0.015 (0.013)
Train: 39 [ 750/1251 ( 60%)]  Loss:  4.003008 (3.9037)  Time: 1.095s,  934.88/s  (1.091s,  938.50/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [ 800/1251 ( 64%)]  Loss:  3.712537 (3.8924)  Time: 1.097s,  933.59/s  (1.092s,  937.92/s)  LR: 9.593e-04  Data: 0.015 (0.013)
Train: 39 [ 850/1251 ( 68%)]  Loss:  3.759246 (3.8850)  Time: 1.120s,  914.05/s  (1.092s,  937.91/s)  LR: 9.593e-04  Data: 0.012 (0.013)
Train: 39 [ 900/1251 ( 72%)]  Loss:  3.811686 (3.8812)  Time: 1.104s,  927.55/s  (1.092s,  937.94/s)  LR: 9.593e-04  Data: 0.016 (0.013)
Train: 39 [ 950/1251 ( 76%)]  Loss:  4.346775 (3.9044)  Time: 1.081s,  946.84/s  (1.092s,  937.62/s)  LR: 9.593e-04  Data: 0.016 (0.013)
Train: 39 [1000/1251 ( 80%)]  Loss:  3.605882 (3.8902)  Time: 1.083s,  945.27/s  (1.092s,  937.76/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [1050/1251 ( 84%)]  Loss:  3.879077 (3.8897)  Time: 1.078s,  949.47/s  (1.092s,  938.13/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [1100/1251 ( 88%)]  Loss:  3.732213 (3.8829)  Time: 1.087s,  941.74/s  (1.091s,  938.35/s)  LR: 9.593e-04  Data: 0.011 (0.013)
Train: 39 [1150/1251 ( 92%)]  Loss:  4.034538 (3.8892)  Time: 1.094s,  935.63/s  (1.092s,  938.08/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [1200/1251 ( 96%)]  Loss:  3.742284 (3.8833)  Time: 1.078s,  950.03/s  (1.092s,  938.05/s)  LR: 9.593e-04  Data: 0.013 (0.013)
Train: 39 [1250/1251 (100%)]  Loss:  3.910048 (3.8843)  Time: 1.089s,  939.89/s  (1.091s,  938.26/s)  LR: 9.593e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.844 (5.844)  Loss:  0.7056 (0.7056)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.8050 (1.3020)  Acc@1: 82.7830 (70.5920)  Acc@5: 95.5189 (90.3700)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-30.pth.tar', 68.26600004394531)

Train: 40 [   0/1251 (  0%)]  Loss:  4.054940 (4.0549)  Time: 1.094s,  936.01/s  (1.094s,  936.01/s)  LR: 9.572e-04  Data: 0.032 (0.032)
Train: 40 [  50/1251 (  4%)]  Loss:  3.996296 (4.0256)  Time: 1.084s,  944.23/s  (1.090s,  939.24/s)  LR: 9.572e-04  Data: 0.011 (0.014)
Train: 40 [ 100/1251 (  8%)]  Loss:  4.063911 (4.0384)  Time: 1.081s,  947.18/s  (1.093s,  937.00/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [ 150/1251 ( 12%)]  Loss:  4.031280 (4.0366)  Time: 1.078s,  950.06/s  (1.091s,  938.81/s)  LR: 9.572e-04  Data: 0.014 (0.013)
Train: 40 [ 200/1251 ( 16%)]  Loss:  4.062149 (4.0417)  Time: 1.095s,  935.39/s  (1.090s,  939.22/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [ 250/1251 ( 20%)]  Loss:  4.021766 (4.0384)  Time: 1.078s,  950.17/s  (1.090s,  939.60/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [ 300/1251 ( 24%)]  Loss:  3.876738 (4.0153)  Time: 1.077s,  951.05/s  (1.089s,  939.95/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [ 350/1251 ( 28%)]  Loss:  4.072379 (4.0224)  Time: 1.082s,  946.51/s  (1.089s,  940.74/s)  LR: 9.572e-04  Data: 0.020 (0.013)
Train: 40 [ 400/1251 ( 32%)]  Loss:  3.695786 (3.9861)  Time: 1.094s,  935.60/s  (1.089s,  940.09/s)  LR: 9.572e-04  Data: 0.017 (0.013)
Train: 40 [ 450/1251 ( 36%)]  Loss:  4.019287 (3.9895)  Time: 1.077s,  950.61/s  (1.089s,  940.15/s)  LR: 9.572e-04  Data: 0.015 (0.013)
Train: 40 [ 500/1251 ( 40%)]  Loss:  3.912698 (3.9825)  Time: 1.080s,  948.23/s  (1.089s,  940.08/s)  LR: 9.572e-04  Data: 0.016 (0.013)
Train: 40 [ 550/1251 ( 44%)]  Loss:  3.775961 (3.9653)  Time: 1.082s,  946.18/s  (1.089s,  940.20/s)  LR: 9.572e-04  Data: 0.014 (0.013)
Train: 40 [ 600/1251 ( 48%)]  Loss:  3.958696 (3.9648)  Time: 1.078s,  950.07/s  (1.089s,  939.99/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [ 650/1251 ( 52%)]  Loss:  4.170198 (3.9794)  Time: 1.093s,  937.00/s  (1.090s,  939.81/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [ 700/1251 ( 56%)]  Loss:  3.643002 (3.9570)  Time: 1.083s,  945.57/s  (1.090s,  939.75/s)  LR: 9.572e-04  Data: 0.019 (0.013)
Train: 40 [ 750/1251 ( 60%)]  Loss:  3.881113 (3.9523)  Time: 1.077s,  950.86/s  (1.090s,  939.65/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [ 800/1251 ( 64%)]  Loss:  3.914962 (3.9501)  Time: 1.094s,  936.31/s  (1.090s,  939.80/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [ 850/1251 ( 68%)]  Loss:  3.809554 (3.9423)  Time: 1.079s,  949.21/s  (1.090s,  939.79/s)  LR: 9.572e-04  Data: 0.015 (0.013)
Train: 40 [ 900/1251 ( 72%)]  Loss:  3.890755 (3.9396)  Time: 1.084s,  944.30/s  (1.089s,  940.03/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [ 950/1251 ( 76%)]  Loss:  3.669003 (3.9260)  Time: 1.081s,  947.06/s  (1.089s,  939.94/s)  LR: 9.572e-04  Data: 0.014 (0.013)
Train: 40 [1000/1251 ( 80%)]  Loss:  3.472055 (3.9044)  Time: 1.081s,  947.03/s  (1.089s,  940.01/s)  LR: 9.572e-04  Data: 0.016 (0.013)
Train: 40 [1050/1251 ( 84%)]  Loss:  4.065601 (3.9117)  Time: 1.087s,  942.06/s  (1.090s,  939.80/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [1100/1251 ( 88%)]  Loss:  4.000248 (3.9156)  Time: 1.100s,  930.55/s  (1.090s,  939.56/s)  LR: 9.572e-04  Data: 0.015 (0.013)
Train: 40 [1150/1251 ( 92%)]  Loss:  3.897800 (3.9148)  Time: 1.095s,  935.29/s  (1.090s,  939.37/s)  LR: 9.572e-04  Data: 0.012 (0.013)
Train: 40 [1200/1251 ( 96%)]  Loss:  3.426854 (3.8953)  Time: 1.081s,  947.35/s  (1.090s,  939.61/s)  LR: 9.572e-04  Data: 0.013 (0.013)
Train: 40 [1250/1251 (100%)]  Loss:  3.913914 (3.8960)  Time: 1.070s,  956.88/s  (1.090s,  939.60/s)  LR: 9.572e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.852 (5.852)  Loss:  0.6977 (0.6977)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.7639 (1.2847)  Acc@1: 83.1368 (70.7040)  Acc@5: 95.7547 (90.4860)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-31.pth.tar', 68.95399999267578)

Train: 41 [   0/1251 (  0%)]  Loss:  4.083335 (4.0833)  Time: 1.089s,  940.48/s  (1.089s,  940.48/s)  LR: 9.551e-04  Data: 0.026 (0.026)
Train: 41 [  50/1251 (  4%)]  Loss:  3.570512 (3.8269)  Time: 1.095s,  935.12/s  (1.092s,  937.60/s)  LR: 9.551e-04  Data: 0.012 (0.014)
Train: 41 [ 100/1251 (  8%)]  Loss:  4.003827 (3.8859)  Time: 1.078s,  950.12/s  (1.093s,  937.17/s)  LR: 9.551e-04  Data: 0.012 (0.014)
Train: 41 [ 150/1251 ( 12%)]  Loss:  3.579942 (3.8094)  Time: 1.083s,  945.33/s  (1.093s,  937.17/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [ 200/1251 ( 16%)]  Loss:  3.514560 (3.7504)  Time: 1.077s,  950.59/s  (1.092s,  937.42/s)  LR: 9.551e-04  Data: 0.014 (0.013)
Train: 41 [ 250/1251 ( 20%)]  Loss:  3.501799 (3.7090)  Time: 1.083s,  945.96/s  (1.093s,  936.86/s)  LR: 9.551e-04  Data: 0.012 (0.013)
Train: 41 [ 300/1251 ( 24%)]  Loss:  3.803160 (3.7224)  Time: 1.095s,  934.79/s  (1.092s,  937.55/s)  LR: 9.551e-04  Data: 0.015 (0.013)
Train: 41 [ 350/1251 ( 28%)]  Loss:  3.782382 (3.7299)  Time: 1.080s,  948.09/s  (1.092s,  937.90/s)  LR: 9.551e-04  Data: 0.014 (0.013)
Train: 41 [ 400/1251 ( 32%)]  Loss:  4.088833 (3.7698)  Time: 1.101s,  930.34/s  (1.092s,  937.97/s)  LR: 9.551e-04  Data: 0.016 (0.013)
Train: 41 [ 450/1251 ( 36%)]  Loss:  4.207148 (3.8135)  Time: 1.102s,  928.93/s  (1.092s,  937.71/s)  LR: 9.551e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 41 [ 500/1251 ( 40%)]  Loss:  3.950811 (3.8260)  Time: 1.096s,  934.23/s  (1.092s,  937.86/s)  LR: 9.551e-04  Data: 0.015 (0.013)
Train: 41 [ 550/1251 ( 44%)]  Loss:  3.736636 (3.8186)  Time: 1.097s,  933.86/s  (1.092s,  937.92/s)  LR: 9.551e-04  Data: 0.017 (0.013)
Train: 41 [ 600/1251 ( 48%)]  Loss:  3.941746 (3.8281)  Time: 1.076s,  952.01/s  (1.092s,  937.85/s)  LR: 9.551e-04  Data: 0.012 (0.013)
Train: 41 [ 650/1251 ( 52%)]  Loss:  4.244127 (3.8578)  Time: 1.088s,  941.14/s  (1.092s,  938.01/s)  LR: 9.551e-04  Data: 0.012 (0.013)
Train: 41 [ 700/1251 ( 56%)]  Loss:  3.714854 (3.8482)  Time: 1.075s,  952.14/s  (1.092s,  937.99/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [ 750/1251 ( 60%)]  Loss:  3.799321 (3.8452)  Time: 1.102s,  929.56/s  (1.091s,  938.34/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [ 800/1251 ( 64%)]  Loss:  3.976730 (3.8529)  Time: 1.103s,  928.66/s  (1.091s,  938.50/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [ 850/1251 ( 68%)]  Loss:  4.051849 (3.8640)  Time: 1.077s,  950.74/s  (1.091s,  938.37/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [ 900/1251 ( 72%)]  Loss:  3.542568 (3.8471)  Time: 1.081s,  946.95/s  (1.091s,  938.32/s)  LR: 9.551e-04  Data: 0.014 (0.013)
Train: 41 [ 950/1251 ( 76%)]  Loss:  3.771856 (3.8433)  Time: 1.079s,  949.33/s  (1.091s,  938.39/s)  LR: 9.551e-04  Data: 0.017 (0.013)
Train: 41 [1000/1251 ( 80%)]  Loss:  4.013112 (3.8514)  Time: 1.079s,  948.72/s  (1.091s,  938.66/s)  LR: 9.551e-04  Data: 0.016 (0.013)
Train: 41 [1050/1251 ( 84%)]  Loss:  3.794357 (3.8488)  Time: 1.084s,  944.75/s  (1.091s,  938.62/s)  LR: 9.551e-04  Data: 0.012 (0.013)
Train: 41 [1100/1251 ( 88%)]  Loss:  4.086441 (3.8591)  Time: 1.077s,  950.39/s  (1.091s,  938.69/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [1150/1251 ( 92%)]  Loss:  4.026372 (3.8661)  Time: 1.091s,  938.49/s  (1.091s,  938.85/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [1200/1251 ( 96%)]  Loss:  3.888273 (3.8670)  Time: 1.173s,  872.71/s  (1.091s,  938.83/s)  LR: 9.551e-04  Data: 0.013 (0.013)
Train: 41 [1250/1251 (100%)]  Loss:  3.585969 (3.8562)  Time: 1.065s,  961.84/s  (1.091s,  938.93/s)  LR: 9.551e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.848 (5.848)  Loss:  0.6419 (0.6419)  Acc@1: 86.1328 (86.1328)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7970 (1.2847)  Acc@1: 82.7830 (70.6080)  Acc@5: 95.2830 (90.4880)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-32.pth.tar', 69.01199989257813)

Train: 42 [   0/1251 (  0%)]  Loss:  3.832733 (3.8327)  Time: 1.095s,  935.51/s  (1.095s,  935.51/s)  LR: 9.529e-04  Data: 0.031 (0.031)
Train: 42 [  50/1251 (  4%)]  Loss:  3.998321 (3.9155)  Time: 1.080s,  948.00/s  (1.086s,  942.63/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 100/1251 (  8%)]  Loss:  3.721778 (3.8509)  Time: 1.077s,  950.36/s  (1.088s,  941.46/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 150/1251 ( 12%)]  Loss:  3.690476 (3.8108)  Time: 1.103s,  927.98/s  (1.089s,  940.34/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 200/1251 ( 16%)]  Loss:  4.161354 (3.8809)  Time: 1.088s,  940.89/s  (1.088s,  941.24/s)  LR: 9.529e-04  Data: 0.012 (0.014)
Train: 42 [ 250/1251 ( 20%)]  Loss:  3.533616 (3.8230)  Time: 1.076s,  951.46/s  (1.087s,  941.70/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 300/1251 ( 24%)]  Loss:  3.792202 (3.8186)  Time: 1.077s,  950.68/s  (1.088s,  941.10/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 350/1251 ( 28%)]  Loss:  3.854092 (3.8231)  Time: 1.076s,  951.26/s  (1.089s,  940.26/s)  LR: 9.529e-04  Data: 0.012 (0.014)
Train: 42 [ 400/1251 ( 32%)]  Loss:  4.222529 (3.8675)  Time: 1.081s,  947.08/s  (1.089s,  940.61/s)  LR: 9.529e-04  Data: 0.014 (0.014)
Train: 42 [ 450/1251 ( 36%)]  Loss:  3.900147 (3.8707)  Time: 1.077s,  950.45/s  (1.089s,  940.26/s)  LR: 9.529e-04  Data: 0.014 (0.014)
Train: 42 [ 500/1251 ( 40%)]  Loss:  3.788923 (3.8633)  Time: 1.079s,  949.09/s  (1.089s,  939.99/s)  LR: 9.529e-04  Data: 0.012 (0.014)
Train: 42 [ 550/1251 ( 44%)]  Loss:  4.021776 (3.8765)  Time: 1.078s,  949.91/s  (1.089s,  940.21/s)  LR: 9.529e-04  Data: 0.014 (0.014)
Train: 42 [ 600/1251 ( 48%)]  Loss:  3.807739 (3.8712)  Time: 1.077s,  950.68/s  (1.089s,  940.28/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 650/1251 ( 52%)]  Loss:  4.000204 (3.8804)  Time: 1.078s,  950.23/s  (1.089s,  940.26/s)  LR: 9.529e-04  Data: 0.013 (0.014)
Train: 42 [ 700/1251 ( 56%)]  Loss:  3.582701 (3.8606)  Time: 1.094s,  936.26/s  (1.089s,  940.14/s)  LR: 9.529e-04  Data: 0.012 (0.014)
Train: 42 [ 750/1251 ( 60%)]  Loss:  3.975630 (3.8678)  Time: 1.074s,  953.82/s  (1.089s,  939.93/s)  LR: 9.529e-04  Data: 0.012 (0.014)
Train: 42 [ 800/1251 ( 64%)]  Loss:  3.645265 (3.8547)  Time: 1.083s,  945.88/s  (1.089s,  940.19/s)  LR: 9.529e-04  Data: 0.016 (0.014)
Train: 42 [ 850/1251 ( 68%)]  Loss:  3.829257 (3.8533)  Time: 1.079s,  949.31/s  (1.089s,  940.20/s)  LR: 9.529e-04  Data: 0.015 (0.014)
Train: 42 [ 900/1251 ( 72%)]  Loss:  4.161060 (3.8695)  Time: 1.104s,  927.21/s  (1.089s,  940.41/s)  LR: 9.529e-04  Data: 0.012 (0.014)
Train: 42 [ 950/1251 ( 76%)]  Loss:  3.787997 (3.8654)  Time: 1.095s,  935.54/s  (1.089s,  940.33/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1000/1251 ( 80%)]  Loss:  4.008253 (3.8722)  Time: 1.171s,  874.26/s  (1.089s,  940.08/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1050/1251 ( 84%)]  Loss:  4.360349 (3.8944)  Time: 1.076s,  951.45/s  (1.089s,  940.20/s)  LR: 9.529e-04  Data: 0.013 (0.013)
Train: 42 [1100/1251 ( 88%)]  Loss:  3.712017 (3.8865)  Time: 1.094s,  935.70/s  (1.089s,  939.90/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1150/1251 ( 92%)]  Loss:  4.151996 (3.8975)  Time: 1.074s,  953.38/s  (1.089s,  939.96/s)  LR: 9.529e-04  Data: 0.012 (0.013)
Train: 42 [1200/1251 ( 96%)]  Loss:  3.538527 (3.8832)  Time: 1.078s,  949.96/s  (1.090s,  939.74/s)  LR: 9.529e-04  Data: 0.015 (0.013)
Train: 42 [1250/1251 (100%)]  Loss:  3.480350 (3.8677)  Time: 1.080s,  948.21/s  (1.090s,  939.52/s)  LR: 9.529e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.781 (5.781)  Loss:  0.7030 (0.7030)  Acc@1: 84.7656 (84.7656)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.8097 (1.3043)  Acc@1: 82.5472 (70.3020)  Acc@5: 95.4009 (90.4240)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.30199991699219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-33.pth.tar', 69.36600001464844)

Train: 43 [   0/1251 (  0%)]  Loss:  4.113425 (4.1134)  Time: 1.114s,  919.28/s  (1.114s,  919.28/s)  LR: 9.507e-04  Data: 0.032 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 43 [  50/1251 (  4%)]  Loss:  3.991466 (4.0524)  Time: 1.080s,  948.10/s  (1.086s,  942.69/s)  LR: 9.507e-04  Data: 0.014 (0.014)
Train: 43 [ 100/1251 (  8%)]  Loss:  3.739463 (3.9481)  Time: 1.079s,  948.79/s  (1.092s,  937.79/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [ 150/1251 ( 12%)]  Loss:  3.482595 (3.8317)  Time: 1.093s,  937.00/s  (1.091s,  938.52/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [ 200/1251 ( 16%)]  Loss:  3.794816 (3.8244)  Time: 1.081s,  947.51/s  (1.090s,  939.64/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [ 250/1251 ( 20%)]  Loss:  3.866618 (3.8314)  Time: 1.079s,  949.45/s  (1.090s,  939.88/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [ 300/1251 ( 24%)]  Loss:  3.990467 (3.8541)  Time: 1.077s,  950.51/s  (1.088s,  940.93/s)  LR: 9.507e-04  Data: 0.015 (0.013)
Train: 43 [ 350/1251 ( 28%)]  Loss:  3.535573 (3.8143)  Time: 1.095s,  934.87/s  (1.089s,  940.37/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [ 400/1251 ( 32%)]  Loss:  3.584957 (3.7888)  Time: 1.077s,  950.94/s  (1.089s,  940.07/s)  LR: 9.507e-04  Data: 0.014 (0.013)
Train: 43 [ 450/1251 ( 36%)]  Loss:  3.969910 (3.8069)  Time: 1.083s,  945.31/s  (1.090s,  939.53/s)  LR: 9.507e-04  Data: 0.014 (0.013)
Train: 43 [ 500/1251 ( 40%)]  Loss:  3.630362 (3.7909)  Time: 1.078s,  950.30/s  (1.090s,  939.54/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [ 550/1251 ( 44%)]  Loss:  3.886227 (3.7988)  Time: 1.076s,  952.00/s  (1.090s,  939.74/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [ 600/1251 ( 48%)]  Loss:  3.893556 (3.8061)  Time: 1.079s,  948.84/s  (1.090s,  939.34/s)  LR: 9.507e-04  Data: 0.016 (0.013)
Train: 43 [ 650/1251 ( 52%)]  Loss:  3.725917 (3.8004)  Time: 1.079s,  949.23/s  (1.090s,  939.06/s)  LR: 9.507e-04  Data: 0.016 (0.013)
Train: 43 [ 700/1251 ( 56%)]  Loss:  3.811325 (3.8011)  Time: 1.078s,  949.80/s  (1.090s,  939.32/s)  LR: 9.507e-04  Data: 0.015 (0.013)
Train: 43 [ 750/1251 ( 60%)]  Loss:  3.675580 (3.7933)  Time: 1.098s,  932.71/s  (1.090s,  939.48/s)  LR: 9.507e-04  Data: 0.015 (0.013)
Train: 43 [ 800/1251 ( 64%)]  Loss:  3.896516 (3.7993)  Time: 1.080s,  948.05/s  (1.090s,  939.51/s)  LR: 9.507e-04  Data: 0.015 (0.013)
Train: 43 [ 850/1251 ( 68%)]  Loss:  4.143167 (3.8184)  Time: 1.095s,  934.84/s  (1.090s,  939.31/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [ 900/1251 ( 72%)]  Loss:  3.910175 (3.8233)  Time: 1.104s,  927.43/s  (1.090s,  939.42/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [ 950/1251 ( 76%)]  Loss:  3.607799 (3.8125)  Time: 1.078s,  950.24/s  (1.090s,  939.10/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [1000/1251 ( 80%)]  Loss:  3.802411 (3.8120)  Time: 1.081s,  947.69/s  (1.090s,  939.03/s)  LR: 9.507e-04  Data: 0.016 (0.013)
Train: 43 [1050/1251 ( 84%)]  Loss:  4.048107 (3.8227)  Time: 1.079s,  949.11/s  (1.091s,  938.92/s)  LR: 9.507e-04  Data: 0.016 (0.013)
Train: 43 [1100/1251 ( 88%)]  Loss:  3.663305 (3.8158)  Time: 1.079s,  948.61/s  (1.090s,  939.08/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [1150/1251 ( 92%)]  Loss:  4.242183 (3.8336)  Time: 1.084s,  944.31/s  (1.090s,  939.13/s)  LR: 9.507e-04  Data: 0.012 (0.013)
Train: 43 [1200/1251 ( 96%)]  Loss:  3.785432 (3.8317)  Time: 1.075s,  952.61/s  (1.091s,  938.96/s)  LR: 9.507e-04  Data: 0.013 (0.013)
Train: 43 [1250/1251 (100%)]  Loss:  4.323897 (3.8506)  Time: 1.079s,  948.60/s  (1.091s,  938.96/s)  LR: 9.507e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.786 (5.786)  Loss:  0.7371 (0.7371)  Acc@1: 84.2773 (84.2773)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7986 (1.2910)  Acc@1: 82.6651 (70.9980)  Acc@5: 95.1651 (90.7860)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.30199991699219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-34.pth.tar', 69.48999997070312)

Train: 44 [   0/1251 (  0%)]  Loss:  3.456605 (3.4566)  Time: 1.136s,  901.71/s  (1.136s,  901.71/s)  LR: 9.484e-04  Data: 0.065 (0.065)
Train: 44 [  50/1251 (  4%)]  Loss:  3.797431 (3.6270)  Time: 1.077s,  950.57/s  (1.088s,  940.98/s)  LR: 9.484e-04  Data: 0.013 (0.014)
Train: 44 [ 100/1251 (  8%)]  Loss:  3.787727 (3.6806)  Time: 1.095s,  935.10/s  (1.087s,  941.70/s)  LR: 9.484e-04  Data: 0.011 (0.014)
Train: 44 [ 150/1251 ( 12%)]  Loss:  3.910519 (3.7381)  Time: 1.115s,  918.37/s  (1.092s,  938.00/s)  LR: 9.484e-04  Data: 0.013 (0.014)
Train: 44 [ 200/1251 ( 16%)]  Loss:  3.674288 (3.7253)  Time: 1.096s,  934.32/s  (1.092s,  937.87/s)  LR: 9.484e-04  Data: 0.012 (0.014)
Train: 44 [ 250/1251 ( 20%)]  Loss:  3.909418 (3.7560)  Time: 1.083s,  945.66/s  (1.091s,  938.64/s)  LR: 9.484e-04  Data: 0.014 (0.014)
Train: 44 [ 300/1251 ( 24%)]  Loss:  3.860491 (3.7709)  Time: 1.104s,  927.75/s  (1.092s,  938.07/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 350/1251 ( 28%)]  Loss:  3.946690 (3.7929)  Time: 1.079s,  949.16/s  (1.091s,  938.84/s)  LR: 9.484e-04  Data: 0.017 (0.013)
Train: 44 [ 400/1251 ( 32%)]  Loss:  4.019352 (3.8181)  Time: 1.096s,  934.61/s  (1.091s,  938.74/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 450/1251 ( 36%)]  Loss:  4.053742 (3.8416)  Time: 1.079s,  949.44/s  (1.090s,  939.58/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 500/1251 ( 40%)]  Loss:  3.314928 (3.7937)  Time: 1.078s,  950.00/s  (1.090s,  939.33/s)  LR: 9.484e-04  Data: 0.015 (0.013)
Train: 44 [ 550/1251 ( 44%)]  Loss:  3.385575 (3.7597)  Time: 1.104s,  927.54/s  (1.090s,  939.33/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 600/1251 ( 48%)]  Loss:  4.054219 (3.7824)  Time: 1.079s,  949.39/s  (1.090s,  939.34/s)  LR: 9.484e-04  Data: 0.016 (0.013)
Train: 44 [ 650/1251 ( 52%)]  Loss:  3.682312 (3.7752)  Time: 1.099s,  931.66/s  (1.090s,  939.38/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [ 700/1251 ( 56%)]  Loss:  3.980422 (3.7889)  Time: 1.099s,  931.57/s  (1.090s,  939.33/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [ 750/1251 ( 60%)]  Loss:  3.772903 (3.7879)  Time: 1.077s,  951.09/s  (1.090s,  939.33/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Train: 44 [ 800/1251 ( 64%)]  Loss:  3.681067 (3.7816)  Time: 1.094s,  936.22/s  (1.090s,  939.56/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [ 850/1251 ( 68%)]  Loss:  3.911926 (3.7889)  Time: 1.080s,  947.87/s  (1.090s,  939.69/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [ 900/1251 ( 72%)]  Loss:  3.765589 (3.7876)  Time: 1.076s,  951.60/s  (1.089s,  939.95/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [ 950/1251 ( 76%)]  Loss:  4.148989 (3.8057)  Time: 1.113s,  919.66/s  (1.090s,  939.67/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [1000/1251 ( 80%)]  Loss:  4.151282 (3.8222)  Time: 1.074s,  953.75/s  (1.090s,  939.49/s)  LR: 9.484e-04  Data: 0.011 (0.013)
Train: 44 [1050/1251 ( 84%)]  Loss:  3.866781 (3.8242)  Time: 1.093s,  936.90/s  (1.090s,  939.56/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [1100/1251 ( 88%)]  Loss:  3.658001 (3.8170)  Time: 1.099s,  931.98/s  (1.090s,  939.61/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [1150/1251 ( 92%)]  Loss:  3.836161 (3.8178)  Time: 1.079s,  949.07/s  (1.090s,  939.65/s)  LR: 9.484e-04  Data: 0.012 (0.013)
Train: 44 [1200/1251 ( 96%)]  Loss:  3.725161 (3.8141)  Time: 1.113s,  920.34/s  (1.090s,  939.49/s)  LR: 9.484e-04  Data: 0.013 (0.013)
Train: 44 [1250/1251 (100%)]  Loss:  3.534112 (3.8033)  Time: 1.080s,  947.80/s  (1.090s,  939.41/s)  LR: 9.484e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.742 (5.742)  Loss:  0.6291 (0.6291)  Acc@1: 86.2305 (86.2305)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.230 (0.442)  Loss:  0.7477 (1.2501)  Acc@1: 83.6085 (71.2120)  Acc@5: 95.8726 (90.8140)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.30199991699219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-35.pth.tar', 69.87599997558594)

Train: 45 [   0/1251 (  0%)]  Loss:  3.716249 (3.7162)  Time: 1.094s,  936.23/s  (1.094s,  936.23/s)  LR: 9.460e-04  Data: 0.031 (0.031)
Train: 45 [  50/1251 (  4%)]  Loss:  3.636045 (3.6761)  Time: 1.086s,  943.06/s  (1.094s,  935.69/s)  LR: 9.460e-04  Data: 0.012 (0.014)
Train: 45 [ 100/1251 (  8%)]  Loss:  3.963580 (3.7720)  Time: 1.085s,  943.68/s  (1.091s,  938.76/s)  LR: 9.460e-04  Data: 0.012 (0.014)
Train: 45 [ 150/1251 ( 12%)]  Loss:  3.832798 (3.7872)  Time: 1.075s,  952.83/s  (1.093s,  936.89/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [ 200/1251 ( 16%)]  Loss:  3.798134 (3.7894)  Time: 1.173s,  873.00/s  (1.093s,  937.10/s)  LR: 9.460e-04  Data: 0.011 (0.013)
Train: 45 [ 250/1251 ( 20%)]  Loss:  4.127398 (3.8457)  Time: 1.078s,  949.50/s  (1.092s,  937.77/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 300/1251 ( 24%)]  Loss:  3.833170 (3.8439)  Time: 1.099s,  931.90/s  (1.092s,  937.58/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 350/1251 ( 28%)]  Loss:  4.062619 (3.8712)  Time: 1.079s,  948.99/s  (1.091s,  938.57/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 400/1251 ( 32%)]  Loss:  3.399910 (3.8189)  Time: 1.078s,  950.35/s  (1.091s,  938.83/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 450/1251 ( 36%)]  Loss:  4.014083 (3.8384)  Time: 1.097s,  933.79/s  (1.090s,  939.46/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [ 500/1251 ( 40%)]  Loss:  3.696355 (3.8255)  Time: 1.084s,  944.64/s  (1.090s,  939.47/s)  LR: 9.460e-04  Data: 0.015 (0.013)
Train: 45 [ 550/1251 ( 44%)]  Loss:  3.988263 (3.8391)  Time: 1.112s,  920.51/s  (1.090s,  939.37/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 600/1251 ( 48%)]  Loss:  3.688245 (3.8274)  Time: 1.077s,  950.47/s  (1.090s,  939.38/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 650/1251 ( 52%)]  Loss:  3.930830 (3.8348)  Time: 1.081s,  947.63/s  (1.090s,  939.34/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [ 700/1251 ( 56%)]  Loss:  3.903785 (3.8394)  Time: 1.122s,  912.90/s  (1.090s,  939.26/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 750/1251 ( 60%)]  Loss:  3.836906 (3.8393)  Time: 1.094s,  936.07/s  (1.091s,  938.92/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 800/1251 ( 64%)]  Loss:  3.868006 (3.8410)  Time: 1.078s,  950.06/s  (1.091s,  938.74/s)  LR: 9.460e-04  Data: 0.014 (0.013)
Train: 45 [ 850/1251 ( 68%)]  Loss:  3.550391 (3.8248)  Time: 1.103s,  928.02/s  (1.091s,  938.81/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [ 900/1251 ( 72%)]  Loss:  3.726408 (3.8196)  Time: 1.099s,  931.48/s  (1.091s,  938.60/s)  LR: 9.460e-04  Data: 0.015 (0.013)
Train: 45 [ 950/1251 ( 76%)]  Loss:  3.876711 (3.8225)  Time: 1.094s,  935.85/s  (1.091s,  938.73/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [1000/1251 ( 80%)]  Loss:  3.878630 (3.8252)  Time: 1.104s,  927.49/s  (1.091s,  938.47/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [1050/1251 ( 84%)]  Loss:  3.708180 (3.8198)  Time: 1.085s,  943.91/s  (1.091s,  938.58/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [1100/1251 ( 88%)]  Loss:  3.618262 (3.8111)  Time: 1.082s,  946.56/s  (1.091s,  938.58/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [1150/1251 ( 92%)]  Loss:  4.265604 (3.8300)  Time: 1.084s,  944.67/s  (1.091s,  938.67/s)  LR: 9.460e-04  Data: 0.012 (0.013)
Train: 45 [1200/1251 ( 96%)]  Loss:  3.852828 (3.8309)  Time: 1.095s,  934.81/s  (1.091s,  938.61/s)  LR: 9.460e-04  Data: 0.013 (0.013)
Train: 45 [1250/1251 (100%)]  Loss:  4.158831 (3.8435)  Time: 1.078s,  949.95/s  (1.091s,  938.37/s)  LR: 9.460e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.786 (5.786)  Loss:  0.6222 (0.6222)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.230 (0.441)  Loss:  0.7302 (1.2451)  Acc@1: 84.1981 (71.1560)  Acc@5: 95.7547 (90.6400)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.30199991699219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-37.pth.tar', 70.10600010009766)

Train: 46 [   0/1251 (  0%)]  Loss:  3.620190 (3.6202)  Time: 1.089s,  940.30/s  (1.089s,  940.30/s)  LR: 9.437e-04  Data: 0.027 (0.027)
Train: 46 [  50/1251 (  4%)]  Loss:  4.113299 (3.8667)  Time: 1.082s,  946.77/s  (1.090s,  939.82/s)  LR: 9.437e-04  Data: 0.012 (0.014)
Train: 46 [ 100/1251 (  8%)]  Loss:  4.024751 (3.9194)  Time: 1.079s,  949.30/s  (1.090s,  939.69/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 150/1251 ( 12%)]  Loss:  4.088349 (3.9616)  Time: 1.079s,  948.81/s  (1.088s,  940.83/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 200/1251 ( 16%)]  Loss:  3.349764 (3.8393)  Time: 1.080s,  948.45/s  (1.088s,  941.03/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 250/1251 ( 20%)]  Loss:  3.696765 (3.8155)  Time: 1.099s,  931.95/s  (1.088s,  941.15/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 300/1251 ( 24%)]  Loss:  4.193240 (3.8695)  Time: 1.094s,  936.42/s  (1.089s,  940.70/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 46 [ 350/1251 ( 28%)]  Loss:  3.457180 (3.8179)  Time: 1.097s,  933.36/s  (1.089s,  940.18/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 400/1251 ( 32%)]  Loss:  3.965479 (3.8343)  Time: 1.077s,  950.50/s  (1.090s,  939.49/s)  LR: 9.437e-04  Data: 0.014 (0.013)
Train: 46 [ 450/1251 ( 36%)]  Loss:  3.645487 (3.8155)  Time: 1.082s,  946.62/s  (1.090s,  939.64/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 500/1251 ( 40%)]  Loss:  3.731032 (3.8078)  Time: 1.082s,  946.29/s  (1.090s,  939.37/s)  LR: 9.437e-04  Data: 0.015 (0.013)
Train: 46 [ 550/1251 ( 44%)]  Loss:  3.615692 (3.7918)  Time: 1.095s,  934.75/s  (1.090s,  939.70/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 600/1251 ( 48%)]  Loss:  4.017788 (3.8092)  Time: 1.109s,  923.73/s  (1.090s,  939.47/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 650/1251 ( 52%)]  Loss:  3.563974 (3.7916)  Time: 1.106s,  926.27/s  (1.090s,  939.51/s)  LR: 9.437e-04  Data: 0.014 (0.013)
Train: 46 [ 700/1251 ( 56%)]  Loss:  4.232352 (3.8210)  Time: 1.076s,  951.63/s  (1.090s,  939.30/s)  LR: 9.437e-04  Data: 0.015 (0.013)
Train: 46 [ 750/1251 ( 60%)]  Loss:  3.978762 (3.8309)  Time: 1.096s,  934.32/s  (1.090s,  939.04/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 800/1251 ( 64%)]  Loss:  3.611654 (3.8180)  Time: 1.081s,  947.35/s  (1.090s,  939.18/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [ 850/1251 ( 68%)]  Loss:  3.804906 (3.8173)  Time: 1.101s,  930.29/s  (1.090s,  939.04/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [ 900/1251 ( 72%)]  Loss:  4.005957 (3.8272)  Time: 1.080s,  948.45/s  (1.090s,  939.16/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [ 950/1251 ( 76%)]  Loss:  3.797795 (3.8257)  Time: 1.079s,  948.99/s  (1.090s,  939.03/s)  LR: 9.437e-04  Data: 0.016 (0.013)
Train: 46 [1000/1251 ( 80%)]  Loss:  3.653849 (3.8175)  Time: 1.135s,  902.31/s  (1.090s,  939.12/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [1050/1251 ( 84%)]  Loss:  3.949636 (3.8235)  Time: 1.074s,  953.24/s  (1.090s,  939.05/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [1100/1251 ( 88%)]  Loss:  4.076613 (3.8345)  Time: 1.075s,  952.90/s  (1.090s,  939.23/s)  LR: 9.437e-04  Data: 0.011 (0.013)
Train: 46 [1150/1251 ( 92%)]  Loss:  3.931699 (3.8386)  Time: 1.099s,  931.48/s  (1.090s,  939.14/s)  LR: 9.437e-04  Data: 0.012 (0.013)
Train: 46 [1200/1251 ( 96%)]  Loss:  3.588210 (3.8286)  Time: 1.079s,  949.18/s  (1.091s,  938.91/s)  LR: 9.437e-04  Data: 0.013 (0.013)
Train: 46 [1250/1251 (100%)]  Loss:  4.177446 (3.8420)  Time: 1.083s,  945.82/s  (1.091s,  939.02/s)  LR: 9.437e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.800 (5.800)  Loss:  0.7306 (0.7306)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.7418 (1.2843)  Acc@1: 82.7830 (71.0060)  Acc@5: 95.9906 (90.8220)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.30199991699219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-36.pth.tar', 70.11800002197266)

Train: 47 [   0/1251 (  0%)]  Loss:  3.877036 (3.8770)  Time: 1.089s,  940.28/s  (1.089s,  940.28/s)  LR: 9.412e-04  Data: 0.027 (0.027)
Train: 47 [  50/1251 (  4%)]  Loss:  3.753419 (3.8152)  Time: 1.087s,  942.18/s  (1.084s,  944.50/s)  LR: 9.412e-04  Data: 0.013 (0.014)
Train: 47 [ 100/1251 (  8%)]  Loss:  3.496734 (3.7091)  Time: 1.099s,  931.75/s  (1.087s,  942.36/s)  LR: 9.412e-04  Data: 0.014 (0.014)
Train: 47 [ 150/1251 ( 12%)]  Loss:  4.082071 (3.8023)  Time: 1.095s,  935.29/s  (1.088s,  941.21/s)  LR: 9.412e-04  Data: 0.013 (0.014)
Train: 47 [ 200/1251 ( 16%)]  Loss:  3.529579 (3.7478)  Time: 1.095s,  935.17/s  (1.090s,  939.70/s)  LR: 9.412e-04  Data: 0.012 (0.014)
Train: 47 [ 250/1251 ( 20%)]  Loss:  3.811109 (3.7583)  Time: 1.095s,  935.49/s  (1.091s,  938.73/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 300/1251 ( 24%)]  Loss:  3.768006 (3.7597)  Time: 1.106s,  926.18/s  (1.090s,  939.09/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 350/1251 ( 28%)]  Loss:  3.630942 (3.7436)  Time: 1.082s,  946.70/s  (1.091s,  938.75/s)  LR: 9.412e-04  Data: 0.016 (0.013)
Train: 47 [ 400/1251 ( 32%)]  Loss:  3.812669 (3.7513)  Time: 1.081s,  947.16/s  (1.090s,  939.13/s)  LR: 9.412e-04  Data: 0.012 (0.014)
Train: 47 [ 450/1251 ( 36%)]  Loss:  3.946578 (3.7708)  Time: 1.077s,  950.58/s  (1.090s,  939.10/s)  LR: 9.412e-04  Data: 0.013 (0.014)
Train: 47 [ 500/1251 ( 40%)]  Loss:  3.604807 (3.7557)  Time: 1.082s,  946.23/s  (1.090s,  939.42/s)  LR: 9.412e-04  Data: 0.013 (0.014)
Train: 47 [ 550/1251 ( 44%)]  Loss:  3.779121 (3.7577)  Time: 1.105s,  926.68/s  (1.090s,  939.62/s)  LR: 9.412e-04  Data: 0.013 (0.014)
Train: 47 [ 600/1251 ( 48%)]  Loss:  4.025934 (3.7783)  Time: 1.081s,  947.54/s  (1.090s,  939.64/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 650/1251 ( 52%)]  Loss:  3.769774 (3.7777)  Time: 1.079s,  949.20/s  (1.089s,  940.09/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 700/1251 ( 56%)]  Loss:  3.484368 (3.7581)  Time: 1.104s,  927.35/s  (1.089s,  940.27/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 750/1251 ( 60%)]  Loss:  3.651238 (3.7515)  Time: 1.091s,  938.81/s  (1.089s,  940.15/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [ 800/1251 ( 64%)]  Loss:  3.875393 (3.7588)  Time: 1.081s,  947.46/s  (1.089s,  940.18/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [ 850/1251 ( 68%)]  Loss:  4.016513 (3.7731)  Time: 1.094s,  936.24/s  (1.090s,  939.85/s)  LR: 9.412e-04  Data: 0.014 (0.013)
Train: 47 [ 900/1251 ( 72%)]  Loss:  3.710064 (3.7698)  Time: 1.104s,  927.81/s  (1.089s,  939.90/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [ 950/1251 ( 76%)]  Loss:  3.460938 (3.7543)  Time: 1.095s,  935.49/s  (1.089s,  939.91/s)  LR: 9.412e-04  Data: 0.014 (0.013)
Train: 47 [1000/1251 ( 80%)]  Loss:  3.744342 (3.7538)  Time: 1.080s,  948.13/s  (1.089s,  939.95/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Train: 47 [1050/1251 ( 84%)]  Loss:  3.850748 (3.7582)  Time: 1.077s,  950.52/s  (1.089s,  940.21/s)  LR: 9.412e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 47 [1100/1251 ( 88%)]  Loss:  4.174438 (3.7763)  Time: 1.104s,  927.93/s  (1.089s,  939.91/s)  LR: 9.412e-04  Data: 0.014 (0.013)
Train: 47 [1150/1251 ( 92%)]  Loss:  3.706255 (3.7734)  Time: 1.080s,  948.29/s  (1.089s,  939.90/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [1200/1251 ( 96%)]  Loss:  3.976200 (3.7815)  Time: 1.094s,  936.21/s  (1.089s,  939.93/s)  LR: 9.412e-04  Data: 0.012 (0.013)
Train: 47 [1250/1251 (100%)]  Loss:  4.106702 (3.7940)  Time: 1.067s,  959.73/s  (1.089s,  939.88/s)  LR: 9.412e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.714 (5.714)  Loss:  0.6779 (0.6779)  Acc@1: 86.3281 (86.3281)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.8033 (1.3025)  Acc@1: 82.6651 (70.7120)  Acc@5: 95.1651 (90.6680)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 70.71200009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-42.pth.tar', 70.30199991699219)

Train: 48 [   0/1251 (  0%)]  Loss:  3.887777 (3.8878)  Time: 1.111s,  921.95/s  (1.111s,  921.95/s)  LR: 9.388e-04  Data: 0.027 (0.027)
Train: 48 [  50/1251 (  4%)]  Loss:  3.998258 (3.9430)  Time: 1.094s,  936.15/s  (1.088s,  941.26/s)  LR: 9.388e-04  Data: 0.015 (0.014)
Train: 48 [ 100/1251 (  8%)]  Loss:  3.869785 (3.9186)  Time: 1.093s,  936.71/s  (1.093s,  936.50/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [ 150/1251 ( 12%)]  Loss:  3.664524 (3.8551)  Time: 1.082s,  946.54/s  (1.095s,  935.36/s)  LR: 9.388e-04  Data: 0.012 (0.013)
Train: 48 [ 200/1251 ( 16%)]  Loss:  3.670115 (3.8181)  Time: 1.082s,  946.27/s  (1.093s,  937.29/s)  LR: 9.388e-04  Data: 0.013 (0.014)
Train: 48 [ 250/1251 ( 20%)]  Loss:  3.651114 (3.7903)  Time: 1.088s,  941.36/s  (1.092s,  937.48/s)  LR: 9.388e-04  Data: 0.014 (0.013)
Train: 48 [ 300/1251 ( 24%)]  Loss:  3.926936 (3.8098)  Time: 1.082s,  946.83/s  (1.092s,  937.34/s)  LR: 9.388e-04  Data: 0.012 (0.014)
Train: 48 [ 350/1251 ( 28%)]  Loss:  3.622993 (3.7864)  Time: 1.089s,  940.19/s  (1.091s,  938.20/s)  LR: 9.388e-04  Data: 0.013 (0.014)
Train: 48 [ 400/1251 ( 32%)]  Loss:  4.007933 (3.8110)  Time: 1.177s,  870.11/s  (1.091s,  938.69/s)  LR: 9.388e-04  Data: 0.013 (0.014)
Train: 48 [ 450/1251 ( 36%)]  Loss:  3.889934 (3.8189)  Time: 1.107s,  924.93/s  (1.091s,  938.50/s)  LR: 9.388e-04  Data: 0.014 (0.014)
Train: 48 [ 500/1251 ( 40%)]  Loss:  3.720963 (3.8100)  Time: 1.079s,  949.12/s  (1.091s,  938.49/s)  LR: 9.388e-04  Data: 0.012 (0.014)
Train: 48 [ 550/1251 ( 44%)]  Loss:  3.804493 (3.8096)  Time: 1.082s,  946.75/s  (1.091s,  938.92/s)  LR: 9.388e-04  Data: 0.015 (0.014)
Train: 48 [ 600/1251 ( 48%)]  Loss:  4.174927 (3.8377)  Time: 1.078s,  949.74/s  (1.091s,  938.70/s)  LR: 9.388e-04  Data: 0.013 (0.014)
Train: 48 [ 650/1251 ( 52%)]  Loss:  3.582021 (3.8194)  Time: 1.078s,  950.21/s  (1.091s,  938.60/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [ 700/1251 ( 56%)]  Loss:  3.536320 (3.8005)  Time: 1.085s,  943.50/s  (1.091s,  938.62/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [ 750/1251 ( 60%)]  Loss:  3.792123 (3.8000)  Time: 1.097s,  933.18/s  (1.091s,  938.44/s)  LR: 9.388e-04  Data: 0.012 (0.013)
Train: 48 [ 800/1251 ( 64%)]  Loss:  4.084831 (3.8168)  Time: 1.079s,  948.80/s  (1.091s,  938.62/s)  LR: 9.388e-04  Data: 0.014 (0.013)
Train: 48 [ 850/1251 ( 68%)]  Loss:  3.778338 (3.8146)  Time: 1.076s,  951.33/s  (1.091s,  938.65/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [ 900/1251 ( 72%)]  Loss:  3.390902 (3.7923)  Time: 1.107s,  925.07/s  (1.091s,  938.81/s)  LR: 9.388e-04  Data: 0.012 (0.013)
Train: 48 [ 950/1251 ( 76%)]  Loss:  3.644820 (3.7850)  Time: 1.084s,  944.35/s  (1.091s,  938.62/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [1000/1251 ( 80%)]  Loss:  3.772289 (3.7844)  Time: 1.106s,  926.24/s  (1.091s,  938.58/s)  LR: 9.388e-04  Data: 0.012 (0.013)
Train: 48 [1050/1251 ( 84%)]  Loss:  3.802201 (3.7852)  Time: 1.098s,  932.86/s  (1.091s,  938.77/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [1100/1251 ( 88%)]  Loss:  3.534253 (3.7743)  Time: 1.096s,  934.37/s  (1.091s,  938.95/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [1150/1251 ( 92%)]  Loss:  3.663143 (3.7696)  Time: 1.083s,  945.57/s  (1.091s,  938.87/s)  LR: 9.388e-04  Data: 0.020 (0.013)
Train: 48 [1200/1251 ( 96%)]  Loss:  3.772587 (3.7697)  Time: 1.095s,  934.78/s  (1.091s,  938.67/s)  LR: 9.388e-04  Data: 0.013 (0.013)
Train: 48 [1250/1251 (100%)]  Loss:  4.084561 (3.7819)  Time: 1.080s,  947.75/s  (1.091s,  938.65/s)  LR: 9.388e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 5.834 (5.834)  Loss:  0.6640 (0.6640)  Acc@1: 86.8164 (86.8164)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.7467 (1.2501)  Acc@1: 83.9623 (71.5920)  Acc@5: 95.7547 (90.9360)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 70.71200009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-38.pth.tar', 70.44599999267578)

Train: 49 [   0/1251 (  0%)]  Loss:  3.887284 (3.8873)  Time: 1.108s,  924.12/s  (1.108s,  924.12/s)  LR: 9.363e-04  Data: 0.026 (0.026)
Train: 49 [  50/1251 (  4%)]  Loss:  3.437040 (3.6622)  Time: 1.105s,  926.46/s  (1.100s,  930.76/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Train: 49 [ 100/1251 (  8%)]  Loss:  3.786253 (3.7035)  Time: 1.096s,  934.11/s  (1.095s,  935.06/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 150/1251 ( 12%)]  Loss:  3.585807 (3.6741)  Time: 1.081s,  947.63/s  (1.094s,  935.92/s)  LR: 9.363e-04  Data: 0.016 (0.013)
Train: 49 [ 200/1251 ( 16%)]  Loss:  3.926780 (3.7246)  Time: 1.081s,  947.46/s  (1.095s,  935.52/s)  LR: 9.363e-04  Data: 0.013 (0.014)
Train: 49 [ 250/1251 ( 20%)]  Loss:  4.020136 (3.7739)  Time: 1.115s,  918.56/s  (1.094s,  936.34/s)  LR: 9.363e-04  Data: 0.013 (0.013)
Train: 49 [ 300/1251 ( 24%)]  Loss:  3.736546 (3.7685)  Time: 1.077s,  950.63/s  (1.092s,  937.41/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 350/1251 ( 28%)]  Loss:  3.732736 (3.7641)  Time: 1.076s,  951.69/s  (1.092s,  938.07/s)  LR: 9.363e-04  Data: 0.012 (0.014)
Train: 49 [ 400/1251 ( 32%)]  Loss:  3.799744 (3.7680)  Time: 1.106s,  926.17/s  (1.091s,  938.35/s)  LR: 9.363e-04  Data: 0.013 (0.013)
Train: 49 [ 450/1251 ( 36%)]  Loss:  3.592917 (3.7505)  Time: 1.095s,  935.20/s  (1.091s,  938.54/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 500/1251 ( 40%)]  Loss:  3.772001 (3.7525)  Time: 1.097s,  933.51/s  (1.091s,  938.20/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 550/1251 ( 44%)]  Loss:  3.638888 (3.7430)  Time: 1.081s,  947.40/s  (1.091s,  938.61/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 49 [ 600/1251 ( 48%)]  Loss:  3.771273 (3.7452)  Time: 1.079s,  948.78/s  (1.091s,  938.69/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Train: 49 [ 650/1251 ( 52%)]  Loss:  4.157056 (3.7746)  Time: 1.083s,  945.45/s  (1.090s,  939.07/s)  LR: 9.363e-04  Data: 0.017 (0.013)
Train: 49 [ 700/1251 ( 56%)]  Loss:  3.629791 (3.7650)  Time: 1.079s,  948.64/s  (1.091s,  938.97/s)  LR: 9.363e-04  Data: 0.013 (0.013)
Train: 49 [ 750/1251 ( 60%)]  Loss:  4.076593 (3.7844)  Time: 1.082s,  946.21/s  (1.091s,  939.00/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 800/1251 ( 64%)]  Loss:  3.921780 (3.7925)  Time: 1.096s,  934.60/s  (1.090s,  939.07/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [ 850/1251 ( 68%)]  Loss:  3.377237 (3.7694)  Time: 1.080s,  948.14/s  (1.090s,  939.21/s)  LR: 9.363e-04  Data: 0.013 (0.013)
Train: 49 [ 900/1251 ( 72%)]  Loss:  4.099336 (3.7868)  Time: 1.077s,  950.61/s  (1.090s,  939.09/s)  LR: 9.363e-04  Data: 0.013 (0.013)
Train: 49 [ 950/1251 ( 76%)]  Loss:  3.707091 (3.7828)  Time: 1.090s,  939.48/s  (1.091s,  939.01/s)  LR: 9.363e-04  Data: 0.011 (0.013)
Train: 49 [1000/1251 ( 80%)]  Loss:  3.802226 (3.7837)  Time: 1.082s,  946.04/s  (1.090s,  939.13/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Train: 49 [1050/1251 ( 84%)]  Loss:  3.901243 (3.7891)  Time: 1.112s,  921.18/s  (1.090s,  939.25/s)  LR: 9.363e-04  Data: 0.012 (0.013)
Train: 49 [1100/1251 ( 88%)]  Loss:  3.924844 (3.7950)  Time: 1.098s,  932.65/s  (1.090s,  939.21/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Train: 49 [1150/1251 ( 92%)]  Loss:  3.909104 (3.7997)  Time: 1.080s,  948.43/s  (1.090s,  939.18/s)  LR: 9.363e-04  Data: 0.014 (0.013)
Train: 49 [1200/1251 ( 96%)]  Loss:  3.897904 (3.8037)  Time: 1.096s,  933.95/s  (1.090s,  939.48/s)  LR: 9.363e-04  Data: 0.015 (0.013)
Train: 49 [1250/1251 (100%)]  Loss:  3.869801 (3.8062)  Time: 1.079s,  949.38/s  (1.090s,  939.46/s)  LR: 9.363e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.792 (5.792)  Loss:  0.6449 (0.6449)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7129 (1.2424)  Acc@1: 83.9623 (71.3960)  Acc@5: 96.3443 (90.8920)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 70.71200009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-39.pth.tar', 70.59200001953126)

Train: 50 [   0/1251 (  0%)]  Loss:  3.838500 (3.8385)  Time: 1.089s,  940.59/s  (1.089s,  940.59/s)  LR: 9.337e-04  Data: 0.024 (0.024)
Train: 50 [  50/1251 (  4%)]  Loss:  3.898824 (3.8687)  Time: 1.100s,  930.73/s  (1.098s,  932.77/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 100/1251 (  8%)]  Loss:  3.628475 (3.7886)  Time: 1.096s,  934.42/s  (1.096s,  934.26/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 150/1251 ( 12%)]  Loss:  4.104111 (3.8675)  Time: 1.084s,  944.26/s  (1.098s,  932.18/s)  LR: 9.337e-04  Data: 0.022 (0.013)
Train: 50 [ 200/1251 ( 16%)]  Loss:  3.597192 (3.8134)  Time: 1.082s,  946.77/s  (1.095s,  935.01/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 50 [ 250/1251 ( 20%)]  Loss:  3.485944 (3.7588)  Time: 1.171s,  874.09/s  (1.094s,  936.23/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 300/1251 ( 24%)]  Loss:  3.636806 (3.7414)  Time: 1.098s,  932.85/s  (1.093s,  936.78/s)  LR: 9.337e-04  Data: 0.015 (0.013)
Train: 50 [ 350/1251 ( 28%)]  Loss:  3.751493 (3.7427)  Time: 1.107s,  925.30/s  (1.094s,  936.01/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 400/1251 ( 32%)]  Loss:  3.827236 (3.7521)  Time: 1.079s,  948.75/s  (1.093s,  936.56/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 50 [ 450/1251 ( 36%)]  Loss:  3.834713 (3.7603)  Time: 1.077s,  950.35/s  (1.094s,  936.37/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 50 [ 500/1251 ( 40%)]  Loss:  3.625551 (3.7481)  Time: 1.096s,  934.51/s  (1.093s,  936.77/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 550/1251 ( 44%)]  Loss:  3.871075 (3.7583)  Time: 1.080s,  948.10/s  (1.093s,  937.27/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 600/1251 ( 48%)]  Loss:  3.944366 (3.7726)  Time: 1.097s,  933.30/s  (1.092s,  937.73/s)  LR: 9.337e-04  Data: 0.015 (0.013)
Train: 50 [ 650/1251 ( 52%)]  Loss:  4.002019 (3.7890)  Time: 1.102s,  928.80/s  (1.093s,  937.30/s)  LR: 9.337e-04  Data: 0.018 (0.013)
Train: 50 [ 700/1251 ( 56%)]  Loss:  3.830612 (3.7918)  Time: 1.081s,  947.46/s  (1.092s,  937.53/s)  LR: 9.337e-04  Data: 0.014 (0.013)
Train: 50 [ 750/1251 ( 60%)]  Loss:  3.729434 (3.7879)  Time: 1.081s,  947.67/s  (1.092s,  937.61/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 50 [ 800/1251 ( 64%)]  Loss:  3.780246 (3.7874)  Time: 1.096s,  934.04/s  (1.093s,  937.22/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 50 [ 850/1251 ( 68%)]  Loss:  4.062660 (3.8027)  Time: 1.099s,  931.53/s  (1.092s,  937.45/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 900/1251 ( 72%)]  Loss:  3.843630 (3.8049)  Time: 1.081s,  946.96/s  (1.092s,  937.67/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [ 950/1251 ( 76%)]  Loss:  3.873775 (3.8083)  Time: 1.077s,  951.17/s  (1.092s,  937.67/s)  LR: 9.337e-04  Data: 0.013 (0.013)
Train: 50 [1000/1251 ( 80%)]  Loss:  3.779277 (3.8069)  Time: 1.079s,  949.14/s  (1.092s,  938.00/s)  LR: 9.337e-04  Data: 0.014 (0.013)
Train: 50 [1050/1251 ( 84%)]  Loss:  3.954711 (3.8137)  Time: 1.079s,  949.17/s  (1.092s,  938.06/s)  LR: 9.337e-04  Data: 0.014 (0.013)
Train: 50 [1100/1251 ( 88%)]  Loss:  4.093554 (3.8258)  Time: 1.078s,  949.92/s  (1.092s,  938.16/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [1150/1251 ( 92%)]  Loss:  3.725506 (3.8217)  Time: 1.087s,  941.69/s  (1.091s,  938.23/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [1200/1251 ( 96%)]  Loss:  3.589752 (3.8124)  Time: 1.094s,  935.64/s  (1.091s,  938.18/s)  LR: 9.337e-04  Data: 0.012 (0.013)
Train: 50 [1250/1251 (100%)]  Loss:  3.914817 (3.8163)  Time: 1.063s,  963.27/s  (1.091s,  938.22/s)  LR: 9.337e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.808 (5.808)  Loss:  0.6482 (0.6482)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.7171 (1.2317)  Acc@1: 83.6085 (71.9460)  Acc@5: 96.1085 (91.0520)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 70.71200009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-41.pth.tar', 70.60800001953125)

Train: 51 [   0/1251 (  0%)]  Loss:  3.626211 (3.6262)  Time: 1.087s,  942.07/s  (1.087s,  942.07/s)  LR: 9.311e-04  Data: 0.023 (0.023)
Train: 51 [  50/1251 (  4%)]  Loss:  4.020455 (3.8233)  Time: 1.113s,  919.74/s  (1.093s,  936.67/s)  LR: 9.311e-04  Data: 0.014 (0.014)
Train: 51 [ 100/1251 (  8%)]  Loss:  3.770807 (3.8058)  Time: 1.095s,  935.17/s  (1.093s,  937.10/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 150/1251 ( 12%)]  Loss:  4.133631 (3.8878)  Time: 1.093s,  936.79/s  (1.093s,  937.12/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 51 [ 200/1251 ( 16%)]  Loss:  3.659842 (3.8422)  Time: 1.082s,  946.75/s  (1.091s,  938.88/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 250/1251 ( 20%)]  Loss:  3.574739 (3.7976)  Time: 1.078s,  949.95/s  (1.090s,  939.16/s)  LR: 9.311e-04  Data: 0.014 (0.013)
Train: 51 [ 300/1251 ( 24%)]  Loss:  4.104510 (3.8415)  Time: 1.098s,  932.72/s  (1.091s,  938.39/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 350/1251 ( 28%)]  Loss:  3.731744 (3.8277)  Time: 1.077s,  950.89/s  (1.092s,  938.11/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [ 400/1251 ( 32%)]  Loss:  3.539265 (3.7957)  Time: 1.097s,  933.82/s  (1.091s,  938.19/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [ 450/1251 ( 36%)]  Loss:  3.867347 (3.8029)  Time: 1.089s,  940.14/s  (1.092s,  938.07/s)  LR: 9.311e-04  Data: 0.025 (0.013)
Train: 51 [ 500/1251 ( 40%)]  Loss:  3.882374 (3.8101)  Time: 1.095s,  935.04/s  (1.091s,  938.69/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 550/1251 ( 44%)]  Loss:  3.789176 (3.8083)  Time: 1.081s,  946.84/s  (1.091s,  938.67/s)  LR: 9.311e-04  Data: 0.018 (0.013)
Train: 51 [ 600/1251 ( 48%)]  Loss:  3.866488 (3.8128)  Time: 1.096s,  934.37/s  (1.091s,  938.70/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 650/1251 ( 52%)]  Loss:  3.704728 (3.8051)  Time: 1.078s,  949.47/s  (1.091s,  938.18/s)  LR: 9.311e-04  Data: 0.016 (0.013)
Train: 51 [ 700/1251 ( 56%)]  Loss:  3.676333 (3.7965)  Time: 1.087s,  942.08/s  (1.091s,  938.36/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 750/1251 ( 60%)]  Loss:  3.663289 (3.7882)  Time: 1.117s,  916.56/s  (1.091s,  938.51/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [ 800/1251 ( 64%)]  Loss:  3.744448 (3.7856)  Time: 1.111s,  922.07/s  (1.092s,  938.14/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [ 850/1251 ( 68%)]  Loss:  3.987285 (3.7968)  Time: 1.080s,  948.37/s  (1.091s,  938.36/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [ 900/1251 ( 72%)]  Loss:  4.144316 (3.8151)  Time: 1.187s,  862.62/s  (1.091s,  938.24/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [ 950/1251 ( 76%)]  Loss:  3.496878 (3.7992)  Time: 1.085s,  943.40/s  (1.091s,  938.22/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [1000/1251 ( 80%)]  Loss:  3.973992 (3.8075)  Time: 1.084s,  944.85/s  (1.091s,  938.23/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [1050/1251 ( 84%)]  Loss:  3.964463 (3.8147)  Time: 1.103s,  928.00/s  (1.091s,  938.52/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [1100/1251 ( 88%)]  Loss:  3.763828 (3.8124)  Time: 1.094s,  935.60/s  (1.091s,  938.34/s)  LR: 9.311e-04  Data: 0.012 (0.013)
Train: 51 [1150/1251 ( 92%)]  Loss:  3.899492 (3.8161)  Time: 1.078s,  950.05/s  (1.091s,  938.20/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [1200/1251 ( 96%)]  Loss:  3.767200 (3.8141)  Time: 1.116s,  917.24/s  (1.092s,  938.03/s)  LR: 9.311e-04  Data: 0.013 (0.013)
Train: 51 [1250/1251 (100%)]  Loss:  4.159506 (3.8274)  Time: 1.060s,  965.69/s  (1.092s,  938.11/s)  LR: 9.311e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.837 (5.837)  Loss:  0.7025 (0.7025)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7945 (1.2952)  Acc@1: 83.7264 (71.7820)  Acc@5: 95.1651 (91.0020)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 70.71200009765624)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-40.pth.tar', 70.70400004394531)

Train: 52 [   0/1251 (  0%)]  Loss:  3.574511 (3.5745)  Time: 1.094s,  936.37/s  (1.094s,  936.37/s)  LR: 9.284e-04  Data: 0.029 (0.029)
Train: 52 [  50/1251 (  4%)]  Loss:  3.894228 (3.7344)  Time: 1.097s,  933.82/s  (1.090s,  939.65/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 100/1251 (  8%)]  Loss:  3.725029 (3.7313)  Time: 1.083s,  945.89/s  (1.090s,  939.73/s)  LR: 9.284e-04  Data: 0.014 (0.013)
Train: 52 [ 150/1251 ( 12%)]  Loss:  4.148584 (3.8356)  Time: 1.097s,  933.18/s  (1.091s,  938.60/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [ 200/1251 ( 16%)]  Loss:  3.720953 (3.8127)  Time: 1.077s,  950.81/s  (1.091s,  938.25/s)  LR: 9.284e-04  Data: 0.014 (0.013)
Train: 52 [ 250/1251 ( 20%)]  Loss:  3.857809 (3.8202)  Time: 1.093s,  936.48/s  (1.091s,  938.35/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [ 300/1251 ( 24%)]  Loss:  3.869468 (3.8272)  Time: 1.172s,  873.81/s  (1.091s,  938.36/s)  LR: 9.284e-04  Data: 0.011 (0.013)
Train: 52 [ 350/1251 ( 28%)]  Loss:  3.607921 (3.7998)  Time: 1.076s,  951.56/s  (1.091s,  938.60/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [ 400/1251 ( 32%)]  Loss:  3.642635 (3.7823)  Time: 1.168s,  876.70/s  (1.091s,  938.82/s)  LR: 9.284e-04  Data: 0.015 (0.013)
Train: 52 [ 450/1251 ( 36%)]  Loss:  3.928030 (3.7969)  Time: 1.101s,  930.35/s  (1.091s,  938.86/s)  LR: 9.284e-04  Data: 0.014 (0.013)
Train: 52 [ 500/1251 ( 40%)]  Loss:  3.938242 (3.8098)  Time: 1.190s,  860.61/s  (1.092s,  937.99/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [ 550/1251 ( 44%)]  Loss:  4.181045 (3.8407)  Time: 1.081s,  947.29/s  (1.091s,  938.38/s)  LR: 9.284e-04  Data: 0.014 (0.013)
Train: 52 [ 600/1251 ( 48%)]  Loss:  3.684464 (3.8287)  Time: 1.099s,  931.82/s  (1.091s,  938.27/s)  LR: 9.284e-04  Data: 0.019 (0.013)
Train: 52 [ 650/1251 ( 52%)]  Loss:  3.470266 (3.8031)  Time: 1.081s,  947.21/s  (1.091s,  938.56/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [ 700/1251 ( 56%)]  Loss:  3.839625 (3.8055)  Time: 1.080s,  947.98/s  (1.091s,  938.54/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [ 750/1251 ( 60%)]  Loss:  4.150520 (3.8271)  Time: 1.080s,  948.36/s  (1.091s,  938.58/s)  LR: 9.284e-04  Data: 0.015 (0.013)
Train: 52 [ 800/1251 ( 64%)]  Loss:  3.318999 (3.7972)  Time: 1.080s,  948.46/s  (1.091s,  938.51/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [ 850/1251 ( 68%)]  Loss:  3.670469 (3.7902)  Time: 1.080s,  947.95/s  (1.091s,  938.52/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [ 900/1251 ( 72%)]  Loss:  3.989437 (3.8006)  Time: 1.095s,  934.78/s  (1.091s,  938.49/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 52 [ 950/1251 ( 76%)]  Loss:  3.821732 (3.8017)  Time: 1.075s,  952.16/s  (1.091s,  938.47/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [1000/1251 ( 80%)]  Loss:  4.038558 (3.8130)  Time: 1.083s,  945.93/s  (1.091s,  938.43/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [1050/1251 ( 84%)]  Loss:  3.918604 (3.8178)  Time: 1.098s,  932.75/s  (1.091s,  938.43/s)  LR: 9.284e-04  Data: 0.012 (0.013)
Train: 52 [1100/1251 ( 88%)]  Loss:  4.043951 (3.8276)  Time: 1.082s,  946.38/s  (1.091s,  938.46/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [1150/1251 ( 92%)]  Loss:  4.041883 (3.8365)  Time: 1.087s,  941.76/s  (1.091s,  938.76/s)  LR: 9.284e-04  Data: 0.013 (0.013)
Train: 52 [1200/1251 ( 96%)]  Loss:  3.620135 (3.8279)  Time: 1.081s,  947.61/s  (1.091s,  938.65/s)  LR: 9.284e-04  Data: 0.016 (0.013)
Train: 52 [1250/1251 (100%)]  Loss:  3.701544 (3.8230)  Time: 1.079s,  948.73/s  (1.091s,  938.76/s)  LR: 9.284e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 5.802 (5.802)  Loss:  0.6959 (0.6959)  Acc@1: 86.7188 (86.7188)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7398 (1.2641)  Acc@1: 83.4906 (71.7120)  Acc@5: 95.7547 (90.9340)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-47.pth.tar', 70.71200009765624)

Train: 53 [   0/1251 (  0%)]  Loss:  3.537327 (3.5373)  Time: 1.092s,  937.89/s  (1.092s,  937.89/s)  LR: 9.257e-04  Data: 0.029 (0.029)
Train: 53 [  50/1251 (  4%)]  Loss:  4.015044 (3.7762)  Time: 1.081s,  947.69/s  (1.092s,  937.50/s)  LR: 9.257e-04  Data: 0.017 (0.014)
Train: 53 [ 100/1251 (  8%)]  Loss:  3.630422 (3.7276)  Time: 1.091s,  938.42/s  (1.093s,  937.10/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [ 150/1251 ( 12%)]  Loss:  3.565916 (3.6872)  Time: 1.093s,  937.00/s  (1.094s,  936.03/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [ 200/1251 ( 16%)]  Loss:  3.536323 (3.6570)  Time: 1.077s,  950.51/s  (1.095s,  935.42/s)  LR: 9.257e-04  Data: 0.013 (0.013)
Train: 53 [ 250/1251 ( 20%)]  Loss:  4.005539 (3.7151)  Time: 1.079s,  949.11/s  (1.095s,  935.50/s)  LR: 9.257e-04  Data: 0.015 (0.013)
Train: 53 [ 300/1251 ( 24%)]  Loss:  3.683112 (3.7105)  Time: 1.078s,  949.52/s  (1.094s,  936.32/s)  LR: 9.257e-04  Data: 0.013 (0.013)
Train: 53 [ 350/1251 ( 28%)]  Loss:  3.958102 (3.7415)  Time: 1.084s,  944.48/s  (1.094s,  936.40/s)  LR: 9.257e-04  Data: 0.021 (0.013)
Train: 53 [ 400/1251 ( 32%)]  Loss:  3.845472 (3.7530)  Time: 1.082s,  946.83/s  (1.093s,  936.78/s)  LR: 9.257e-04  Data: 0.014 (0.013)
Train: 53 [ 450/1251 ( 36%)]  Loss:  3.745869 (3.7523)  Time: 1.103s,  928.31/s  (1.093s,  937.08/s)  LR: 9.257e-04  Data: 0.020 (0.013)
Train: 53 [ 500/1251 ( 40%)]  Loss:  3.896328 (3.7654)  Time: 1.082s,  946.02/s  (1.092s,  937.49/s)  LR: 9.257e-04  Data: 0.019 (0.013)
Train: 53 [ 550/1251 ( 44%)]  Loss:  3.871553 (3.7743)  Time: 1.078s,  949.77/s  (1.092s,  937.51/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [ 600/1251 ( 48%)]  Loss:  3.487986 (3.7522)  Time: 1.085s,  943.89/s  (1.092s,  937.60/s)  LR: 9.257e-04  Data: 0.013 (0.013)
Train: 53 [ 650/1251 ( 52%)]  Loss:  3.611876 (3.7422)  Time: 1.081s,  947.47/s  (1.092s,  937.79/s)  LR: 9.257e-04  Data: 0.014 (0.013)
Train: 53 [ 700/1251 ( 56%)]  Loss:  3.535848 (3.7284)  Time: 1.077s,  950.72/s  (1.092s,  937.72/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [ 750/1251 ( 60%)]  Loss:  3.758060 (3.7303)  Time: 1.084s,  945.05/s  (1.092s,  937.81/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [ 800/1251 ( 64%)]  Loss:  3.910072 (3.7409)  Time: 1.101s,  930.38/s  (1.092s,  937.92/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [ 850/1251 ( 68%)]  Loss:  3.512757 (3.7282)  Time: 1.095s,  935.51/s  (1.092s,  937.68/s)  LR: 9.257e-04  Data: 0.011 (0.013)
Train: 53 [ 900/1251 ( 72%)]  Loss:  3.893949 (3.7369)  Time: 1.077s,  950.40/s  (1.092s,  937.71/s)  LR: 9.257e-04  Data: 0.013 (0.013)
Train: 53 [ 950/1251 ( 76%)]  Loss:  3.523265 (3.7262)  Time: 1.081s,  947.34/s  (1.092s,  937.83/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [1000/1251 ( 80%)]  Loss:  3.467065 (3.7139)  Time: 1.078s,  950.17/s  (1.092s,  937.69/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [1050/1251 ( 84%)]  Loss:  3.918086 (3.7232)  Time: 1.101s,  929.78/s  (1.092s,  937.77/s)  LR: 9.257e-04  Data: 0.013 (0.013)
Train: 53 [1100/1251 ( 88%)]  Loss:  3.906316 (3.7311)  Time: 1.098s,  932.20/s  (1.092s,  937.71/s)  LR: 9.257e-04  Data: 0.015 (0.013)
Train: 53 [1150/1251 ( 92%)]  Loss:  3.796446 (3.7339)  Time: 1.073s,  954.34/s  (1.092s,  937.58/s)  LR: 9.257e-04  Data: 0.010 (0.013)
Train: 53 [1200/1251 ( 96%)]  Loss:  3.542220 (3.7262)  Time: 1.077s,  950.53/s  (1.093s,  937.20/s)  LR: 9.257e-04  Data: 0.012 (0.013)
Train: 53 [1250/1251 (100%)]  Loss:  3.625141 (3.7223)  Time: 1.080s,  948.48/s  (1.093s,  937.22/s)  LR: 9.257e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.760 (5.760)  Loss:  0.6539 (0.6539)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7872 (1.2345)  Acc@1: 82.1934 (72.1040)  Acc@5: 96.1085 (91.1700)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-43.pth.tar', 70.99800009765625)

Train: 54 [   0/1251 (  0%)]  Loss:  3.909109 (3.9091)  Time: 1.089s,  940.02/s  (1.089s,  940.02/s)  LR: 9.229e-04  Data: 0.026 (0.026)
Train: 54 [  50/1251 (  4%)]  Loss:  3.811622 (3.8604)  Time: 1.105s,  926.30/s  (1.089s,  939.91/s)  LR: 9.229e-04  Data: 0.013 (0.014)
Train: 54 [ 100/1251 (  8%)]  Loss:  3.741384 (3.8207)  Time: 1.082s,  946.40/s  (1.090s,  939.33/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [ 150/1251 ( 12%)]  Loss:  3.956438 (3.8546)  Time: 1.095s,  934.88/s  (1.091s,  938.96/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 200/1251 ( 16%)]  Loss:  3.336262 (3.7510)  Time: 1.082s,  946.75/s  (1.091s,  939.00/s)  LR: 9.229e-04  Data: 0.015 (0.014)
Train: 54 [ 250/1251 ( 20%)]  Loss:  3.693480 (3.7414)  Time: 1.084s,  944.90/s  (1.090s,  939.39/s)  LR: 9.229e-04  Data: 0.013 (0.014)
Train: 54 [ 300/1251 ( 24%)]  Loss:  3.575438 (3.7177)  Time: 1.101s,  930.09/s  (1.092s,  938.15/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [ 350/1251 ( 28%)]  Loss:  3.577380 (3.7001)  Time: 1.096s,  934.03/s  (1.091s,  938.29/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [ 400/1251 ( 32%)]  Loss:  3.738158 (3.7044)  Time: 1.095s,  934.98/s  (1.091s,  938.21/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 54 [ 450/1251 ( 36%)]  Loss:  3.794038 (3.7133)  Time: 1.094s,  935.73/s  (1.092s,  937.93/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [ 500/1251 ( 40%)]  Loss:  3.945889 (3.7345)  Time: 1.082s,  946.57/s  (1.092s,  938.05/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 550/1251 ( 44%)]  Loss:  3.666983 (3.7288)  Time: 1.095s,  934.88/s  (1.092s,  937.93/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 600/1251 ( 48%)]  Loss:  3.751253 (3.7306)  Time: 1.108s,  924.43/s  (1.092s,  938.07/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 650/1251 ( 52%)]  Loss:  3.790853 (3.7349)  Time: 1.077s,  950.57/s  (1.092s,  937.95/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [ 700/1251 ( 56%)]  Loss:  3.827406 (3.7410)  Time: 1.079s,  949.36/s  (1.091s,  938.29/s)  LR: 9.229e-04  Data: 0.014 (0.013)
Train: 54 [ 750/1251 ( 60%)]  Loss:  3.814499 (3.7456)  Time: 1.096s,  934.20/s  (1.091s,  938.37/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 800/1251 ( 64%)]  Loss:  3.799813 (3.7488)  Time: 1.093s,  936.49/s  (1.092s,  938.10/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [ 850/1251 ( 68%)]  Loss:  3.877394 (3.7560)  Time: 1.084s,  944.74/s  (1.092s,  937.89/s)  LR: 9.229e-04  Data: 0.014 (0.013)
Train: 54 [ 900/1251 ( 72%)]  Loss:  3.870635 (3.7620)  Time: 1.078s,  949.81/s  (1.092s,  937.81/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [ 950/1251 ( 76%)]  Loss:  3.741289 (3.7610)  Time: 1.083s,  945.88/s  (1.092s,  937.73/s)  LR: 9.229e-04  Data: 0.018 (0.013)
Train: 54 [1000/1251 ( 80%)]  Loss:  4.154421 (3.7797)  Time: 1.109s,  923.69/s  (1.092s,  937.59/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [1050/1251 ( 84%)]  Loss:  3.817478 (3.7814)  Time: 1.083s,  945.11/s  (1.092s,  937.71/s)  LR: 9.229e-04  Data: 0.015 (0.013)
Train: 54 [1100/1251 ( 88%)]  Loss:  3.318625 (3.7613)  Time: 1.103s,  928.51/s  (1.092s,  937.68/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [1150/1251 ( 92%)]  Loss:  3.708194 (3.7591)  Time: 1.078s,  950.06/s  (1.092s,  937.69/s)  LR: 9.229e-04  Data: 0.012 (0.013)
Train: 54 [1200/1251 ( 96%)]  Loss:  3.504551 (3.7489)  Time: 1.077s,  951.22/s  (1.092s,  937.69/s)  LR: 9.229e-04  Data: 0.013 (0.013)
Train: 54 [1250/1251 (100%)]  Loss:  3.705640 (3.7472)  Time: 1.059s,  966.93/s  (1.092s,  937.79/s)  LR: 9.229e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.818 (5.818)  Loss:  0.6305 (0.6305)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7740 (1.2187)  Acc@1: 83.6085 (72.3660)  Acc@5: 95.7547 (91.2700)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-46.pth.tar', 71.00600001953126)

Train: 55 [   0/1251 (  0%)]  Loss:  3.617284 (3.6173)  Time: 1.092s,  937.73/s  (1.092s,  937.73/s)  LR: 9.201e-04  Data: 0.030 (0.030)
Train: 55 [  50/1251 (  4%)]  Loss:  3.375211 (3.4962)  Time: 1.086s,  943.09/s  (1.092s,  938.04/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [ 100/1251 (  8%)]  Loss:  3.673416 (3.5553)  Time: 1.080s,  948.08/s  (1.093s,  936.71/s)  LR: 9.201e-04  Data: 0.017 (0.014)
Train: 55 [ 150/1251 ( 12%)]  Loss:  3.959605 (3.6564)  Time: 1.103s,  928.36/s  (1.092s,  937.94/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [ 200/1251 ( 16%)]  Loss:  3.851975 (3.6955)  Time: 1.096s,  934.33/s  (1.093s,  936.80/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 250/1251 ( 20%)]  Loss:  3.847927 (3.7209)  Time: 1.079s,  948.88/s  (1.092s,  937.32/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [ 300/1251 ( 24%)]  Loss:  3.656775 (3.7117)  Time: 1.096s,  933.88/s  (1.092s,  937.99/s)  LR: 9.201e-04  Data: 0.015 (0.013)
Train: 55 [ 350/1251 ( 28%)]  Loss:  3.693087 (3.7094)  Time: 1.073s,  954.31/s  (1.093s,  936.95/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 400/1251 ( 32%)]  Loss:  4.029716 (3.7450)  Time: 1.171s,  874.45/s  (1.093s,  937.16/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 450/1251 ( 36%)]  Loss:  4.002625 (3.7708)  Time: 1.083s,  945.41/s  (1.092s,  937.39/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [ 500/1251 ( 40%)]  Loss:  3.768567 (3.7706)  Time: 1.083s,  945.17/s  (1.092s,  937.35/s)  LR: 9.201e-04  Data: 0.014 (0.013)
Train: 55 [ 550/1251 ( 44%)]  Loss:  3.619679 (3.7580)  Time: 1.081s,  947.50/s  (1.093s,  937.05/s)  LR: 9.201e-04  Data: 0.014 (0.013)
Train: 55 [ 600/1251 ( 48%)]  Loss:  3.436025 (3.7332)  Time: 1.073s,  953.92/s  (1.093s,  936.95/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 650/1251 ( 52%)]  Loss:  3.981254 (3.7509)  Time: 1.081s,  947.45/s  (1.093s,  936.99/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 700/1251 ( 56%)]  Loss:  3.388014 (3.7267)  Time: 1.073s,  954.38/s  (1.093s,  937.05/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [ 750/1251 ( 60%)]  Loss:  3.923957 (3.7391)  Time: 1.101s,  930.04/s  (1.093s,  936.89/s)  LR: 9.201e-04  Data: 0.016 (0.013)
Train: 55 [ 800/1251 ( 64%)]  Loss:  3.921245 (3.7498)  Time: 1.084s,  944.62/s  (1.093s,  937.17/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 850/1251 ( 68%)]  Loss:  3.604543 (3.7417)  Time: 1.076s,  951.41/s  (1.093s,  937.28/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [ 900/1251 ( 72%)]  Loss:  3.792461 (3.7444)  Time: 1.082s,  946.81/s  (1.093s,  937.10/s)  LR: 9.201e-04  Data: 0.014 (0.013)
Train: 55 [ 950/1251 ( 76%)]  Loss:  3.594271 (3.7369)  Time: 1.087s,  941.96/s  (1.093s,  937.26/s)  LR: 9.201e-04  Data: 0.011 (0.013)
Train: 55 [1000/1251 ( 80%)]  Loss:  3.731235 (3.7366)  Time: 1.097s,  933.54/s  (1.093s,  937.22/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Train: 55 [1050/1251 ( 84%)]  Loss:  3.688056 (3.7344)  Time: 1.081s,  947.16/s  (1.092s,  937.32/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [1100/1251 ( 88%)]  Loss:  3.786914 (3.7367)  Time: 1.080s,  947.94/s  (1.092s,  937.31/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [1150/1251 ( 92%)]  Loss:  3.950050 (3.7456)  Time: 1.107s,  925.12/s  (1.092s,  937.38/s)  LR: 9.201e-04  Data: 0.013 (0.013)
Train: 55 [1200/1251 ( 96%)]  Loss:  3.761569 (3.7462)  Time: 1.091s,  938.52/s  (1.093s,  937.14/s)  LR: 9.201e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 55 [1250/1251 (100%)]  Loss:  3.706367 (3.7447)  Time: 1.077s,  950.41/s  (1.093s,  937.09/s)  LR: 9.201e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.913 (5.913)  Loss:  0.6733 (0.6733)  Acc@1: 86.5234 (86.5234)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7463 (1.2338)  Acc@1: 83.7264 (72.2060)  Acc@5: 96.2264 (91.4280)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-45.pth.tar', 71.15599998779297)

Train: 56 [   0/1251 (  0%)]  Loss:  4.078542 (4.0785)  Time: 1.108s,  923.96/s  (1.108s,  923.96/s)  LR: 9.173e-04  Data: 0.024 (0.024)
Train: 56 [  50/1251 (  4%)]  Loss:  4.069326 (4.0739)  Time: 1.078s,  950.06/s  (1.089s,  939.91/s)  LR: 9.173e-04  Data: 0.013 (0.014)
Train: 56 [ 100/1251 (  8%)]  Loss:  4.168428 (4.1054)  Time: 1.079s,  948.93/s  (1.089s,  939.93/s)  LR: 9.173e-04  Data: 0.015 (0.014)
Train: 56 [ 150/1251 ( 12%)]  Loss:  4.044585 (4.0902)  Time: 1.081s,  947.50/s  (1.092s,  938.06/s)  LR: 9.173e-04  Data: 0.014 (0.013)
Train: 56 [ 200/1251 ( 16%)]  Loss:  3.735668 (4.0193)  Time: 1.092s,  937.59/s  (1.091s,  938.81/s)  LR: 9.173e-04  Data: 0.011 (0.013)
Train: 56 [ 250/1251 ( 20%)]  Loss:  3.868891 (3.9942)  Time: 1.079s,  948.96/s  (1.091s,  938.29/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 300/1251 ( 24%)]  Loss:  3.552093 (3.9311)  Time: 1.100s,  930.66/s  (1.092s,  937.80/s)  LR: 9.173e-04  Data: 0.013 (0.013)
Train: 56 [ 350/1251 ( 28%)]  Loss:  3.955451 (3.9341)  Time: 1.098s,  932.36/s  (1.091s,  938.37/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 400/1251 ( 32%)]  Loss:  3.817897 (3.9212)  Time: 1.076s,  951.40/s  (1.092s,  938.14/s)  LR: 9.173e-04  Data: 0.013 (0.013)
Train: 56 [ 450/1251 ( 36%)]  Loss:  3.501230 (3.8792)  Time: 1.081s,  946.94/s  (1.091s,  938.17/s)  LR: 9.173e-04  Data: 0.016 (0.013)
Train: 56 [ 500/1251 ( 40%)]  Loss:  3.745481 (3.8671)  Time: 1.099s,  932.11/s  (1.092s,  938.15/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 550/1251 ( 44%)]  Loss:  3.868091 (3.8671)  Time: 1.080s,  948.06/s  (1.092s,  938.08/s)  LR: 9.173e-04  Data: 0.017 (0.013)
Train: 56 [ 600/1251 ( 48%)]  Loss:  3.980432 (3.8759)  Time: 1.100s,  930.64/s  (1.092s,  937.97/s)  LR: 9.173e-04  Data: 0.015 (0.013)
Train: 56 [ 650/1251 ( 52%)]  Loss:  3.769962 (3.8683)  Time: 1.174s,  872.46/s  (1.092s,  938.06/s)  LR: 9.173e-04  Data: 0.015 (0.013)
Train: 56 [ 700/1251 ( 56%)]  Loss:  4.179062 (3.8890)  Time: 1.080s,  948.12/s  (1.092s,  938.10/s)  LR: 9.173e-04  Data: 0.013 (0.013)
Train: 56 [ 750/1251 ( 60%)]  Loss:  3.942452 (3.8923)  Time: 1.081s,  947.19/s  (1.091s,  938.36/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 800/1251 ( 64%)]  Loss:  4.050436 (3.9016)  Time: 1.096s,  934.33/s  (1.092s,  938.06/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 850/1251 ( 68%)]  Loss:  3.960814 (3.9049)  Time: 1.081s,  947.25/s  (1.091s,  938.17/s)  LR: 9.173e-04  Data: 0.013 (0.013)
Train: 56 [ 900/1251 ( 72%)]  Loss:  3.674526 (3.8928)  Time: 1.079s,  948.99/s  (1.092s,  938.09/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [ 950/1251 ( 76%)]  Loss:  3.437422 (3.8700)  Time: 1.083s,  945.57/s  (1.092s,  938.02/s)  LR: 9.173e-04  Data: 0.014 (0.013)
Train: 56 [1000/1251 ( 80%)]  Loss:  3.910633 (3.8720)  Time: 1.099s,  931.78/s  (1.092s,  937.94/s)  LR: 9.173e-04  Data: 0.015 (0.013)
Train: 56 [1050/1251 ( 84%)]  Loss:  3.739871 (3.8660)  Time: 1.081s,  947.39/s  (1.092s,  938.00/s)  LR: 9.173e-04  Data: 0.015 (0.013)
Train: 56 [1100/1251 ( 88%)]  Loss:  3.701638 (3.8588)  Time: 1.094s,  935.83/s  (1.092s,  937.86/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [1150/1251 ( 92%)]  Loss:  3.455009 (3.8420)  Time: 1.077s,  950.61/s  (1.092s,  937.89/s)  LR: 9.173e-04  Data: 0.013 (0.013)
Train: 56 [1200/1251 ( 96%)]  Loss:  3.594650 (3.8321)  Time: 1.082s,  946.63/s  (1.092s,  937.92/s)  LR: 9.173e-04  Data: 0.012 (0.013)
Train: 56 [1250/1251 (100%)]  Loss:  3.924557 (3.8357)  Time: 1.079s,  948.68/s  (1.092s,  937.86/s)  LR: 9.173e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.869 (5.869)  Loss:  0.6249 (0.6249)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7540 (1.2134)  Acc@1: 83.6085 (72.2800)  Acc@5: 95.7547 (91.4500)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-44.pth.tar', 71.21199986083984)

Train: 57 [   0/1251 (  0%)]  Loss:  4.057991 (4.0580)  Time: 1.084s,  944.54/s  (1.084s,  944.54/s)  LR: 9.144e-04  Data: 0.022 (0.022)
Train: 57 [  50/1251 (  4%)]  Loss:  3.605124 (3.8316)  Time: 1.082s,  946.82/s  (1.094s,  935.97/s)  LR: 9.144e-04  Data: 0.012 (0.014)
Train: 57 [ 100/1251 (  8%)]  Loss:  3.570108 (3.7444)  Time: 1.078s,  950.22/s  (1.092s,  937.39/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [ 150/1251 ( 12%)]  Loss:  3.552421 (3.6964)  Time: 1.104s,  927.81/s  (1.091s,  938.40/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [ 200/1251 ( 16%)]  Loss:  3.690768 (3.6953)  Time: 1.079s,  948.93/s  (1.092s,  938.08/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [ 250/1251 ( 20%)]  Loss:  3.799435 (3.7126)  Time: 1.099s,  931.36/s  (1.091s,  938.84/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [ 300/1251 ( 24%)]  Loss:  3.636484 (3.7018)  Time: 1.078s,  950.21/s  (1.090s,  939.59/s)  LR: 9.144e-04  Data: 0.014 (0.013)
Train: 57 [ 350/1251 ( 28%)]  Loss:  3.889417 (3.7252)  Time: 1.078s,  949.65/s  (1.090s,  939.20/s)  LR: 9.144e-04  Data: 0.014 (0.013)
Train: 57 [ 400/1251 ( 32%)]  Loss:  3.767917 (3.7300)  Time: 1.080s,  948.03/s  (1.090s,  939.75/s)  LR: 9.144e-04  Data: 0.016 (0.013)
Train: 57 [ 450/1251 ( 36%)]  Loss:  4.080081 (3.7650)  Time: 1.077s,  950.95/s  (1.090s,  939.67/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [ 500/1251 ( 40%)]  Loss:  3.945111 (3.7814)  Time: 1.080s,  947.88/s  (1.090s,  939.62/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [ 550/1251 ( 44%)]  Loss:  3.757827 (3.7794)  Time: 1.095s,  934.94/s  (1.090s,  939.57/s)  LR: 9.144e-04  Data: 0.012 (0.013)
Train: 57 [ 600/1251 ( 48%)]  Loss:  3.954858 (3.7929)  Time: 1.204s,  850.62/s  (1.090s,  939.23/s)  LR: 9.144e-04  Data: 0.017 (0.013)
Train: 57 [ 650/1251 ( 52%)]  Loss:  3.621998 (3.7807)  Time: 1.079s,  949.13/s  (1.090s,  939.67/s)  LR: 9.144e-04  Data: 0.016 (0.013)
Train: 57 [ 700/1251 ( 56%)]  Loss:  3.593233 (3.7682)  Time: 1.261s,  812.29/s  (1.090s,  939.81/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Train: 57 [ 750/1251 ( 60%)]  Loss:  3.584018 (3.7567)  Time: 1.080s,  948.14/s  (1.089s,  940.10/s)  LR: 9.144e-04  Data: 0.015 (0.013)
Train: 57 [ 800/1251 ( 64%)]  Loss:  3.920578 (3.7663)  Time: 1.081s,  947.42/s  (1.089s,  940.38/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [ 850/1251 ( 68%)]  Loss:  3.790341 (3.7677)  Time: 1.087s,  942.37/s  (1.089s,  940.31/s)  LR: 9.144e-04  Data: 0.012 (0.013)
Train: 57 [ 900/1251 ( 72%)]  Loss:  3.862226 (3.7726)  Time: 1.110s,  922.78/s  (1.089s,  940.42/s)  LR: 9.144e-04  Data: 0.012 (0.013)
Train: 57 [ 950/1251 ( 76%)]  Loss:  3.579249 (3.7630)  Time: 1.077s,  951.13/s  (1.089s,  939.96/s)  LR: 9.144e-04  Data: 0.011 (0.013)
Train: 57 [1000/1251 ( 80%)]  Loss:  3.870818 (3.7681)  Time: 1.079s,  949.44/s  (1.089s,  940.03/s)  LR: 9.144e-04  Data: 0.014 (0.013)
Train: 57 [1050/1251 ( 84%)]  Loss:  3.995349 (3.7784)  Time: 1.080s,  948.48/s  (1.090s,  939.86/s)  LR: 9.144e-04  Data: 0.017 (0.013)
Train: 57 [1100/1251 ( 88%)]  Loss:  3.772473 (3.7782)  Time: 1.095s,  935.06/s  (1.089s,  939.96/s)  LR: 9.144e-04  Data: 0.015 (0.013)
Train: 57 [1150/1251 ( 92%)]  Loss:  3.581027 (3.7700)  Time: 1.092s,  937.83/s  (1.090s,  939.79/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [1200/1251 ( 96%)]  Loss:  3.661916 (3.7656)  Time: 1.095s,  935.22/s  (1.090s,  939.71/s)  LR: 9.144e-04  Data: 0.013 (0.013)
Train: 57 [1250/1251 (100%)]  Loss:  4.065130 (3.7771)  Time: 1.063s,  963.62/s  (1.090s,  939.73/s)  LR: 9.144e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.864 (5.864)  Loss:  0.6569 (0.6569)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.7291 (1.2295)  Acc@1: 84.3160 (72.0680)  Acc@5: 95.7547 (91.3760)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 72.0680000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-49.pth.tar', 71.39600001464844)

Train: 58 [   0/1251 (  0%)]  Loss:  4.041550 (4.0415)  Time: 1.095s,  935.38/s  (1.095s,  935.38/s)  LR: 9.115e-04  Data: 0.030 (0.030)
Train: 58 [  50/1251 (  4%)]  Loss:  4.029242 (4.0354)  Time: 1.079s,  948.67/s  (1.091s,  938.55/s)  LR: 9.115e-04  Data: 0.013 (0.014)
Train: 58 [ 100/1251 (  8%)]  Loss:  3.925634 (3.9988)  Time: 1.100s,  930.63/s  (1.093s,  937.26/s)  LR: 9.115e-04  Data: 0.014 (0.014)
Train: 58 [ 150/1251 ( 12%)]  Loss:  3.865710 (3.9655)  Time: 1.103s,  928.35/s  (1.090s,  939.19/s)  LR: 9.115e-04  Data: 0.013 (0.014)
Train: 58 [ 200/1251 ( 16%)]  Loss:  3.768015 (3.9260)  Time: 1.093s,  937.01/s  (1.090s,  939.34/s)  LR: 9.115e-04  Data: 0.012 (0.014)
Train: 58 [ 250/1251 ( 20%)]  Loss:  3.777672 (3.9013)  Time: 1.096s,  934.35/s  (1.090s,  939.05/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 300/1251 ( 24%)]  Loss:  3.513449 (3.8459)  Time: 1.158s,  884.14/s  (1.090s,  939.46/s)  LR: 9.115e-04  Data: 0.016 (0.013)
Train: 58 [ 350/1251 ( 28%)]  Loss:  3.776793 (3.8373)  Time: 1.075s,  952.57/s  (1.091s,  938.69/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 400/1251 ( 32%)]  Loss:  3.865003 (3.8403)  Time: 1.091s,  938.38/s  (1.091s,  938.70/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 450/1251 ( 36%)]  Loss:  3.813650 (3.8377)  Time: 1.099s,  931.82/s  (1.091s,  938.64/s)  LR: 9.115e-04  Data: 0.015 (0.013)
Train: 58 [ 500/1251 ( 40%)]  Loss:  3.806727 (3.8349)  Time: 1.079s,  949.42/s  (1.091s,  938.77/s)  LR: 9.115e-04  Data: 0.013 (0.013)
Train: 58 [ 550/1251 ( 44%)]  Loss:  3.963929 (3.8456)  Time: 1.171s,  874.78/s  (1.091s,  938.49/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 600/1251 ( 48%)]  Loss:  3.623303 (3.8285)  Time: 1.095s,  935.32/s  (1.091s,  938.53/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 650/1251 ( 52%)]  Loss:  3.631084 (3.8144)  Time: 1.094s,  935.67/s  (1.091s,  938.65/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 700/1251 ( 56%)]  Loss:  3.478440 (3.7920)  Time: 1.078s,  950.05/s  (1.091s,  938.51/s)  LR: 9.115e-04  Data: 0.016 (0.013)
Train: 58 [ 750/1251 ( 60%)]  Loss:  3.694797 (3.7859)  Time: 1.080s,  947.78/s  (1.091s,  938.83/s)  LR: 9.115e-04  Data: 0.016 (0.013)
Train: 58 [ 800/1251 ( 64%)]  Loss:  3.434634 (3.7653)  Time: 1.083s,  945.64/s  (1.090s,  939.14/s)  LR: 9.115e-04  Data: 0.014 (0.013)
Train: 58 [ 850/1251 ( 68%)]  Loss:  4.200972 (3.7895)  Time: 1.094s,  935.72/s  (1.090s,  939.04/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 900/1251 ( 72%)]  Loss:  4.062662 (3.8039)  Time: 1.082s,  946.57/s  (1.090s,  939.19/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [ 950/1251 ( 76%)]  Loss:  3.788878 (3.8031)  Time: 1.094s,  935.89/s  (1.091s,  938.74/s)  LR: 9.115e-04  Data: 0.012 (0.013)
Train: 58 [1000/1251 ( 80%)]  Loss:  3.607235 (3.7938)  Time: 1.100s,  931.02/s  (1.091s,  938.67/s)  LR: 9.115e-04  Data: 0.016 (0.013)
Train: 58 [1050/1251 ( 84%)]  Loss:  3.893430 (3.7983)  Time: 1.077s,  950.36/s  (1.091s,  938.67/s)  LR: 9.115e-04  Data: 0.013 (0.013)
Train: 58 [1100/1251 ( 88%)]  Loss:  3.895238 (3.8025)  Time: 1.111s,  921.46/s  (1.091s,  938.77/s)  LR: 9.115e-04  Data: 0.013 (0.013)
Train: 58 [1150/1251 ( 92%)]  Loss:  3.728489 (3.7994)  Time: 1.095s,  935.15/s  (1.091s,  938.68/s)  LR: 9.115e-04  Data: 0.013 (0.013)
Train: 58 [1200/1251 ( 96%)]  Loss:  3.741586 (3.7971)  Time: 1.078s,  949.81/s  (1.091s,  938.59/s)  LR: 9.115e-04  Data: 0.013 (0.013)
Train: 58 [1250/1251 (100%)]  Loss:  3.623558 (3.7904)  Time: 1.061s,  964.95/s  (1.091s,  938.59/s)  LR: 9.115e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.771 (5.771)  Loss:  0.6186 (0.6186)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7604 (1.2440)  Acc@1: 83.7264 (72.2560)  Acc@5: 95.8726 (91.3720)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 72.0680000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-48.pth.tar', 71.59200001464843)

Train: 59 [   0/1251 (  0%)]  Loss:  3.599450 (3.5995)  Time: 1.087s,  942.13/s  (1.087s,  942.13/s)  LR: 9.085e-04  Data: 0.023 (0.023)
Train: 59 [  50/1251 (  4%)]  Loss:  3.863501 (3.7315)  Time: 1.077s,  950.46/s  (1.086s,  943.28/s)  LR: 9.085e-04  Data: 0.013 (0.014)
Train: 59 [ 100/1251 (  8%)]  Loss:  4.003055 (3.8220)  Time: 1.079s,  948.88/s  (1.090s,  939.51/s)  LR: 9.085e-04  Data: 0.015 (0.013)
Train: 59 [ 150/1251 ( 12%)]  Loss:  3.683654 (3.7874)  Time: 1.095s,  935.39/s  (1.091s,  938.73/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 200/1251 ( 16%)]  Loss:  3.837699 (3.7975)  Time: 1.080s,  948.19/s  (1.090s,  939.70/s)  LR: 9.085e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 59 [ 250/1251 ( 20%)]  Loss:  3.511242 (3.7498)  Time: 1.094s,  935.65/s  (1.090s,  939.84/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 300/1251 ( 24%)]  Loss:  4.006957 (3.7865)  Time: 1.095s,  935.12/s  (1.089s,  940.24/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 350/1251 ( 28%)]  Loss:  3.849789 (3.7944)  Time: 1.077s,  951.20/s  (1.089s,  940.11/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 59 [ 400/1251 ( 32%)]  Loss:  3.804249 (3.7955)  Time: 1.104s,  927.29/s  (1.090s,  939.87/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 59 [ 450/1251 ( 36%)]  Loss:  3.750282 (3.7910)  Time: 1.174s,  872.36/s  (1.090s,  939.74/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 59 [ 500/1251 ( 40%)]  Loss:  3.992925 (3.8093)  Time: 1.080s,  947.71/s  (1.089s,  940.19/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 550/1251 ( 44%)]  Loss:  3.687988 (3.7992)  Time: 1.104s,  927.45/s  (1.089s,  940.16/s)  LR: 9.085e-04  Data: 0.019 (0.013)
Train: 59 [ 600/1251 ( 48%)]  Loss:  3.737067 (3.7945)  Time: 1.081s,  947.66/s  (1.089s,  940.17/s)  LR: 9.085e-04  Data: 0.016 (0.013)
Train: 59 [ 650/1251 ( 52%)]  Loss:  3.529727 (3.7755)  Time: 1.096s,  934.68/s  (1.090s,  939.88/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 700/1251 ( 56%)]  Loss:  3.708943 (3.7711)  Time: 1.078s,  949.97/s  (1.089s,  939.91/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 750/1251 ( 60%)]  Loss:  4.062855 (3.7893)  Time: 1.080s,  947.98/s  (1.089s,  940.18/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 59 [ 800/1251 ( 64%)]  Loss:  3.753479 (3.7872)  Time: 1.093s,  936.46/s  (1.089s,  939.92/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [ 850/1251 ( 68%)]  Loss:  3.794574 (3.7876)  Time: 1.078s,  949.71/s  (1.090s,  939.84/s)  LR: 9.085e-04  Data: 0.014 (0.013)
Train: 59 [ 900/1251 ( 72%)]  Loss:  3.846359 (3.7907)  Time: 1.103s,  928.07/s  (1.090s,  939.86/s)  LR: 9.085e-04  Data: 0.011 (0.013)
Train: 59 [ 950/1251 ( 76%)]  Loss:  3.686109 (3.7855)  Time: 1.095s,  934.89/s  (1.090s,  939.75/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [1000/1251 ( 80%)]  Loss:  3.916390 (3.7917)  Time: 1.097s,  933.57/s  (1.090s,  939.54/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 59 [1050/1251 ( 84%)]  Loss:  3.927178 (3.7979)  Time: 1.084s,  944.70/s  (1.090s,  939.50/s)  LR: 9.085e-04  Data: 0.016 (0.013)
Train: 59 [1100/1251 ( 88%)]  Loss:  3.824722 (3.7991)  Time: 1.074s,  953.21/s  (1.090s,  939.51/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [1150/1251 ( 92%)]  Loss:  3.713498 (3.7955)  Time: 1.077s,  951.16/s  (1.090s,  939.53/s)  LR: 9.085e-04  Data: 0.013 (0.013)
Train: 59 [1200/1251 ( 96%)]  Loss:  4.053806 (3.8058)  Time: 1.099s,  931.73/s  (1.090s,  939.50/s)  LR: 9.085e-04  Data: 0.012 (0.013)
Train: 59 [1250/1251 (100%)]  Loss:  3.456560 (3.7924)  Time: 1.079s,  949.40/s  (1.090s,  939.57/s)  LR: 9.085e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.835 (5.835)  Loss:  0.6453 (0.6453)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7759 (1.2180)  Acc@1: 83.7264 (72.7820)  Acc@5: 95.8726 (91.6380)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 72.0680000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-52.pth.tar', 71.71200006835937)

Train: 60 [   0/1251 (  0%)]  Loss:  3.896313 (3.8963)  Time: 1.085s,  943.69/s  (1.085s,  943.69/s)  LR: 9.055e-04  Data: 0.023 (0.023)
Train: 60 [  50/1251 (  4%)]  Loss:  3.815497 (3.8559)  Time: 1.091s,  938.92/s  (1.091s,  938.55/s)  LR: 9.055e-04  Data: 0.015 (0.014)
Train: 60 [ 100/1251 (  8%)]  Loss:  3.660288 (3.7907)  Time: 1.098s,  932.51/s  (1.092s,  938.07/s)  LR: 9.055e-04  Data: 0.015 (0.013)
Train: 60 [ 150/1251 ( 12%)]  Loss:  3.842571 (3.8037)  Time: 1.079s,  948.72/s  (1.090s,  939.29/s)  LR: 9.055e-04  Data: 0.015 (0.013)
Train: 60 [ 200/1251 ( 16%)]  Loss:  3.729275 (3.7888)  Time: 1.107s,  924.72/s  (1.091s,  938.35/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 250/1251 ( 20%)]  Loss:  3.648203 (3.7654)  Time: 1.088s,  940.94/s  (1.091s,  938.59/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [ 300/1251 ( 24%)]  Loss:  3.828980 (3.7744)  Time: 1.078s,  950.11/s  (1.090s,  939.42/s)  LR: 9.055e-04  Data: 0.014 (0.013)
Train: 60 [ 350/1251 ( 28%)]  Loss:  3.802716 (3.7780)  Time: 1.095s,  935.36/s  (1.090s,  939.17/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 400/1251 ( 32%)]  Loss:  3.799760 (3.7804)  Time: 1.085s,  944.16/s  (1.090s,  939.44/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [ 450/1251 ( 36%)]  Loss:  3.958000 (3.7982)  Time: 1.095s,  934.81/s  (1.090s,  939.78/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 500/1251 ( 40%)]  Loss:  3.953526 (3.8123)  Time: 1.078s,  950.20/s  (1.090s,  939.55/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [ 550/1251 ( 44%)]  Loss:  3.682163 (3.8014)  Time: 1.083s,  945.48/s  (1.090s,  939.75/s)  LR: 9.055e-04  Data: 0.014 (0.013)
Train: 60 [ 600/1251 ( 48%)]  Loss:  3.752431 (3.7977)  Time: 1.096s,  934.05/s  (1.089s,  939.90/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 650/1251 ( 52%)]  Loss:  3.636111 (3.7861)  Time: 1.095s,  934.90/s  (1.090s,  939.76/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 700/1251 ( 56%)]  Loss:  3.688015 (3.7796)  Time: 1.082s,  946.12/s  (1.090s,  939.81/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [ 750/1251 ( 60%)]  Loss:  3.613727 (3.7692)  Time: 1.082s,  946.24/s  (1.090s,  939.72/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [ 800/1251 ( 64%)]  Loss:  3.655601 (3.7625)  Time: 1.095s,  935.11/s  (1.090s,  939.59/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [ 850/1251 ( 68%)]  Loss:  3.490413 (3.7474)  Time: 1.103s,  928.40/s  (1.090s,  939.20/s)  LR: 9.055e-04  Data: 0.015 (0.013)
Train: 60 [ 900/1251 ( 72%)]  Loss:  3.832751 (3.7519)  Time: 1.093s,  936.87/s  (1.090s,  939.23/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [ 950/1251 ( 76%)]  Loss:  3.426986 (3.7357)  Time: 1.078s,  949.77/s  (1.090s,  939.17/s)  LR: 9.055e-04  Data: 0.016 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 60 [1000/1251 ( 80%)]  Loss:  3.977972 (3.7472)  Time: 1.080s,  948.57/s  (1.091s,  938.99/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [1050/1251 ( 84%)]  Loss:  3.682371 (3.7443)  Time: 1.094s,  935.73/s  (1.090s,  939.17/s)  LR: 9.055e-04  Data: 0.016 (0.013)
Train: 60 [1100/1251 ( 88%)]  Loss:  3.825145 (3.7478)  Time: 1.093s,  936.58/s  (1.090s,  939.14/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [1150/1251 ( 92%)]  Loss:  4.171546 (3.7654)  Time: 1.096s,  934.60/s  (1.090s,  939.35/s)  LR: 9.055e-04  Data: 0.012 (0.013)
Train: 60 [1200/1251 ( 96%)]  Loss:  3.494191 (3.7546)  Time: 1.078s,  950.04/s  (1.090s,  939.34/s)  LR: 9.055e-04  Data: 0.013 (0.013)
Train: 60 [1250/1251 (100%)]  Loss:  3.571520 (3.7475)  Time: 1.080s,  948.50/s  (1.090s,  939.30/s)  LR: 9.055e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.840 (5.840)  Loss:  0.6364 (0.6364)  Acc@1: 86.7188 (86.7188)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7637 (1.2134)  Acc@1: 84.0802 (72.6680)  Acc@5: 95.9906 (91.6260)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 72.0680000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-51.pth.tar', 71.7820000415039)

Train: 61 [   0/1251 (  0%)]  Loss:  3.393355 (3.3934)  Time: 1.099s,  932.08/s  (1.099s,  932.08/s)  LR: 9.024e-04  Data: 0.023 (0.023)
Train: 61 [  50/1251 (  4%)]  Loss:  3.920582 (3.6570)  Time: 1.098s,  932.73/s  (1.084s,  944.59/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 100/1251 (  8%)]  Loss:  3.508864 (3.6076)  Time: 1.122s,  912.33/s  (1.088s,  941.56/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 150/1251 ( 12%)]  Loss:  3.565595 (3.5971)  Time: 1.096s,  934.48/s  (1.088s,  941.06/s)  LR: 9.024e-04  Data: 0.012 (0.013)
Train: 61 [ 200/1251 ( 16%)]  Loss:  3.833559 (3.6444)  Time: 1.077s,  951.09/s  (1.088s,  941.26/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 250/1251 ( 20%)]  Loss:  3.484896 (3.6178)  Time: 1.096s,  934.70/s  (1.087s,  941.87/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 300/1251 ( 24%)]  Loss:  3.789680 (3.6424)  Time: 1.081s,  947.49/s  (1.088s,  940.95/s)  LR: 9.024e-04  Data: 0.016 (0.014)
Train: 61 [ 350/1251 ( 28%)]  Loss:  3.744684 (3.6552)  Time: 1.096s,  934.51/s  (1.088s,  940.92/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 400/1251 ( 32%)]  Loss:  3.869203 (3.6789)  Time: 1.104s,  927.61/s  (1.089s,  940.66/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 450/1251 ( 36%)]  Loss:  3.829072 (3.6939)  Time: 1.080s,  948.01/s  (1.090s,  939.63/s)  LR: 9.024e-04  Data: 0.014 (0.014)
Train: 61 [ 500/1251 ( 40%)]  Loss:  3.711038 (3.6955)  Time: 1.104s,  927.63/s  (1.090s,  939.67/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 550/1251 ( 44%)]  Loss:  3.824115 (3.7062)  Time: 1.080s,  948.46/s  (1.090s,  939.16/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 600/1251 ( 48%)]  Loss:  3.677379 (3.7040)  Time: 1.095s,  935.03/s  (1.090s,  939.31/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 650/1251 ( 52%)]  Loss:  4.006577 (3.7256)  Time: 1.079s,  949.20/s  (1.090s,  939.43/s)  LR: 9.024e-04  Data: 0.015 (0.014)
Train: 61 [ 700/1251 ( 56%)]  Loss:  3.869145 (3.7352)  Time: 1.097s,  933.05/s  (1.090s,  939.42/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 750/1251 ( 60%)]  Loss:  3.508099 (3.7210)  Time: 1.078s,  950.33/s  (1.090s,  939.29/s)  LR: 9.024e-04  Data: 0.013 (0.014)
Train: 61 [ 800/1251 ( 64%)]  Loss:  3.681077 (3.7186)  Time: 1.082s,  946.11/s  (1.090s,  939.21/s)  LR: 9.024e-04  Data: 0.012 (0.014)
Train: 61 [ 850/1251 ( 68%)]  Loss:  3.717195 (3.7186)  Time: 1.080s,  948.45/s  (1.090s,  939.10/s)  LR: 9.024e-04  Data: 0.013 (0.013)
Train: 61 [ 900/1251 ( 72%)]  Loss:  3.574853 (3.7110)  Time: 1.104s,  927.77/s  (1.090s,  939.16/s)  LR: 9.024e-04  Data: 0.012 (0.013)
Train: 61 [ 950/1251 ( 76%)]  Loss:  3.967049 (3.7238)  Time: 1.084s,  944.60/s  (1.090s,  939.11/s)  LR: 9.024e-04  Data: 0.012 (0.013)
Train: 61 [1000/1251 ( 80%)]  Loss:  3.630526 (3.7194)  Time: 1.170s,  874.97/s  (1.090s,  939.36/s)  LR: 9.024e-04  Data: 0.016 (0.013)
Train: 61 [1050/1251 ( 84%)]  Loss:  3.945342 (3.7296)  Time: 1.096s,  934.13/s  (1.091s,  938.83/s)  LR: 9.024e-04  Data: 0.016 (0.013)
Train: 61 [1100/1251 ( 88%)]  Loss:  3.843746 (3.7346)  Time: 1.105s,  926.90/s  (1.091s,  938.59/s)  LR: 9.024e-04  Data: 0.013 (0.013)
Train: 61 [1150/1251 ( 92%)]  Loss:  3.848921 (3.7394)  Time: 1.103s,  928.00/s  (1.091s,  938.35/s)  LR: 9.024e-04  Data: 0.012 (0.013)
Train: 61 [1200/1251 ( 96%)]  Loss:  3.955415 (3.7480)  Time: 1.077s,  951.17/s  (1.091s,  938.30/s)  LR: 9.024e-04  Data: 0.013 (0.013)
Train: 61 [1250/1251 (100%)]  Loss:  3.846060 (3.7518)  Time: 1.063s,  963.64/s  (1.092s,  937.99/s)  LR: 9.024e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.899 (5.899)  Loss:  0.6212 (0.6212)  Acc@1: 87.1094 (87.1094)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.6796 (1.1881)  Acc@1: 84.9057 (72.6160)  Acc@5: 95.9906 (91.6600)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 72.0680000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-50.pth.tar', 71.94599999023437)

Train: 62 [   0/1251 (  0%)]  Loss:  3.502721 (3.5027)  Time: 1.088s,  940.79/s  (1.088s,  940.79/s)  LR: 8.993e-04  Data: 0.025 (0.025)
Train: 62 [  50/1251 (  4%)]  Loss:  4.057554 (3.7801)  Time: 1.099s,  932.14/s  (1.090s,  939.50/s)  LR: 8.993e-04  Data: 0.012 (0.014)
Train: 62 [ 100/1251 (  8%)]  Loss:  3.592957 (3.7177)  Time: 1.079s,  949.04/s  (1.089s,  940.12/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [ 150/1251 ( 12%)]  Loss:  3.860937 (3.7535)  Time: 1.077s,  950.95/s  (1.090s,  939.08/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [ 200/1251 ( 16%)]  Loss:  3.662670 (3.7354)  Time: 1.083s,  945.10/s  (1.090s,  939.87/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [ 250/1251 ( 20%)]  Loss:  3.727532 (3.7341)  Time: 1.095s,  935.27/s  (1.088s,  940.85/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [ 300/1251 ( 24%)]  Loss:  3.745049 (3.7356)  Time: 1.156s,  885.71/s  (1.089s,  940.42/s)  LR: 8.993e-04  Data: 0.017 (0.013)
Train: 62 [ 350/1251 ( 28%)]  Loss:  3.828251 (3.7472)  Time: 1.078s,  949.64/s  (1.089s,  940.27/s)  LR: 8.993e-04  Data: 0.015 (0.013)
Train: 62 [ 400/1251 ( 32%)]  Loss:  3.746784 (3.7472)  Time: 1.081s,  947.57/s  (1.089s,  940.43/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [ 450/1251 ( 36%)]  Loss:  3.921576 (3.7646)  Time: 1.080s,  947.88/s  (1.088s,  940.88/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 62 [ 500/1251 ( 40%)]  Loss:  4.063900 (3.7918)  Time: 1.097s,  933.34/s  (1.089s,  940.56/s)  LR: 8.993e-04  Data: 0.016 (0.013)
Train: 62 [ 550/1251 ( 44%)]  Loss:  3.361870 (3.7560)  Time: 1.084s,  944.32/s  (1.088s,  940.89/s)  LR: 8.993e-04  Data: 0.015 (0.013)
Train: 62 [ 600/1251 ( 48%)]  Loss:  4.002076 (3.7749)  Time: 1.102s,  928.89/s  (1.088s,  941.15/s)  LR: 8.993e-04  Data: 0.015 (0.013)
Train: 62 [ 650/1251 ( 52%)]  Loss:  3.427816 (3.7501)  Time: 1.170s,  875.53/s  (1.088s,  940.96/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [ 700/1251 ( 56%)]  Loss:  4.020661 (3.7682)  Time: 1.093s,  937.09/s  (1.088s,  940.99/s)  LR: 8.993e-04  Data: 0.015 (0.013)
Train: 62 [ 750/1251 ( 60%)]  Loss:  3.387935 (3.7444)  Time: 1.094s,  936.38/s  (1.088s,  941.02/s)  LR: 8.993e-04  Data: 0.014 (0.013)
Train: 62 [ 800/1251 ( 64%)]  Loss:  3.777546 (3.7463)  Time: 1.093s,  936.45/s  (1.088s,  940.96/s)  LR: 8.993e-04  Data: 0.011 (0.013)
Train: 62 [ 850/1251 ( 68%)]  Loss:  3.790986 (3.7488)  Time: 1.080s,  948.59/s  (1.088s,  940.91/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [ 900/1251 ( 72%)]  Loss:  3.697178 (3.7461)  Time: 1.078s,  949.74/s  (1.088s,  941.07/s)  LR: 8.993e-04  Data: 0.014 (0.013)
Train: 62 [ 950/1251 ( 76%)]  Loss:  3.590201 (3.7383)  Time: 1.079s,  948.91/s  (1.088s,  941.15/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [1000/1251 ( 80%)]  Loss:  3.769767 (3.7398)  Time: 1.096s,  934.26/s  (1.088s,  940.97/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [1050/1251 ( 84%)]  Loss:  4.018037 (3.7525)  Time: 1.080s,  947.73/s  (1.088s,  941.06/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [1100/1251 ( 88%)]  Loss:  3.850375 (3.7567)  Time: 1.095s,  935.49/s  (1.089s,  940.70/s)  LR: 8.993e-04  Data: 0.012 (0.013)
Train: 62 [1150/1251 ( 92%)]  Loss:  3.599124 (3.7501)  Time: 1.078s,  949.97/s  (1.089s,  940.58/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [1200/1251 ( 96%)]  Loss:  3.708639 (3.7485)  Time: 1.110s,  922.26/s  (1.089s,  940.28/s)  LR: 8.993e-04  Data: 0.013 (0.013)
Train: 62 [1250/1251 (100%)]  Loss:  4.031901 (3.7594)  Time: 1.062s,  963.97/s  (1.089s,  940.11/s)  LR: 8.993e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.849 (5.849)  Loss:  0.6152 (0.6152)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6804 (1.1943)  Acc@1: 84.5519 (72.7000)  Acc@5: 96.2264 (91.6000)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-57.pth.tar', 72.0680000390625)

Train: 63 [   0/1251 (  0%)]  Loss:  3.922860 (3.9229)  Time: 1.087s,  942.27/s  (1.087s,  942.27/s)  LR: 8.961e-04  Data: 0.023 (0.023)
Train: 63 [  50/1251 (  4%)]  Loss:  3.669918 (3.7964)  Time: 1.080s,  948.01/s  (1.092s,  937.65/s)  LR: 8.961e-04  Data: 0.017 (0.014)
Train: 63 [ 100/1251 (  8%)]  Loss:  3.897220 (3.8300)  Time: 1.085s,  943.38/s  (1.091s,  938.87/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 150/1251 ( 12%)]  Loss:  4.066047 (3.8890)  Time: 1.099s,  931.64/s  (1.090s,  939.64/s)  LR: 8.961e-04  Data: 0.017 (0.013)
Train: 63 [ 200/1251 ( 16%)]  Loss:  3.532802 (3.8178)  Time: 1.083s,  945.32/s  (1.089s,  940.05/s)  LR: 8.961e-04  Data: 0.017 (0.013)
Train: 63 [ 250/1251 ( 20%)]  Loss:  3.747023 (3.8060)  Time: 1.077s,  950.72/s  (1.088s,  940.96/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 300/1251 ( 24%)]  Loss:  3.610265 (3.7780)  Time: 1.123s,  911.72/s  (1.088s,  940.97/s)  LR: 8.961e-04  Data: 0.018 (0.013)
Train: 63 [ 350/1251 ( 28%)]  Loss:  3.433360 (3.7349)  Time: 1.078s,  949.97/s  (1.088s,  941.40/s)  LR: 8.961e-04  Data: 0.014 (0.013)
Train: 63 [ 400/1251 ( 32%)]  Loss:  3.601901 (3.7202)  Time: 1.075s,  952.57/s  (1.087s,  941.74/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 450/1251 ( 36%)]  Loss:  3.513909 (3.6995)  Time: 1.077s,  950.87/s  (1.088s,  941.02/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 500/1251 ( 40%)]  Loss:  3.575998 (3.6883)  Time: 1.085s,  944.15/s  (1.088s,  941.17/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 63 [ 550/1251 ( 44%)]  Loss:  3.117948 (3.6408)  Time: 1.083s,  945.68/s  (1.088s,  941.27/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 63 [ 600/1251 ( 48%)]  Loss:  3.688787 (3.6445)  Time: 1.079s,  949.00/s  (1.088s,  941.27/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 650/1251 ( 52%)]  Loss:  3.576069 (3.6396)  Time: 1.114s,  919.57/s  (1.089s,  940.63/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 63 [ 700/1251 ( 56%)]  Loss:  4.031838 (3.6657)  Time: 1.083s,  945.82/s  (1.088s,  940.87/s)  LR: 8.961e-04  Data: 0.014 (0.013)
Train: 63 [ 750/1251 ( 60%)]  Loss:  3.428301 (3.6509)  Time: 1.097s,  933.88/s  (1.088s,  940.77/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 63 [ 800/1251 ( 64%)]  Loss:  3.442345 (3.6386)  Time: 1.090s,  939.81/s  (1.089s,  940.52/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 63 [ 850/1251 ( 68%)]  Loss:  3.349130 (3.6225)  Time: 1.104s,  927.70/s  (1.089s,  940.31/s)  LR: 8.961e-04  Data: 0.021 (0.013)
Train: 63 [ 900/1251 ( 72%)]  Loss:  3.833265 (3.6336)  Time: 1.074s,  953.38/s  (1.089s,  940.19/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [ 950/1251 ( 76%)]  Loss:  3.842823 (3.6441)  Time: 1.104s,  927.85/s  (1.089s,  940.15/s)  LR: 8.961e-04  Data: 0.013 (0.013)
Train: 63 [1000/1251 ( 80%)]  Loss:  3.531115 (3.6387)  Time: 1.081s,  947.29/s  (1.089s,  939.98/s)  LR: 8.961e-04  Data: 0.019 (0.013)
Train: 63 [1050/1251 ( 84%)]  Loss:  3.689584 (3.6410)  Time: 1.081s,  947.44/s  (1.089s,  940.08/s)  LR: 8.961e-04  Data: 0.017 (0.013)
Train: 63 [1100/1251 ( 88%)]  Loss:  3.941262 (3.6541)  Time: 1.095s,  934.84/s  (1.089s,  940.07/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1150/1251 ( 92%)]  Loss:  3.899547 (3.6643)  Time: 1.084s,  944.73/s  (1.089s,  939.92/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Train: 63 [1200/1251 ( 96%)]  Loss:  3.818729 (3.6705)  Time: 1.084s,  944.67/s  (1.089s,  939.91/s)  LR: 8.961e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 63 [1250/1251 (100%)]  Loss:  4.055348 (3.6853)  Time: 1.060s,  965.97/s  (1.089s,  940.10/s)  LR: 8.961e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.966 (5.966)  Loss:  0.6035 (0.6035)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6744 (1.1910)  Acc@1: 84.3160 (72.6460)  Acc@5: 96.8160 (91.5560)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-53.pth.tar', 72.10400015136719)

Train: 64 [   0/1251 (  0%)]  Loss:  3.395964 (3.3960)  Time: 1.086s,  942.49/s  (1.086s,  942.49/s)  LR: 8.929e-04  Data: 0.024 (0.024)
Train: 64 [  50/1251 (  4%)]  Loss:  3.555926 (3.4759)  Time: 1.096s,  934.08/s  (1.101s,  930.40/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 100/1251 (  8%)]  Loss:  4.059303 (3.6704)  Time: 1.081s,  946.88/s  (1.099s,  932.18/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 150/1251 ( 12%)]  Loss:  3.626291 (3.6594)  Time: 1.094s,  935.94/s  (1.093s,  936.74/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 200/1251 ( 16%)]  Loss:  3.742476 (3.6760)  Time: 1.082s,  946.73/s  (1.095s,  935.52/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 250/1251 ( 20%)]  Loss:  3.770190 (3.6917)  Time: 1.075s,  952.85/s  (1.094s,  935.61/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 300/1251 ( 24%)]  Loss:  3.824046 (3.7106)  Time: 1.077s,  950.87/s  (1.093s,  936.55/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 64 [ 350/1251 ( 28%)]  Loss:  3.545024 (3.6899)  Time: 1.092s,  937.65/s  (1.093s,  937.24/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 400/1251 ( 32%)]  Loss:  3.628897 (3.6831)  Time: 1.096s,  933.92/s  (1.092s,  938.09/s)  LR: 8.929e-04  Data: 0.015 (0.013)
Train: 64 [ 450/1251 ( 36%)]  Loss:  3.676030 (3.6824)  Time: 1.097s,  933.50/s  (1.091s,  938.52/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 500/1251 ( 40%)]  Loss:  3.922420 (3.7042)  Time: 1.192s,  859.14/s  (1.091s,  938.73/s)  LR: 8.929e-04  Data: 0.011 (0.013)
Train: 64 [ 550/1251 ( 44%)]  Loss:  3.623529 (3.6975)  Time: 1.094s,  936.24/s  (1.091s,  938.42/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 600/1251 ( 48%)]  Loss:  3.950933 (3.7170)  Time: 1.078s,  949.63/s  (1.091s,  938.59/s)  LR: 8.929e-04  Data: 0.015 (0.013)
Train: 64 [ 650/1251 ( 52%)]  Loss:  3.586019 (3.7076)  Time: 1.220s,  839.00/s  (1.091s,  938.80/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 64 [ 700/1251 ( 56%)]  Loss:  3.910141 (3.7211)  Time: 1.084s,  945.04/s  (1.091s,  938.70/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [ 750/1251 ( 60%)]  Loss:  3.830623 (3.7280)  Time: 1.080s,  948.44/s  (1.090s,  939.17/s)  LR: 8.929e-04  Data: 0.015 (0.013)
Train: 64 [ 800/1251 ( 64%)]  Loss:  3.616105 (3.7214)  Time: 1.079s,  949.45/s  (1.090s,  939.08/s)  LR: 8.929e-04  Data: 0.014 (0.013)
Train: 64 [ 850/1251 ( 68%)]  Loss:  3.663074 (3.7182)  Time: 1.082s,  946.60/s  (1.090s,  939.14/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 64 [ 900/1251 ( 72%)]  Loss:  3.778936 (3.7214)  Time: 1.080s,  947.87/s  (1.090s,  939.33/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 64 [ 950/1251 ( 76%)]  Loss:  3.925508 (3.7316)  Time: 1.083s,  945.95/s  (1.090s,  939.72/s)  LR: 8.929e-04  Data: 0.015 (0.013)
Train: 64 [1000/1251 ( 80%)]  Loss:  3.635690 (3.7270)  Time: 1.078s,  949.90/s  (1.090s,  939.84/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [1050/1251 ( 84%)]  Loss:  3.287353 (3.7070)  Time: 1.077s,  950.76/s  (1.090s,  939.85/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 64 [1100/1251 ( 88%)]  Loss:  3.800460 (3.7111)  Time: 1.078s,  949.74/s  (1.089s,  940.12/s)  LR: 8.929e-04  Data: 0.015 (0.013)
Train: 64 [1150/1251 ( 92%)]  Loss:  3.774964 (3.7137)  Time: 1.095s,  935.02/s  (1.089s,  940.28/s)  LR: 8.929e-04  Data: 0.012 (0.013)
Train: 64 [1200/1251 ( 96%)]  Loss:  3.873901 (3.7202)  Time: 1.094s,  935.62/s  (1.089s,  940.13/s)  LR: 8.929e-04  Data: 0.013 (0.013)
Train: 64 [1250/1251 (100%)]  Loss:  3.671909 (3.7183)  Time: 1.080s,  948.37/s  (1.090s,  939.73/s)  LR: 8.929e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.876 (5.876)  Loss:  0.6195 (0.6195)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.7176 (1.1972)  Acc@1: 84.5519 (72.5580)  Acc@5: 95.9906 (91.6520)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 72.55800001220703)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-55.pth.tar', 72.20600004150391)

Train: 65 [   0/1251 (  0%)]  Loss:  3.773820 (3.7738)  Time: 1.092s,  937.79/s  (1.092s,  937.79/s)  LR: 8.897e-04  Data: 0.022 (0.022)
Train: 65 [  50/1251 (  4%)]  Loss:  3.851254 (3.8125)  Time: 1.172s,  873.67/s  (1.091s,  938.97/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [ 100/1251 (  8%)]  Loss:  3.779047 (3.8014)  Time: 1.077s,  950.86/s  (1.088s,  941.25/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [ 150/1251 ( 12%)]  Loss:  3.750573 (3.7887)  Time: 1.105s,  926.64/s  (1.090s,  939.88/s)  LR: 8.897e-04  Data: 0.015 (0.013)
Train: 65 [ 200/1251 ( 16%)]  Loss:  3.936721 (3.8183)  Time: 1.080s,  948.46/s  (1.089s,  940.65/s)  LR: 8.897e-04  Data: 0.014 (0.013)
Train: 65 [ 250/1251 ( 20%)]  Loss:  3.339879 (3.7385)  Time: 1.106s,  925.92/s  (1.088s,  940.89/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [ 300/1251 ( 24%)]  Loss:  3.864989 (3.7566)  Time: 1.171s,  874.23/s  (1.089s,  940.54/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [ 350/1251 ( 28%)]  Loss:  3.630231 (3.7408)  Time: 1.078s,  950.13/s  (1.088s,  941.09/s)  LR: 8.897e-04  Data: 0.015 (0.013)
Train: 65 [ 400/1251 ( 32%)]  Loss:  3.745815 (3.7414)  Time: 1.098s,  932.69/s  (1.088s,  940.81/s)  LR: 8.897e-04  Data: 0.015 (0.013)
Train: 65 [ 450/1251 ( 36%)]  Loss:  3.895905 (3.7568)  Time: 1.078s,  949.69/s  (1.088s,  941.11/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [ 500/1251 ( 40%)]  Loss:  3.805768 (3.7613)  Time: 1.095s,  935.23/s  (1.089s,  940.73/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [ 550/1251 ( 44%)]  Loss:  3.702924 (3.7564)  Time: 1.079s,  949.25/s  (1.089s,  940.67/s)  LR: 8.897e-04  Data: 0.016 (0.013)
Train: 65 [ 600/1251 ( 48%)]  Loss:  3.616510 (3.7456)  Time: 1.084s,  944.38/s  (1.089s,  940.41/s)  LR: 8.897e-04  Data: 0.016 (0.013)
Train: 65 [ 650/1251 ( 52%)]  Loss:  3.905131 (3.7570)  Time: 1.076s,  951.26/s  (1.089s,  940.20/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [ 700/1251 ( 56%)]  Loss:  3.738843 (3.7558)  Time: 1.172s,  873.74/s  (1.090s,  939.83/s)  LR: 8.897e-04  Data: 0.018 (0.013)
Train: 65 [ 750/1251 ( 60%)]  Loss:  3.800572 (3.7586)  Time: 1.096s,  934.58/s  (1.090s,  939.79/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 65 [ 800/1251 ( 64%)]  Loss:  3.666319 (3.7532)  Time: 1.106s,  925.65/s  (1.090s,  939.63/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [ 850/1251 ( 68%)]  Loss:  3.746724 (3.7528)  Time: 1.078s,  949.90/s  (1.090s,  939.69/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [ 900/1251 ( 72%)]  Loss:  3.723719 (3.7513)  Time: 1.105s,  926.38/s  (1.089s,  939.93/s)  LR: 8.897e-04  Data: 0.015 (0.013)
Train: 65 [ 950/1251 ( 76%)]  Loss:  3.545606 (3.7410)  Time: 1.100s,  930.72/s  (1.089s,  939.97/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [1000/1251 ( 80%)]  Loss:  3.603405 (3.7345)  Time: 1.100s,  930.96/s  (1.089s,  939.98/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [1050/1251 ( 84%)]  Loss:  3.568377 (3.7269)  Time: 1.095s,  935.34/s  (1.089s,  940.01/s)  LR: 8.897e-04  Data: 0.012 (0.013)
Train: 65 [1100/1251 ( 88%)]  Loss:  3.927815 (3.7356)  Time: 1.081s,  947.13/s  (1.089s,  940.09/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 65 [1150/1251 ( 92%)]  Loss:  3.583619 (3.7293)  Time: 1.078s,  950.18/s  (1.089s,  940.16/s)  LR: 8.897e-04  Data: 0.013 (0.013)
Train: 65 [1200/1251 ( 96%)]  Loss:  4.050954 (3.7422)  Time: 1.099s,  931.99/s  (1.089s,  940.35/s)  LR: 8.897e-04  Data: 0.015 (0.013)
Train: 65 [1250/1251 (100%)]  Loss:  3.750408 (3.7425)  Time: 1.079s,  948.85/s  (1.089s,  940.06/s)  LR: 8.897e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.847 (5.847)  Loss:  0.6726 (0.6726)  Acc@1: 86.8164 (86.8164)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6922 (1.1989)  Acc@1: 86.0849 (72.8020)  Acc@5: 96.1085 (91.7720)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 72.55800001220703)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-58.pth.tar', 72.25599991210937)

Train: 66 [   0/1251 (  0%)]  Loss:  3.731253 (3.7313)  Time: 1.092s,  937.45/s  (1.092s,  937.45/s)  LR: 8.864e-04  Data: 0.030 (0.030)
Train: 66 [  50/1251 (  4%)]  Loss:  3.438195 (3.5847)  Time: 1.081s,  947.33/s  (1.087s,  941.98/s)  LR: 8.864e-04  Data: 0.012 (0.014)
Train: 66 [ 100/1251 (  8%)]  Loss:  3.658043 (3.6092)  Time: 1.096s,  934.45/s  (1.089s,  940.33/s)  LR: 8.864e-04  Data: 0.018 (0.014)
Train: 66 [ 150/1251 ( 12%)]  Loss:  3.580039 (3.6019)  Time: 1.098s,  932.61/s  (1.089s,  939.89/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 200/1251 ( 16%)]  Loss:  3.857370 (3.6530)  Time: 1.100s,  930.49/s  (1.089s,  940.58/s)  LR: 8.864e-04  Data: 0.016 (0.013)
Train: 66 [ 250/1251 ( 20%)]  Loss:  3.510524 (3.6292)  Time: 1.080s,  947.74/s  (1.089s,  940.24/s)  LR: 8.864e-04  Data: 0.013 (0.013)
Train: 66 [ 300/1251 ( 24%)]  Loss:  3.756951 (3.6475)  Time: 1.081s,  946.88/s  (1.089s,  940.30/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 350/1251 ( 28%)]  Loss:  3.628382 (3.6451)  Time: 1.089s,  940.28/s  (1.088s,  940.79/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [ 400/1251 ( 32%)]  Loss:  3.391901 (3.6170)  Time: 1.094s,  935.95/s  (1.089s,  940.24/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 450/1251 ( 36%)]  Loss:  3.231345 (3.5784)  Time: 1.078s,  949.62/s  (1.089s,  940.37/s)  LR: 8.864e-04  Data: 0.013 (0.013)
Train: 66 [ 500/1251 ( 40%)]  Loss:  4.081817 (3.6242)  Time: 1.174s,  872.08/s  (1.089s,  940.20/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 550/1251 ( 44%)]  Loss:  3.623323 (3.6241)  Time: 1.106s,  926.23/s  (1.089s,  940.67/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 600/1251 ( 48%)]  Loss:  3.721312 (3.6316)  Time: 1.079s,  949.21/s  (1.089s,  940.17/s)  LR: 8.864e-04  Data: 0.014 (0.013)
Train: 66 [ 650/1251 ( 52%)]  Loss:  3.482221 (3.6209)  Time: 1.095s,  934.93/s  (1.089s,  940.08/s)  LR: 8.864e-04  Data: 0.013 (0.013)
Train: 66 [ 700/1251 ( 56%)]  Loss:  4.104066 (3.6531)  Time: 1.094s,  935.80/s  (1.089s,  939.92/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 750/1251 ( 60%)]  Loss:  3.397494 (3.6371)  Time: 1.089s,  940.18/s  (1.089s,  939.93/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 800/1251 ( 64%)]  Loss:  3.758734 (3.6443)  Time: 1.082s,  946.28/s  (1.090s,  939.80/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 850/1251 ( 68%)]  Loss:  3.805775 (3.6533)  Time: 1.095s,  935.48/s  (1.090s,  939.78/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 900/1251 ( 72%)]  Loss:  3.438028 (3.6419)  Time: 1.096s,  934.50/s  (1.090s,  939.73/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [ 950/1251 ( 76%)]  Loss:  3.717076 (3.6457)  Time: 1.079s,  949.46/s  (1.090s,  939.50/s)  LR: 8.864e-04  Data: 0.013 (0.013)
Train: 66 [1000/1251 ( 80%)]  Loss:  3.275705 (3.6281)  Time: 1.078s,  950.13/s  (1.090s,  939.25/s)  LR: 8.864e-04  Data: 0.014 (0.013)
Train: 66 [1050/1251 ( 84%)]  Loss:  3.859853 (3.6386)  Time: 1.100s,  930.93/s  (1.090s,  939.03/s)  LR: 8.864e-04  Data: 0.017 (0.013)
Train: 66 [1100/1251 ( 88%)]  Loss:  3.799035 (3.6456)  Time: 1.080s,  947.91/s  (1.091s,  938.94/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [1150/1251 ( 92%)]  Loss:  3.402780 (3.6355)  Time: 1.108s,  924.29/s  (1.091s,  938.82/s)  LR: 8.864e-04  Data: 0.011 (0.013)
Train: 66 [1200/1251 ( 96%)]  Loss:  3.569477 (3.6328)  Time: 1.095s,  934.73/s  (1.091s,  938.71/s)  LR: 8.864e-04  Data: 0.012 (0.013)
Train: 66 [1250/1251 (100%)]  Loss:  3.596000 (3.6314)  Time: 1.078s,  949.75/s  (1.091s,  938.61/s)  LR: 8.864e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.934 (5.934)  Loss:  0.6224 (0.6224)  Acc@1: 85.8398 (85.8398)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7200 (1.2047)  Acc@1: 85.0236 (72.6560)  Acc@5: 96.5802 (91.6760)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 72.65600008789062)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 72.55800001220703)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-56.pth.tar', 72.27999999023437)

Train: 67 [   0/1251 (  0%)]  Loss:  3.729561 (3.7296)  Time: 1.087s,  941.67/s  (1.087s,  941.67/s)  LR: 8.831e-04  Data: 0.025 (0.025)
Train: 67 [  50/1251 (  4%)]  Loss:  3.669287 (3.6994)  Time: 1.079s,  949.06/s  (1.087s,  942.25/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 100/1251 (  8%)]  Loss:  3.401104 (3.6000)  Time: 1.084s,  944.53/s  (1.085s,  943.39/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [ 150/1251 ( 12%)]  Loss:  3.695894 (3.6240)  Time: 1.094s,  936.15/s  (1.086s,  943.18/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [ 200/1251 ( 16%)]  Loss:  3.589153 (3.6170)  Time: 1.102s,  929.35/s  (1.087s,  942.23/s)  LR: 8.831e-04  Data: 0.011 (0.013)
Train: 67 [ 250/1251 ( 20%)]  Loss:  3.977841 (3.6771)  Time: 1.079s,  948.66/s  (1.087s,  942.31/s)  LR: 8.831e-04  Data: 0.014 (0.013)
Train: 67 [ 300/1251 ( 24%)]  Loss:  3.879416 (3.7060)  Time: 1.076s,  951.46/s  (1.087s,  941.69/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 350/1251 ( 28%)]  Loss:  3.877329 (3.7274)  Time: 1.080s,  947.97/s  (1.087s,  941.64/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 400/1251 ( 32%)]  Loss:  3.636173 (3.7173)  Time: 1.104s,  927.34/s  (1.088s,  940.95/s)  LR: 8.831e-04  Data: 0.014 (0.013)
Train: 67 [ 450/1251 ( 36%)]  Loss:  3.903288 (3.7359)  Time: 1.077s,  950.40/s  (1.088s,  941.26/s)  LR: 8.831e-04  Data: 0.015 (0.013)
Train: 67 [ 500/1251 ( 40%)]  Loss:  3.657653 (3.7288)  Time: 1.092s,  938.05/s  (1.088s,  941.50/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [ 550/1251 ( 44%)]  Loss:  3.695165 (3.7260)  Time: 1.098s,  932.74/s  (1.088s,  940.82/s)  LR: 8.831e-04  Data: 0.014 (0.013)
Train: 67 [ 600/1251 ( 48%)]  Loss:  3.693850 (3.7235)  Time: 1.099s,  931.86/s  (1.088s,  940.79/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 650/1251 ( 52%)]  Loss:  3.665548 (3.7194)  Time: 1.097s,  933.18/s  (1.089s,  940.53/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 700/1251 ( 56%)]  Loss:  3.877179 (3.7299)  Time: 1.092s,  937.93/s  (1.089s,  940.08/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [ 750/1251 ( 60%)]  Loss:  3.258303 (3.7004)  Time: 1.078s,  949.96/s  (1.089s,  940.08/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 800/1251 ( 64%)]  Loss:  3.511465 (3.6893)  Time: 1.104s,  927.19/s  (1.089s,  940.10/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 850/1251 ( 68%)]  Loss:  3.685364 (3.6891)  Time: 1.096s,  934.52/s  (1.090s,  939.61/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [ 900/1251 ( 72%)]  Loss:  3.753918 (3.6925)  Time: 1.095s,  935.11/s  (1.090s,  939.52/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [ 950/1251 ( 76%)]  Loss:  3.769489 (3.6963)  Time: 1.097s,  933.26/s  (1.090s,  939.36/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [1000/1251 ( 80%)]  Loss:  3.923078 (3.7071)  Time: 1.085s,  943.52/s  (1.090s,  939.49/s)  LR: 8.831e-04  Data: 0.021 (0.013)
Train: 67 [1050/1251 ( 84%)]  Loss:  3.638131 (3.7040)  Time: 1.078s,  949.65/s  (1.090s,  939.11/s)  LR: 8.831e-04  Data: 0.014 (0.013)
Train: 67 [1100/1251 ( 88%)]  Loss:  3.978200 (3.7159)  Time: 1.085s,  943.40/s  (1.090s,  939.25/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [1150/1251 ( 92%)]  Loss:  3.729936 (3.7165)  Time: 1.095s,  934.79/s  (1.090s,  939.16/s)  LR: 8.831e-04  Data: 0.012 (0.013)
Train: 67 [1200/1251 ( 96%)]  Loss:  3.867592 (3.7226)  Time: 1.106s,  926.01/s  (1.090s,  939.03/s)  LR: 8.831e-04  Data: 0.013 (0.013)
Train: 67 [1250/1251 (100%)]  Loss:  4.189674 (3.7405)  Time: 1.059s,  966.64/s  (1.091s,  938.83/s)  LR: 8.831e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.889 (5.889)  Loss:  0.7428 (0.7428)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.230 (0.447)  Loss:  0.7749 (1.2429)  Acc@1: 84.3160 (72.9660)  Acc@5: 96.8160 (91.8640)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 72.65600008789062)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 72.55800001220703)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-54.pth.tar', 72.36599999023437)

Train: 68 [   0/1251 (  0%)]  Loss:  3.957536 (3.9575)  Time: 1.086s,  942.93/s  (1.086s,  942.93/s)  LR: 8.797e-04  Data: 0.023 (0.023)
Train: 68 [  50/1251 (  4%)]  Loss:  3.654837 (3.8062)  Time: 1.080s,  948.58/s  (1.088s,  941.18/s)  LR: 8.797e-04  Data: 0.012 (0.013)
Train: 68 [ 100/1251 (  8%)]  Loss:  3.517003 (3.7098)  Time: 1.079s,  949.25/s  (1.087s,  941.76/s)  LR: 8.797e-04  Data: 0.014 (0.013)
Train: 68 [ 150/1251 ( 12%)]  Loss:  3.389421 (3.6297)  Time: 1.095s,  935.33/s  (1.088s,  940.75/s)  LR: 8.797e-04  Data: 0.012 (0.013)
Train: 68 [ 200/1251 ( 16%)]  Loss:  3.862522 (3.6763)  Time: 1.078s,  949.75/s  (1.089s,  940.70/s)  LR: 8.797e-04  Data: 0.014 (0.013)
Train: 68 [ 250/1251 ( 20%)]  Loss:  3.553753 (3.6558)  Time: 1.079s,  949.18/s  (1.089s,  940.45/s)  LR: 8.797e-04  Data: 0.012 (0.013)
Train: 68 [ 300/1251 ( 24%)]  Loss:  3.714621 (3.6642)  Time: 1.097s,  933.66/s  (1.089s,  940.44/s)  LR: 8.797e-04  Data: 0.012 (0.013)
Train: 68 [ 350/1251 ( 28%)]  Loss:  3.333624 (3.6229)  Time: 1.094s,  935.76/s  (1.090s,  939.63/s)  LR: 8.797e-04  Data: 0.012 (0.013)
Train: 68 [ 400/1251 ( 32%)]  Loss:  3.554016 (3.6153)  Time: 1.099s,  931.76/s  (1.090s,  939.16/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [ 450/1251 ( 36%)]  Loss:  3.622949 (3.6160)  Time: 1.098s,  932.30/s  (1.091s,  938.84/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [ 500/1251 ( 40%)]  Loss:  3.270099 (3.5846)  Time: 1.075s,  952.40/s  (1.091s,  938.31/s)  LR: 8.797e-04  Data: 0.011 (0.013)
Train: 68 [ 550/1251 ( 44%)]  Loss:  3.986511 (3.6181)  Time: 1.076s,  951.42/s  (1.091s,  938.58/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [ 600/1251 ( 48%)]  Loss:  3.582114 (3.6153)  Time: 1.087s,  942.19/s  (1.091s,  938.58/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [ 650/1251 ( 52%)]  Loss:  3.435155 (3.6024)  Time: 1.077s,  950.70/s  (1.092s,  938.16/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [ 700/1251 ( 56%)]  Loss:  3.839957 (3.6183)  Time: 1.082s,  946.21/s  (1.092s,  938.01/s)  LR: 8.797e-04  Data: 0.015 (0.013)
Train: 68 [ 750/1251 ( 60%)]  Loss:  3.917099 (3.6370)  Time: 1.084s,  945.08/s  (1.091s,  938.26/s)  LR: 8.797e-04  Data: 0.015 (0.013)
Train: 68 [ 800/1251 ( 64%)]  Loss:  3.800873 (3.6466)  Time: 1.098s,  932.83/s  (1.091s,  938.26/s)  LR: 8.797e-04  Data: 0.012 (0.013)
Train: 68 [ 850/1251 ( 68%)]  Loss:  3.805309 (3.6554)  Time: 1.095s,  935.05/s  (1.092s,  938.13/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [ 900/1251 ( 72%)]  Loss:  3.380396 (3.6409)  Time: 1.108s,  923.92/s  (1.091s,  938.22/s)  LR: 8.797e-04  Data: 0.015 (0.013)
Train: 68 [ 950/1251 ( 76%)]  Loss:  3.898355 (3.6538)  Time: 1.170s,  875.11/s  (1.091s,  938.31/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [1000/1251 ( 80%)]  Loss:  3.260130 (3.6351)  Time: 1.098s,  932.90/s  (1.091s,  938.37/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [1050/1251 ( 84%)]  Loss:  3.732348 (3.6395)  Time: 1.079s,  949.14/s  (1.091s,  938.54/s)  LR: 8.797e-04  Data: 0.014 (0.013)
Train: 68 [1100/1251 ( 88%)]  Loss:  3.361200 (3.6274)  Time: 1.078s,  949.81/s  (1.091s,  938.72/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [1150/1251 ( 92%)]  Loss:  3.668886 (3.6291)  Time: 1.078s,  949.87/s  (1.091s,  938.80/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [1200/1251 ( 96%)]  Loss:  3.401193 (3.6200)  Time: 1.096s,  934.44/s  (1.091s,  938.62/s)  LR: 8.797e-04  Data: 0.013 (0.013)
Train: 68 [1250/1251 (100%)]  Loss:  3.847628 (3.6288)  Time: 1.082s,  946.35/s  (1.091s,  938.45/s)  LR: 8.797e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.790 (5.790)  Loss:  0.6458 (0.6458)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.448)  Loss:  0.7675 (1.1993)  Acc@1: 82.3113 (73.1180)  Acc@5: 96.3443 (91.8200)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 72.65600008789062)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-64.pth.tar', 72.55800001220703)

Train: 69 [   0/1251 (  0%)]  Loss:  3.608580 (3.6086)  Time: 1.087s,  942.45/s  (1.087s,  942.45/s)  LR: 8.763e-04  Data: 0.024 (0.024)
Train: 69 [  50/1251 (  4%)]  Loss:  3.762092 (3.6853)  Time: 1.079s,  949.03/s  (1.087s,  941.68/s)  LR: 8.763e-04  Data: 0.015 (0.014)
Train: 69 [ 100/1251 (  8%)]  Loss:  3.569138 (3.6466)  Time: 1.085s,  943.88/s  (1.089s,  940.64/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 69 [ 150/1251 ( 12%)]  Loss:  3.666621 (3.6516)  Time: 1.084s,  944.98/s  (1.088s,  941.09/s)  LR: 8.763e-04  Data: 0.015 (0.013)
Train: 69 [ 200/1251 ( 16%)]  Loss:  3.562471 (3.6338)  Time: 1.102s,  929.46/s  (1.088s,  941.19/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 250/1251 ( 20%)]  Loss:  3.681149 (3.6417)  Time: 1.080s,  948.28/s  (1.089s,  940.45/s)  LR: 8.763e-04  Data: 0.017 (0.013)
Train: 69 [ 300/1251 ( 24%)]  Loss:  3.525970 (3.6251)  Time: 1.106s,  926.06/s  (1.089s,  940.12/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [ 350/1251 ( 28%)]  Loss:  3.419703 (3.5995)  Time: 1.079s,  948.59/s  (1.089s,  940.41/s)  LR: 8.763e-04  Data: 0.018 (0.013)
Train: 69 [ 400/1251 ( 32%)]  Loss:  3.725541 (3.6135)  Time: 1.096s,  934.29/s  (1.089s,  940.17/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [ 450/1251 ( 36%)]  Loss:  3.487982 (3.6009)  Time: 1.078s,  950.07/s  (1.089s,  940.37/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [ 500/1251 ( 40%)]  Loss:  3.824688 (3.6213)  Time: 1.080s,  948.39/s  (1.088s,  940.80/s)  LR: 8.763e-04  Data: 0.017 (0.013)
Train: 69 [ 550/1251 ( 44%)]  Loss:  3.726497 (3.6300)  Time: 1.094s,  936.26/s  (1.089s,  940.41/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [ 600/1251 ( 48%)]  Loss:  3.643145 (3.6310)  Time: 1.087s,  942.04/s  (1.089s,  940.26/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [ 650/1251 ( 52%)]  Loss:  3.786835 (3.6422)  Time: 1.080s,  947.82/s  (1.089s,  940.23/s)  LR: 8.763e-04  Data: 0.014 (0.013)
Train: 69 [ 700/1251 ( 56%)]  Loss:  3.859165 (3.6566)  Time: 1.075s,  952.27/s  (1.089s,  940.06/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 750/1251 ( 60%)]  Loss:  3.648108 (3.6561)  Time: 1.077s,  950.53/s  (1.090s,  939.49/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [ 800/1251 ( 64%)]  Loss:  3.720033 (3.6599)  Time: 1.082s,  946.38/s  (1.090s,  939.51/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [ 850/1251 ( 68%)]  Loss:  3.819248 (3.6687)  Time: 1.114s,  919.55/s  (1.090s,  939.51/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [ 900/1251 ( 72%)]  Loss:  3.679578 (3.6693)  Time: 1.086s,  942.65/s  (1.090s,  939.23/s)  LR: 8.763e-04  Data: 0.011 (0.013)
Train: 69 [ 950/1251 ( 76%)]  Loss:  3.579712 (3.6648)  Time: 1.081s,  947.16/s  (1.090s,  939.06/s)  LR: 8.763e-04  Data: 0.012 (0.013)
Train: 69 [1000/1251 ( 80%)]  Loss:  3.740650 (3.6684)  Time: 1.078s,  949.95/s  (1.090s,  939.24/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [1050/1251 ( 84%)]  Loss:  4.087739 (3.6875)  Time: 1.105s,  926.42/s  (1.090s,  939.32/s)  LR: 8.763e-04  Data: 0.014 (0.013)
Train: 69 [1100/1251 ( 88%)]  Loss:  3.701659 (3.6881)  Time: 1.078s,  949.91/s  (1.090s,  939.13/s)  LR: 8.763e-04  Data: 0.014 (0.013)
Train: 69 [1150/1251 ( 92%)]  Loss:  4.031278 (3.7024)  Time: 1.078s,  950.26/s  (1.090s,  939.36/s)  LR: 8.763e-04  Data: 0.013 (0.013)
Train: 69 [1200/1251 ( 96%)]  Loss:  3.425244 (3.6913)  Time: 1.095s,  935.10/s  (1.090s,  939.24/s)  LR: 8.763e-04  Data: 0.017 (0.013)
Train: 69 [1250/1251 (100%)]  Loss:  3.763331 (3.6941)  Time: 1.062s,  963.91/s  (1.090s,  939.08/s)  LR: 8.763e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.895 (5.895)  Loss:  0.6263 (0.6263)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.230 (0.447)  Loss:  0.6787 (1.1688)  Acc@1: 84.4340 (73.3420)  Acc@5: 95.7547 (91.7480)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 72.65600008789062)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-61.pth.tar', 72.61600003662109)

Train: 70 [   0/1251 (  0%)]  Loss:  3.247426 (3.2474)  Time: 1.088s,  940.78/s  (1.088s,  940.78/s)  LR: 8.729e-04  Data: 0.025 (0.025)
Train: 70 [  50/1251 (  4%)]  Loss:  3.612746 (3.4301)  Time: 1.096s,  934.04/s  (1.089s,  940.29/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 100/1251 (  8%)]  Loss:  3.698906 (3.5197)  Time: 1.098s,  932.89/s  (1.090s,  939.37/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 150/1251 ( 12%)]  Loss:  3.820146 (3.5948)  Time: 1.095s,  935.17/s  (1.091s,  938.26/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 200/1251 ( 16%)]  Loss:  3.740813 (3.6240)  Time: 1.076s,  951.93/s  (1.092s,  937.34/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 250/1251 ( 20%)]  Loss:  3.701124 (3.6369)  Time: 1.077s,  951.06/s  (1.091s,  938.19/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 300/1251 ( 24%)]  Loss:  3.573913 (3.6279)  Time: 1.080s,  948.01/s  (1.091s,  938.50/s)  LR: 8.729e-04  Data: 0.014 (0.013)
Train: 70 [ 350/1251 ( 28%)]  Loss:  3.620257 (3.6269)  Time: 1.084s,  945.05/s  (1.092s,  937.53/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 400/1251 ( 32%)]  Loss:  3.485490 (3.6112)  Time: 1.084s,  945.08/s  (1.092s,  937.96/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 450/1251 ( 36%)]  Loss:  3.770544 (3.6271)  Time: 1.094s,  936.32/s  (1.091s,  938.18/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 500/1251 ( 40%)]  Loss:  3.591284 (3.6239)  Time: 1.095s,  935.26/s  (1.091s,  938.30/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [ 550/1251 ( 44%)]  Loss:  3.725751 (3.6324)  Time: 1.099s,  931.99/s  (1.091s,  938.22/s)  LR: 8.729e-04  Data: 0.015 (0.013)
Train: 70 [ 600/1251 ( 48%)]  Loss:  3.753827 (3.6417)  Time: 1.073s,  954.38/s  (1.091s,  938.32/s)  LR: 8.729e-04  Data: 0.011 (0.013)
Train: 70 [ 650/1251 ( 52%)]  Loss:  3.625075 (3.6405)  Time: 1.081s,  946.87/s  (1.091s,  938.29/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 700/1251 ( 56%)]  Loss:  3.966445 (3.6622)  Time: 1.084s,  944.45/s  (1.091s,  938.29/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 750/1251 ( 60%)]  Loss:  3.568792 (3.6564)  Time: 1.078s,  949.73/s  (1.091s,  938.22/s)  LR: 8.729e-04  Data: 0.014 (0.013)
Train: 70 [ 800/1251 ( 64%)]  Loss:  4.011373 (3.6773)  Time: 1.085s,  943.83/s  (1.091s,  938.41/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 850/1251 ( 68%)]  Loss:  3.661210 (3.6764)  Time: 1.093s,  936.77/s  (1.091s,  938.17/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 70 [ 900/1251 ( 72%)]  Loss:  3.391548 (3.6614)  Time: 1.079s,  948.67/s  (1.091s,  938.56/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [ 950/1251 ( 76%)]  Loss:  3.718578 (3.6643)  Time: 1.080s,  948.01/s  (1.091s,  938.76/s)  LR: 8.729e-04  Data: 0.016 (0.013)
Train: 70 [1000/1251 ( 80%)]  Loss:  3.871798 (3.6741)  Time: 1.094s,  935.75/s  (1.091s,  938.60/s)  LR: 8.729e-04  Data: 0.013 (0.013)
Train: 70 [1050/1251 ( 84%)]  Loss:  3.818703 (3.6807)  Time: 1.083s,  945.45/s  (1.091s,  938.43/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [1100/1251 ( 88%)]  Loss:  4.119525 (3.6998)  Time: 1.082s,  946.01/s  (1.091s,  938.78/s)  LR: 8.729e-04  Data: 0.014 (0.013)
Train: 70 [1150/1251 ( 92%)]  Loss:  3.837950 (3.7056)  Time: 1.080s,  948.50/s  (1.091s,  938.83/s)  LR: 8.729e-04  Data: 0.014 (0.013)
Train: 70 [1200/1251 ( 96%)]  Loss:  4.049293 (3.7193)  Time: 1.079s,  949.25/s  (1.091s,  938.82/s)  LR: 8.729e-04  Data: 0.012 (0.013)
Train: 70 [1250/1251 (100%)]  Loss:  3.571533 (3.7136)  Time: 1.061s,  964.97/s  (1.091s,  938.94/s)  LR: 8.729e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.893 (5.893)  Loss:  0.6934 (0.6934)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.7683 (1.2311)  Acc@1: 83.6085 (73.0000)  Acc@5: 96.3443 (91.8480)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 72.65600008789062)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-63.pth.tar', 72.6460000390625)

Train: 71 [   0/1251 (  0%)]  Loss:  3.684082 (3.6841)  Time: 1.086s,  943.22/s  (1.086s,  943.22/s)  LR: 8.694e-04  Data: 0.022 (0.022)
Train: 71 [  50/1251 (  4%)]  Loss:  3.361661 (3.5229)  Time: 1.078s,  949.91/s  (1.094s,  936.30/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [ 100/1251 (  8%)]  Loss:  3.652635 (3.5661)  Time: 1.082s,  946.39/s  (1.095s,  935.48/s)  LR: 8.694e-04  Data: 0.014 (0.013)
Train: 71 [ 150/1251 ( 12%)]  Loss:  3.833236 (3.6329)  Time: 1.082s,  946.17/s  (1.095s,  935.10/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 200/1251 ( 16%)]  Loss:  4.041484 (3.7146)  Time: 1.101s,  930.17/s  (1.093s,  936.81/s)  LR: 8.694e-04  Data: 0.015 (0.013)
Train: 71 [ 250/1251 ( 20%)]  Loss:  3.827336 (3.7334)  Time: 1.103s,  928.47/s  (1.094s,  935.99/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [ 300/1251 ( 24%)]  Loss:  3.700797 (3.7287)  Time: 1.076s,  951.27/s  (1.093s,  936.49/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 350/1251 ( 28%)]  Loss:  3.788366 (3.7362)  Time: 1.104s,  927.43/s  (1.093s,  936.49/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 400/1251 ( 32%)]  Loss:  3.829170 (3.7465)  Time: 1.085s,  943.52/s  (1.093s,  937.20/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 450/1251 ( 36%)]  Loss:  3.706903 (3.7426)  Time: 1.096s,  934.58/s  (1.092s,  937.45/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [ 500/1251 ( 40%)]  Loss:  3.918639 (3.7586)  Time: 1.079s,  948.72/s  (1.092s,  937.85/s)  LR: 8.694e-04  Data: 0.014 (0.013)
Train: 71 [ 550/1251 ( 44%)]  Loss:  3.677831 (3.7518)  Time: 1.096s,  933.99/s  (1.092s,  938.08/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 600/1251 ( 48%)]  Loss:  3.384121 (3.7236)  Time: 1.079s,  948.88/s  (1.092s,  937.89/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 650/1251 ( 52%)]  Loss:  3.460320 (3.7048)  Time: 1.082s,  946.06/s  (1.092s,  937.76/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [ 700/1251 ( 56%)]  Loss:  3.718410 (3.7057)  Time: 1.078s,  949.69/s  (1.092s,  938.06/s)  LR: 8.694e-04  Data: 0.015 (0.013)
Train: 71 [ 750/1251 ( 60%)]  Loss:  3.748355 (3.7083)  Time: 1.102s,  929.58/s  (1.092s,  938.05/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 800/1251 ( 64%)]  Loss:  3.490465 (3.6955)  Time: 1.080s,  948.18/s  (1.092s,  938.07/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [ 850/1251 ( 68%)]  Loss:  3.688935 (3.6952)  Time: 1.077s,  950.41/s  (1.092s,  937.94/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [ 900/1251 ( 72%)]  Loss:  3.626141 (3.6915)  Time: 1.095s,  935.18/s  (1.091s,  938.20/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [ 950/1251 ( 76%)]  Loss:  3.696278 (3.6918)  Time: 1.159s,  883.23/s  (1.092s,  938.16/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1000/1251 ( 80%)]  Loss:  3.714646 (3.6928)  Time: 1.077s,  950.70/s  (1.092s,  937.98/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [1050/1251 ( 84%)]  Loss:  3.594268 (3.6884)  Time: 1.080s,  948.14/s  (1.091s,  938.23/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1100/1251 ( 88%)]  Loss:  3.560961 (3.6828)  Time: 1.084s,  944.32/s  (1.091s,  938.46/s)  LR: 8.694e-04  Data: 0.013 (0.013)
Train: 71 [1150/1251 ( 92%)]  Loss:  3.869433 (3.6906)  Time: 1.082s,  946.81/s  (1.091s,  938.48/s)  LR: 8.694e-04  Data: 0.015 (0.013)
Train: 71 [1200/1251 ( 96%)]  Loss:  3.653805 (3.6891)  Time: 1.079s,  949.45/s  (1.091s,  938.65/s)  LR: 8.694e-04  Data: 0.012 (0.013)
Train: 71 [1250/1251 (100%)]  Loss:  3.538650 (3.6833)  Time: 1.081s,  947.47/s  (1.091s,  938.51/s)  LR: 8.694e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.742 (5.742)  Loss:  0.5602 (0.5602)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.6288 (1.1404)  Acc@1: 85.7311 (73.5880)  Acc@5: 97.0519 (91.9340)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-66.pth.tar', 72.65600008789062)

Train: 72 [   0/1251 (  0%)]  Loss:  3.772477 (3.7725)  Time: 1.093s,  936.51/s  (1.093s,  936.51/s)  LR: 8.658e-04  Data: 0.030 (0.030)
Train: 72 [  50/1251 (  4%)]  Loss:  3.455560 (3.6140)  Time: 1.081s,  947.17/s  (1.085s,  943.77/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 100/1251 (  8%)]  Loss:  3.714671 (3.6476)  Time: 1.094s,  935.83/s  (1.091s,  938.80/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 150/1251 ( 12%)]  Loss:  3.747655 (3.6726)  Time: 1.097s,  933.11/s  (1.090s,  939.41/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 200/1251 ( 16%)]  Loss:  3.507083 (3.6395)  Time: 1.083s,  945.52/s  (1.090s,  939.73/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 250/1251 ( 20%)]  Loss:  4.002861 (3.7001)  Time: 1.180s,  867.73/s  (1.091s,  938.52/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 300/1251 ( 24%)]  Loss:  3.451661 (3.6646)  Time: 1.096s,  934.55/s  (1.091s,  938.55/s)  LR: 8.658e-04  Data: 0.016 (0.013)
Train: 72 [ 350/1251 ( 28%)]  Loss:  3.768642 (3.6776)  Time: 1.096s,  934.20/s  (1.091s,  938.78/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 72 [ 400/1251 ( 32%)]  Loss:  3.554309 (3.6639)  Time: 1.085s,  944.09/s  (1.090s,  939.36/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 450/1251 ( 36%)]  Loss:  4.082750 (3.7058)  Time: 1.076s,  951.34/s  (1.090s,  939.69/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [ 500/1251 ( 40%)]  Loss:  3.655557 (3.7012)  Time: 1.095s,  934.92/s  (1.089s,  940.11/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 550/1251 ( 44%)]  Loss:  3.733315 (3.7039)  Time: 1.096s,  934.18/s  (1.089s,  940.03/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [ 600/1251 ( 48%)]  Loss:  3.670364 (3.7013)  Time: 1.173s,  872.72/s  (1.089s,  939.98/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [ 650/1251 ( 52%)]  Loss:  3.588915 (3.6933)  Time: 1.086s,  942.95/s  (1.089s,  939.92/s)  LR: 8.658e-04  Data: 0.015 (0.013)
Train: 72 [ 700/1251 ( 56%)]  Loss:  3.746974 (3.6969)  Time: 1.077s,  950.63/s  (1.090s,  939.88/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [ 750/1251 ( 60%)]  Loss:  3.612498 (3.6916)  Time: 1.094s,  936.19/s  (1.089s,  940.07/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 800/1251 ( 64%)]  Loss:  3.385262 (3.6736)  Time: 1.078s,  950.10/s  (1.089s,  939.99/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 850/1251 ( 68%)]  Loss:  3.687985 (3.6744)  Time: 1.095s,  935.52/s  (1.090s,  939.68/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [ 900/1251 ( 72%)]  Loss:  3.590550 (3.6700)  Time: 1.080s,  947.77/s  (1.090s,  939.68/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [ 950/1251 ( 76%)]  Loss:  3.498361 (3.6614)  Time: 1.085s,  943.96/s  (1.090s,  939.54/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [1000/1251 ( 80%)]  Loss:  3.533043 (3.6553)  Time: 1.080s,  948.44/s  (1.090s,  939.59/s)  LR: 8.658e-04  Data: 0.013 (0.013)
Train: 72 [1050/1251 ( 84%)]  Loss:  3.651174 (3.6551)  Time: 1.094s,  935.60/s  (1.090s,  939.45/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [1100/1251 ( 88%)]  Loss:  3.499078 (3.6483)  Time: 1.098s,  932.29/s  (1.090s,  939.29/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [1150/1251 ( 92%)]  Loss:  3.915005 (3.6594)  Time: 1.083s,  945.44/s  (1.090s,  939.07/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [1200/1251 ( 96%)]  Loss:  3.653470 (3.6592)  Time: 1.076s,  951.62/s  (1.090s,  939.04/s)  LR: 8.658e-04  Data: 0.012 (0.013)
Train: 72 [1250/1251 (100%)]  Loss:  3.706308 (3.6610)  Time: 1.084s,  944.48/s  (1.090s,  939.08/s)  LR: 8.658e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.727 (5.727)  Loss:  0.6095 (0.6095)  Acc@1: 87.4023 (87.4023)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.7037 (1.1652)  Acc@1: 85.0236 (73.2260)  Acc@5: 95.6368 (91.9680)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-60.pth.tar', 72.66800006591797)

Train: 73 [   0/1251 (  0%)]  Loss:  3.629738 (3.6297)  Time: 1.089s,  940.04/s  (1.089s,  940.04/s)  LR: 8.623e-04  Data: 0.026 (0.026)
Train: 73 [  50/1251 (  4%)]  Loss:  3.881355 (3.7555)  Time: 1.083s,  945.88/s  (1.090s,  939.16/s)  LR: 8.623e-04  Data: 0.013 (0.014)
Train: 73 [ 100/1251 (  8%)]  Loss:  3.837739 (3.7829)  Time: 1.100s,  931.02/s  (1.089s,  940.27/s)  LR: 8.623e-04  Data: 0.015 (0.013)
Train: 73 [ 150/1251 ( 12%)]  Loss:  3.721105 (3.7675)  Time: 1.093s,  936.80/s  (1.089s,  940.60/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [ 200/1251 ( 16%)]  Loss:  3.702119 (3.7544)  Time: 1.107s,  924.70/s  (1.089s,  940.22/s)  LR: 8.623e-04  Data: 0.014 (0.013)
Train: 73 [ 250/1251 ( 20%)]  Loss:  3.559387 (3.7219)  Time: 1.077s,  951.03/s  (1.090s,  939.88/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 73 [ 300/1251 ( 24%)]  Loss:  3.583315 (3.7021)  Time: 1.076s,  951.27/s  (1.090s,  939.29/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 73 [ 350/1251 ( 28%)]  Loss:  3.631705 (3.6933)  Time: 1.079s,  949.37/s  (1.090s,  939.74/s)  LR: 8.623e-04  Data: 0.014 (0.013)
Train: 73 [ 400/1251 ( 32%)]  Loss:  3.706515 (3.6948)  Time: 1.080s,  948.14/s  (1.090s,  939.68/s)  LR: 8.623e-04  Data: 0.014 (0.013)
Train: 73 [ 450/1251 ( 36%)]  Loss:  3.303521 (3.6556)  Time: 1.077s,  950.88/s  (1.089s,  940.02/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 73 [ 500/1251 ( 40%)]  Loss:  3.763272 (3.6654)  Time: 1.078s,  950.10/s  (1.089s,  939.88/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 73 [ 550/1251 ( 44%)]  Loss:  3.789120 (3.6757)  Time: 1.080s,  948.57/s  (1.089s,  940.23/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 73 [ 600/1251 ( 48%)]  Loss:  3.215556 (3.6403)  Time: 1.087s,  942.10/s  (1.089s,  940.08/s)  LR: 8.623e-04  Data: 0.014 (0.013)
Train: 73 [ 650/1251 ( 52%)]  Loss:  3.502755 (3.6305)  Time: 1.077s,  950.73/s  (1.089s,  939.89/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [ 700/1251 ( 56%)]  Loss:  3.664909 (3.6328)  Time: 1.102s,  929.28/s  (1.089s,  940.09/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [ 750/1251 ( 60%)]  Loss:  3.498836 (3.6244)  Time: 1.103s,  928.65/s  (1.089s,  940.20/s)  LR: 8.623e-04  Data: 0.018 (0.013)
Train: 73 [ 800/1251 ( 64%)]  Loss:  3.643811 (3.6256)  Time: 1.078s,  949.89/s  (1.089s,  940.21/s)  LR: 8.623e-04  Data: 0.014 (0.013)
Train: 73 [ 850/1251 ( 68%)]  Loss:  3.831439 (3.6370)  Time: 1.099s,  932.14/s  (1.089s,  939.98/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [ 900/1251 ( 72%)]  Loss:  3.988448 (3.6555)  Time: 1.095s,  934.78/s  (1.090s,  939.59/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [ 950/1251 ( 76%)]  Loss:  3.669426 (3.6562)  Time: 1.087s,  941.80/s  (1.090s,  939.71/s)  LR: 8.623e-04  Data: 0.011 (0.013)
Train: 73 [1000/1251 ( 80%)]  Loss:  3.914005 (3.6685)  Time: 1.084s,  945.03/s  (1.090s,  939.62/s)  LR: 8.623e-04  Data: 0.013 (0.013)
Train: 73 [1050/1251 ( 84%)]  Loss:  3.726275 (3.6711)  Time: 1.079s,  948.60/s  (1.090s,  939.50/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Train: 73 [1100/1251 ( 88%)]  Loss:  3.468632 (3.6623)  Time: 1.076s,  951.44/s  (1.090s,  939.36/s)  LR: 8.623e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 73 [1150/1251 ( 92%)]  Loss:  3.825472 (3.6691)  Time: 1.085s,  944.19/s  (1.090s,  939.34/s)  LR: 8.623e-04  Data: 0.016 (0.013)
Train: 73 [1200/1251 ( 96%)]  Loss:  3.623736 (3.6673)  Time: 1.079s,  949.42/s  (1.090s,  939.27/s)  LR: 8.623e-04  Data: 0.014 (0.013)
Train: 73 [1250/1251 (100%)]  Loss:  3.529080 (3.6620)  Time: 1.060s,  965.86/s  (1.090s,  939.45/s)  LR: 8.623e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 5.747 (5.747)  Loss:  0.5691 (0.5691)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7660 (1.1489)  Acc@1: 82.9009 (73.5620)  Acc@5: 95.6368 (92.0640)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-62.pth.tar', 72.6999998828125)

Train: 74 [   0/1251 (  0%)]  Loss:  3.464118 (3.4641)  Time: 1.085s,  943.63/s  (1.085s,  943.63/s)  LR: 8.587e-04  Data: 0.023 (0.023)
Train: 74 [  50/1251 (  4%)]  Loss:  3.276772 (3.3704)  Time: 1.080s,  947.82/s  (1.087s,  941.63/s)  LR: 8.587e-04  Data: 0.015 (0.014)
Train: 74 [ 100/1251 (  8%)]  Loss:  3.514575 (3.4185)  Time: 1.172s,  873.38/s  (1.093s,  937.30/s)  LR: 8.587e-04  Data: 0.011 (0.014)
Train: 74 [ 150/1251 ( 12%)]  Loss:  3.726568 (3.4955)  Time: 1.091s,  938.33/s  (1.091s,  938.77/s)  LR: 8.587e-04  Data: 0.016 (0.013)
Train: 74 [ 200/1251 ( 16%)]  Loss:  3.707078 (3.5378)  Time: 1.076s,  951.24/s  (1.090s,  939.39/s)  LR: 8.587e-04  Data: 0.013 (0.014)
Train: 74 [ 250/1251 ( 20%)]  Loss:  3.661970 (3.5585)  Time: 1.079s,  949.13/s  (1.090s,  939.08/s)  LR: 8.587e-04  Data: 0.016 (0.014)
Train: 74 [ 300/1251 ( 24%)]  Loss:  3.492742 (3.5491)  Time: 1.095s,  935.19/s  (1.090s,  939.17/s)  LR: 8.587e-04  Data: 0.012 (0.014)
Train: 74 [ 350/1251 ( 28%)]  Loss:  3.974661 (3.6023)  Time: 1.080s,  948.02/s  (1.090s,  939.88/s)  LR: 8.587e-04  Data: 0.013 (0.014)
Train: 74 [ 400/1251 ( 32%)]  Loss:  3.858095 (3.6307)  Time: 1.095s,  934.89/s  (1.089s,  940.03/s)  LR: 8.587e-04  Data: 0.012 (0.014)
Train: 74 [ 450/1251 ( 36%)]  Loss:  3.702085 (3.6379)  Time: 1.172s,  873.47/s  (1.090s,  939.13/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [ 500/1251 ( 40%)]  Loss:  3.947228 (3.6660)  Time: 1.122s,  912.79/s  (1.091s,  938.87/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [ 550/1251 ( 44%)]  Loss:  3.639951 (3.6638)  Time: 1.095s,  935.49/s  (1.090s,  939.02/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [ 600/1251 ( 48%)]  Loss:  3.898131 (3.6818)  Time: 1.078s,  949.71/s  (1.091s,  938.64/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [ 650/1251 ( 52%)]  Loss:  3.991325 (3.7039)  Time: 1.094s,  935.75/s  (1.091s,  938.28/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [ 700/1251 ( 56%)]  Loss:  3.703717 (3.7039)  Time: 1.077s,  950.98/s  (1.091s,  938.37/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [ 750/1251 ( 60%)]  Loss:  3.469786 (3.6893)  Time: 1.078s,  949.87/s  (1.091s,  938.50/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [ 800/1251 ( 64%)]  Loss:  3.621974 (3.6853)  Time: 1.084s,  944.88/s  (1.091s,  938.71/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [ 850/1251 ( 68%)]  Loss:  3.634592 (3.6825)  Time: 1.080s,  947.92/s  (1.091s,  938.97/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [ 900/1251 ( 72%)]  Loss:  3.756838 (3.6864)  Time: 1.093s,  937.12/s  (1.090s,  939.02/s)  LR: 8.587e-04  Data: 0.019 (0.013)
Train: 74 [ 950/1251 ( 76%)]  Loss:  3.969944 (3.7006)  Time: 1.078s,  949.67/s  (1.091s,  938.96/s)  LR: 8.587e-04  Data: 0.016 (0.013)
Train: 74 [1000/1251 ( 80%)]  Loss:  3.798194 (3.7053)  Time: 1.104s,  927.83/s  (1.090s,  939.13/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [1050/1251 ( 84%)]  Loss:  4.037177 (3.7203)  Time: 1.079s,  949.13/s  (1.090s,  939.28/s)  LR: 8.587e-04  Data: 0.013 (0.013)
Train: 74 [1100/1251 ( 88%)]  Loss:  3.870533 (3.7269)  Time: 1.084s,  945.09/s  (1.090s,  939.36/s)  LR: 8.587e-04  Data: 0.018 (0.013)
Train: 74 [1150/1251 ( 92%)]  Loss:  3.911929 (3.7346)  Time: 1.076s,  951.25/s  (1.090s,  939.07/s)  LR: 8.587e-04  Data: 0.012 (0.013)
Train: 74 [1200/1251 ( 96%)]  Loss:  3.974969 (3.7442)  Time: 1.116s,  917.24/s  (1.090s,  939.06/s)  LR: 8.587e-04  Data: 0.016 (0.013)
Train: 74 [1250/1251 (100%)]  Loss:  3.114263 (3.7200)  Time: 1.079s,  948.60/s  (1.090s,  939.06/s)  LR: 8.587e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.848 (5.848)  Loss:  0.6260 (0.6260)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7028 (1.1827)  Acc@1: 84.9057 (73.2500)  Acc@5: 96.5802 (91.8600)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-59.pth.tar', 72.7820000415039)

Train: 75 [   0/1251 (  0%)]  Loss:  3.801149 (3.8011)  Time: 1.091s,  938.23/s  (1.091s,  938.23/s)  LR: 8.550e-04  Data: 0.028 (0.028)
Train: 75 [  50/1251 (  4%)]  Loss:  3.587263 (3.6942)  Time: 1.082s,  945.97/s  (1.089s,  940.21/s)  LR: 8.550e-04  Data: 0.014 (0.013)
Train: 75 [ 100/1251 (  8%)]  Loss:  4.023842 (3.8041)  Time: 1.097s,  933.85/s  (1.092s,  937.86/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 150/1251 ( 12%)]  Loss:  3.659052 (3.7678)  Time: 1.078s,  950.14/s  (1.090s,  939.49/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 75 [ 200/1251 ( 16%)]  Loss:  3.556096 (3.7255)  Time: 1.110s,  922.30/s  (1.091s,  938.37/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 250/1251 ( 20%)]  Loss:  3.899113 (3.7544)  Time: 1.096s,  934.46/s  (1.092s,  937.96/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 75 [ 300/1251 ( 24%)]  Loss:  3.099552 (3.6609)  Time: 1.086s,  943.27/s  (1.091s,  938.68/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 350/1251 ( 28%)]  Loss:  3.757813 (3.6730)  Time: 1.103s,  928.09/s  (1.091s,  938.81/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 400/1251 ( 32%)]  Loss:  3.849138 (3.6926)  Time: 1.086s,  942.89/s  (1.091s,  938.45/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 75 [ 450/1251 ( 36%)]  Loss:  3.619460 (3.6852)  Time: 1.086s,  943.19/s  (1.091s,  938.49/s)  LR: 8.550e-04  Data: 0.014 (0.013)
Train: 75 [ 500/1251 ( 40%)]  Loss:  3.267518 (3.6473)  Time: 1.095s,  934.92/s  (1.090s,  939.07/s)  LR: 8.550e-04  Data: 0.011 (0.013)
Train: 75 [ 550/1251 ( 44%)]  Loss:  3.643855 (3.6470)  Time: 1.097s,  933.36/s  (1.091s,  939.01/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 600/1251 ( 48%)]  Loss:  3.660367 (3.6480)  Time: 1.176s,  870.95/s  (1.090s,  939.22/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 75 [ 650/1251 ( 52%)]  Loss:  3.881066 (3.6647)  Time: 1.078s,  949.56/s  (1.090s,  939.62/s)  LR: 8.550e-04  Data: 0.014 (0.013)
Train: 75 [ 700/1251 ( 56%)]  Loss:  3.709569 (3.6677)  Time: 1.101s,  929.86/s  (1.090s,  939.31/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 750/1251 ( 60%)]  Loss:  3.809126 (3.6765)  Time: 1.108s,  924.02/s  (1.090s,  939.39/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 800/1251 ( 64%)]  Loss:  3.621425 (3.6733)  Time: 1.079s,  949.13/s  (1.090s,  939.16/s)  LR: 8.550e-04  Data: 0.014 (0.013)
Train: 75 [ 850/1251 ( 68%)]  Loss:  3.647482 (3.6718)  Time: 1.075s,  952.12/s  (1.090s,  939.06/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 75 [ 900/1251 ( 72%)]  Loss:  3.823884 (3.6798)  Time: 1.077s,  950.89/s  (1.090s,  939.14/s)  LR: 8.550e-04  Data: 0.012 (0.013)
Train: 75 [ 950/1251 ( 76%)]  Loss:  3.859520 (3.6888)  Time: 1.077s,  950.98/s  (1.090s,  939.40/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 75 [1000/1251 ( 80%)]  Loss:  3.654326 (3.6872)  Time: 1.079s,  949.19/s  (1.090s,  939.57/s)  LR: 8.550e-04  Data: 0.014 (0.013)
Train: 75 [1050/1251 ( 84%)]  Loss:  3.922225 (3.6979)  Time: 1.096s,  934.64/s  (1.090s,  939.53/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 75 [1100/1251 ( 88%)]  Loss:  3.897860 (3.7066)  Time: 1.097s,  933.59/s  (1.090s,  939.40/s)  LR: 8.550e-04  Data: 0.021 (0.013)
Train: 75 [1150/1251 ( 92%)]  Loss:  3.590145 (3.7017)  Time: 1.080s,  948.06/s  (1.090s,  939.18/s)  LR: 8.550e-04  Data: 0.014 (0.013)
Train: 75 [1200/1251 ( 96%)]  Loss:  3.239238 (3.6832)  Time: 1.103s,  928.72/s  (1.090s,  939.03/s)  LR: 8.550e-04  Data: 0.013 (0.013)
Train: 75 [1250/1251 (100%)]  Loss:  3.749460 (3.6858)  Time: 1.080s,  948.46/s  (1.091s,  938.74/s)  LR: 8.550e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.738 (5.738)  Loss:  0.5658 (0.5658)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7131 (1.1603)  Acc@1: 84.0802 (73.5340)  Acc@5: 96.2264 (92.1240)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-65.pth.tar', 72.80200003173829)

Train: 76 [   0/1251 (  0%)]  Loss:  3.717377 (3.7174)  Time: 1.090s,  939.84/s  (1.090s,  939.84/s)  LR: 8.513e-04  Data: 0.027 (0.027)
Train: 76 [  50/1251 (  4%)]  Loss:  3.561483 (3.6394)  Time: 1.081s,  947.12/s  (1.092s,  938.05/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 100/1251 (  8%)]  Loss:  4.038015 (3.7723)  Time: 1.097s,  933.29/s  (1.089s,  940.67/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 150/1251 ( 12%)]  Loss:  3.469940 (3.6967)  Time: 1.079s,  949.11/s  (1.093s,  936.78/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 200/1251 ( 16%)]  Loss:  3.811453 (3.7197)  Time: 1.094s,  935.69/s  (1.092s,  937.36/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 250/1251 ( 20%)]  Loss:  3.665280 (3.7106)  Time: 1.091s,  938.65/s  (1.092s,  937.65/s)  LR: 8.513e-04  Data: 0.015 (0.013)
Train: 76 [ 300/1251 ( 24%)]  Loss:  3.714374 (3.7111)  Time: 1.105s,  926.90/s  (1.093s,  937.26/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 350/1251 ( 28%)]  Loss:  3.685729 (3.7080)  Time: 1.081s,  947.61/s  (1.092s,  937.76/s)  LR: 8.513e-04  Data: 0.019 (0.013)
Train: 76 [ 400/1251 ( 32%)]  Loss:  3.525824 (3.6877)  Time: 1.076s,  951.35/s  (1.092s,  937.68/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 450/1251 ( 36%)]  Loss:  3.749771 (3.6939)  Time: 1.096s,  934.23/s  (1.092s,  937.60/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 500/1251 ( 40%)]  Loss:  3.739777 (3.6981)  Time: 1.088s,  941.21/s  (1.092s,  937.94/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 550/1251 ( 44%)]  Loss:  3.323679 (3.6669)  Time: 1.077s,  950.78/s  (1.092s,  937.46/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 600/1251 ( 48%)]  Loss:  3.555757 (3.6583)  Time: 1.079s,  948.93/s  (1.092s,  937.75/s)  LR: 8.513e-04  Data: 0.014 (0.013)
Train: 76 [ 650/1251 ( 52%)]  Loss:  3.592057 (3.6536)  Time: 1.097s,  933.79/s  (1.092s,  937.63/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 700/1251 ( 56%)]  Loss:  3.319265 (3.6313)  Time: 1.087s,  942.21/s  (1.092s,  938.15/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 750/1251 ( 60%)]  Loss:  3.433142 (3.6189)  Time: 1.084s,  944.65/s  (1.091s,  938.40/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [ 800/1251 ( 64%)]  Loss:  3.432491 (3.6080)  Time: 1.077s,  950.40/s  (1.091s,  938.47/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 850/1251 ( 68%)]  Loss:  3.592615 (3.6071)  Time: 1.078s,  949.61/s  (1.091s,  938.29/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 900/1251 ( 72%)]  Loss:  3.453965 (3.5991)  Time: 1.095s,  935.44/s  (1.091s,  938.77/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [ 950/1251 ( 76%)]  Loss:  3.792397 (3.6087)  Time: 1.080s,  948.35/s  (1.091s,  938.84/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [1000/1251 ( 80%)]  Loss:  3.997696 (3.6272)  Time: 1.078s,  949.55/s  (1.091s,  938.87/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [1050/1251 ( 84%)]  Loss:  3.823752 (3.6362)  Time: 1.078s,  949.78/s  (1.091s,  938.75/s)  LR: 8.513e-04  Data: 0.014 (0.013)
Train: 76 [1100/1251 ( 88%)]  Loss:  3.362785 (3.6243)  Time: 1.076s,  951.26/s  (1.091s,  938.76/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [1150/1251 ( 92%)]  Loss:  3.504953 (3.6193)  Time: 1.098s,  932.87/s  (1.091s,  938.83/s)  LR: 8.513e-04  Data: 0.013 (0.013)
Train: 76 [1200/1251 ( 96%)]  Loss:  3.574275 (3.6175)  Time: 1.106s,  926.16/s  (1.091s,  938.66/s)  LR: 8.513e-04  Data: 0.012 (0.013)
Train: 76 [1250/1251 (100%)]  Loss:  3.359479 (3.6076)  Time: 1.099s,  931.94/s  (1.091s,  938.72/s)  LR: 8.513e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.897 (5.897)  Loss:  0.7037 (0.7037)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7100 (1.1743)  Acc@1: 84.5519 (73.4380)  Acc@5: 96.8160 (92.0800)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-67.pth.tar', 72.9660000390625)

Train: 77 [   0/1251 (  0%)]  Loss:  3.442889 (3.4429)  Time: 1.090s,  939.30/s  (1.090s,  939.30/s)  LR: 8.476e-04  Data: 0.027 (0.027)
Train: 77 [  50/1251 (  4%)]  Loss:  3.731196 (3.5870)  Time: 1.086s,  942.52/s  (1.094s,  935.62/s)  LR: 8.476e-04  Data: 0.014 (0.013)
Train: 77 [ 100/1251 (  8%)]  Loss:  3.319989 (3.4980)  Time: 1.081s,  947.45/s  (1.096s,  934.19/s)  LR: 8.476e-04  Data: 0.017 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 77 [ 150/1251 ( 12%)]  Loss:  3.561285 (3.5138)  Time: 1.076s,  951.25/s  (1.094s,  935.96/s)  LR: 8.476e-04  Data: 0.013 (0.013)
Train: 77 [ 200/1251 ( 16%)]  Loss:  3.379018 (3.4869)  Time: 1.075s,  952.39/s  (1.093s,  936.66/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 250/1251 ( 20%)]  Loss:  3.985671 (3.5700)  Time: 1.079s,  949.00/s  (1.092s,  938.10/s)  LR: 8.476e-04  Data: 0.013 (0.013)
Train: 77 [ 300/1251 ( 24%)]  Loss:  3.733004 (3.5933)  Time: 1.079s,  948.66/s  (1.091s,  938.21/s)  LR: 8.476e-04  Data: 0.016 (0.013)
Train: 77 [ 350/1251 ( 28%)]  Loss:  3.696711 (3.6062)  Time: 1.096s,  934.46/s  (1.091s,  938.77/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 400/1251 ( 32%)]  Loss:  3.475959 (3.5917)  Time: 1.082s,  946.74/s  (1.091s,  939.01/s)  LR: 8.476e-04  Data: 0.015 (0.013)
Train: 77 [ 450/1251 ( 36%)]  Loss:  3.694681 (3.6020)  Time: 1.109s,  923.09/s  (1.091s,  938.56/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 500/1251 ( 40%)]  Loss:  3.573328 (3.5994)  Time: 1.081s,  947.15/s  (1.091s,  938.70/s)  LR: 8.476e-04  Data: 0.016 (0.013)
Train: 77 [ 550/1251 ( 44%)]  Loss:  3.450454 (3.5870)  Time: 1.095s,  935.25/s  (1.091s,  938.50/s)  LR: 8.476e-04  Data: 0.019 (0.013)
Train: 77 [ 600/1251 ( 48%)]  Loss:  3.654212 (3.5922)  Time: 1.094s,  936.11/s  (1.091s,  938.41/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 650/1251 ( 52%)]  Loss:  3.590810 (3.5921)  Time: 1.076s,  951.47/s  (1.091s,  938.56/s)  LR: 8.476e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 77 [ 700/1251 ( 56%)]  Loss:  3.581556 (3.5914)  Time: 1.098s,  932.38/s  (1.091s,  938.90/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 750/1251 ( 60%)]  Loss:  3.842563 (3.6071)  Time: 1.082s,  946.64/s  (1.090s,  939.09/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [ 800/1251 ( 64%)]  Loss:  3.375847 (3.5935)  Time: 1.079s,  949.00/s  (1.091s,  938.91/s)  LR: 8.476e-04  Data: 0.013 (0.013)
Train: 77 [ 850/1251 ( 68%)]  Loss:  3.600931 (3.5939)  Time: 1.084s,  944.83/s  (1.091s,  939.00/s)  LR: 8.476e-04  Data: 0.016 (0.013)
Train: 77 [ 900/1251 ( 72%)]  Loss:  3.549650 (3.5916)  Time: 1.222s,  837.67/s  (1.091s,  938.62/s)  LR: 8.476e-04  Data: 0.014 (0.013)
Train: 77 [ 950/1251 ( 76%)]  Loss:  3.720468 (3.5980)  Time: 1.111s,  921.55/s  (1.091s,  938.56/s)  LR: 8.476e-04  Data: 0.048 (0.013)
Train: 77 [1000/1251 ( 80%)]  Loss:  3.760220 (3.6057)  Time: 1.110s,  922.63/s  (1.091s,  938.80/s)  LR: 8.476e-04  Data: 0.012 (0.013)
Train: 77 [1050/1251 ( 84%)]  Loss:  3.582192 (3.6047)  Time: 1.103s,  928.44/s  (1.091s,  938.42/s)  LR: 8.476e-04  Data: 0.020 (0.013)
Train: 77 [1100/1251 ( 88%)]  Loss:  3.831405 (3.6145)  Time: 1.080s,  947.91/s  (1.091s,  938.41/s)  LR: 8.476e-04  Data: 0.013 (0.013)
Train: 77 [1150/1251 ( 92%)]  Loss:  3.862922 (3.6249)  Time: 1.080s,  947.76/s  (1.091s,  938.63/s)  LR: 8.476e-04  Data: 0.015 (0.013)
Train: 77 [1200/1251 ( 96%)]  Loss:  3.640240 (3.6255)  Time: 1.076s,  951.61/s  (1.091s,  938.52/s)  LR: 8.476e-04  Data: 0.013 (0.013)
Train: 77 [1250/1251 (100%)]  Loss:  3.784243 (3.6316)  Time: 1.062s,  964.47/s  (1.091s,  938.51/s)  LR: 8.476e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.888 (5.888)  Loss:  0.6340 (0.6340)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.7623 (1.1573)  Acc@1: 83.7264 (73.6420)  Acc@5: 96.2264 (92.2360)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-70.pth.tar', 72.99999999023437)

Train: 78 [   0/1251 (  0%)]  Loss:  3.517820 (3.5178)  Time: 1.094s,  935.94/s  (1.094s,  935.94/s)  LR: 8.439e-04  Data: 0.029 (0.029)
Train: 78 [  50/1251 (  4%)]  Loss:  3.688168 (3.6030)  Time: 1.102s,  929.11/s  (1.096s,  934.33/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 100/1251 (  8%)]  Loss:  3.742041 (3.6493)  Time: 1.097s,  933.67/s  (1.094s,  935.84/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 150/1251 ( 12%)]  Loss:  3.917902 (3.7165)  Time: 1.107s,  925.41/s  (1.095s,  935.03/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 200/1251 ( 16%)]  Loss:  3.572859 (3.6878)  Time: 1.099s,  931.45/s  (1.096s,  934.73/s)  LR: 8.439e-04  Data: 0.015 (0.013)
Train: 78 [ 250/1251 ( 20%)]  Loss:  3.412962 (3.6420)  Time: 1.096s,  934.08/s  (1.094s,  936.18/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 300/1251 ( 24%)]  Loss:  3.846893 (3.6712)  Time: 1.077s,  950.99/s  (1.093s,  936.87/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 350/1251 ( 28%)]  Loss:  3.506922 (3.6507)  Time: 1.081s,  947.53/s  (1.093s,  936.90/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 400/1251 ( 32%)]  Loss:  3.537288 (3.6381)  Time: 1.095s,  934.82/s  (1.093s,  936.99/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 450/1251 ( 36%)]  Loss:  3.389424 (3.6132)  Time: 1.095s,  934.79/s  (1.093s,  937.04/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 500/1251 ( 40%)]  Loss:  3.416326 (3.5953)  Time: 1.082s,  946.25/s  (1.092s,  937.36/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 550/1251 ( 44%)]  Loss:  3.498203 (3.5872)  Time: 1.078s,  949.64/s  (1.092s,  937.72/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 600/1251 ( 48%)]  Loss:  3.861829 (3.6084)  Time: 1.094s,  936.42/s  (1.092s,  937.96/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [ 650/1251 ( 52%)]  Loss:  3.629924 (3.6099)  Time: 1.161s,  881.71/s  (1.092s,  937.99/s)  LR: 8.439e-04  Data: 0.014 (0.013)
Train: 78 [ 700/1251 ( 56%)]  Loss:  3.678994 (3.6145)  Time: 1.081s,  946.89/s  (1.092s,  937.80/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 750/1251 ( 60%)]  Loss:  4.064900 (3.6427)  Time: 1.095s,  935.28/s  (1.092s,  937.95/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [ 800/1251 ( 64%)]  Loss:  3.817928 (3.6530)  Time: 1.076s,  951.28/s  (1.091s,  938.44/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [ 850/1251 ( 68%)]  Loss:  3.856993 (3.6643)  Time: 1.076s,  951.68/s  (1.091s,  938.22/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [ 900/1251 ( 72%)]  Loss:  3.767828 (3.6697)  Time: 1.078s,  949.96/s  (1.091s,  938.52/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [ 950/1251 ( 76%)]  Loss:  3.582099 (3.6654)  Time: 1.106s,  926.04/s  (1.091s,  938.70/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [1000/1251 ( 80%)]  Loss:  3.255286 (3.6458)  Time: 1.175s,  871.74/s  (1.091s,  938.42/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [1050/1251 ( 84%)]  Loss:  3.920032 (3.6583)  Time: 1.078s,  949.85/s  (1.091s,  938.39/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [1100/1251 ( 88%)]  Loss:  3.641395 (3.6576)  Time: 1.094s,  936.06/s  (1.091s,  938.21/s)  LR: 8.439e-04  Data: 0.012 (0.013)
Train: 78 [1150/1251 ( 92%)]  Loss:  3.373720 (3.6457)  Time: 1.078s,  949.77/s  (1.092s,  938.13/s)  LR: 8.439e-04  Data: 0.013 (0.013)
Train: 78 [1200/1251 ( 96%)]  Loss:  3.561440 (3.6424)  Time: 1.098s,  932.38/s  (1.092s,  937.85/s)  LR: 8.439e-04  Data: 0.016 (0.013)
Train: 78 [1250/1251 (100%)]  Loss:  3.297379 (3.6291)  Time: 1.089s,  940.74/s  (1.092s,  937.84/s)  LR: 8.439e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.886 (5.886)  Loss:  0.6343 (0.6343)  Acc@1: 86.8164 (86.8164)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.6913 (1.1631)  Acc@1: 85.6132 (73.7200)  Acc@5: 96.1085 (92.1620)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-68.pth.tar', 73.11800007324219)

Train: 79 [   0/1251 (  0%)]  Loss:  3.542919 (3.5429)  Time: 1.091s,  938.59/s  (1.091s,  938.59/s)  LR: 8.401e-04  Data: 0.027 (0.027)
Train: 79 [  50/1251 (  4%)]  Loss:  3.455101 (3.4990)  Time: 1.077s,  950.52/s  (1.084s,  945.06/s)  LR: 8.401e-04  Data: 0.012 (0.014)
Train: 79 [ 100/1251 (  8%)]  Loss:  3.924726 (3.6409)  Time: 1.083s,  945.91/s  (1.087s,  942.37/s)  LR: 8.401e-04  Data: 0.012 (0.014)
Train: 79 [ 150/1251 ( 12%)]  Loss:  3.619665 (3.6356)  Time: 1.114s,  919.46/s  (1.088s,  941.56/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 200/1251 ( 16%)]  Loss:  3.571179 (3.6227)  Time: 1.073s,  954.65/s  (1.088s,  941.40/s)  LR: 8.401e-04  Data: 0.011 (0.013)
Train: 79 [ 250/1251 ( 20%)]  Loss:  3.601646 (3.6192)  Time: 1.075s,  952.19/s  (1.089s,  940.04/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 300/1251 ( 24%)]  Loss:  3.676359 (3.6274)  Time: 1.095s,  935.20/s  (1.089s,  940.00/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 350/1251 ( 28%)]  Loss:  3.606311 (3.6247)  Time: 1.077s,  951.14/s  (1.089s,  939.96/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 400/1251 ( 32%)]  Loss:  3.552758 (3.6167)  Time: 1.088s,  941.04/s  (1.089s,  940.09/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [ 450/1251 ( 36%)]  Loss:  3.805226 (3.6356)  Time: 1.080s,  947.86/s  (1.090s,  939.74/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 500/1251 ( 40%)]  Loss:  3.717318 (3.6430)  Time: 1.080s,  948.30/s  (1.090s,  939.87/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 550/1251 ( 44%)]  Loss:  3.815362 (3.6574)  Time: 1.080s,  947.84/s  (1.089s,  940.24/s)  LR: 8.401e-04  Data: 0.014 (0.013)
Train: 79 [ 600/1251 ( 48%)]  Loss:  3.766556 (3.6658)  Time: 1.103s,  928.01/s  (1.089s,  940.30/s)  LR: 8.401e-04  Data: 0.015 (0.013)
Train: 79 [ 650/1251 ( 52%)]  Loss:  3.611476 (3.6619)  Time: 1.076s,  952.05/s  (1.089s,  940.28/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [ 700/1251 ( 56%)]  Loss:  3.588821 (3.6570)  Time: 1.109s,  923.66/s  (1.089s,  940.45/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 750/1251 ( 60%)]  Loss:  3.691137 (3.6592)  Time: 1.097s,  933.65/s  (1.089s,  940.67/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 800/1251 ( 64%)]  Loss:  3.482480 (3.6488)  Time: 1.078s,  950.19/s  (1.089s,  940.72/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [ 850/1251 ( 68%)]  Loss:  3.569082 (3.6443)  Time: 1.095s,  934.84/s  (1.088s,  940.78/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [ 900/1251 ( 72%)]  Loss:  3.463813 (3.6348)  Time: 1.082s,  946.00/s  (1.088s,  940.90/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [ 950/1251 ( 76%)]  Loss:  3.946007 (3.6504)  Time: 1.086s,  942.92/s  (1.088s,  940.95/s)  LR: 8.401e-04  Data: 0.015 (0.013)
Train: 79 [1000/1251 ( 80%)]  Loss:  3.811597 (3.6581)  Time: 1.079s,  949.28/s  (1.088s,  940.78/s)  LR: 8.401e-04  Data: 0.012 (0.013)
Train: 79 [1050/1251 ( 84%)]  Loss:  3.590610 (3.6550)  Time: 1.078s,  950.25/s  (1.089s,  940.62/s)  LR: 8.401e-04  Data: 0.013 (0.013)
Train: 79 [1100/1251 ( 88%)]  Loss:  3.327634 (3.6408)  Time: 1.084s,  944.41/s  (1.089s,  940.54/s)  LR: 8.401e-04  Data: 0.014 (0.013)
Train: 79 [1150/1251 ( 92%)]  Loss:  3.646325 (3.6410)  Time: 1.079s,  949.09/s  (1.089s,  940.16/s)  LR: 8.401e-04  Data: 0.015 (0.013)
Train: 79 [1200/1251 ( 96%)]  Loss:  3.544040 (3.6371)  Time: 1.079s,  948.63/s  (1.089s,  940.33/s)  LR: 8.401e-04  Data: 0.014 (0.013)
Train: 79 [1250/1251 (100%)]  Loss:  3.407650 (3.6283)  Time: 1.061s,  965.44/s  (1.089s,  940.37/s)  LR: 8.401e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.811 (5.811)  Loss:  0.5464 (0.5464)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.6856 (1.1275)  Acc@1: 83.8443 (73.8280)  Acc@5: 95.8726 (92.3100)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-72.pth.tar', 73.22600008789063)

Train: 80 [   0/1251 (  0%)]  Loss:  3.594979 (3.5950)  Time: 1.090s,  939.29/s  (1.090s,  939.29/s)  LR: 8.362e-04  Data: 0.026 (0.026)
Train: 80 [  50/1251 (  4%)]  Loss:  3.723906 (3.6594)  Time: 1.078s,  949.61/s  (1.091s,  938.38/s)  LR: 8.362e-04  Data: 0.013 (0.014)
Train: 80 [ 100/1251 (  8%)]  Loss:  4.020662 (3.7798)  Time: 1.100s,  930.78/s  (1.092s,  938.15/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 150/1251 ( 12%)]  Loss:  3.236183 (3.6439)  Time: 1.103s,  928.44/s  (1.092s,  937.66/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 200/1251 ( 16%)]  Loss:  3.471885 (3.6095)  Time: 1.172s,  873.71/s  (1.092s,  937.81/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [ 250/1251 ( 20%)]  Loss:  3.744074 (3.6319)  Time: 1.077s,  950.73/s  (1.092s,  937.44/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [ 300/1251 ( 24%)]  Loss:  3.478085 (3.6100)  Time: 1.103s,  928.27/s  (1.093s,  936.79/s)  LR: 8.362e-04  Data: 0.011 (0.013)
Train: 80 [ 350/1251 ( 28%)]  Loss:  3.830487 (3.6375)  Time: 1.104s,  927.39/s  (1.094s,  936.31/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 400/1251 ( 32%)]  Loss:  3.391621 (3.6102)  Time: 1.085s,  943.36/s  (1.094s,  935.98/s)  LR: 8.362e-04  Data: 0.020 (0.013)
Train: 80 [ 450/1251 ( 36%)]  Loss:  3.855826 (3.6348)  Time: 1.078s,  949.51/s  (1.094s,  936.42/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 500/1251 ( 40%)]  Loss:  3.532659 (3.6255)  Time: 1.084s,  944.63/s  (1.093s,  937.08/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 550/1251 ( 44%)]  Loss:  3.751683 (3.6360)  Time: 1.204s,  850.32/s  (1.092s,  937.37/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [ 600/1251 ( 48%)]  Loss:  3.866828 (3.6538)  Time: 1.097s,  933.40/s  (1.093s,  936.67/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 650/1251 ( 52%)]  Loss:  3.900975 (3.6714)  Time: 1.078s,  950.17/s  (1.093s,  937.02/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [ 700/1251 ( 56%)]  Loss:  3.773702 (3.6782)  Time: 1.094s,  935.99/s  (1.092s,  937.32/s)  LR: 8.362e-04  Data: 0.015 (0.013)
Train: 80 [ 750/1251 ( 60%)]  Loss:  3.695677 (3.6793)  Time: 1.080s,  947.78/s  (1.093s,  937.10/s)  LR: 8.362e-04  Data: 0.015 (0.013)
Train: 80 [ 800/1251 ( 64%)]  Loss:  3.580542 (3.6735)  Time: 1.092s,  938.13/s  (1.092s,  937.49/s)  LR: 8.362e-04  Data: 0.015 (0.013)
Train: 80 [ 850/1251 ( 68%)]  Loss:  3.829526 (3.6822)  Time: 1.093s,  936.89/s  (1.092s,  937.47/s)  LR: 8.362e-04  Data: 0.014 (0.013)
Train: 80 [ 900/1251 ( 72%)]  Loss:  3.375278 (3.6660)  Time: 1.099s,  931.50/s  (1.092s,  937.37/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 80 [ 950/1251 ( 76%)]  Loss:  3.866101 (3.6760)  Time: 1.086s,  942.74/s  (1.092s,  937.44/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [1000/1251 ( 80%)]  Loss:  3.843465 (3.6840)  Time: 1.078s,  950.11/s  (1.092s,  937.85/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [1050/1251 ( 84%)]  Loss:  3.760994 (3.6875)  Time: 1.078s,  950.34/s  (1.092s,  937.98/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [1100/1251 ( 88%)]  Loss:  4.016299 (3.7018)  Time: 1.079s,  948.98/s  (1.091s,  938.19/s)  LR: 8.362e-04  Data: 0.012 (0.013)
Train: 80 [1150/1251 ( 92%)]  Loss:  3.582150 (3.6968)  Time: 1.096s,  933.91/s  (1.091s,  938.16/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [1200/1251 ( 96%)]  Loss:  3.357361 (3.6832)  Time: 1.080s,  947.96/s  (1.092s,  938.11/s)  LR: 8.362e-04  Data: 0.013 (0.013)
Train: 80 [1250/1251 (100%)]  Loss:  3.799811 (3.6877)  Time: 1.062s,  963.88/s  (1.091s,  938.25/s)  LR: 8.362e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.866 (5.866)  Loss:  0.6284 (0.6284)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7465 (1.1854)  Acc@1: 82.7830 (73.6020)  Acc@5: 95.8726 (92.1000)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-74.pth.tar', 73.25000003662109)

Train: 81 [   0/1251 (  0%)]  Loss:  3.687440 (3.6874)  Time: 1.092s,  937.84/s  (1.092s,  937.84/s)  LR: 8.323e-04  Data: 0.026 (0.026)
Train: 81 [  50/1251 (  4%)]  Loss:  3.551718 (3.6196)  Time: 1.079s,  948.64/s  (1.093s,  936.71/s)  LR: 8.323e-04  Data: 0.015 (0.014)
Train: 81 [ 100/1251 (  8%)]  Loss:  3.730707 (3.6566)  Time: 1.076s,  951.38/s  (1.089s,  940.29/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 150/1251 ( 12%)]  Loss:  3.527389 (3.6243)  Time: 1.122s,  912.94/s  (1.089s,  940.52/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 200/1251 ( 16%)]  Loss:  3.616623 (3.6228)  Time: 1.098s,  932.65/s  (1.089s,  940.36/s)  LR: 8.323e-04  Data: 0.015 (0.013)
Train: 81 [ 250/1251 ( 20%)]  Loss:  3.546898 (3.6101)  Time: 1.095s,  935.41/s  (1.091s,  938.53/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 300/1251 ( 24%)]  Loss:  3.707813 (3.6241)  Time: 1.079s,  948.85/s  (1.091s,  939.01/s)  LR: 8.323e-04  Data: 0.016 (0.013)
Train: 81 [ 350/1251 ( 28%)]  Loss:  3.793436 (3.6453)  Time: 1.094s,  935.69/s  (1.091s,  938.45/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 400/1251 ( 32%)]  Loss:  3.644276 (3.6451)  Time: 1.096s,  934.69/s  (1.091s,  938.88/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 450/1251 ( 36%)]  Loss:  3.823699 (3.6630)  Time: 1.081s,  947.09/s  (1.091s,  939.01/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 500/1251 ( 40%)]  Loss:  3.886162 (3.6833)  Time: 1.172s,  873.96/s  (1.091s,  938.70/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 550/1251 ( 44%)]  Loss:  3.590202 (3.6755)  Time: 1.104s,  927.38/s  (1.090s,  939.11/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 600/1251 ( 48%)]  Loss:  3.608866 (3.6704)  Time: 1.108s,  924.38/s  (1.090s,  939.09/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 650/1251 ( 52%)]  Loss:  3.706390 (3.6730)  Time: 1.076s,  951.28/s  (1.090s,  939.07/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 700/1251 ( 56%)]  Loss:  3.836672 (3.6839)  Time: 1.096s,  934.45/s  (1.091s,  938.90/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 750/1251 ( 60%)]  Loss:  3.916435 (3.6984)  Time: 1.099s,  931.65/s  (1.091s,  938.68/s)  LR: 8.323e-04  Data: 0.015 (0.013)
Train: 81 [ 800/1251 ( 64%)]  Loss:  3.798977 (3.7043)  Time: 1.078s,  950.23/s  (1.091s,  938.49/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [ 850/1251 ( 68%)]  Loss:  3.839045 (3.7118)  Time: 1.187s,  862.82/s  (1.091s,  938.59/s)  LR: 8.323e-04  Data: 0.014 (0.013)
Train: 81 [ 900/1251 ( 72%)]  Loss:  3.632312 (3.7076)  Time: 1.084s,  944.89/s  (1.091s,  938.48/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [ 950/1251 ( 76%)]  Loss:  3.571769 (3.7008)  Time: 1.089s,  940.60/s  (1.091s,  938.64/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [1000/1251 ( 80%)]  Loss:  3.535327 (3.6930)  Time: 1.136s,  901.03/s  (1.091s,  938.49/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [1050/1251 ( 84%)]  Loss:  3.585296 (3.6881)  Time: 1.112s,  920.88/s  (1.091s,  938.28/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [1100/1251 ( 88%)]  Loss:  3.594784 (3.6840)  Time: 1.095s,  934.85/s  (1.091s,  938.36/s)  LR: 8.323e-04  Data: 0.013 (0.013)
Train: 81 [1150/1251 ( 92%)]  Loss:  3.690379 (3.6843)  Time: 1.101s,  930.19/s  (1.091s,  938.63/s)  LR: 8.323e-04  Data: 0.012 (0.013)
Train: 81 [1200/1251 ( 96%)]  Loss:  3.736349 (3.6864)  Time: 1.101s,  930.24/s  (1.091s,  938.65/s)  LR: 8.323e-04  Data: 0.016 (0.013)
Train: 81 [1250/1251 (100%)]  Loss:  3.931873 (3.6958)  Time: 1.079s,  949.24/s  (1.091s,  938.45/s)  LR: 8.323e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.813 (5.813)  Loss:  0.6265 (0.6265)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6998 (1.1642)  Acc@1: 85.0236 (73.9700)  Acc@5: 96.9340 (92.2060)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-69.pth.tar', 73.3419999609375)

Train: 82 [   0/1251 (  0%)]  Loss:  3.546758 (3.5468)  Time: 1.087s,  942.44/s  (1.087s,  942.44/s)  LR: 8.284e-04  Data: 0.024 (0.024)
Train: 82 [  50/1251 (  4%)]  Loss:  3.811394 (3.6791)  Time: 1.079s,  948.89/s  (1.085s,  943.73/s)  LR: 8.284e-04  Data: 0.013 (0.014)
Train: 82 [ 100/1251 (  8%)]  Loss:  3.655338 (3.6712)  Time: 1.106s,  926.13/s  (1.090s,  939.82/s)  LR: 8.284e-04  Data: 0.013 (0.014)
Train: 82 [ 150/1251 ( 12%)]  Loss:  3.491416 (3.6262)  Time: 1.082s,  946.25/s  (1.092s,  937.74/s)  LR: 8.284e-04  Data: 0.012 (0.014)
Train: 82 [ 200/1251 ( 16%)]  Loss:  3.961707 (3.6933)  Time: 1.099s,  931.83/s  (1.091s,  938.84/s)  LR: 8.284e-04  Data: 0.017 (0.013)
Train: 82 [ 250/1251 ( 20%)]  Loss:  3.545898 (3.6688)  Time: 1.079s,  949.20/s  (1.090s,  939.60/s)  LR: 8.284e-04  Data: 0.014 (0.013)
Train: 82 [ 300/1251 ( 24%)]  Loss:  3.940681 (3.7076)  Time: 1.082s,  946.10/s  (1.091s,  938.91/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [ 350/1251 ( 28%)]  Loss:  3.763957 (3.7146)  Time: 1.082s,  945.98/s  (1.091s,  938.94/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 400/1251 ( 32%)]  Loss:  3.974453 (3.7435)  Time: 1.077s,  950.45/s  (1.090s,  939.47/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 82 [ 450/1251 ( 36%)]  Loss:  3.570994 (3.7263)  Time: 1.080s,  948.58/s  (1.090s,  939.38/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 500/1251 ( 40%)]  Loss:  3.665416 (3.7207)  Time: 1.096s,  934.05/s  (1.090s,  939.15/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [ 550/1251 ( 44%)]  Loss:  3.377679 (3.6921)  Time: 1.095s,  935.35/s  (1.090s,  939.32/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 600/1251 ( 48%)]  Loss:  3.649626 (3.6889)  Time: 1.102s,  928.83/s  (1.090s,  939.38/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 650/1251 ( 52%)]  Loss:  3.700596 (3.6897)  Time: 1.080s,  948.21/s  (1.090s,  939.59/s)  LR: 8.284e-04  Data: 0.016 (0.013)
Train: 82 [ 700/1251 ( 56%)]  Loss:  3.659727 (3.6877)  Time: 1.096s,  934.51/s  (1.090s,  939.76/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [ 750/1251 ( 60%)]  Loss:  3.806595 (3.6951)  Time: 1.095s,  934.81/s  (1.090s,  939.88/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [ 800/1251 ( 64%)]  Loss:  3.335657 (3.6740)  Time: 1.089s,  940.01/s  (1.090s,  939.83/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [ 850/1251 ( 68%)]  Loss:  3.822781 (3.6823)  Time: 1.079s,  949.33/s  (1.089s,  939.91/s)  LR: 8.284e-04  Data: 0.016 (0.013)
Train: 82 [ 900/1251 ( 72%)]  Loss:  3.647373 (3.6804)  Time: 1.078s,  949.62/s  (1.089s,  940.07/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [ 950/1251 ( 76%)]  Loss:  3.332591 (3.6630)  Time: 1.075s,  952.30/s  (1.089s,  940.12/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [1000/1251 ( 80%)]  Loss:  3.634831 (3.6617)  Time: 1.078s,  949.73/s  (1.089s,  940.12/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [1050/1251 ( 84%)]  Loss:  3.972131 (3.6758)  Time: 1.098s,  932.76/s  (1.090s,  939.84/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [1100/1251 ( 88%)]  Loss:  3.380201 (3.6629)  Time: 1.106s,  925.50/s  (1.089s,  940.05/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [1150/1251 ( 92%)]  Loss:  3.671914 (3.6633)  Time: 1.098s,  932.55/s  (1.089s,  940.08/s)  LR: 8.284e-04  Data: 0.012 (0.013)
Train: 82 [1200/1251 ( 96%)]  Loss:  3.918757 (3.6735)  Time: 1.078s,  949.59/s  (1.089s,  940.15/s)  LR: 8.284e-04  Data: 0.013 (0.013)
Train: 82 [1250/1251 (100%)]  Loss:  4.074671 (3.6890)  Time: 1.064s,  962.36/s  (1.089s,  940.03/s)  LR: 8.284e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.847 (5.847)  Loss:  0.6122 (0.6122)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.7380 (1.1493)  Acc@1: 83.2547 (73.9820)  Acc@5: 95.8726 (92.1620)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-76.pth.tar', 73.43800014160156)

Train: 83 [   0/1251 (  0%)]  Loss:  3.758163 (3.7582)  Time: 1.087s,  941.80/s  (1.087s,  941.80/s)  LR: 8.245e-04  Data: 0.023 (0.023)
Train: 83 [  50/1251 (  4%)]  Loss:  3.752361 (3.7553)  Time: 1.100s,  930.81/s  (1.091s,  938.68/s)  LR: 8.245e-04  Data: 0.016 (0.014)
Train: 83 [ 100/1251 (  8%)]  Loss:  4.079153 (3.8632)  Time: 1.097s,  933.80/s  (1.091s,  938.48/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 150/1251 ( 12%)]  Loss:  3.852223 (3.8605)  Time: 1.082s,  946.74/s  (1.090s,  939.04/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 200/1251 ( 16%)]  Loss:  3.594245 (3.8072)  Time: 1.151s,  889.42/s  (1.092s,  937.56/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [ 250/1251 ( 20%)]  Loss:  3.828894 (3.8108)  Time: 1.096s,  934.44/s  (1.094s,  936.33/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 300/1251 ( 24%)]  Loss:  4.087831 (3.8504)  Time: 1.092s,  937.41/s  (1.092s,  937.33/s)  LR: 8.245e-04  Data: 0.021 (0.013)
Train: 83 [ 350/1251 ( 28%)]  Loss:  3.765184 (3.8398)  Time: 1.096s,  934.49/s  (1.092s,  937.33/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [ 400/1251 ( 32%)]  Loss:  3.528468 (3.8052)  Time: 1.083s,  945.77/s  (1.092s,  937.99/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 450/1251 ( 36%)]  Loss:  3.725982 (3.7973)  Time: 1.080s,  947.85/s  (1.092s,  938.08/s)  LR: 8.245e-04  Data: 0.014 (0.013)
Train: 83 [ 500/1251 ( 40%)]  Loss:  3.901556 (3.8067)  Time: 1.082s,  946.10/s  (1.092s,  938.10/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 550/1251 ( 44%)]  Loss:  3.156219 (3.7525)  Time: 1.078s,  950.21/s  (1.091s,  938.21/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 600/1251 ( 48%)]  Loss:  3.977244 (3.7698)  Time: 1.081s,  947.66/s  (1.091s,  938.22/s)  LR: 8.245e-04  Data: 0.015 (0.013)
Train: 83 [ 650/1251 ( 52%)]  Loss:  3.931057 (3.7813)  Time: 1.202s,  852.22/s  (1.091s,  938.31/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [ 700/1251 ( 56%)]  Loss:  3.959156 (3.7932)  Time: 1.073s,  954.21/s  (1.091s,  938.26/s)  LR: 8.245e-04  Data: 0.011 (0.013)
Train: 83 [ 750/1251 ( 60%)]  Loss:  3.779833 (3.7923)  Time: 1.077s,  950.48/s  (1.091s,  938.23/s)  LR: 8.245e-04  Data: 0.015 (0.013)
Train: 83 [ 800/1251 ( 64%)]  Loss:  3.530473 (3.7769)  Time: 1.079s,  949.33/s  (1.091s,  938.67/s)  LR: 8.245e-04  Data: 0.015 (0.013)
Train: 83 [ 850/1251 ( 68%)]  Loss:  3.683863 (3.7718)  Time: 1.099s,  931.36/s  (1.091s,  938.64/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [ 900/1251 ( 72%)]  Loss:  3.811013 (3.7738)  Time: 1.080s,  948.15/s  (1.091s,  938.68/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [ 950/1251 ( 76%)]  Loss:  3.577469 (3.7640)  Time: 1.096s,  934.11/s  (1.091s,  938.73/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [1000/1251 ( 80%)]  Loss:  3.739232 (3.7628)  Time: 1.077s,  950.63/s  (1.091s,  938.86/s)  LR: 8.245e-04  Data: 0.013 (0.013)
Train: 83 [1050/1251 ( 84%)]  Loss:  3.962656 (3.7719)  Time: 1.078s,  949.80/s  (1.091s,  938.96/s)  LR: 8.245e-04  Data: 0.014 (0.013)
Train: 83 [1100/1251 ( 88%)]  Loss:  3.629058 (3.7657)  Time: 1.098s,  932.75/s  (1.091s,  938.96/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Train: 83 [1150/1251 ( 92%)]  Loss:  3.331435 (3.7476)  Time: 1.095s,  934.95/s  (1.090s,  939.20/s)  LR: 8.245e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 83 [1200/1251 ( 96%)]  Loss:  3.679317 (3.7449)  Time: 1.079s,  949.22/s  (1.090s,  939.28/s)  LR: 8.245e-04  Data: 0.015 (0.013)
Train: 83 [1250/1251 (100%)]  Loss:  3.939334 (3.7524)  Time: 1.081s,  947.46/s  (1.090s,  939.12/s)  LR: 8.245e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.845 (5.845)  Loss:  0.5926 (0.5926)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6908 (1.1477)  Acc@1: 84.5519 (73.7740)  Acc@5: 96.3443 (92.2140)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-75.pth.tar', 73.53400006591797)

Train: 84 [   0/1251 (  0%)]  Loss:  3.423748 (3.4237)  Time: 1.089s,  940.44/s  (1.089s,  940.44/s)  LR: 8.205e-04  Data: 0.025 (0.025)
Train: 84 [  50/1251 (  4%)]  Loss:  3.816128 (3.6199)  Time: 1.081s,  947.52/s  (1.086s,  943.15/s)  LR: 8.205e-04  Data: 0.017 (0.014)
Train: 84 [ 100/1251 (  8%)]  Loss:  3.157710 (3.4659)  Time: 1.103s,  928.57/s  (1.087s,  942.14/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [ 150/1251 ( 12%)]  Loss:  3.684801 (3.5206)  Time: 1.091s,  938.58/s  (1.086s,  942.58/s)  LR: 8.205e-04  Data: 0.014 (0.013)
Train: 84 [ 200/1251 ( 16%)]  Loss:  3.803374 (3.5772)  Time: 1.080s,  947.87/s  (1.087s,  942.06/s)  LR: 8.205e-04  Data: 0.018 (0.013)
Train: 84 [ 250/1251 ( 20%)]  Loss:  3.559728 (3.5742)  Time: 1.097s,  933.04/s  (1.088s,  941.41/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [ 300/1251 ( 24%)]  Loss:  3.654764 (3.5858)  Time: 1.182s,  866.16/s  (1.088s,  940.98/s)  LR: 8.205e-04  Data: 0.014 (0.013)
Train: 84 [ 350/1251 ( 28%)]  Loss:  3.879771 (3.6225)  Time: 1.082s,  946.68/s  (1.088s,  941.39/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [ 400/1251 ( 32%)]  Loss:  3.921499 (3.6557)  Time: 1.076s,  951.25/s  (1.088s,  940.85/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [ 450/1251 ( 36%)]  Loss:  3.810666 (3.6712)  Time: 1.080s,  947.86/s  (1.089s,  940.44/s)  LR: 8.205e-04  Data: 0.014 (0.013)
Train: 84 [ 500/1251 ( 40%)]  Loss:  3.825978 (3.6853)  Time: 1.098s,  932.57/s  (1.089s,  940.35/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [ 550/1251 ( 44%)]  Loss:  3.687235 (3.6855)  Time: 1.076s,  951.85/s  (1.089s,  940.33/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [ 600/1251 ( 48%)]  Loss:  3.872779 (3.6999)  Time: 1.084s,  944.84/s  (1.089s,  940.51/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [ 650/1251 ( 52%)]  Loss:  3.894036 (3.7137)  Time: 1.095s,  935.08/s  (1.089s,  939.97/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [ 700/1251 ( 56%)]  Loss:  3.601944 (3.7063)  Time: 1.095s,  935.09/s  (1.090s,  939.83/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [ 750/1251 ( 60%)]  Loss:  3.845630 (3.7150)  Time: 1.079s,  948.91/s  (1.089s,  939.89/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [ 800/1251 ( 64%)]  Loss:  3.540066 (3.7047)  Time: 1.096s,  934.04/s  (1.089s,  939.91/s)  LR: 8.205e-04  Data: 0.020 (0.013)
Train: 84 [ 850/1251 ( 68%)]  Loss:  3.962016 (3.7190)  Time: 1.083s,  945.89/s  (1.089s,  939.97/s)  LR: 8.205e-04  Data: 0.015 (0.013)
Train: 84 [ 900/1251 ( 72%)]  Loss:  3.521007 (3.7086)  Time: 1.079s,  948.72/s  (1.089s,  939.91/s)  LR: 8.205e-04  Data: 0.016 (0.013)
Train: 84 [ 950/1251 ( 76%)]  Loss:  3.487972 (3.6975)  Time: 1.084s,  944.95/s  (1.089s,  940.02/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [1000/1251 ( 80%)]  Loss:  3.651333 (3.6953)  Time: 1.077s,  950.52/s  (1.089s,  940.01/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [1050/1251 ( 84%)]  Loss:  3.864648 (3.7030)  Time: 1.078s,  950.16/s  (1.089s,  940.24/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [1100/1251 ( 88%)]  Loss:  3.728773 (3.7042)  Time: 1.078s,  949.58/s  (1.089s,  940.30/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [1150/1251 ( 92%)]  Loss:  3.906123 (3.7126)  Time: 1.077s,  950.69/s  (1.089s,  940.51/s)  LR: 8.205e-04  Data: 0.013 (0.013)
Train: 84 [1200/1251 ( 96%)]  Loss:  3.277513 (3.6952)  Time: 1.109s,  923.26/s  (1.089s,  940.48/s)  LR: 8.205e-04  Data: 0.012 (0.013)
Train: 84 [1250/1251 (100%)]  Loss:  3.600389 (3.6915)  Time: 1.070s,  956.85/s  (1.089s,  940.45/s)  LR: 8.205e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.866 (5.866)  Loss:  0.6101 (0.6101)  Acc@1: 87.1094 (87.1094)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.6574 (1.1478)  Acc@1: 85.1415 (74.1360)  Acc@5: 96.8160 (92.2620)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-73.pth.tar', 73.56199994140626)

Train: 85 [   0/1251 (  0%)]  Loss:  3.464433 (3.4644)  Time: 1.105s,  926.42/s  (1.105s,  926.42/s)  LR: 8.165e-04  Data: 0.023 (0.023)
Train: 85 [  50/1251 (  4%)]  Loss:  3.521011 (3.4927)  Time: 1.098s,  932.29/s  (1.093s,  936.68/s)  LR: 8.165e-04  Data: 0.012 (0.014)
Train: 85 [ 100/1251 (  8%)]  Loss:  3.699444 (3.5616)  Time: 1.078s,  949.48/s  (1.092s,  937.36/s)  LR: 8.165e-04  Data: 0.013 (0.014)
Train: 85 [ 150/1251 ( 12%)]  Loss:  3.439552 (3.5311)  Time: 1.078s,  950.02/s  (1.093s,  937.00/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [ 200/1251 ( 16%)]  Loss:  3.743513 (3.5736)  Time: 1.078s,  950.32/s  (1.091s,  938.83/s)  LR: 8.165e-04  Data: 0.013 (0.013)
Train: 85 [ 250/1251 ( 20%)]  Loss:  3.941407 (3.6349)  Time: 1.077s,  950.80/s  (1.090s,  939.51/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [ 300/1251 ( 24%)]  Loss:  3.581057 (3.6272)  Time: 1.175s,  871.43/s  (1.091s,  938.98/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [ 350/1251 ( 28%)]  Loss:  3.514703 (3.6131)  Time: 1.125s,  910.21/s  (1.090s,  939.35/s)  LR: 8.165e-04  Data: 0.015 (0.013)
Train: 85 [ 400/1251 ( 32%)]  Loss:  3.267319 (3.5747)  Time: 1.081s,  946.92/s  (1.089s,  939.92/s)  LR: 8.165e-04  Data: 0.016 (0.013)
Train: 85 [ 450/1251 ( 36%)]  Loss:  3.849105 (3.6022)  Time: 1.097s,  933.83/s  (1.090s,  939.77/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [ 500/1251 ( 40%)]  Loss:  4.028324 (3.6409)  Time: 1.084s,  945.07/s  (1.090s,  939.34/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [ 550/1251 ( 44%)]  Loss:  3.498075 (3.6290)  Time: 1.077s,  950.36/s  (1.090s,  939.28/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [ 600/1251 ( 48%)]  Loss:  3.599234 (3.6267)  Time: 1.083s,  945.37/s  (1.091s,  938.95/s)  LR: 8.165e-04  Data: 0.013 (0.013)
Train: 85 [ 650/1251 ( 52%)]  Loss:  3.840361 (3.6420)  Time: 1.099s,  931.43/s  (1.091s,  938.68/s)  LR: 8.165e-04  Data: 0.016 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 85 [ 700/1251 ( 56%)]  Loss:  3.652955 (3.6427)  Time: 1.080s,  948.43/s  (1.091s,  938.63/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [ 750/1251 ( 60%)]  Loss:  3.920277 (3.6600)  Time: 1.078s,  949.93/s  (1.091s,  938.74/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [ 800/1251 ( 64%)]  Loss:  3.853777 (3.6714)  Time: 1.088s,  940.84/s  (1.091s,  938.95/s)  LR: 8.165e-04  Data: 0.016 (0.013)
Train: 85 [ 850/1251 ( 68%)]  Loss:  3.530809 (3.6636)  Time: 1.078s,  950.06/s  (1.090s,  939.24/s)  LR: 8.165e-04  Data: 0.015 (0.013)
Train: 85 [ 900/1251 ( 72%)]  Loss:  3.232951 (3.6410)  Time: 1.079s,  948.93/s  (1.090s,  939.46/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [ 950/1251 ( 76%)]  Loss:  3.636197 (3.6407)  Time: 1.093s,  936.47/s  (1.090s,  939.60/s)  LR: 8.165e-04  Data: 0.012 (0.013)
Train: 85 [1000/1251 ( 80%)]  Loss:  3.576693 (3.6377)  Time: 1.081s,  947.35/s  (1.090s,  939.39/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [1050/1251 ( 84%)]  Loss:  3.460349 (3.6296)  Time: 1.100s,  931.11/s  (1.090s,  939.22/s)  LR: 8.165e-04  Data: 0.015 (0.013)
Train: 85 [1100/1251 ( 88%)]  Loss:  3.590455 (3.6279)  Time: 1.082s,  946.23/s  (1.090s,  939.05/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [1150/1251 ( 92%)]  Loss:  3.370870 (3.6172)  Time: 1.084s,  944.50/s  (1.090s,  939.20/s)  LR: 8.165e-04  Data: 0.013 (0.013)
Train: 85 [1200/1251 ( 96%)]  Loss:  3.469432 (3.6113)  Time: 1.080s,  947.83/s  (1.090s,  939.31/s)  LR: 8.165e-04  Data: 0.014 (0.013)
Train: 85 [1250/1251 (100%)]  Loss:  4.138577 (3.6316)  Time: 1.076s,  952.06/s  (1.090s,  939.14/s)  LR: 8.165e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.842 (5.842)  Loss:  0.6090 (0.6090)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.448)  Loss:  0.7542 (1.1443)  Acc@1: 84.3160 (74.0460)  Acc@5: 95.4009 (92.2060)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 74.0460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-71.pth.tar', 73.58800000732423)

Train: 86 [   0/1251 (  0%)]  Loss:  3.636921 (3.6369)  Time: 1.087s,  941.69/s  (1.087s,  941.69/s)  LR: 8.125e-04  Data: 0.023 (0.023)
Train: 86 [  50/1251 (  4%)]  Loss:  3.636210 (3.6366)  Time: 1.095s,  935.12/s  (1.094s,  936.43/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 100/1251 (  8%)]  Loss:  3.594513 (3.6225)  Time: 1.102s,  929.41/s  (1.091s,  938.45/s)  LR: 8.125e-04  Data: 0.014 (0.013)
Train: 86 [ 150/1251 ( 12%)]  Loss:  3.685442 (3.6383)  Time: 1.172s,  873.78/s  (1.091s,  938.80/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [ 200/1251 ( 16%)]  Loss:  3.780356 (3.6667)  Time: 1.079s,  949.02/s  (1.092s,  937.99/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 250/1251 ( 20%)]  Loss:  3.282455 (3.6026)  Time: 1.077s,  951.15/s  (1.090s,  939.09/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [ 300/1251 ( 24%)]  Loss:  3.523067 (3.5913)  Time: 1.094s,  936.17/s  (1.090s,  939.67/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 350/1251 ( 28%)]  Loss:  3.996216 (3.6419)  Time: 1.186s,  863.40/s  (1.091s,  938.27/s)  LR: 8.125e-04  Data: 0.011 (0.013)
Train: 86 [ 400/1251 ( 32%)]  Loss:  3.366139 (3.6113)  Time: 1.097s,  933.35/s  (1.091s,  938.79/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [ 450/1251 ( 36%)]  Loss:  3.487842 (3.5989)  Time: 1.094s,  935.83/s  (1.091s,  938.22/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 500/1251 ( 40%)]  Loss:  3.392886 (3.5802)  Time: 1.094s,  935.96/s  (1.091s,  938.16/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 550/1251 ( 44%)]  Loss:  3.628371 (3.5842)  Time: 1.079s,  949.41/s  (1.092s,  938.03/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [ 600/1251 ( 48%)]  Loss:  3.236478 (3.5575)  Time: 1.099s,  931.50/s  (1.092s,  937.74/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 650/1251 ( 52%)]  Loss:  3.701425 (3.5677)  Time: 1.076s,  951.47/s  (1.092s,  938.00/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [ 700/1251 ( 56%)]  Loss:  3.672873 (3.5747)  Time: 1.080s,  948.22/s  (1.091s,  938.57/s)  LR: 8.125e-04  Data: 0.014 (0.013)
Train: 86 [ 750/1251 ( 60%)]  Loss:  3.123429 (3.5465)  Time: 1.082s,  946.35/s  (1.091s,  938.84/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 800/1251 ( 64%)]  Loss:  3.668182 (3.5537)  Time: 1.085s,  944.20/s  (1.091s,  938.68/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 850/1251 ( 68%)]  Loss:  3.677947 (3.5606)  Time: 1.079s,  948.65/s  (1.091s,  938.68/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 900/1251 ( 72%)]  Loss:  3.745448 (3.5703)  Time: 1.079s,  949.40/s  (1.091s,  938.76/s)  LR: 8.125e-04  Data: 0.012 (0.013)
Train: 86 [ 950/1251 ( 76%)]  Loss:  3.635803 (3.5736)  Time: 1.096s,  934.12/s  (1.091s,  938.75/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [1000/1251 ( 80%)]  Loss:  3.518014 (3.5710)  Time: 1.097s,  933.53/s  (1.091s,  938.73/s)  LR: 8.125e-04  Data: 0.016 (0.013)
Train: 86 [1050/1251 ( 84%)]  Loss:  3.540709 (3.5696)  Time: 1.078s,  949.91/s  (1.091s,  938.84/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [1100/1251 ( 88%)]  Loss:  3.723410 (3.5763)  Time: 1.082s,  946.13/s  (1.091s,  938.96/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [1150/1251 ( 92%)]  Loss:  3.366584 (3.5675)  Time: 1.080s,  948.14/s  (1.091s,  938.81/s)  LR: 8.125e-04  Data: 0.014 (0.013)
Train: 86 [1200/1251 ( 96%)]  Loss:  3.921528 (3.5817)  Time: 1.077s,  951.00/s  (1.091s,  938.60/s)  LR: 8.125e-04  Data: 0.013 (0.013)
Train: 86 [1250/1251 (100%)]  Loss:  3.767419 (3.5888)  Time: 1.078s,  949.51/s  (1.091s,  938.62/s)  LR: 8.125e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.827 (5.827)  Loss:  0.6371 (0.6371)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6974 (1.1321)  Acc@1: 84.3160 (73.9640)  Acc@5: 96.5802 (92.2900)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 74.0460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 73.9640000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-80.pth.tar', 73.60200001953125)

Train: 87 [   0/1251 (  0%)]  Loss:  3.511140 (3.5111)  Time: 1.101s,  930.29/s  (1.101s,  930.29/s)  LR: 8.084e-04  Data: 0.030 (0.030)
Train: 87 [  50/1251 (  4%)]  Loss:  3.631500 (3.5713)  Time: 1.077s,  950.88/s  (1.093s,  937.06/s)  LR: 8.084e-04  Data: 0.013 (0.013)
Train: 87 [ 100/1251 (  8%)]  Loss:  3.600820 (3.5812)  Time: 1.079s,  949.22/s  (1.091s,  938.68/s)  LR: 8.084e-04  Data: 0.016 (0.014)
Train: 87 [ 150/1251 ( 12%)]  Loss:  3.461214 (3.5512)  Time: 1.101s,  930.15/s  (1.090s,  939.45/s)  LR: 8.084e-04  Data: 0.013 (0.014)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 87 [ 200/1251 ( 16%)]  Loss:  3.524905 (3.5459)  Time: 1.095s,  935.29/s  (1.091s,  938.62/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 250/1251 ( 20%)]  Loss:  3.646876 (3.5627)  Time: 1.081s,  946.89/s  (1.091s,  938.63/s)  LR: 8.084e-04  Data: 0.013 (0.013)
Train: 87 [ 300/1251 ( 24%)]  Loss:  4.005635 (3.6260)  Time: 1.124s,  911.33/s  (1.091s,  938.28/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 350/1251 ( 28%)]  Loss:  3.717687 (3.6375)  Time: 1.077s,  950.56/s  (1.091s,  938.85/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 400/1251 ( 32%)]  Loss:  3.515085 (3.6239)  Time: 1.077s,  951.08/s  (1.090s,  939.26/s)  LR: 8.084e-04  Data: 0.013 (0.013)
Train: 87 [ 450/1251 ( 36%)]  Loss:  3.831726 (3.6447)  Time: 1.074s,  953.43/s  (1.090s,  939.24/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 500/1251 ( 40%)]  Loss:  3.709336 (3.6505)  Time: 1.086s,  943.15/s  (1.091s,  938.82/s)  LR: 8.084e-04  Data: 0.013 (0.013)
Train: 87 [ 550/1251 ( 44%)]  Loss:  3.922567 (3.6732)  Time: 1.099s,  931.51/s  (1.091s,  938.93/s)  LR: 8.084e-04  Data: 0.015 (0.013)
Train: 87 [ 600/1251 ( 48%)]  Loss:  3.894419 (3.6902)  Time: 1.093s,  936.56/s  (1.090s,  939.22/s)  LR: 8.084e-04  Data: 0.012 (0.014)
Train: 87 [ 650/1251 ( 52%)]  Loss:  3.582299 (3.6825)  Time: 1.080s,  948.28/s  (1.090s,  939.17/s)  LR: 8.084e-04  Data: 0.013 (0.014)
Train: 87 [ 700/1251 ( 56%)]  Loss:  4.002679 (3.7039)  Time: 1.074s,  953.84/s  (1.090s,  939.15/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 750/1251 ( 60%)]  Loss:  3.549513 (3.6942)  Time: 1.081s,  947.39/s  (1.091s,  938.98/s)  LR: 8.084e-04  Data: 0.016 (0.013)
Train: 87 [ 800/1251 ( 64%)]  Loss:  3.729149 (3.6963)  Time: 1.078s,  949.48/s  (1.090s,  939.22/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 850/1251 ( 68%)]  Loss:  3.858494 (3.7053)  Time: 1.095s,  935.12/s  (1.091s,  938.98/s)  LR: 8.084e-04  Data: 0.017 (0.013)
Train: 87 [ 900/1251 ( 72%)]  Loss:  3.701751 (3.7051)  Time: 1.104s,  927.37/s  (1.091s,  938.52/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [ 950/1251 ( 76%)]  Loss:  3.928659 (3.7163)  Time: 1.078s,  949.86/s  (1.091s,  938.60/s)  LR: 8.084e-04  Data: 0.014 (0.013)
Train: 87 [1000/1251 ( 80%)]  Loss:  3.749071 (3.7178)  Time: 1.081s,  947.63/s  (1.091s,  938.71/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [1050/1251 ( 84%)]  Loss:  3.626183 (3.7137)  Time: 1.081s,  947.33/s  (1.091s,  938.81/s)  LR: 8.084e-04  Data: 0.015 (0.013)
Train: 87 [1100/1251 ( 88%)]  Loss:  3.626760 (3.7099)  Time: 1.103s,  928.25/s  (1.091s,  938.89/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [1150/1251 ( 92%)]  Loss:  3.870025 (3.7166)  Time: 1.095s,  934.86/s  (1.091s,  938.82/s)  LR: 8.084e-04  Data: 0.014 (0.013)
Train: 87 [1200/1251 ( 96%)]  Loss:  3.785392 (3.7193)  Time: 1.078s,  950.18/s  (1.091s,  938.90/s)  LR: 8.084e-04  Data: 0.012 (0.013)
Train: 87 [1250/1251 (100%)]  Loss:  3.833935 (3.7237)  Time: 1.082s,  946.48/s  (1.091s,  938.77/s)  LR: 8.084e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.778 (5.778)  Loss:  0.6122 (0.6122)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7014 (1.1386)  Acc@1: 84.9057 (74.2020)  Acc@5: 96.2264 (92.4200)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 74.2020000366211)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 74.0460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 73.9640000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-77.pth.tar', 73.6420000415039)

Train: 88 [   0/1251 (  0%)]  Loss:  3.568517 (3.5685)  Time: 1.086s,  942.87/s  (1.086s,  942.87/s)  LR: 8.043e-04  Data: 0.023 (0.023)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 88 [  50/1251 (  4%)]  Loss:  3.376093 (3.4723)  Time: 1.096s,  934.52/s  (1.087s,  941.70/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 100/1251 (  8%)]  Loss:  3.567325 (3.5040)  Time: 1.081s,  947.39/s  (1.089s,  940.08/s)  LR: 8.043e-04  Data: 0.015 (0.013)
Train: 88 [ 150/1251 ( 12%)]  Loss:  3.639054 (3.5377)  Time: 1.091s,  938.89/s  (1.087s,  942.06/s)  LR: 8.043e-04  Data: 0.014 (0.013)
Train: 88 [ 200/1251 ( 16%)]  Loss:  3.726884 (3.5756)  Time: 1.096s,  934.11/s  (1.088s,  941.11/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 250/1251 ( 20%)]  Loss:  3.571607 (3.5749)  Time: 1.080s,  948.33/s  (1.088s,  941.58/s)  LR: 8.043e-04  Data: 0.016 (0.013)
Train: 88 [ 300/1251 ( 24%)]  Loss:  4.101238 (3.6501)  Time: 1.099s,  931.67/s  (1.087s,  941.93/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 350/1251 ( 28%)]  Loss:  3.688711 (3.6549)  Time: 1.103s,  928.38/s  (1.088s,  941.19/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 400/1251 ( 32%)]  Loss:  3.761356 (3.6668)  Time: 1.097s,  933.78/s  (1.089s,  940.60/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 450/1251 ( 36%)]  Loss:  3.733083 (3.6734)  Time: 1.172s,  873.59/s  (1.089s,  940.26/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 500/1251 ( 40%)]  Loss:  3.674411 (3.6735)  Time: 1.077s,  950.95/s  (1.090s,  939.80/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 550/1251 ( 44%)]  Loss:  3.529034 (3.6614)  Time: 1.075s,  952.74/s  (1.090s,  939.61/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 600/1251 ( 48%)]  Loss:  3.230985 (3.6283)  Time: 1.080s,  947.98/s  (1.090s,  939.29/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 650/1251 ( 52%)]  Loss:  3.516850 (3.6204)  Time: 1.077s,  950.67/s  (1.090s,  939.47/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 700/1251 ( 56%)]  Loss:  3.969249 (3.6436)  Time: 1.081s,  947.42/s  (1.090s,  939.75/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [ 750/1251 ( 60%)]  Loss:  3.830152 (3.6553)  Time: 1.078s,  950.00/s  (1.090s,  939.76/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 800/1251 ( 64%)]  Loss:  3.595107 (3.6517)  Time: 1.077s,  950.48/s  (1.090s,  939.58/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 850/1251 ( 68%)]  Loss:  3.593353 (3.6485)  Time: 1.077s,  950.43/s  (1.090s,  939.61/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 900/1251 ( 72%)]  Loss:  3.696299 (3.6510)  Time: 1.097s,  933.08/s  (1.090s,  939.86/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [ 950/1251 ( 76%)]  Loss:  3.500626 (3.6435)  Time: 1.085s,  944.06/s  (1.090s,  939.75/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [1000/1251 ( 80%)]  Loss:  3.504352 (3.6369)  Time: 1.186s,  863.65/s  (1.090s,  939.66/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1050/1251 ( 84%)]  Loss:  3.516929 (3.6314)  Time: 1.095s,  935.56/s  (1.090s,  939.65/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1100/1251 ( 88%)]  Loss:  3.491338 (3.6253)  Time: 1.079s,  949.37/s  (1.090s,  939.59/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1150/1251 ( 92%)]  Loss:  3.484664 (3.6195)  Time: 1.094s,  935.89/s  (1.090s,  939.68/s)  LR: 8.043e-04  Data: 0.012 (0.013)
Train: 88 [1200/1251 ( 96%)]  Loss:  3.728047 (3.6238)  Time: 1.077s,  950.82/s  (1.090s,  939.57/s)  LR: 8.043e-04  Data: 0.013 (0.013)
Train: 88 [1250/1251 (100%)]  Loss:  3.418483 (3.6159)  Time: 1.080s,  947.86/s  (1.090s,  939.50/s)  LR: 8.043e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.888 (5.888)  Loss:  0.5532 (0.5532)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6612 (1.0956)  Acc@1: 85.8491 (74.5380)  Acc@5: 96.3443 (92.6260)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 74.53800005859375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 74.2020000366211)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 74.0460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 73.9640000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-78.pth.tar', 73.72000008544921)

Train: 89 [   0/1251 (  0%)]  Loss:  3.451358 (3.4514)  Time: 1.108s,  924.02/s  (1.108s,  924.02/s)  LR: 8.001e-04  Data: 0.025 (0.025)
Train: 89 [  50/1251 (  4%)]  Loss:  3.455534 (3.4534)  Time: 1.081s,  946.93/s  (1.089s,  940.74/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 100/1251 (  8%)]  Loss:  3.590293 (3.4991)  Time: 1.078s,  950.34/s  (1.088s,  941.24/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 150/1251 ( 12%)]  Loss:  3.551327 (3.5121)  Time: 1.097s,  933.81/s  (1.087s,  941.81/s)  LR: 8.001e-04  Data: 0.012 (0.013)
Train: 89 [ 200/1251 ( 16%)]  Loss:  3.782589 (3.5662)  Time: 1.080s,  947.73/s  (1.087s,  941.91/s)  LR: 8.001e-04  Data: 0.014 (0.013)
Train: 89 [ 250/1251 ( 20%)]  Loss:  3.413139 (3.5407)  Time: 1.079s,  948.91/s  (1.087s,  942.11/s)  LR: 8.001e-04  Data: 0.012 (0.013)
Train: 89 [ 300/1251 ( 24%)]  Loss:  3.462516 (3.5295)  Time: 1.077s,  950.78/s  (1.087s,  942.30/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 350/1251 ( 28%)]  Loss:  3.677456 (3.5480)  Time: 1.078s,  949.82/s  (1.087s,  942.02/s)  LR: 8.001e-04  Data: 0.014 (0.013)
Train: 89 [ 400/1251 ( 32%)]  Loss:  3.759941 (3.5716)  Time: 1.078s,  950.30/s  (1.087s,  942.11/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 450/1251 ( 36%)]  Loss:  3.312588 (3.5457)  Time: 1.080s,  948.11/s  (1.087s,  941.98/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 500/1251 ( 40%)]  Loss:  3.605565 (3.5511)  Time: 1.097s,  933.38/s  (1.088s,  941.42/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 550/1251 ( 44%)]  Loss:  3.799137 (3.5718)  Time: 1.080s,  947.73/s  (1.088s,  941.36/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 600/1251 ( 48%)]  Loss:  3.235365 (3.5459)  Time: 1.172s,  873.41/s  (1.088s,  941.24/s)  LR: 8.001e-04  Data: 0.015 (0.013)
Train: 89 [ 650/1251 ( 52%)]  Loss:  3.527335 (3.5446)  Time: 1.182s,  866.40/s  (1.088s,  940.76/s)  LR: 8.001e-04  Data: 0.011 (0.013)
Train: 89 [ 700/1251 ( 56%)]  Loss:  3.521225 (3.5430)  Time: 1.076s,  951.51/s  (1.089s,  940.41/s)  LR: 8.001e-04  Data: 0.012 (0.013)
Train: 89 [ 750/1251 ( 60%)]  Loss:  4.049129 (3.5747)  Time: 1.079s,  948.59/s  (1.089s,  940.72/s)  LR: 8.001e-04  Data: 0.015 (0.013)
Train: 89 [ 800/1251 ( 64%)]  Loss:  3.510367 (3.5709)  Time: 1.081s,  947.61/s  (1.089s,  940.71/s)  LR: 8.001e-04  Data: 0.016 (0.013)
Train: 89 [ 850/1251 ( 68%)]  Loss:  3.757577 (3.5812)  Time: 1.083s,  945.63/s  (1.089s,  940.55/s)  LR: 8.001e-04  Data: 0.015 (0.013)
Train: 89 [ 900/1251 ( 72%)]  Loss:  3.999063 (3.6032)  Time: 1.082s,  946.48/s  (1.089s,  940.53/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [ 950/1251 ( 76%)]  Loss:  3.774496 (3.6118)  Time: 1.080s,  948.32/s  (1.089s,  940.45/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [1000/1251 ( 80%)]  Loss:  3.486379 (3.6058)  Time: 1.079s,  949.00/s  (1.089s,  940.39/s)  LR: 8.001e-04  Data: 0.014 (0.013)
Train: 89 [1050/1251 ( 84%)]  Loss:  3.419413 (3.5974)  Time: 1.102s,  928.83/s  (1.089s,  940.15/s)  LR: 8.001e-04  Data: 0.012 (0.013)
Train: 89 [1100/1251 ( 88%)]  Loss:  3.691016 (3.6014)  Time: 1.094s,  935.92/s  (1.090s,  939.71/s)  LR: 8.001e-04  Data: 0.012 (0.013)
Train: 89 [1150/1251 ( 92%)]  Loss:  3.676562 (3.6046)  Time: 1.120s,  914.00/s  (1.090s,  939.83/s)  LR: 8.001e-04  Data: 0.013 (0.013)
Train: 89 [1200/1251 ( 96%)]  Loss:  3.413464 (3.5969)  Time: 1.098s,  933.02/s  (1.090s,  939.66/s)  LR: 8.001e-04  Data: 0.015 (0.013)
Train: 89 [1250/1251 (100%)]  Loss:  3.686507 (3.6004)  Time: 1.062s,  964.26/s  (1.090s,  939.62/s)  LR: 8.001e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.861 (5.861)  Loss:  0.5913 (0.5913)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6621 (1.1257)  Acc@1: 85.9670 (74.4200)  Acc@5: 96.1085 (92.4500)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 74.53800005859375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 74.41999998046875)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 74.2020000366211)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 74.0460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 73.9640000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-83.pth.tar', 73.77400014160156)

Train: 90 [   0/1251 (  0%)]  Loss:  3.490294 (3.4903)  Time: 1.090s,  939.35/s  (1.090s,  939.35/s)  LR: 7.960e-04  Data: 0.027 (0.027)
Train: 90 [  50/1251 (  4%)]  Loss:  3.854105 (3.6722)  Time: 1.101s,  929.73/s  (1.098s,  932.80/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 100/1251 (  8%)]  Loss:  3.789890 (3.7114)  Time: 1.082s,  946.52/s  (1.095s,  935.20/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 150/1251 ( 12%)]  Loss:  3.481769 (3.6540)  Time: 1.095s,  935.06/s  (1.094s,  936.10/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 200/1251 ( 16%)]  Loss:  3.420435 (3.6073)  Time: 1.079s,  949.30/s  (1.095s,  935.26/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [ 250/1251 ( 20%)]  Loss:  3.393013 (3.5716)  Time: 1.095s,  935.05/s  (1.094s,  936.40/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 300/1251 ( 24%)]  Loss:  3.351054 (3.5401)  Time: 1.080s,  948.14/s  (1.093s,  936.49/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [ 350/1251 ( 28%)]  Loss:  3.802615 (3.5729)  Time: 1.172s,  873.51/s  (1.093s,  937.19/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 400/1251 ( 32%)]  Loss:  3.817506 (3.6001)  Time: 1.083s,  945.77/s  (1.092s,  938.06/s)  LR: 7.960e-04  Data: 0.014 (0.013)
Train: 90 [ 450/1251 ( 36%)]  Loss:  3.585780 (3.5986)  Time: 1.103s,  928.12/s  (1.091s,  938.54/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [ 500/1251 ( 40%)]  Loss:  3.193699 (3.5618)  Time: 1.088s,  940.76/s  (1.091s,  938.39/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [ 550/1251 ( 44%)]  Loss:  3.710916 (3.5743)  Time: 1.075s,  952.24/s  (1.091s,  938.65/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 600/1251 ( 48%)]  Loss:  3.860465 (3.5963)  Time: 1.098s,  932.27/s  (1.091s,  938.49/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [ 650/1251 ( 52%)]  Loss:  3.585137 (3.5955)  Time: 1.174s,  872.14/s  (1.091s,  938.70/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 700/1251 ( 56%)]  Loss:  4.024500 (3.6241)  Time: 1.074s,  953.46/s  (1.091s,  938.67/s)  LR: 7.960e-04  Data: 0.011 (0.013)
Train: 90 [ 750/1251 ( 60%)]  Loss:  3.604225 (3.6228)  Time: 1.096s,  934.70/s  (1.091s,  938.71/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 800/1251 ( 64%)]  Loss:  3.714366 (3.6282)  Time: 1.106s,  925.50/s  (1.091s,  938.77/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [ 850/1251 ( 68%)]  Loss:  3.593610 (3.6263)  Time: 1.096s,  934.29/s  (1.091s,  938.30/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [ 900/1251 ( 72%)]  Loss:  3.486268 (3.6189)  Time: 1.117s,  916.84/s  (1.091s,  938.45/s)  LR: 7.960e-04  Data: 0.015 (0.013)
Train: 90 [ 950/1251 ( 76%)]  Loss:  3.275591 (3.6018)  Time: 1.099s,  932.03/s  (1.091s,  938.64/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [1000/1251 ( 80%)]  Loss:  3.646997 (3.6039)  Time: 1.098s,  932.23/s  (1.091s,  938.42/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [1050/1251 ( 84%)]  Loss:  3.493271 (3.5989)  Time: 1.078s,  950.27/s  (1.091s,  938.37/s)  LR: 7.960e-04  Data: 0.015 (0.013)
Train: 90 [1100/1251 ( 88%)]  Loss:  3.829604 (3.6089)  Time: 1.095s,  935.37/s  (1.091s,  938.55/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [1150/1251 ( 92%)]  Loss:  3.503973 (3.6045)  Time: 1.084s,  944.87/s  (1.091s,  938.64/s)  LR: 7.960e-04  Data: 0.013 (0.013)
Train: 90 [1200/1251 ( 96%)]  Loss:  3.326654 (3.5934)  Time: 1.095s,  935.42/s  (1.091s,  938.38/s)  LR: 7.960e-04  Data: 0.012 (0.013)
Train: 90 [1250/1251 (100%)]  Loss:  3.344926 (3.5839)  Time: 1.062s,  964.35/s  (1.091s,  938.48/s)  LR: 7.960e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.836 (5.836)  Loss:  0.5897 (0.5897)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6558 (1.1356)  Acc@1: 85.7311 (74.3700)  Acc@5: 96.5802 (92.3520)
Current checkpoints:
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-88.pth.tar', 74.53800005859375)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar', 74.41999998046875)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.37000000732422)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-87.pth.tar', 74.2020000366211)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-84.pth.tar', 74.13600000976562)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-85.pth.tar', 74.0460000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-82.pth.tar', 73.98199996582031)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-81.pth.tar', 73.96999995849609)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-86.pth.tar', 73.9640000390625)
 ('./output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-79.pth.tar', 73.82800009277344)

Train: 91 [   0/1251 (  0%)]  Loss:  3.537426 (3.5374)  Time: 1.092s,  937.92/s  (1.092s,  937.92/s)  LR: 7.917e-04  Data: 0.028 (0.028)
Train: 91 [  50/1251 (  4%)]  Loss:  3.658536 (3.5980)  Time: 1.079s,  949.09/s  (1.098s,  932.53/s)  LR: 7.917e-04  Data: 0.014 (0.013)
Train: 91 [ 100/1251 (  8%)]  Loss:  3.682103 (3.6260)  Time: 1.078s,  949.68/s  (1.091s,  938.21/s)  LR: 7.917e-04  Data: 0.013 (0.013)
Train: 91 [ 150/1251 ( 12%)]  Loss:  3.810001 (3.6720)  Time: 1.077s,  950.81/s  (1.093s,  936.53/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [ 200/1251 ( 16%)]  Loss:  3.703228 (3.6783)  Time: 1.079s,  948.78/s  (1.091s,  938.62/s)  LR: 7.917e-04  Data: 0.013 (0.013)
Train: 91 [ 250/1251 ( 20%)]  Loss:  3.738301 (3.6883)  Time: 1.101s,  929.94/s  (1.090s,  939.09/s)  LR: 7.917e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Model swin_tiny_patch4_window7_224 created, param count:27110704
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Using NVIDIA APEX AMP. Training in mixed precision.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Restoring AMP loss scaler state from checkpoint...
Loaded checkpoint 'output/train/20220307-215816-swin_tiny_patch4_window7_224-224/checkpoint-89.pth.tar' (epoch 89)
Using NVIDIA APEX DistributedDataParallel.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Scheduled epochs: 310
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Train: 90 [   0/1251 (  0%)]  Loss:  3.608330 (3.6083)  Time: 17.404s,   58.84/s  (17.404s,   58.84/s)  LR: 7.960e-04  Data: 4.238 (4.238)
Train: 90 [  50/1251 (  4%)]  Loss:  3.835093 (3.7217)  Time: 1.081s,  947.01/s  (1.422s,  720.31/s)  LR: 7.960e-04  Data: 0.012 (0.096)
Train: 90 [ 100/1251 (  8%)]  Loss:  3.794909 (3.7461)  Time: 1.096s,  934.62/s  (1.256s,  814.98/s)  LR: 7.960e-04  Data: 0.011 (0.055)
Train: 90 [ 150/1251 ( 12%)]  Loss:  3.809693 (3.7620)  Time: 1.076s,  951.96/s  (1.200s,  853.47/s)  LR: 7.960e-04  Data: 0.011 (0.041)
Train: 90 [ 200/1251 ( 16%)]  Loss:  3.483654 (3.7063)  Time: 1.083s,  945.40/s  (1.171s,  874.45/s)  LR: 7.960e-04  Data: 0.014 (0.034)
Train: 90 [ 250/1251 ( 20%)]  Loss:  3.640754 (3.6954)  Time: 1.075s,  952.23/s  (1.154s,  887.36/s)  LR: 7.960e-04  Data: 0.010 (0.029)
Train: 90 [ 300/1251 ( 24%)]  Loss:  3.382782 (3.6507)  Time: 1.093s,  936.89/s  (1.143s,  895.82/s)  LR: 7.960e-04  Data: 0.012 (0.026)
Train: 90 [ 350/1251 ( 28%)]  Loss:  3.488284 (3.6304)  Time: 1.081s,  947.17/s  (1.135s,  901.82/s)  LR: 7.960e-04  Data: 0.012 (0.024)
Train: 90 [ 400/1251 ( 32%)]  Loss:  4.012893 (3.6729)  Time: 1.085s,  943.44/s  (1.130s,  906.24/s)  LR: 7.960e-04  Data: 0.021 (0.023)
Train: 90 [ 450/1251 ( 36%)]  Loss:  3.758976 (3.6815)  Time: 1.083s,  945.77/s  (1.125s,  910.18/s)  LR: 7.960e-04  Data: 0.011 (0.022)
Train: 90 [ 500/1251 ( 40%)]  Loss:  3.635047 (3.6773)  Time: 1.080s,  948.35/s  (1.121s,  913.13/s)  LR: 7.960e-04  Data: 0.012 (0.021)
Train: 90 [ 550/1251 ( 44%)]  Loss:  3.500645 (3.6626)  Time: 1.079s,  948.84/s  (1.118s,  915.72/s)  LR: 7.960e-04  Data: 0.011 (0.020)
Train: 90 [ 600/1251 ( 48%)]  Loss:  3.638384 (3.6607)  Time: 1.100s,  931.17/s  (1.115s,  918.06/s)  LR: 7.960e-04  Data: 0.011 (0.019)
Train: 90 [ 650/1251 ( 52%)]  Loss:  3.694327 (3.6631)  Time: 1.077s,  950.38/s  (1.113s,  919.99/s)  LR: 7.960e-04  Data: 0.011 (0.019)
Train: 90 [ 700/1251 ( 56%)]  Loss:  3.750061 (3.6689)  Time: 1.079s,  949.13/s  (1.111s,  921.74/s)  LR: 7.960e-04  Data: 0.011 (0.018)
Train: 90 [ 750/1251 ( 60%)]  Loss:  3.400858 (3.6522)  Time: 1.100s,  930.87/s  (1.109s,  923.23/s)  LR: 7.960e-04  Data: 0.015 (0.018)
Train: 90 [ 800/1251 ( 64%)]  Loss:  3.664286 (3.6529)  Time: 1.080s,  948.56/s  (1.108s,  924.22/s)  LR: 7.960e-04  Data: 0.011 (0.017)
Train: 90 [ 850/1251 ( 68%)]  Loss:  3.642500 (3.6523)  Time: 1.093s,  937.00/s  (1.107s,  925.09/s)  LR: 7.960e-04  Data: 0.011 (0.017)
Train: 90 [ 900/1251 ( 72%)]  Loss:  3.363755 (3.6371)  Time: 1.098s,  932.76/s  (1.106s,  925.72/s)  LR: 7.960e-04  Data: 0.011 (0.017)
Train: 90 [ 950/1251 ( 76%)]  Loss:  3.720416 (3.6413)  Time: 1.079s,  948.66/s  (1.105s,  926.55/s)  LR: 7.960e-04  Data: 0.012 (0.016)
Train: 90 [1000/1251 ( 80%)]  Loss:  3.715185 (3.6448)  Time: 1.096s,  933.94/s  (1.105s,  927.11/s)  LR: 7.960e-04  Data: 0.012 (0.016)
Train: 90 [1050/1251 ( 84%)]  Loss:  3.865916 (3.6549)  Time: 1.093s,  937.16/s  (1.104s,  927.87/s)  LR: 7.960e-04  Data: 0.016 (0.016)
Train: 90 [1100/1251 ( 88%)]  Loss:  3.221477 (3.6360)  Time: 1.079s,  948.84/s  (1.103s,  928.39/s)  LR: 7.960e-04  Data: 0.011 (0.016)
Train: 90 [1150/1251 ( 92%)]  Loss:  3.684056 (3.6380)  Time: 1.094s,  936.07/s  (1.102s,  928.90/s)  LR: 7.960e-04  Data: 0.014 (0.016)
Train: 90 [1200/1251 ( 96%)]  Loss:  3.221741 (3.6214)  Time: 1.110s,  922.85/s  (1.102s,  929.54/s)  LR: 7.960e-04  Data: 0.011 (0.016)
Train: 90 [1250/1251 (100%)]  Loss:  3.402434 (3.6129)  Time: 1.082s,  946.38/s  (1.101s,  929.97/s)  LR: 7.960e-04  Data: 0.000 (0.015)
Test: [   0/48]  Time: 8.543 (8.543)  Loss:  0.6106 (0.6106)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 2.609 (0.544)  Loss:  0.6981 (1.1417)  Acc@1: 84.7877 (74.1500)  Acc@5: 96.4623 (92.4260)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 91 [   0/1251 (  0%)]  Loss:  3.617257 (3.6173)  Time: 1.105s,  926.64/s  (1.105s,  926.64/s)  LR: 7.917e-04  Data: 0.036 (0.036)
Train: 91 [  50/1251 (  4%)]  Loss:  3.793402 (3.7053)  Time: 1.103s,  928.44/s  (1.096s,  933.99/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [ 100/1251 (  8%)]  Loss:  3.789536 (3.7334)  Time: 1.084s,  944.99/s  (1.094s,  936.03/s)  LR: 7.917e-04  Data: 0.012 (0.013)
Train: 91 [ 150/1251 ( 12%)]  Loss:  3.615529 (3.7039)  Time: 1.078s,  950.05/s  (1.092s,  937.75/s)  LR: 7.917e-04  Data: 0.014 (0.012)
Train: 91 [ 200/1251 ( 16%)]  Loss:  3.307652 (3.6247)  Time: 1.093s,  937.09/s  (1.091s,  938.30/s)  LR: 7.917e-04  Data: 0.010 (0.012)
Train: 91 [ 250/1251 ( 20%)]  Loss:  3.600655 (3.6207)  Time: 1.077s,  950.36/s  (1.092s,  938.07/s)  LR: 7.917e-04  Data: 0.010 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 91 [ 300/1251 ( 24%)]  Loss:  3.781066 (3.6436)  Time: 1.076s,  951.49/s  (1.090s,  939.30/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [ 350/1251 ( 28%)]  Loss:  3.693760 (3.6499)  Time: 1.076s,  951.51/s  (1.090s,  939.77/s)  LR: 7.917e-04  Data: 0.015 (0.012)
Train: 91 [ 400/1251 ( 32%)]  Loss:  3.520806 (3.6355)  Time: 1.079s,  949.05/s  (1.089s,  940.10/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [ 450/1251 ( 36%)]  Loss:  3.302300 (3.6022)  Time: 1.076s,  951.71/s  (1.089s,  939.91/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 91 [ 500/1251 ( 40%)]  Loss:  3.425139 (3.5861)  Time: 1.077s,  950.52/s  (1.089s,  940.48/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [ 550/1251 ( 44%)]  Loss:  3.589692 (3.5864)  Time: 1.074s,  953.40/s  (1.089s,  940.66/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [ 600/1251 ( 48%)]  Loss:  3.713299 (3.5962)  Time: 1.101s,  930.10/s  (1.088s,  941.15/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [ 650/1251 ( 52%)]  Loss:  3.588840 (3.5956)  Time: 1.082s,  946.31/s  (1.088s,  941.56/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [ 700/1251 ( 56%)]  Loss:  3.505996 (3.5897)  Time: 1.076s,  951.94/s  (1.087s,  941.68/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [ 750/1251 ( 60%)]  Loss:  3.477772 (3.5827)  Time: 1.094s,  935.80/s  (1.087s,  941.73/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [ 800/1251 ( 64%)]  Loss:  3.351546 (3.5691)  Time: 1.077s,  950.47/s  (1.087s,  941.64/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [ 850/1251 ( 68%)]  Loss:  3.488120 (3.5646)  Time: 1.093s,  937.01/s  (1.087s,  941.79/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [ 900/1251 ( 72%)]  Loss:  3.510411 (3.5617)  Time: 1.077s,  950.44/s  (1.087s,  941.72/s)  LR: 7.917e-04  Data: 0.014 (0.012)
Train: 91 [ 950/1251 ( 76%)]  Loss:  3.673630 (3.5673)  Time: 1.080s,  948.43/s  (1.087s,  941.67/s)  LR: 7.917e-04  Data: 0.013 (0.012)
Train: 91 [1000/1251 ( 80%)]  Loss:  3.575982 (3.5677)  Time: 1.081s,  946.90/s  (1.088s,  941.61/s)  LR: 7.917e-04  Data: 0.011 (0.012)
Train: 91 [1050/1251 ( 84%)]  Loss:  3.612856 (3.5698)  Time: 1.079s,  948.82/s  (1.088s,  941.43/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [1100/1251 ( 88%)]  Loss:  3.773168 (3.5786)  Time: 1.075s,  952.14/s  (1.088s,  941.43/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [1150/1251 ( 92%)]  Loss:  3.546709 (3.5773)  Time: 1.079s,  949.01/s  (1.087s,  941.61/s)  LR: 7.917e-04  Data: 0.015 (0.012)
Train: 91 [1200/1251 ( 96%)]  Loss:  3.598634 (3.5782)  Time: 1.081s,  947.49/s  (1.088s,  941.52/s)  LR: 7.917e-04  Data: 0.012 (0.012)
Train: 91 [1250/1251 (100%)]  Loss:  3.760102 (3.5851)  Time: 1.079s,  949.32/s  (1.088s,  941.56/s)  LR: 7.917e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.886 (5.886)  Loss:  0.5297 (0.5297)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.6965 (1.1113)  Acc@1: 84.4340 (74.5220)  Acc@5: 95.9906 (92.4840)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 92 [   0/1251 (  0%)]  Loss:  3.558810 (3.5588)  Time: 1.088s,  941.30/s  (1.088s,  941.30/s)  LR: 7.875e-04  Data: 0.026 (0.026)
Train: 92 [  50/1251 (  4%)]  Loss:  3.759017 (3.6589)  Time: 1.082s,  946.02/s  (1.085s,  943.80/s)  LR: 7.875e-04  Data: 0.011 (0.013)
Train: 92 [ 100/1251 (  8%)]  Loss:  3.506065 (3.6080)  Time: 1.077s,  951.03/s  (1.086s,  942.68/s)  LR: 7.875e-04  Data: 0.012 (0.013)
Train: 92 [ 150/1251 ( 12%)]  Loss:  3.640461 (3.6161)  Time: 1.106s,  925.97/s  (1.088s,  941.11/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [ 200/1251 ( 16%)]  Loss:  3.509080 (3.5947)  Time: 1.079s,  949.44/s  (1.089s,  939.91/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 250/1251 ( 20%)]  Loss:  3.812715 (3.6310)  Time: 1.094s,  935.89/s  (1.088s,  940.77/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 300/1251 ( 24%)]  Loss:  3.671604 (3.6368)  Time: 1.075s,  952.81/s  (1.088s,  941.19/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 350/1251 ( 28%)]  Loss:  3.660537 (3.6398)  Time: 1.092s,  937.37/s  (1.089s,  940.01/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [ 400/1251 ( 32%)]  Loss:  3.602626 (3.6357)  Time: 1.094s,  935.97/s  (1.089s,  939.94/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 450/1251 ( 36%)]  Loss:  3.726332 (3.6447)  Time: 1.077s,  950.36/s  (1.089s,  940.16/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [ 500/1251 ( 40%)]  Loss:  3.318793 (3.6151)  Time: 1.078s,  949.73/s  (1.089s,  940.08/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [ 550/1251 ( 44%)]  Loss:  3.821541 (3.6323)  Time: 1.104s,  927.93/s  (1.090s,  939.49/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 600/1251 ( 48%)]  Loss:  3.758162 (3.6420)  Time: 1.101s,  930.43/s  (1.090s,  939.16/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [ 650/1251 ( 52%)]  Loss:  3.914965 (3.6615)  Time: 1.094s,  936.24/s  (1.091s,  938.71/s)  LR: 7.875e-04  Data: 0.013 (0.012)
Train: 92 [ 700/1251 ( 56%)]  Loss:  3.413435 (3.6449)  Time: 1.094s,  936.19/s  (1.091s,  938.60/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 750/1251 ( 60%)]  Loss:  3.393219 (3.6292)  Time: 1.081s,  947.70/s  (1.091s,  938.53/s)  LR: 7.875e-04  Data: 0.013 (0.012)
Train: 92 [ 800/1251 ( 64%)]  Loss:  3.604676 (3.6278)  Time: 1.081s,  947.65/s  (1.091s,  938.56/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 850/1251 ( 68%)]  Loss:  3.827928 (3.6389)  Time: 1.075s,  952.51/s  (1.091s,  938.80/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [ 900/1251 ( 72%)]  Loss:  3.813772 (3.6481)  Time: 1.077s,  950.93/s  (1.090s,  939.02/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [ 950/1251 ( 76%)]  Loss:  3.824774 (3.6569)  Time: 1.081s,  947.51/s  (1.090s,  939.13/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [1000/1251 ( 80%)]  Loss:  3.870651 (3.6671)  Time: 1.110s,  922.56/s  (1.090s,  939.20/s)  LR: 7.875e-04  Data: 0.016 (0.012)
Train: 92 [1050/1251 ( 84%)]  Loss:  3.699914 (3.6686)  Time: 1.079s,  948.74/s  (1.090s,  939.45/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [1100/1251 ( 88%)]  Loss:  3.845100 (3.6763)  Time: 1.094s,  936.34/s  (1.090s,  939.70/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [1150/1251 ( 92%)]  Loss:  3.591341 (3.6727)  Time: 1.097s,  933.53/s  (1.090s,  939.49/s)  LR: 7.875e-04  Data: 0.011 (0.012)
Train: 92 [1200/1251 ( 96%)]  Loss:  3.334519 (3.6592)  Time: 1.081s,  947.45/s  (1.090s,  939.74/s)  LR: 7.875e-04  Data: 0.012 (0.012)
Train: 92 [1250/1251 (100%)]  Loss:  3.725997 (3.6618)  Time: 1.061s,  964.69/s  (1.090s,  939.65/s)  LR: 7.875e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.850 (5.850)  Loss:  0.5509 (0.5509)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.6630 (1.1008)  Acc@1: 84.9057 (74.2680)  Acc@5: 96.4623 (92.5100)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 93 [   0/1251 (  0%)]  Loss:  3.574147 (3.5741)  Time: 1.089s,  940.03/s  (1.089s,  940.03/s)  LR: 7.832e-04  Data: 0.026 (0.026)
Train: 93 [  50/1251 (  4%)]  Loss:  3.607115 (3.5906)  Time: 1.079s,  948.92/s  (1.090s,  939.71/s)  LR: 7.832e-04  Data: 0.011 (0.013)
Train: 93 [ 100/1251 (  8%)]  Loss:  3.430779 (3.5373)  Time: 1.097s,  933.53/s  (1.093s,  936.87/s)  LR: 7.832e-04  Data: 0.014 (0.013)
Train: 93 [ 150/1251 ( 12%)]  Loss:  3.999467 (3.6529)  Time: 1.087s,  941.78/s  (1.091s,  938.80/s)  LR: 7.832e-04  Data: 0.011 (0.012)
Train: 93 [ 200/1251 ( 16%)]  Loss:  3.626907 (3.6477)  Time: 1.079s,  948.88/s  (1.088s,  941.15/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 250/1251 ( 20%)]  Loss:  3.581946 (3.6367)  Time: 1.079s,  948.79/s  (1.088s,  941.35/s)  LR: 7.832e-04  Data: 0.015 (0.012)
Train: 93 [ 300/1251 ( 24%)]  Loss:  3.707066 (3.6468)  Time: 1.096s,  934.05/s  (1.088s,  941.36/s)  LR: 7.832e-04  Data: 0.014 (0.012)
Train: 93 [ 350/1251 ( 28%)]  Loss:  3.592591 (3.6400)  Time: 1.078s,  949.50/s  (1.087s,  941.62/s)  LR: 7.832e-04  Data: 0.011 (0.012)
Train: 93 [ 400/1251 ( 32%)]  Loss:  4.097204 (3.6908)  Time: 1.108s,  924.24/s  (1.088s,  941.36/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 450/1251 ( 36%)]  Loss:  3.829672 (3.7047)  Time: 1.084s,  944.33/s  (1.088s,  940.75/s)  LR: 7.832e-04  Data: 0.013 (0.012)
Train: 93 [ 500/1251 ( 40%)]  Loss:  3.166491 (3.6558)  Time: 1.103s,  928.00/s  (1.088s,  941.07/s)  LR: 7.832e-04  Data: 0.014 (0.012)
Train: 93 [ 550/1251 ( 44%)]  Loss:  3.406507 (3.6350)  Time: 1.078s,  949.97/s  (1.088s,  941.50/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 600/1251 ( 48%)]  Loss:  3.793047 (3.6471)  Time: 1.078s,  949.84/s  (1.088s,  940.75/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 650/1251 ( 52%)]  Loss:  3.704164 (3.6512)  Time: 1.103s,  928.60/s  (1.088s,  940.84/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 700/1251 ( 56%)]  Loss:  3.320979 (3.6292)  Time: 1.097s,  933.56/s  (1.088s,  940.94/s)  LR: 7.832e-04  Data: 0.011 (0.013)
Train: 93 [ 750/1251 ( 60%)]  Loss:  3.566497 (3.6253)  Time: 1.095s,  934.79/s  (1.089s,  940.68/s)  LR: 7.832e-04  Data: 0.012 (0.013)
Train: 93 [ 800/1251 ( 64%)]  Loss:  3.465693 (3.6159)  Time: 1.077s,  950.40/s  (1.089s,  940.72/s)  LR: 7.832e-04  Data: 0.013 (0.012)
Train: 93 [ 850/1251 ( 68%)]  Loss:  3.629115 (3.6166)  Time: 1.083s,  945.86/s  (1.088s,  940.78/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 900/1251 ( 72%)]  Loss:  3.480814 (3.6095)  Time: 1.078s,  949.91/s  (1.088s,  941.01/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [ 950/1251 ( 76%)]  Loss:  3.756290 (3.6168)  Time: 1.094s,  936.42/s  (1.088s,  941.05/s)  LR: 7.832e-04  Data: 0.011 (0.012)
Train: 93 [1000/1251 ( 80%)]  Loss:  3.792147 (3.6252)  Time: 1.094s,  935.86/s  (1.088s,  940.86/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [1050/1251 ( 84%)]  Loss:  3.756400 (3.6311)  Time: 1.076s,  951.54/s  (1.088s,  940.91/s)  LR: 7.832e-04  Data: 0.014 (0.012)
Train: 93 [1100/1251 ( 88%)]  Loss:  3.555124 (3.6278)  Time: 1.105s,  926.32/s  (1.088s,  941.04/s)  LR: 7.832e-04  Data: 0.011 (0.012)
Train: 93 [1150/1251 ( 92%)]  Loss:  3.385369 (3.6177)  Time: 1.098s,  932.22/s  (1.089s,  940.61/s)  LR: 7.832e-04  Data: 0.012 (0.012)
Train: 93 [1200/1251 ( 96%)]  Loss:  3.316376 (3.6057)  Time: 1.081s,  947.15/s  (1.088s,  940.76/s)  LR: 7.832e-04  Data: 0.014 (0.012)
Train: 93 [1250/1251 (100%)]  Loss:  3.527249 (3.6027)  Time: 1.065s,  961.56/s  (1.088s,  940.90/s)  LR: 7.832e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.849 (5.849)  Loss:  0.5431 (0.5431)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6757 (1.0983)  Acc@1: 84.9057 (74.4660)  Acc@5: 97.2877 (92.5200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 94 [   0/1251 (  0%)]  Loss:  3.282342 (3.2823)  Time: 1.088s,  941.43/s  (1.088s,  941.43/s)  LR: 7.789e-04  Data: 0.025 (0.025)
Train: 94 [  50/1251 (  4%)]  Loss:  3.582701 (3.4325)  Time: 1.176s,  870.86/s  (1.087s,  941.72/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [ 100/1251 (  8%)]  Loss:  3.239246 (3.3681)  Time: 1.103s,  928.02/s  (1.087s,  942.43/s)  LR: 7.789e-04  Data: 0.014 (0.013)
Train: 94 [ 150/1251 ( 12%)]  Loss:  3.637080 (3.4353)  Time: 1.093s,  936.59/s  (1.087s,  941.92/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [ 200/1251 ( 16%)]  Loss:  3.588619 (3.4660)  Time: 1.077s,  950.88/s  (1.087s,  941.97/s)  LR: 7.789e-04  Data: 0.012 (0.013)
Train: 94 [ 250/1251 ( 20%)]  Loss:  3.564985 (3.4825)  Time: 1.077s,  950.72/s  (1.087s,  941.71/s)  LR: 7.789e-04  Data: 0.011 (0.013)
Train: 94 [ 300/1251 ( 24%)]  Loss:  3.654536 (3.5071)  Time: 1.077s,  950.36/s  (1.088s,  940.97/s)  LR: 7.789e-04  Data: 0.014 (0.012)
Train: 94 [ 350/1251 ( 28%)]  Loss:  3.752725 (3.5378)  Time: 1.168s,  876.41/s  (1.089s,  940.35/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [ 400/1251 ( 32%)]  Loss:  3.689292 (3.5546)  Time: 1.078s,  949.94/s  (1.088s,  941.28/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [ 450/1251 ( 36%)]  Loss:  3.651085 (3.5643)  Time: 1.077s,  950.42/s  (1.087s,  941.98/s)  LR: 7.789e-04  Data: 0.012 (0.012)
Train: 94 [ 500/1251 ( 40%)]  Loss:  3.488443 (3.5574)  Time: 1.080s,  948.46/s  (1.088s,  941.52/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [ 550/1251 ( 44%)]  Loss:  3.570959 (3.5585)  Time: 1.082s,  946.03/s  (1.088s,  941.55/s)  LR: 7.789e-04  Data: 0.012 (0.012)
Train: 94 [ 600/1251 ( 48%)]  Loss:  3.806380 (3.5776)  Time: 1.079s,  949.16/s  (1.087s,  941.82/s)  LR: 7.789e-04  Data: 0.012 (0.012)
Train: 94 [ 650/1251 ( 52%)]  Loss:  3.401623 (3.5650)  Time: 1.078s,  950.04/s  (1.088s,  941.53/s)  LR: 7.789e-04  Data: 0.012 (0.012)
Train: 94 [ 700/1251 ( 56%)]  Loss:  3.542135 (3.5635)  Time: 1.079s,  948.67/s  (1.088s,  941.60/s)  LR: 7.789e-04  Data: 0.015 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 94 [ 750/1251 ( 60%)]  Loss:  3.605961 (3.5661)  Time: 1.096s,  934.69/s  (1.087s,  941.63/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [ 800/1251 ( 64%)]  Loss:  3.559108 (3.5657)  Time: 1.077s,  950.71/s  (1.088s,  941.23/s)  LR: 7.789e-04  Data: 0.012 (0.012)
Train: 94 [ 850/1251 ( 68%)]  Loss:  3.679469 (3.5720)  Time: 1.093s,  937.13/s  (1.088s,  941.28/s)  LR: 7.789e-04  Data: 0.013 (0.012)
Train: 94 [ 900/1251 ( 72%)]  Loss:  3.527517 (3.5697)  Time: 1.103s,  928.12/s  (1.088s,  941.45/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [ 950/1251 ( 76%)]  Loss:  3.270609 (3.5547)  Time: 1.101s,  930.39/s  (1.088s,  941.13/s)  LR: 7.789e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 94 [1000/1251 ( 80%)]  Loss:  3.552732 (3.5546)  Time: 1.191s,  859.92/s  (1.088s,  941.11/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [1050/1251 ( 84%)]  Loss:  3.619927 (3.5576)  Time: 1.095s,  935.14/s  (1.088s,  940.99/s)  LR: 7.789e-04  Data: 0.013 (0.012)
Train: 94 [1100/1251 ( 88%)]  Loss:  4.040704 (3.5786)  Time: 1.105s,  926.63/s  (1.088s,  940.84/s)  LR: 7.789e-04  Data: 0.012 (0.012)
Train: 94 [1150/1251 ( 92%)]  Loss:  3.654030 (3.5818)  Time: 1.083s,  945.16/s  (1.088s,  940.90/s)  LR: 7.789e-04  Data: 0.014 (0.012)
Train: 94 [1200/1251 ( 96%)]  Loss:  3.714231 (3.5871)  Time: 1.094s,  936.37/s  (1.088s,  940.81/s)  LR: 7.789e-04  Data: 0.011 (0.012)
Train: 94 [1250/1251 (100%)]  Loss:  3.458008 (3.5821)  Time: 1.061s,  964.95/s  (1.088s,  941.04/s)  LR: 7.789e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.932 (5.932)  Loss:  0.5226 (0.5226)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.452)  Loss:  0.6470 (1.0993)  Acc@1: 85.4953 (74.6500)  Acc@5: 96.8160 (92.5780)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 95 [   0/1251 (  0%)]  Loss:  3.363682 (3.3637)  Time: 1.089s,  940.15/s  (1.089s,  940.15/s)  LR: 7.746e-04  Data: 0.027 (0.027)
Train: 95 [  50/1251 (  4%)]  Loss:  3.579890 (3.4718)  Time: 1.077s,  950.35/s  (1.089s,  940.46/s)  LR: 7.746e-04  Data: 0.012 (0.013)
Train: 95 [ 100/1251 (  8%)]  Loss:  3.531839 (3.4918)  Time: 1.096s,  934.58/s  (1.091s,  938.80/s)  LR: 7.746e-04  Data: 0.011 (0.013)
Train: 95 [ 150/1251 ( 12%)]  Loss:  3.759842 (3.5588)  Time: 1.073s,  954.77/s  (1.090s,  939.64/s)  LR: 7.746e-04  Data: 0.010 (0.013)
Train: 95 [ 200/1251 ( 16%)]  Loss:  3.437731 (3.5346)  Time: 1.082s,  946.46/s  (1.090s,  939.16/s)  LR: 7.746e-04  Data: 0.012 (0.013)
Train: 95 [ 250/1251 ( 20%)]  Loss:  3.768892 (3.5736)  Time: 1.077s,  950.80/s  (1.088s,  941.16/s)  LR: 7.746e-04  Data: 0.015 (0.012)
Train: 95 [ 300/1251 ( 24%)]  Loss:  3.502146 (3.5634)  Time: 1.098s,  932.30/s  (1.088s,  941.10/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [ 350/1251 ( 28%)]  Loss:  3.924829 (3.6086)  Time: 1.078s,  949.67/s  (1.089s,  940.56/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 400/1251 ( 32%)]  Loss:  4.068408 (3.6597)  Time: 1.076s,  951.77/s  (1.089s,  940.54/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 450/1251 ( 36%)]  Loss:  3.821981 (3.6759)  Time: 1.079s,  948.95/s  (1.089s,  940.71/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [ 500/1251 ( 40%)]  Loss:  3.254987 (3.6377)  Time: 1.084s,  945.07/s  (1.088s,  941.07/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 550/1251 ( 44%)]  Loss:  3.669152 (3.6403)  Time: 1.095s,  935.13/s  (1.089s,  940.59/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 600/1251 ( 48%)]  Loss:  3.359693 (3.6187)  Time: 1.096s,  934.51/s  (1.090s,  939.85/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [ 650/1251 ( 52%)]  Loss:  3.895452 (3.6385)  Time: 1.099s,  931.68/s  (1.090s,  939.88/s)  LR: 7.746e-04  Data: 0.018 (0.012)
Train: 95 [ 700/1251 ( 56%)]  Loss:  3.648187 (3.6391)  Time: 1.096s,  934.72/s  (1.090s,  939.72/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [ 750/1251 ( 60%)]  Loss:  3.543004 (3.6331)  Time: 1.078s,  949.88/s  (1.090s,  939.45/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 800/1251 ( 64%)]  Loss:  3.850071 (3.6459)  Time: 1.096s,  934.44/s  (1.090s,  939.65/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [ 850/1251 ( 68%)]  Loss:  3.605380 (3.6436)  Time: 1.173s,  873.09/s  (1.090s,  939.75/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 900/1251 ( 72%)]  Loss:  3.637533 (3.6433)  Time: 1.106s,  925.56/s  (1.090s,  939.85/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [ 950/1251 ( 76%)]  Loss:  3.581046 (3.6402)  Time: 1.085s,  944.00/s  (1.089s,  940.02/s)  LR: 7.746e-04  Data: 0.011 (0.012)
Train: 95 [1000/1251 ( 80%)]  Loss:  3.519178 (3.6344)  Time: 1.098s,  932.92/s  (1.089s,  940.34/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [1050/1251 ( 84%)]  Loss:  3.590562 (3.6324)  Time: 1.194s,  857.66/s  (1.090s,  939.79/s)  LR: 7.746e-04  Data: 0.013 (0.012)
Train: 95 [1100/1251 ( 88%)]  Loss:  3.348868 (3.6201)  Time: 1.094s,  936.14/s  (1.090s,  939.67/s)  LR: 7.746e-04  Data: 0.015 (0.012)
Train: 95 [1150/1251 ( 92%)]  Loss:  3.869678 (3.6305)  Time: 1.083s,  945.21/s  (1.090s,  939.75/s)  LR: 7.746e-04  Data: 0.014 (0.012)
Train: 95 [1200/1251 ( 96%)]  Loss:  3.507024 (3.6256)  Time: 1.080s,  948.45/s  (1.090s,  939.76/s)  LR: 7.746e-04  Data: 0.012 (0.012)
Train: 95 [1250/1251 (100%)]  Loss:  3.624711 (3.6255)  Time: 1.060s,  965.70/s  (1.090s,  939.58/s)  LR: 7.746e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.868 (5.868)  Loss:  0.6069 (0.6069)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6410 (1.1214)  Acc@1: 85.7311 (74.5180)  Acc@5: 97.2877 (92.5100)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 96 [   0/1251 (  0%)]  Loss:  3.345560 (3.3456)  Time: 1.086s,  942.58/s  (1.086s,  942.58/s)  LR: 7.702e-04  Data: 0.024 (0.024)
Train: 96 [  50/1251 (  4%)]  Loss:  3.517833 (3.4317)  Time: 1.096s,  934.01/s  (1.088s,  941.30/s)  LR: 7.702e-04  Data: 0.010 (0.013)
Train: 96 [ 100/1251 (  8%)]  Loss:  3.489371 (3.4509)  Time: 1.093s,  936.95/s  (1.093s,  937.03/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 150/1251 ( 12%)]  Loss:  3.372763 (3.4314)  Time: 1.108s,  924.38/s  (1.093s,  936.98/s)  LR: 7.702e-04  Data: 0.014 (0.012)
Train: 96 [ 200/1251 ( 16%)]  Loss:  3.922788 (3.5297)  Time: 1.079s,  949.21/s  (1.093s,  936.88/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 250/1251 ( 20%)]  Loss:  3.474416 (3.5205)  Time: 1.083s,  945.69/s  (1.092s,  937.85/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 300/1251 ( 24%)]  Loss:  3.770820 (3.5562)  Time: 1.110s,  922.51/s  (1.092s,  937.81/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [ 350/1251 ( 28%)]  Loss:  3.430350 (3.5405)  Time: 1.078s,  949.52/s  (1.091s,  938.18/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 400/1251 ( 32%)]  Loss:  3.320513 (3.5160)  Time: 1.082s,  946.53/s  (1.091s,  938.71/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [ 450/1251 ( 36%)]  Loss:  3.695386 (3.5340)  Time: 1.094s,  935.78/s  (1.090s,  939.02/s)  LR: 7.702e-04  Data: 0.013 (0.012)
Train: 96 [ 500/1251 ( 40%)]  Loss:  3.802263 (3.5584)  Time: 1.073s,  954.31/s  (1.091s,  938.96/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 550/1251 ( 44%)]  Loss:  3.606507 (3.5624)  Time: 1.095s,  934.86/s  (1.091s,  938.93/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 600/1251 ( 48%)]  Loss:  3.714126 (3.5741)  Time: 1.077s,  950.77/s  (1.091s,  938.27/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 650/1251 ( 52%)]  Loss:  3.688612 (3.5822)  Time: 1.091s,  938.42/s  (1.092s,  938.11/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [ 700/1251 ( 56%)]  Loss:  3.385387 (3.5691)  Time: 1.096s,  934.46/s  (1.091s,  938.51/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 750/1251 ( 60%)]  Loss:  3.787175 (3.5827)  Time: 1.085s,  944.03/s  (1.091s,  938.43/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 800/1251 ( 64%)]  Loss:  3.583045 (3.5828)  Time: 1.094s,  936.03/s  (1.091s,  938.47/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [ 850/1251 ( 68%)]  Loss:  3.701326 (3.5893)  Time: 1.096s,  934.50/s  (1.091s,  938.25/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [ 900/1251 ( 72%)]  Loss:  3.811104 (3.6010)  Time: 1.170s,  875.48/s  (1.091s,  938.47/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [ 950/1251 ( 76%)]  Loss:  4.036847 (3.6228)  Time: 1.098s,  932.96/s  (1.091s,  938.44/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [1000/1251 ( 80%)]  Loss:  3.751817 (3.6290)  Time: 1.073s,  954.50/s  (1.091s,  938.54/s)  LR: 7.702e-04  Data: 0.011 (0.012)
Train: 96 [1050/1251 ( 84%)]  Loss:  3.748597 (3.6344)  Time: 1.077s,  950.48/s  (1.091s,  938.92/s)  LR: 7.702e-04  Data: 0.013 (0.012)
Train: 96 [1100/1251 ( 88%)]  Loss:  3.635766 (3.6345)  Time: 1.096s,  934.57/s  (1.091s,  938.93/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [1150/1251 ( 92%)]  Loss:  3.421751 (3.6256)  Time: 1.171s,  874.57/s  (1.091s,  938.76/s)  LR: 7.702e-04  Data: 0.014 (0.012)
Train: 96 [1200/1251 ( 96%)]  Loss:  3.685976 (3.6280)  Time: 1.096s,  934.68/s  (1.091s,  938.74/s)  LR: 7.702e-04  Data: 0.012 (0.012)
Train: 96 [1250/1251 (100%)]  Loss:  3.551899 (3.6251)  Time: 1.061s,  964.82/s  (1.091s,  938.55/s)  LR: 7.702e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.919 (5.919)  Loss:  0.5937 (0.5937)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7381 (1.1174)  Acc@1: 84.5519 (74.6820)  Acc@5: 96.4623 (92.7040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 97 [   0/1251 (  0%)]  Loss:  3.611018 (3.6110)  Time: 1.090s,  939.87/s  (1.090s,  939.87/s)  LR: 7.658e-04  Data: 0.027 (0.027)
Train: 97 [  50/1251 (  4%)]  Loss:  3.552786 (3.5819)  Time: 1.093s,  936.67/s  (1.099s,  932.03/s)  LR: 7.658e-04  Data: 0.014 (0.013)
Train: 97 [ 100/1251 (  8%)]  Loss:  3.498487 (3.5541)  Time: 1.092s,  938.05/s  (1.093s,  937.07/s)  LR: 7.658e-04  Data: 0.013 (0.012)
Train: 97 [ 150/1251 ( 12%)]  Loss:  3.427211 (3.5224)  Time: 1.095s,  935.51/s  (1.092s,  937.50/s)  LR: 7.658e-04  Data: 0.013 (0.012)
Train: 97 [ 200/1251 ( 16%)]  Loss:  3.781087 (3.5741)  Time: 1.094s,  935.75/s  (1.094s,  936.34/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 250/1251 ( 20%)]  Loss:  3.546008 (3.5694)  Time: 1.093s,  937.13/s  (1.095s,  935.58/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 300/1251 ( 24%)]  Loss:  3.520548 (3.5624)  Time: 1.096s,  934.45/s  (1.095s,  935.53/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 350/1251 ( 28%)]  Loss:  3.602916 (3.5675)  Time: 1.078s,  950.28/s  (1.094s,  935.68/s)  LR: 7.658e-04  Data: 0.014 (0.012)
Train: 97 [ 400/1251 ( 32%)]  Loss:  3.571424 (3.5679)  Time: 1.174s,  872.27/s  (1.095s,  935.06/s)  LR: 7.658e-04  Data: 0.011 (0.012)
Train: 97 [ 450/1251 ( 36%)]  Loss:  3.340393 (3.5452)  Time: 1.095s,  934.84/s  (1.094s,  935.63/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 500/1251 ( 40%)]  Loss:  3.928177 (3.5800)  Time: 1.104s,  927.91/s  (1.094s,  936.24/s)  LR: 7.658e-04  Data: 0.013 (0.012)
Train: 97 [ 550/1251 ( 44%)]  Loss:  3.481621 (3.5718)  Time: 1.098s,  932.78/s  (1.094s,  936.39/s)  LR: 7.658e-04  Data: 0.010 (0.012)
Train: 97 [ 600/1251 ( 48%)]  Loss:  3.677948 (3.5800)  Time: 1.076s,  951.96/s  (1.093s,  936.89/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 650/1251 ( 52%)]  Loss:  3.635633 (3.5839)  Time: 1.083s,  945.71/s  (1.092s,  937.39/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 700/1251 ( 56%)]  Loss:  3.585701 (3.5841)  Time: 1.083s,  945.27/s  (1.092s,  937.71/s)  LR: 7.658e-04  Data: 0.015 (0.012)
Train: 97 [ 750/1251 ( 60%)]  Loss:  3.788932 (3.5969)  Time: 1.081s,  947.09/s  (1.091s,  938.16/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [ 800/1251 ( 64%)]  Loss:  3.941876 (3.6172)  Time: 1.172s,  873.79/s  (1.091s,  938.41/s)  LR: 7.658e-04  Data: 0.011 (0.012)
Train: 97 [ 850/1251 ( 68%)]  Loss:  3.641825 (3.6185)  Time: 1.084s,  944.52/s  (1.091s,  938.27/s)  LR: 7.658e-04  Data: 0.022 (0.012)
Train: 97 [ 900/1251 ( 72%)]  Loss:  3.695780 (3.6226)  Time: 1.079s,  949.44/s  (1.091s,  938.38/s)  LR: 7.658e-04  Data: 0.014 (0.012)
Train: 97 [ 950/1251 ( 76%)]  Loss:  3.647029 (3.6238)  Time: 1.076s,  951.89/s  (1.091s,  938.35/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [1000/1251 ( 80%)]  Loss:  3.925617 (3.6382)  Time: 1.094s,  935.90/s  (1.091s,  938.53/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [1050/1251 ( 84%)]  Loss:  3.299350 (3.6228)  Time: 1.093s,  937.12/s  (1.091s,  938.33/s)  LR: 7.658e-04  Data: 0.011 (0.012)
Train: 97 [1100/1251 ( 88%)]  Loss:  3.619544 (3.6226)  Time: 1.078s,  950.23/s  (1.091s,  938.42/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Train: 97 [1150/1251 ( 92%)]  Loss:  3.907167 (3.6345)  Time: 1.093s,  936.49/s  (1.091s,  938.22/s)  LR: 7.658e-04  Data: 0.011 (0.012)
Train: 97 [1200/1251 ( 96%)]  Loss:  3.806659 (3.6414)  Time: 1.096s,  934.07/s  (1.091s,  938.21/s)  LR: 7.658e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 97 [1250/1251 (100%)]  Loss:  3.343884 (3.6299)  Time: 1.079s,  948.81/s  (1.092s,  938.15/s)  LR: 7.658e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.754 (5.754)  Loss:  0.5538 (0.5538)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.6525 (1.0977)  Acc@1: 85.8491 (74.9260)  Acc@5: 96.9340 (92.7840)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 98 [   0/1251 (  0%)]  Loss:  3.473099 (3.4731)  Time: 1.088s,  940.98/s  (1.088s,  940.98/s)  LR: 7.614e-04  Data: 0.026 (0.026)
Train: 98 [  50/1251 (  4%)]  Loss:  3.396817 (3.4350)  Time: 1.093s,  937.12/s  (1.098s,  932.22/s)  LR: 7.614e-04  Data: 0.012 (0.013)
Train: 98 [ 100/1251 (  8%)]  Loss:  3.638655 (3.5029)  Time: 1.082s,  946.29/s  (1.096s,  934.12/s)  LR: 7.614e-04  Data: 0.012 (0.013)
Train: 98 [ 150/1251 ( 12%)]  Loss:  3.741168 (3.5624)  Time: 1.077s,  950.78/s  (1.093s,  937.02/s)  LR: 7.614e-04  Data: 0.012 (0.013)
Train: 98 [ 200/1251 ( 16%)]  Loss:  3.216879 (3.4933)  Time: 1.078s,  950.14/s  (1.092s,  938.08/s)  LR: 7.614e-04  Data: 0.013 (0.012)
Train: 98 [ 250/1251 ( 20%)]  Loss:  3.571428 (3.5063)  Time: 1.095s,  935.39/s  (1.090s,  939.37/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [ 300/1251 ( 24%)]  Loss:  3.544105 (3.5117)  Time: 1.076s,  951.31/s  (1.090s,  939.58/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [ 350/1251 ( 28%)]  Loss:  3.938218 (3.5650)  Time: 1.094s,  936.42/s  (1.089s,  939.89/s)  LR: 7.614e-04  Data: 0.015 (0.012)
Train: 98 [ 400/1251 ( 32%)]  Loss:  3.426908 (3.5497)  Time: 1.080s,  948.04/s  (1.090s,  939.84/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 98 [ 450/1251 ( 36%)]  Loss:  3.218509 (3.5166)  Time: 1.095s,  935.54/s  (1.089s,  939.96/s)  LR: 7.614e-04  Data: 0.010 (0.012)
Train: 98 [ 500/1251 ( 40%)]  Loss:  3.203681 (3.4881)  Time: 1.081s,  947.12/s  (1.089s,  940.19/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [ 550/1251 ( 44%)]  Loss:  3.776493 (3.5122)  Time: 1.079s,  949.06/s  (1.088s,  940.86/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [ 600/1251 ( 48%)]  Loss:  3.627672 (3.5210)  Time: 1.085s,  943.88/s  (1.089s,  940.59/s)  LR: 7.614e-04  Data: 0.013 (0.012)
Train: 98 [ 650/1251 ( 52%)]  Loss:  3.628837 (3.5287)  Time: 1.076s,  951.72/s  (1.088s,  940.80/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 98 [ 700/1251 ( 56%)]  Loss:  3.547176 (3.5300)  Time: 1.078s,  950.12/s  (1.088s,  940.97/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [ 750/1251 ( 60%)]  Loss:  3.356789 (3.5192)  Time: 1.083s,  945.77/s  (1.089s,  940.74/s)  LR: 7.614e-04  Data: 0.015 (0.012)
Train: 98 [ 800/1251 ( 64%)]  Loss:  3.680159 (3.5286)  Time: 1.169s,  875.95/s  (1.088s,  941.00/s)  LR: 7.614e-04  Data: 0.016 (0.012)
Train: 98 [ 850/1251 ( 68%)]  Loss:  3.435186 (3.5234)  Time: 1.078s,  950.25/s  (1.088s,  940.75/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [ 900/1251 ( 72%)]  Loss:  3.342222 (3.5139)  Time: 1.076s,  951.26/s  (1.089s,  940.38/s)  LR: 7.614e-04  Data: 0.014 (0.012)
Train: 98 [ 950/1251 ( 76%)]  Loss:  3.471094 (3.5118)  Time: 1.078s,  950.00/s  (1.089s,  940.14/s)  LR: 7.614e-04  Data: 0.014 (0.012)
Train: 98 [1000/1251 ( 80%)]  Loss:  3.792026 (3.5251)  Time: 1.094s,  936.08/s  (1.089s,  940.13/s)  LR: 7.614e-04  Data: 0.014 (0.012)
Train: 98 [1050/1251 ( 84%)]  Loss:  3.828208 (3.5389)  Time: 1.078s,  949.55/s  (1.089s,  940.18/s)  LR: 7.614e-04  Data: 0.012 (0.012)
Train: 98 [1100/1251 ( 88%)]  Loss:  3.535022 (3.5387)  Time: 1.092s,  937.69/s  (1.089s,  940.02/s)  LR: 7.614e-04  Data: 0.011 (0.012)
Train: 98 [1150/1251 ( 92%)]  Loss:  3.810176 (3.5500)  Time: 1.078s,  950.03/s  (1.089s,  940.02/s)  LR: 7.614e-04  Data: 0.013 (0.012)
Train: 98 [1200/1251 ( 96%)]  Loss:  3.636384 (3.5535)  Time: 1.094s,  935.94/s  (1.089s,  940.17/s)  LR: 7.614e-04  Data: 0.014 (0.012)
Train: 98 [1250/1251 (100%)]  Loss:  3.824721 (3.5639)  Time: 1.061s,  964.97/s  (1.090s,  939.88/s)  LR: 7.614e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.813 (5.813)  Loss:  0.6007 (0.6007)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.6804 (1.1345)  Acc@1: 85.1415 (74.4560)  Acc@5: 96.8160 (92.6940)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 74.45600000976563)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 99 [   0/1251 (  0%)]  Loss:  3.586468 (3.5865)  Time: 1.088s,  941.53/s  (1.088s,  941.53/s)  LR: 7.570e-04  Data: 0.025 (0.025)
Train: 99 [  50/1251 (  4%)]  Loss:  3.527070 (3.5568)  Time: 1.092s,  938.00/s  (1.091s,  938.26/s)  LR: 7.570e-04  Data: 0.012 (0.013)
Train: 99 [ 100/1251 (  8%)]  Loss:  3.602786 (3.5721)  Time: 1.093s,  936.68/s  (1.091s,  938.83/s)  LR: 7.570e-04  Data: 0.014 (0.012)
Train: 99 [ 150/1251 ( 12%)]  Loss:  3.364771 (3.5203)  Time: 1.078s,  950.03/s  (1.092s,  937.53/s)  LR: 7.570e-04  Data: 0.013 (0.012)
Train: 99 [ 200/1251 ( 16%)]  Loss:  3.598150 (3.5358)  Time: 1.075s,  952.22/s  (1.091s,  938.30/s)  LR: 7.570e-04  Data: 0.014 (0.012)
Train: 99 [ 250/1251 ( 20%)]  Loss:  3.495552 (3.5291)  Time: 1.087s,  942.36/s  (1.091s,  938.42/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [ 300/1251 ( 24%)]  Loss:  3.691410 (3.5523)  Time: 1.095s,  935.19/s  (1.090s,  939.28/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [ 350/1251 ( 28%)]  Loss:  3.931673 (3.5997)  Time: 1.105s,  926.95/s  (1.092s,  938.07/s)  LR: 7.570e-04  Data: 0.013 (0.012)
Train: 99 [ 400/1251 ( 32%)]  Loss:  3.628911 (3.6030)  Time: 1.081s,  947.19/s  (1.092s,  938.01/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [ 450/1251 ( 36%)]  Loss:  3.724183 (3.6151)  Time: 1.080s,  947.93/s  (1.091s,  938.24/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [ 500/1251 ( 40%)]  Loss:  3.795736 (3.6315)  Time: 1.076s,  951.75/s  (1.091s,  938.79/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [ 550/1251 ( 44%)]  Loss:  3.777931 (3.6437)  Time: 1.095s,  935.42/s  (1.091s,  938.60/s)  LR: 7.570e-04  Data: 0.018 (0.012)
Train: 99 [ 600/1251 ( 48%)]  Loss:  3.669539 (3.6457)  Time: 1.080s,  948.41/s  (1.091s,  938.46/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [ 650/1251 ( 52%)]  Loss:  3.572450 (3.6405)  Time: 1.078s,  950.00/s  (1.091s,  938.92/s)  LR: 7.570e-04  Data: 0.013 (0.012)
Train: 99 [ 700/1251 ( 56%)]  Loss:  3.207980 (3.6116)  Time: 1.108s,  923.80/s  (1.091s,  938.99/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 99 [ 750/1251 ( 60%)]  Loss:  3.724326 (3.6187)  Time: 1.078s,  949.84/s  (1.091s,  939.02/s)  LR: 7.570e-04  Data: 0.014 (0.012)
Train: 99 [ 800/1251 ( 64%)]  Loss:  3.667183 (3.6215)  Time: 1.085s,  943.45/s  (1.090s,  939.16/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [ 850/1251 ( 68%)]  Loss:  3.531112 (3.6165)  Time: 1.093s,  936.51/s  (1.090s,  939.39/s)  LR: 7.570e-04  Data: 0.015 (0.012)
Train: 99 [ 900/1251 ( 72%)]  Loss:  3.962751 (3.6347)  Time: 1.096s,  934.59/s  (1.090s,  939.53/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [ 950/1251 ( 76%)]  Loss:  3.579425 (3.6320)  Time: 1.080s,  948.38/s  (1.090s,  939.76/s)  LR: 7.570e-04  Data: 0.016 (0.012)
Train: 99 [1000/1251 ( 80%)]  Loss:  3.309066 (3.6166)  Time: 1.096s,  934.47/s  (1.090s,  939.51/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [1050/1251 ( 84%)]  Loss:  3.453332 (3.6092)  Time: 1.095s,  935.16/s  (1.090s,  939.14/s)  LR: 7.570e-04  Data: 0.012 (0.012)
Train: 99 [1100/1251 ( 88%)]  Loss:  3.812627 (3.6180)  Time: 1.096s,  934.00/s  (1.090s,  939.24/s)  LR: 7.570e-04  Data: 0.015 (0.012)
Train: 99 [1150/1251 ( 92%)]  Loss:  3.170226 (3.5994)  Time: 1.077s,  950.66/s  (1.090s,  939.36/s)  LR: 7.570e-04  Data: 0.014 (0.012)
Train: 99 [1200/1251 ( 96%)]  Loss:  3.649011 (3.6013)  Time: 1.096s,  934.44/s  (1.090s,  939.26/s)  LR: 7.570e-04  Data: 0.011 (0.012)
Train: 99 [1250/1251 (100%)]  Loss:  3.652472 (3.6033)  Time: 1.077s,  951.18/s  (1.091s,  939.00/s)  LR: 7.570e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.765 (5.765)  Loss:  0.6096 (0.6096)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.232 (0.443)  Loss:  0.6840 (1.1071)  Acc@1: 84.4340 (74.6000)  Acc@5: 96.5802 (92.7620)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 74.45600000976563)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-90.pth.tar', 74.15000011474609)

Train: 100 [   0/1251 (  0%)]  Loss:  3.594744 (3.5947)  Time: 1.087s,  941.62/s  (1.087s,  941.62/s)  LR: 7.525e-04  Data: 0.026 (0.026)
Train: 100 [  50/1251 (  4%)]  Loss:  3.521005 (3.5579)  Time: 1.077s,  950.63/s  (1.088s,  941.16/s)  LR: 7.525e-04  Data: 0.012 (0.013)
Train: 100 [ 100/1251 (  8%)]  Loss:  3.716985 (3.6109)  Time: 1.101s,  929.81/s  (1.089s,  940.17/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 150/1251 ( 12%)]  Loss:  3.361942 (3.5487)  Time: 1.077s,  950.44/s  (1.089s,  939.89/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [ 200/1251 ( 16%)]  Loss:  3.168593 (3.4727)  Time: 1.092s,  937.67/s  (1.089s,  940.37/s)  LR: 7.525e-04  Data: 0.014 (0.012)
Train: 100 [ 250/1251 ( 20%)]  Loss:  3.722954 (3.5144)  Time: 1.095s,  935.07/s  (1.089s,  940.58/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 300/1251 ( 24%)]  Loss:  3.805199 (3.5559)  Time: 1.078s,  949.54/s  (1.088s,  941.49/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 350/1251 ( 28%)]  Loss:  3.583299 (3.5593)  Time: 1.077s,  950.96/s  (1.088s,  941.08/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [ 400/1251 ( 32%)]  Loss:  4.099741 (3.6194)  Time: 1.078s,  949.97/s  (1.088s,  941.57/s)  LR: 7.525e-04  Data: 0.013 (0.012)
Train: 100 [ 450/1251 ( 36%)]  Loss:  3.551273 (3.6126)  Time: 1.079s,  948.76/s  (1.087s,  942.30/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 500/1251 ( 40%)]  Loss:  3.484240 (3.6009)  Time: 1.094s,  935.91/s  (1.087s,  942.32/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 550/1251 ( 44%)]  Loss:  3.790249 (3.6167)  Time: 1.076s,  952.11/s  (1.087s,  942.42/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 600/1251 ( 48%)]  Loss:  3.420156 (3.6016)  Time: 1.078s,  950.23/s  (1.087s,  942.48/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [ 650/1251 ( 52%)]  Loss:  3.491428 (3.5937)  Time: 1.093s,  937.25/s  (1.087s,  942.35/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [ 700/1251 ( 56%)]  Loss:  3.722682 (3.6023)  Time: 1.081s,  946.95/s  (1.087s,  942.08/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [ 750/1251 ( 60%)]  Loss:  3.640357 (3.6047)  Time: 1.076s,  951.63/s  (1.087s,  942.35/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [ 800/1251 ( 64%)]  Loss:  3.658364 (3.6078)  Time: 1.100s,  930.49/s  (1.086s,  942.63/s)  LR: 7.525e-04  Data: 0.013 (0.012)
Train: 100 [ 850/1251 ( 68%)]  Loss:  3.550888 (3.6047)  Time: 1.081s,  947.17/s  (1.087s,  942.23/s)  LR: 7.525e-04  Data: 0.013 (0.012)
Train: 100 [ 900/1251 ( 72%)]  Loss:  3.647308 (3.6069)  Time: 1.076s,  951.27/s  (1.087s,  942.22/s)  LR: 7.525e-04  Data: 0.013 (0.012)
Train: 100 [ 950/1251 ( 76%)]  Loss:  3.689525 (3.6110)  Time: 1.096s,  934.69/s  (1.087s,  941.69/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [1000/1251 ( 80%)]  Loss:  3.623099 (3.6116)  Time: 1.077s,  950.50/s  (1.088s,  941.51/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [1050/1251 ( 84%)]  Loss:  3.622614 (3.6121)  Time: 1.100s,  931.29/s  (1.088s,  941.45/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [1100/1251 ( 88%)]  Loss:  3.964636 (3.6274)  Time: 1.092s,  937.56/s  (1.088s,  941.24/s)  LR: 7.525e-04  Data: 0.011 (0.012)
Train: 100 [1150/1251 ( 92%)]  Loss:  3.333839 (3.6152)  Time: 1.095s,  935.48/s  (1.088s,  940.80/s)  LR: 7.525e-04  Data: 0.013 (0.012)
Train: 100 [1200/1251 ( 96%)]  Loss:  3.368275 (3.6053)  Time: 1.096s,  933.96/s  (1.089s,  940.51/s)  LR: 7.525e-04  Data: 0.012 (0.012)
Train: 100 [1250/1251 (100%)]  Loss:  3.481524 (3.6006)  Time: 1.078s,  949.77/s  (1.089s,  940.37/s)  LR: 7.525e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 6.208 (6.208)  Loss:  0.6284 (0.6284)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.461)  Loss:  0.6513 (1.1327)  Acc@1: 84.6698 (74.7060)  Acc@5: 96.9340 (92.5480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 74.45600000976563)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-92.pth.tar', 74.26800003662109)

Train: 101 [   0/1251 (  0%)]  Loss:  3.906529 (3.9065)  Time: 1.089s,  940.52/s  (1.089s,  940.52/s)  LR: 7.480e-04  Data: 0.027 (0.027)
Train: 101 [  50/1251 (  4%)]  Loss:  3.249194 (3.5779)  Time: 1.095s,  934.77/s  (1.084s,  944.43/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [ 100/1251 (  8%)]  Loss:  3.719235 (3.6250)  Time: 1.098s,  932.46/s  (1.091s,  938.30/s)  LR: 7.480e-04  Data: 0.011 (0.013)
Train: 101 [ 150/1251 ( 12%)]  Loss:  3.733678 (3.6522)  Time: 1.077s,  951.02/s  (1.087s,  941.70/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [ 200/1251 ( 16%)]  Loss:  3.327372 (3.5872)  Time: 1.079s,  949.19/s  (1.086s,  942.79/s)  LR: 7.480e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 101 [ 250/1251 ( 20%)]  Loss:  3.425848 (3.5603)  Time: 1.096s,  934.65/s  (1.088s,  941.53/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [ 300/1251 ( 24%)]  Loss:  3.712970 (3.5821)  Time: 1.084s,  944.87/s  (1.089s,  940.66/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [ 350/1251 ( 28%)]  Loss:  3.646309 (3.5901)  Time: 1.073s,  954.36/s  (1.089s,  940.18/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [ 400/1251 ( 32%)]  Loss:  3.553389 (3.5861)  Time: 1.079s,  949.23/s  (1.089s,  940.50/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [ 450/1251 ( 36%)]  Loss:  3.658585 (3.5933)  Time: 1.080s,  947.83/s  (1.088s,  940.87/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [ 500/1251 ( 40%)]  Loss:  3.629910 (3.5966)  Time: 1.078s,  949.70/s  (1.088s,  941.17/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [ 550/1251 ( 44%)]  Loss:  3.740432 (3.6086)  Time: 1.085s,  944.03/s  (1.088s,  941.17/s)  LR: 7.480e-04  Data: 0.021 (0.012)
Train: 101 [ 600/1251 ( 48%)]  Loss:  3.822255 (3.6251)  Time: 1.084s,  944.33/s  (1.088s,  941.32/s)  LR: 7.480e-04  Data: 0.013 (0.012)
Train: 101 [ 650/1251 ( 52%)]  Loss:  3.233739 (3.5971)  Time: 1.099s,  931.87/s  (1.088s,  941.58/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [ 700/1251 ( 56%)]  Loss:  3.522705 (3.5921)  Time: 1.084s,  944.42/s  (1.088s,  941.25/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [ 750/1251 ( 60%)]  Loss:  3.500839 (3.5864)  Time: 1.094s,  936.32/s  (1.088s,  941.00/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [ 800/1251 ( 64%)]  Loss:  3.447766 (3.5783)  Time: 1.079s,  948.66/s  (1.088s,  941.05/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [ 850/1251 ( 68%)]  Loss:  3.486454 (3.5732)  Time: 1.095s,  935.35/s  (1.088s,  941.03/s)  LR: 7.480e-04  Data: 0.015 (0.012)
Train: 101 [ 900/1251 ( 72%)]  Loss:  3.501253 (3.5694)  Time: 1.076s,  951.38/s  (1.088s,  941.13/s)  LR: 7.480e-04  Data: 0.011 (0.012)
Train: 101 [ 950/1251 ( 76%)]  Loss:  3.629137 (3.5724)  Time: 1.079s,  948.79/s  (1.088s,  941.24/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [1000/1251 ( 80%)]  Loss:  3.293007 (3.5591)  Time: 1.077s,  950.53/s  (1.088s,  941.50/s)  LR: 7.480e-04  Data: 0.015 (0.012)
Train: 101 [1050/1251 ( 84%)]  Loss:  3.481027 (3.5555)  Time: 1.095s,  935.27/s  (1.088s,  941.27/s)  LR: 7.480e-04  Data: 0.012 (0.012)
Train: 101 [1100/1251 ( 88%)]  Loss:  3.687231 (3.5613)  Time: 1.077s,  950.69/s  (1.088s,  941.07/s)  LR: 7.480e-04  Data: 0.014 (0.013)
Train: 101 [1150/1251 ( 92%)]  Loss:  3.274769 (3.5493)  Time: 1.076s,  951.56/s  (1.088s,  941.23/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Train: 101 [1200/1251 ( 96%)]  Loss:  3.315186 (3.5400)  Time: 1.108s,  923.97/s  (1.088s,  941.36/s)  LR: 7.480e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 101 [1250/1251 (100%)]  Loss:  3.988472 (3.5572)  Time: 1.064s,  962.30/s  (1.088s,  941.06/s)  LR: 7.480e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.852 (5.852)  Loss:  0.5941 (0.5941)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.7367 (1.1608)  Acc@1: 84.5519 (74.6900)  Acc@5: 97.0519 (92.5520)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-98.pth.tar', 74.45600000976563)

Train: 102 [   0/1251 (  0%)]  Loss:  3.879331 (3.8793)  Time: 1.090s,  939.42/s  (1.090s,  939.42/s)  LR: 7.435e-04  Data: 0.027 (0.027)
Train: 102 [  50/1251 (  4%)]  Loss:  3.250712 (3.5650)  Time: 1.094s,  936.03/s  (1.085s,  943.84/s)  LR: 7.435e-04  Data: 0.011 (0.013)
Train: 102 [ 100/1251 (  8%)]  Loss:  3.440746 (3.5236)  Time: 1.095s,  935.49/s  (1.085s,  943.89/s)  LR: 7.435e-04  Data: 0.013 (0.013)
Train: 102 [ 150/1251 ( 12%)]  Loss:  3.852920 (3.6059)  Time: 1.083s,  945.31/s  (1.088s,  941.08/s)  LR: 7.435e-04  Data: 0.012 (0.013)
Train: 102 [ 200/1251 ( 16%)]  Loss:  3.534619 (3.5917)  Time: 1.079s,  949.41/s  (1.087s,  941.80/s)  LR: 7.435e-04  Data: 0.012 (0.013)
Train: 102 [ 250/1251 ( 20%)]  Loss:  3.353575 (3.5520)  Time: 1.094s,  936.36/s  (1.087s,  941.69/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 102 [ 300/1251 ( 24%)]  Loss:  3.757200 (3.5813)  Time: 1.104s,  927.14/s  (1.088s,  941.33/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 102 [ 350/1251 ( 28%)]  Loss:  3.432515 (3.5627)  Time: 1.077s,  951.10/s  (1.088s,  941.01/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 400/1251 ( 32%)]  Loss:  3.803020 (3.5894)  Time: 1.083s,  945.95/s  (1.088s,  941.11/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 450/1251 ( 36%)]  Loss:  3.597816 (3.5902)  Time: 1.075s,  952.18/s  (1.088s,  941.47/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 102 [ 500/1251 ( 40%)]  Loss:  3.497803 (3.5818)  Time: 1.096s,  934.36/s  (1.088s,  941.27/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 550/1251 ( 44%)]  Loss:  3.817731 (3.6015)  Time: 1.082s,  946.57/s  (1.088s,  940.98/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 102 [ 600/1251 ( 48%)]  Loss:  3.560385 (3.5983)  Time: 1.097s,  933.65/s  (1.088s,  941.10/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 650/1251 ( 52%)]  Loss:  3.706367 (3.6061)  Time: 1.097s,  933.26/s  (1.089s,  940.53/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 700/1251 ( 56%)]  Loss:  3.385895 (3.5914)  Time: 1.103s,  928.31/s  (1.088s,  940.75/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 102 [ 750/1251 ( 60%)]  Loss:  3.400302 (3.5794)  Time: 1.095s,  935.03/s  (1.089s,  940.36/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 800/1251 ( 64%)]  Loss:  3.560337 (3.5783)  Time: 1.077s,  950.56/s  (1.089s,  940.57/s)  LR: 7.435e-04  Data: 0.014 (0.012)
Train: 102 [ 850/1251 ( 68%)]  Loss:  3.578141 (3.5783)  Time: 1.107s,  924.74/s  (1.089s,  940.65/s)  LR: 7.435e-04  Data: 0.011 (0.012)
Train: 102 [ 900/1251 ( 72%)]  Loss:  3.512031 (3.5748)  Time: 1.080s,  947.95/s  (1.088s,  940.78/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [ 950/1251 ( 76%)]  Loss:  3.287013 (3.5604)  Time: 1.086s,  943.25/s  (1.088s,  940.82/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [1000/1251 ( 80%)]  Loss:  3.783411 (3.5710)  Time: 1.099s,  932.15/s  (1.089s,  940.63/s)  LR: 7.435e-04  Data: 0.014 (0.012)
Train: 102 [1050/1251 ( 84%)]  Loss:  3.488831 (3.5673)  Time: 1.076s,  951.45/s  (1.089s,  940.68/s)  LR: 7.435e-04  Data: 0.013 (0.012)
Train: 102 [1100/1251 ( 88%)]  Loss:  3.515591 (3.5651)  Time: 1.080s,  948.34/s  (1.089s,  940.42/s)  LR: 7.435e-04  Data: 0.015 (0.012)
Train: 102 [1150/1251 ( 92%)]  Loss:  3.620590 (3.5674)  Time: 1.076s,  951.76/s  (1.089s,  940.35/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [1200/1251 ( 96%)]  Loss:  3.890773 (3.5803)  Time: 1.077s,  950.50/s  (1.089s,  940.54/s)  LR: 7.435e-04  Data: 0.012 (0.012)
Train: 102 [1250/1251 (100%)]  Loss:  3.726892 (3.5859)  Time: 1.186s,  863.48/s  (1.089s,  940.09/s)  LR: 7.435e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.882 (5.882)  Loss:  0.5792 (0.5792)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.7299 (1.1097)  Acc@1: 84.9057 (75.0040)  Acc@5: 96.5802 (92.8760)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-93.pth.tar', 74.4660000366211)

Train: 103 [   0/1251 (  0%)]  Loss:  3.677974 (3.6780)  Time: 1.088s,  941.58/s  (1.088s,  941.58/s)  LR: 7.389e-04  Data: 0.027 (0.027)
Train: 103 [  50/1251 (  4%)]  Loss:  4.029574 (3.8538)  Time: 1.080s,  948.57/s  (1.083s,  945.19/s)  LR: 7.389e-04  Data: 0.018 (0.013)
Train: 103 [ 100/1251 (  8%)]  Loss:  3.183455 (3.6303)  Time: 1.081s,  947.43/s  (1.089s,  940.58/s)  LR: 7.389e-04  Data: 0.012 (0.013)
Train: 103 [ 150/1251 ( 12%)]  Loss:  3.563818 (3.6137)  Time: 1.094s,  935.99/s  (1.090s,  939.30/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 200/1251 ( 16%)]  Loss:  3.269866 (3.5449)  Time: 1.077s,  950.38/s  (1.091s,  938.78/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 250/1251 ( 20%)]  Loss:  3.362534 (3.5145)  Time: 1.078s,  949.51/s  (1.089s,  940.06/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 300/1251 ( 24%)]  Loss:  3.573050 (3.5229)  Time: 1.077s,  950.68/s  (1.089s,  940.18/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 350/1251 ( 28%)]  Loss:  3.689464 (3.5437)  Time: 1.078s,  949.98/s  (1.089s,  940.15/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 400/1251 ( 32%)]  Loss:  3.458439 (3.5342)  Time: 1.078s,  950.05/s  (1.089s,  940.28/s)  LR: 7.389e-04  Data: 0.011 (0.012)
Train: 103 [ 450/1251 ( 36%)]  Loss:  3.372476 (3.5181)  Time: 1.086s,  942.60/s  (1.089s,  940.56/s)  LR: 7.389e-04  Data: 0.014 (0.012)
Train: 103 [ 500/1251 ( 40%)]  Loss:  3.727370 (3.5371)  Time: 1.099s,  932.10/s  (1.089s,  940.43/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 550/1251 ( 44%)]  Loss:  3.375072 (3.5236)  Time: 1.098s,  932.39/s  (1.089s,  940.14/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 600/1251 ( 48%)]  Loss:  3.604233 (3.5298)  Time: 1.094s,  936.06/s  (1.089s,  940.30/s)  LR: 7.389e-04  Data: 0.011 (0.012)
Train: 103 [ 650/1251 ( 52%)]  Loss:  3.602762 (3.5350)  Time: 1.078s,  949.93/s  (1.089s,  940.46/s)  LR: 7.389e-04  Data: 0.013 (0.012)
Train: 103 [ 700/1251 ( 56%)]  Loss:  3.454720 (3.5297)  Time: 1.077s,  950.83/s  (1.088s,  940.92/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 750/1251 ( 60%)]  Loss:  3.345101 (3.5181)  Time: 1.109s,  922.97/s  (1.088s,  940.97/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 800/1251 ( 64%)]  Loss:  3.364246 (3.5091)  Time: 1.109s,  923.59/s  (1.088s,  941.08/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [ 850/1251 ( 68%)]  Loss:  3.669451 (3.5180)  Time: 1.079s,  949.06/s  (1.088s,  941.21/s)  LR: 7.389e-04  Data: 0.015 (0.012)
Train: 103 [ 900/1251 ( 72%)]  Loss:  3.317827 (3.5074)  Time: 1.077s,  951.06/s  (1.088s,  941.19/s)  LR: 7.389e-04  Data: 0.014 (0.012)
Train: 103 [ 950/1251 ( 76%)]  Loss:  3.992642 (3.5317)  Time: 1.093s,  936.89/s  (1.088s,  941.19/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [1000/1251 ( 80%)]  Loss:  3.137972 (3.5130)  Time: 1.080s,  948.08/s  (1.088s,  941.23/s)  LR: 7.389e-04  Data: 0.013 (0.012)
Train: 103 [1050/1251 ( 84%)]  Loss:  3.794442 (3.5257)  Time: 1.079s,  949.08/s  (1.088s,  941.03/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [1100/1251 ( 88%)]  Loss:  3.712518 (3.5339)  Time: 1.071s,  956.08/s  (1.088s,  941.17/s)  LR: 7.389e-04  Data: 0.011 (0.012)
Train: 103 [1150/1251 ( 92%)]  Loss:  3.749430 (3.5429)  Time: 1.093s,  937.28/s  (1.088s,  941.31/s)  LR: 7.389e-04  Data: 0.012 (0.012)
Train: 103 [1200/1251 ( 96%)]  Loss:  3.830463 (3.5544)  Time: 1.078s,  950.09/s  (1.088s,  941.51/s)  LR: 7.389e-04  Data: 0.014 (0.012)
Train: 103 [1250/1251 (100%)]  Loss:  3.561955 (3.5546)  Time: 1.081s,  947.00/s  (1.088s,  941.32/s)  LR: 7.389e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.842 (5.842)  Loss:  0.5744 (0.5744)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7337 (1.0835)  Acc@1: 84.3160 (75.1120)  Acc@5: 95.5189 (92.7960)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-95.pth.tar', 74.51800000732422)

Train: 104 [   0/1251 (  0%)]  Loss:  3.578536 (3.5785)  Time: 1.093s,  937.24/s  (1.093s,  937.24/s)  LR: 7.343e-04  Data: 0.027 (0.027)
Train: 104 [  50/1251 (  4%)]  Loss:  3.589807 (3.5842)  Time: 1.077s,  951.01/s  (1.084s,  944.41/s)  LR: 7.343e-04  Data: 0.015 (0.013)
Train: 104 [ 100/1251 (  8%)]  Loss:  3.467087 (3.5451)  Time: 1.087s,  941.68/s  (1.088s,  940.88/s)  LR: 7.343e-04  Data: 0.013 (0.013)
Train: 104 [ 150/1251 ( 12%)]  Loss:  3.344434 (3.4950)  Time: 1.177s,  870.03/s  (1.088s,  940.88/s)  LR: 7.343e-04  Data: 0.011 (0.013)
Train: 104 [ 200/1251 ( 16%)]  Loss:  3.556717 (3.5073)  Time: 1.077s,  950.69/s  (1.088s,  941.21/s)  LR: 7.343e-04  Data: 0.013 (0.012)
Train: 104 [ 250/1251 ( 20%)]  Loss:  3.701640 (3.5397)  Time: 1.098s,  932.49/s  (1.088s,  941.48/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 300/1251 ( 24%)]  Loss:  3.708458 (3.5638)  Time: 1.096s,  933.94/s  (1.088s,  941.13/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 104 [ 350/1251 ( 28%)]  Loss:  3.357718 (3.5380)  Time: 1.095s,  934.76/s  (1.089s,  940.23/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 400/1251 ( 32%)]  Loss:  3.660949 (3.5517)  Time: 1.109s,  923.41/s  (1.091s,  938.97/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 450/1251 ( 36%)]  Loss:  3.721247 (3.5687)  Time: 1.086s,  942.74/s  (1.091s,  938.75/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 104 [ 500/1251 ( 40%)]  Loss:  3.596966 (3.5712)  Time: 1.097s,  933.40/s  (1.091s,  938.94/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 550/1251 ( 44%)]  Loss:  3.709810 (3.5828)  Time: 1.092s,  937.68/s  (1.091s,  938.60/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 600/1251 ( 48%)]  Loss:  3.732313 (3.5943)  Time: 1.075s,  952.28/s  (1.091s,  938.67/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 650/1251 ( 52%)]  Loss:  3.568392 (3.5924)  Time: 1.098s,  932.56/s  (1.091s,  938.76/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 700/1251 ( 56%)]  Loss:  3.733405 (3.6018)  Time: 1.078s,  949.82/s  (1.091s,  938.84/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 750/1251 ( 60%)]  Loss:  3.380917 (3.5880)  Time: 1.077s,  950.35/s  (1.091s,  938.97/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 104 [ 800/1251 ( 64%)]  Loss:  3.693790 (3.5942)  Time: 1.099s,  931.54/s  (1.090s,  939.18/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 104 [ 850/1251 ( 68%)]  Loss:  3.683129 (3.5992)  Time: 1.079s,  949.33/s  (1.090s,  939.42/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [ 900/1251 ( 72%)]  Loss:  3.288094 (3.5828)  Time: 1.078s,  949.76/s  (1.090s,  939.50/s)  LR: 7.343e-04  Data: 0.013 (0.012)
Train: 104 [ 950/1251 ( 76%)]  Loss:  3.699247 (3.5886)  Time: 1.077s,  950.53/s  (1.090s,  939.42/s)  LR: 7.343e-04  Data: 0.014 (0.012)
Train: 104 [1000/1251 ( 80%)]  Loss:  3.414920 (3.5804)  Time: 1.095s,  934.79/s  (1.090s,  939.49/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [1050/1251 ( 84%)]  Loss:  3.746532 (3.5879)  Time: 1.098s,  933.01/s  (1.090s,  939.06/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [1100/1251 ( 88%)]  Loss:  3.628211 (3.5897)  Time: 1.080s,  947.75/s  (1.091s,  939.00/s)  LR: 7.343e-04  Data: 0.013 (0.012)
Train: 104 [1150/1251 ( 92%)]  Loss:  3.685160 (3.5936)  Time: 1.097s,  933.21/s  (1.091s,  938.78/s)  LR: 7.343e-04  Data: 0.012 (0.012)
Train: 104 [1200/1251 ( 96%)]  Loss:  3.415223 (3.5865)  Time: 1.097s,  933.80/s  (1.091s,  938.83/s)  LR: 7.343e-04  Data: 0.011 (0.012)
Train: 104 [1250/1251 (100%)]  Loss:  3.689626 (3.5905)  Time: 1.062s,  964.42/s  (1.091s,  938.77/s)  LR: 7.343e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.796 (5.796)  Loss:  0.5860 (0.5860)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.6035 (1.0833)  Acc@1: 87.1462 (75.0220)  Acc@5: 97.6415 (92.9480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-91.pth.tar', 74.5219999609375)

Train: 105 [   0/1251 (  0%)]  Loss:  3.737557 (3.7376)  Time: 2.608s,  392.59/s  (2.608s,  392.59/s)  LR: 7.297e-04  Data: 1.425 (1.425)
Train: 105 [  50/1251 (  4%)]  Loss:  3.472131 (3.6048)  Time: 1.077s,  951.08/s  (1.115s,  918.14/s)  LR: 7.297e-04  Data: 0.012 (0.041)
Train: 105 [ 100/1251 (  8%)]  Loss:  3.652674 (3.6208)  Time: 1.077s,  950.65/s  (1.101s,  930.03/s)  LR: 7.297e-04  Data: 0.012 (0.027)
Train: 105 [ 150/1251 ( 12%)]  Loss:  3.253685 (3.5290)  Time: 1.097s,  933.86/s  (1.095s,  935.35/s)  LR: 7.297e-04  Data: 0.012 (0.022)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 105 [ 200/1251 ( 16%)]  Loss:  3.130656 (3.4493)  Time: 1.081s,  946.91/s  (1.094s,  936.32/s)  LR: 7.297e-04  Data: 0.012 (0.019)
Train: 105 [ 250/1251 ( 20%)]  Loss:  3.666276 (3.4855)  Time: 1.075s,  952.23/s  (1.093s,  936.72/s)  LR: 7.297e-04  Data: 0.012 (0.018)
Train: 105 [ 300/1251 ( 24%)]  Loss:  3.624262 (3.5053)  Time: 1.083s,  945.15/s  (1.092s,  937.66/s)  LR: 7.297e-04  Data: 0.015 (0.017)
Train: 105 [ 350/1251 ( 28%)]  Loss:  3.464652 (3.5002)  Time: 1.078s,  949.87/s  (1.092s,  938.10/s)  LR: 7.297e-04  Data: 0.011 (0.017)
Train: 105 [ 400/1251 ( 32%)]  Loss:  3.844481 (3.5385)  Time: 1.080s,  948.38/s  (1.090s,  939.33/s)  LR: 7.297e-04  Data: 0.011 (0.016)
Train: 105 [ 450/1251 ( 36%)]  Loss:  3.554537 (3.5401)  Time: 1.076s,  951.75/s  (1.091s,  938.73/s)  LR: 7.297e-04  Data: 0.012 (0.016)
Train: 105 [ 500/1251 ( 40%)]  Loss:  3.293260 (3.5177)  Time: 1.096s,  934.46/s  (1.090s,  939.08/s)  LR: 7.297e-04  Data: 0.012 (0.015)
Train: 105 [ 550/1251 ( 44%)]  Loss:  3.908897 (3.5503)  Time: 1.095s,  935.06/s  (1.090s,  939.18/s)  LR: 7.297e-04  Data: 0.012 (0.015)
Train: 105 [ 600/1251 ( 48%)]  Loss:  3.723730 (3.5636)  Time: 1.181s,  866.76/s  (1.090s,  939.22/s)  LR: 7.297e-04  Data: 0.012 (0.015)
Train: 105 [ 650/1251 ( 52%)]  Loss:  3.424256 (3.5536)  Time: 1.086s,  943.17/s  (1.090s,  939.49/s)  LR: 7.297e-04  Data: 0.011 (0.015)
Train: 105 [ 700/1251 ( 56%)]  Loss:  3.391801 (3.5429)  Time: 1.104s,  927.65/s  (1.090s,  939.08/s)  LR: 7.297e-04  Data: 0.012 (0.015)
Train: 105 [ 750/1251 ( 60%)]  Loss:  2.913391 (3.5035)  Time: 1.085s,  943.82/s  (1.090s,  939.49/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [ 800/1251 ( 64%)]  Loss:  3.695220 (3.5148)  Time: 1.097s,  933.46/s  (1.090s,  939.34/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [ 850/1251 ( 68%)]  Loss:  3.679139 (3.5239)  Time: 1.100s,  931.22/s  (1.090s,  939.30/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [ 900/1251 ( 72%)]  Loss:  3.732474 (3.5349)  Time: 1.077s,  950.81/s  (1.090s,  939.44/s)  LR: 7.297e-04  Data: 0.013 (0.014)
Train: 105 [ 950/1251 ( 76%)]  Loss:  3.735688 (3.5449)  Time: 1.096s,  934.34/s  (1.090s,  939.31/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [1000/1251 ( 80%)]  Loss:  3.160301 (3.5266)  Time: 1.083s,  945.14/s  (1.090s,  939.49/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [1050/1251 ( 84%)]  Loss:  3.591371 (3.5296)  Time: 1.094s,  936.39/s  (1.090s,  939.41/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 105 [1100/1251 ( 88%)]  Loss:  3.626694 (3.5338)  Time: 1.079s,  948.71/s  (1.090s,  939.24/s)  LR: 7.297e-04  Data: 0.014 (0.014)
Train: 105 [1150/1251 ( 92%)]  Loss:  3.365085 (3.5268)  Time: 1.079s,  948.75/s  (1.090s,  939.43/s)  LR: 7.297e-04  Data: 0.011 (0.014)
Train: 105 [1200/1251 ( 96%)]  Loss:  3.387109 (3.5212)  Time: 1.098s,  932.20/s  (1.090s,  939.53/s)  LR: 7.297e-04  Data: 0.012 (0.014)
Train: 105 [1250/1251 (100%)]  Loss:  3.687192 (3.5276)  Time: 1.078s,  950.11/s  (1.090s,  939.31/s)  LR: 7.297e-04  Data: 0.000 (0.014)
Test: [   0/48]  Time: 5.796 (5.796)  Loss:  0.5470 (0.5470)  Acc@1: 88.8672 (88.8672)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6741 (1.0975)  Acc@1: 84.7877 (74.7100)  Acc@5: 96.9340 (92.7340)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 74.7100001147461)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-99.pth.tar', 74.5999999609375)

Train: 106 [   0/1251 (  0%)]  Loss:  3.525276 (3.5253)  Time: 1.090s,  939.31/s  (1.090s,  939.31/s)  LR: 7.251e-04  Data: 0.028 (0.028)
Train: 106 [  50/1251 (  4%)]  Loss:  3.714847 (3.6201)  Time: 1.078s,  949.86/s  (1.089s,  940.06/s)  LR: 7.251e-04  Data: 0.015 (0.013)
Train: 106 [ 100/1251 (  8%)]  Loss:  3.846618 (3.6956)  Time: 1.079s,  948.84/s  (1.090s,  939.77/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 150/1251 ( 12%)]  Loss:  3.416696 (3.6259)  Time: 1.077s,  951.05/s  (1.090s,  939.39/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 200/1251 ( 16%)]  Loss:  3.400463 (3.5808)  Time: 1.078s,  949.93/s  (1.092s,  937.50/s)  LR: 7.251e-04  Data: 0.014 (0.013)
Train: 106 [ 250/1251 ( 20%)]  Loss:  3.399377 (3.5505)  Time: 1.094s,  936.09/s  (1.091s,  938.72/s)  LR: 7.251e-04  Data: 0.011 (0.013)
Train: 106 [ 300/1251 ( 24%)]  Loss:  3.569009 (3.5532)  Time: 1.076s,  951.52/s  (1.091s,  938.72/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 350/1251 ( 28%)]  Loss:  3.494434 (3.5458)  Time: 1.087s,  941.87/s  (1.091s,  938.82/s)  LR: 7.251e-04  Data: 0.013 (0.013)
Train: 106 [ 400/1251 ( 32%)]  Loss:  3.269656 (3.5152)  Time: 1.077s,  950.36/s  (1.090s,  939.44/s)  LR: 7.251e-04  Data: 0.013 (0.013)
Train: 106 [ 450/1251 ( 36%)]  Loss:  3.762483 (3.5399)  Time: 1.084s,  944.24/s  (1.089s,  939.96/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 500/1251 ( 40%)]  Loss:  3.636813 (3.5487)  Time: 1.093s,  936.70/s  (1.089s,  939.98/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 550/1251 ( 44%)]  Loss:  3.333525 (3.5308)  Time: 1.077s,  950.66/s  (1.089s,  939.98/s)  LR: 7.251e-04  Data: 0.012 (0.013)
Train: 106 [ 600/1251 ( 48%)]  Loss:  3.098374 (3.4975)  Time: 1.079s,  948.83/s  (1.089s,  940.27/s)  LR: 7.251e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 106 [ 650/1251 ( 52%)]  Loss:  3.773433 (3.5172)  Time: 1.095s,  934.87/s  (1.089s,  940.05/s)  LR: 7.251e-04  Data: 0.011 (0.012)
Train: 106 [ 700/1251 ( 56%)]  Loss:  3.662977 (3.5269)  Time: 1.082s,  946.81/s  (1.089s,  940.11/s)  LR: 7.251e-04  Data: 0.012 (0.012)
Train: 106 [ 750/1251 ( 60%)]  Loss:  3.505570 (3.5256)  Time: 1.079s,  948.82/s  (1.089s,  940.52/s)  LR: 7.251e-04  Data: 0.012 (0.012)
Train: 106 [ 800/1251 ( 64%)]  Loss:  3.582140 (3.5289)  Time: 1.079s,  949.20/s  (1.089s,  940.35/s)  LR: 7.251e-04  Data: 0.013 (0.012)
Train: 106 [ 850/1251 ( 68%)]  Loss:  3.240263 (3.5129)  Time: 1.173s,  872.73/s  (1.089s,  940.26/s)  LR: 7.251e-04  Data: 0.015 (0.012)
Train: 106 [ 900/1251 ( 72%)]  Loss:  3.805483 (3.5283)  Time: 1.077s,  951.14/s  (1.089s,  940.15/s)  LR: 7.251e-04  Data: 0.012 (0.012)
Train: 106 [ 950/1251 ( 76%)]  Loss:  3.432996 (3.5235)  Time: 1.081s,  947.34/s  (1.089s,  940.38/s)  LR: 7.251e-04  Data: 0.014 (0.012)
Train: 106 [1000/1251 ( 80%)]  Loss:  3.431323 (3.5191)  Time: 1.078s,  950.34/s  (1.089s,  940.38/s)  LR: 7.251e-04  Data: 0.011 (0.012)
Train: 106 [1050/1251 ( 84%)]  Loss:  3.539715 (3.5201)  Time: 1.078s,  949.71/s  (1.089s,  940.46/s)  LR: 7.251e-04  Data: 0.012 (0.012)
Train: 106 [1100/1251 ( 88%)]  Loss:  3.568485 (3.5222)  Time: 1.077s,  950.67/s  (1.089s,  940.52/s)  LR: 7.251e-04  Data: 0.012 (0.012)
Train: 106 [1150/1251 ( 92%)]  Loss:  3.545965 (3.5232)  Time: 1.077s,  950.91/s  (1.089s,  940.32/s)  LR: 7.251e-04  Data: 0.013 (0.012)
Train: 106 [1200/1251 ( 96%)]  Loss:  3.646078 (3.5281)  Time: 1.101s,  929.69/s  (1.089s,  940.16/s)  LR: 7.251e-04  Data: 0.011 (0.012)
Train: 106 [1250/1251 (100%)]  Loss:  3.817969 (3.5392)  Time: 1.063s,  963.48/s  (1.089s,  940.06/s)  LR: 7.251e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.856 (5.856)  Loss:  0.5545 (0.5545)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.229 (0.440)  Loss:  0.6441 (1.0813)  Acc@1: 85.8491 (75.2880)  Acc@5: 96.9340 (92.8480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 74.7100001147461)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-94.pth.tar', 74.65000003417968)

Train: 107 [   0/1251 (  0%)]  Loss:  3.704439 (3.7044)  Time: 1.088s,  941.31/s  (1.088s,  941.31/s)  LR: 7.204e-04  Data: 0.025 (0.025)
Train: 107 [  50/1251 (  4%)]  Loss:  3.904886 (3.8047)  Time: 1.083s,  945.60/s  (1.086s,  942.98/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 100/1251 (  8%)]  Loss:  3.462237 (3.6905)  Time: 1.077s,  951.01/s  (1.087s,  941.98/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 150/1251 ( 12%)]  Loss:  3.643896 (3.6789)  Time: 1.096s,  934.29/s  (1.086s,  942.69/s)  LR: 7.204e-04  Data: 0.014 (0.013)
Train: 107 [ 200/1251 ( 16%)]  Loss:  3.372698 (3.6176)  Time: 1.105s,  926.34/s  (1.087s,  941.77/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 107 [ 250/1251 ( 20%)]  Loss:  3.464094 (3.5920)  Time: 1.096s,  934.64/s  (1.088s,  941.33/s)  LR: 7.204e-04  Data: 0.013 (0.012)
Train: 107 [ 300/1251 ( 24%)]  Loss:  3.527498 (3.5828)  Time: 1.096s,  934.68/s  (1.089s,  940.05/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 107 [ 350/1251 ( 28%)]  Loss:  3.412918 (3.5616)  Time: 1.078s,  949.62/s  (1.090s,  939.59/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 107 [ 400/1251 ( 32%)]  Loss:  3.398949 (3.5435)  Time: 1.081s,  947.25/s  (1.089s,  940.04/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 107 [ 450/1251 ( 36%)]  Loss:  3.731669 (3.5623)  Time: 1.085s,  943.50/s  (1.090s,  939.38/s)  LR: 7.204e-04  Data: 0.014 (0.012)
Train: 107 [ 500/1251 ( 40%)]  Loss:  3.516588 (3.5582)  Time: 1.078s,  950.32/s  (1.090s,  939.44/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 550/1251 ( 44%)]  Loss:  3.510383 (3.5542)  Time: 1.077s,  950.81/s  (1.090s,  939.36/s)  LR: 7.204e-04  Data: 0.014 (0.012)
Train: 107 [ 600/1251 ( 48%)]  Loss:  3.738840 (3.5684)  Time: 1.089s,  940.55/s  (1.090s,  939.33/s)  LR: 7.204e-04  Data: 0.018 (0.013)
Train: 107 [ 650/1251 ( 52%)]  Loss:  3.538993 (3.5663)  Time: 1.086s,  942.78/s  (1.090s,  939.72/s)  LR: 7.204e-04  Data: 0.013 (0.013)
Train: 107 [ 700/1251 ( 56%)]  Loss:  3.263150 (3.5461)  Time: 1.083s,  945.42/s  (1.090s,  939.73/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 750/1251 ( 60%)]  Loss:  3.331432 (3.5327)  Time: 1.094s,  936.01/s  (1.090s,  939.78/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 800/1251 ( 64%)]  Loss:  3.549973 (3.5337)  Time: 1.076s,  951.67/s  (1.090s,  939.63/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 850/1251 ( 68%)]  Loss:  3.457551 (3.5295)  Time: 1.096s,  934.11/s  (1.090s,  939.46/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 900/1251 ( 72%)]  Loss:  3.485392 (3.5271)  Time: 1.096s,  934.06/s  (1.090s,  939.11/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [ 950/1251 ( 76%)]  Loss:  3.648913 (3.5332)  Time: 1.096s,  934.07/s  (1.091s,  938.64/s)  LR: 7.204e-04  Data: 0.013 (0.013)
Train: 107 [1000/1251 ( 80%)]  Loss:  3.550836 (3.5341)  Time: 1.080s,  947.86/s  (1.091s,  938.58/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 107 [1050/1251 ( 84%)]  Loss:  3.669489 (3.5402)  Time: 1.103s,  928.77/s  (1.091s,  938.83/s)  LR: 7.204e-04  Data: 0.012 (0.012)
Train: 107 [1100/1251 ( 88%)]  Loss:  3.793331 (3.5512)  Time: 1.086s,  943.33/s  (1.091s,  938.79/s)  LR: 7.204e-04  Data: 0.016 (0.013)
Train: 107 [1150/1251 ( 92%)]  Loss:  3.425026 (3.5460)  Time: 1.077s,  950.37/s  (1.090s,  939.16/s)  LR: 7.204e-04  Data: 0.015 (0.013)
Train: 107 [1200/1251 ( 96%)]  Loss:  3.331532 (3.5374)  Time: 1.096s,  934.30/s  (1.090s,  939.42/s)  LR: 7.204e-04  Data: 0.012 (0.013)
Train: 107 [1250/1251 (100%)]  Loss:  3.739984 (3.5452)  Time: 1.061s,  964.92/s  (1.090s,  939.50/s)  LR: 7.204e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.770 (5.770)  Loss:  0.5695 (0.5695)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.441)  Loss:  0.6774 (1.1069)  Acc@1: 85.3774 (75.1000)  Acc@5: 96.6981 (92.9340)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 74.7100001147461)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-96.pth.tar', 74.68200014160156)

Train: 108 [   0/1251 (  0%)]  Loss:  3.676170 (3.6762)  Time: 1.151s,  889.97/s  (1.151s,  889.97/s)  LR: 7.158e-04  Data: 0.088 (0.088)
Train: 108 [  50/1251 (  4%)]  Loss:  3.610738 (3.6435)  Time: 1.088s,  941.48/s  (1.093s,  937.12/s)  LR: 7.158e-04  Data: 0.012 (0.014)
Train: 108 [ 100/1251 (  8%)]  Loss:  2.974348 (3.4204)  Time: 1.098s,  932.68/s  (1.089s,  940.56/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 150/1251 ( 12%)]  Loss:  3.577612 (3.4597)  Time: 1.095s,  935.18/s  (1.088s,  941.60/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 200/1251 ( 16%)]  Loss:  3.509200 (3.4696)  Time: 1.080s,  947.91/s  (1.087s,  941.61/s)  LR: 7.158e-04  Data: 0.012 (0.013)
Train: 108 [ 250/1251 ( 20%)]  Loss:  3.297266 (3.4409)  Time: 1.090s,  939.34/s  (1.087s,  941.89/s)  LR: 7.158e-04  Data: 0.013 (0.013)
Train: 108 [ 300/1251 ( 24%)]  Loss:  3.458531 (3.4434)  Time: 1.105s,  926.70/s  (1.087s,  942.25/s)  LR: 7.158e-04  Data: 0.013 (0.012)
Train: 108 [ 350/1251 ( 28%)]  Loss:  3.237021 (3.4176)  Time: 1.106s,  925.78/s  (1.087s,  941.71/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 400/1251 ( 32%)]  Loss:  3.580892 (3.4358)  Time: 1.080s,  948.24/s  (1.088s,  940.92/s)  LR: 7.158e-04  Data: 0.011 (0.012)
Train: 108 [ 450/1251 ( 36%)]  Loss:  3.223683 (3.4145)  Time: 1.079s,  948.72/s  (1.088s,  940.87/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 500/1251 ( 40%)]  Loss:  3.646953 (3.4357)  Time: 1.096s,  934.66/s  (1.088s,  941.12/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 550/1251 ( 44%)]  Loss:  3.687314 (3.4566)  Time: 1.076s,  951.50/s  (1.088s,  940.99/s)  LR: 7.158e-04  Data: 0.013 (0.012)
Train: 108 [ 600/1251 ( 48%)]  Loss:  3.725528 (3.4773)  Time: 1.105s,  926.43/s  (1.088s,  940.76/s)  LR: 7.158e-04  Data: 0.013 (0.012)
Train: 108 [ 650/1251 ( 52%)]  Loss:  3.327187 (3.4666)  Time: 1.097s,  933.31/s  (1.089s,  940.67/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 700/1251 ( 56%)]  Loss:  3.738313 (3.4847)  Time: 1.098s,  932.68/s  (1.089s,  940.61/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 750/1251 ( 60%)]  Loss:  3.462304 (3.4833)  Time: 1.097s,  933.49/s  (1.089s,  940.58/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 800/1251 ( 64%)]  Loss:  3.369877 (3.4766)  Time: 1.095s,  935.48/s  (1.089s,  940.25/s)  LR: 7.158e-04  Data: 0.014 (0.012)
Train: 108 [ 850/1251 ( 68%)]  Loss:  3.593051 (3.4831)  Time: 1.094s,  935.62/s  (1.089s,  940.49/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 900/1251 ( 72%)]  Loss:  3.873681 (3.5037)  Time: 1.078s,  950.32/s  (1.089s,  940.34/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [ 950/1251 ( 76%)]  Loss:  3.725183 (3.5147)  Time: 1.097s,  933.80/s  (1.089s,  940.15/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [1000/1251 ( 80%)]  Loss:  3.627318 (3.5201)  Time: 1.096s,  934.58/s  (1.089s,  939.89/s)  LR: 7.158e-04  Data: 0.013 (0.012)
Train: 108 [1050/1251 ( 84%)]  Loss:  3.320443 (3.5110)  Time: 1.095s,  935.50/s  (1.090s,  939.71/s)  LR: 7.158e-04  Data: 0.013 (0.012)
Train: 108 [1100/1251 ( 88%)]  Loss:  3.861878 (3.5263)  Time: 1.099s,  932.00/s  (1.090s,  939.50/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [1150/1251 ( 92%)]  Loss:  3.524823 (3.5262)  Time: 1.079s,  949.18/s  (1.090s,  939.40/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [1200/1251 ( 96%)]  Loss:  3.771799 (3.5360)  Time: 1.095s,  935.47/s  (1.090s,  939.54/s)  LR: 7.158e-04  Data: 0.012 (0.012)
Train: 108 [1250/1251 (100%)]  Loss:  3.891135 (3.5497)  Time: 1.079s,  948.87/s  (1.090s,  939.36/s)  LR: 7.158e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.778 (5.778)  Loss:  0.5727 (0.5727)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.6899 (1.1056)  Acc@1: 85.0236 (75.0420)  Acc@5: 96.5802 (92.8880)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 74.7100001147461)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-101.pth.tar', 74.69000014160156)

Train: 109 [   0/1251 (  0%)]  Loss:  3.798415 (3.7984)  Time: 1.097s,  933.80/s  (1.097s,  933.80/s)  LR: 7.111e-04  Data: 0.034 (0.034)
Train: 109 [  50/1251 (  4%)]  Loss:  3.779132 (3.7888)  Time: 1.076s,  951.52/s  (1.088s,  940.83/s)  LR: 7.111e-04  Data: 0.014 (0.013)
Train: 109 [ 100/1251 (  8%)]  Loss:  3.653299 (3.7436)  Time: 1.081s,  946.95/s  (1.088s,  941.46/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 150/1251 ( 12%)]  Loss:  3.717365 (3.7371)  Time: 1.096s,  934.35/s  (1.087s,  941.86/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 200/1251 ( 16%)]  Loss:  3.561237 (3.7019)  Time: 1.096s,  934.68/s  (1.088s,  941.49/s)  LR: 7.111e-04  Data: 0.013 (0.013)
Train: 109 [ 250/1251 ( 20%)]  Loss:  3.375419 (3.6475)  Time: 1.096s,  934.61/s  (1.087s,  942.09/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 300/1251 ( 24%)]  Loss:  3.434486 (3.6171)  Time: 1.096s,  934.26/s  (1.087s,  941.73/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 350/1251 ( 28%)]  Loss:  3.486461 (3.6007)  Time: 1.096s,  934.50/s  (1.087s,  941.69/s)  LR: 7.111e-04  Data: 0.011 (0.013)
Train: 109 [ 400/1251 ( 32%)]  Loss:  4.004314 (3.6456)  Time: 1.076s,  951.57/s  (1.087s,  942.01/s)  LR: 7.111e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 109 [ 450/1251 ( 36%)]  Loss:  3.330150 (3.6140)  Time: 1.075s,  952.27/s  (1.087s,  942.18/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 500/1251 ( 40%)]  Loss:  3.363584 (3.5913)  Time: 1.080s,  948.45/s  (1.086s,  942.59/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 550/1251 ( 44%)]  Loss:  3.750912 (3.6046)  Time: 1.078s,  949.87/s  (1.086s,  942.77/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 600/1251 ( 48%)]  Loss:  3.148229 (3.5695)  Time: 1.079s,  948.62/s  (1.086s,  942.94/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 650/1251 ( 52%)]  Loss:  3.531175 (3.5667)  Time: 1.076s,  951.63/s  (1.086s,  943.14/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 700/1251 ( 56%)]  Loss:  3.719597 (3.5769)  Time: 1.078s,  949.98/s  (1.086s,  943.33/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 750/1251 ( 60%)]  Loss:  3.837461 (3.5932)  Time: 1.173s,  873.21/s  (1.086s,  942.77/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 800/1251 ( 64%)]  Loss:  3.602318 (3.5937)  Time: 1.096s,  933.94/s  (1.087s,  942.32/s)  LR: 7.111e-04  Data: 0.014 (0.013)
Train: 109 [ 850/1251 ( 68%)]  Loss:  3.782866 (3.6042)  Time: 1.094s,  935.83/s  (1.087s,  941.97/s)  LR: 7.111e-04  Data: 0.013 (0.013)
Train: 109 [ 900/1251 ( 72%)]  Loss:  3.492216 (3.5983)  Time: 1.095s,  935.21/s  (1.087s,  941.78/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [ 950/1251 ( 76%)]  Loss:  3.668925 (3.6019)  Time: 1.078s,  950.05/s  (1.087s,  941.67/s)  LR: 7.111e-04  Data: 0.012 (0.013)
Train: 109 [1000/1251 ( 80%)]  Loss:  3.818252 (3.6122)  Time: 1.082s,  946.41/s  (1.088s,  941.55/s)  LR: 7.111e-04  Data: 0.012 (0.012)
Train: 109 [1050/1251 ( 84%)]  Loss:  3.587581 (3.6111)  Time: 1.094s,  936.04/s  (1.087s,  941.64/s)  LR: 7.111e-04  Data: 0.013 (0.012)
Train: 109 [1100/1251 ( 88%)]  Loss:  3.700466 (3.6150)  Time: 1.099s,  931.90/s  (1.087s,  941.68/s)  LR: 7.111e-04  Data: 0.012 (0.012)
Train: 109 [1150/1251 ( 92%)]  Loss:  3.588314 (3.6138)  Time: 1.078s,  950.28/s  (1.088s,  941.28/s)  LR: 7.111e-04  Data: 0.011 (0.012)
Train: 109 [1200/1251 ( 96%)]  Loss:  3.803387 (3.6214)  Time: 1.106s,  926.04/s  (1.088s,  941.13/s)  LR: 7.111e-04  Data: 0.013 (0.012)
Train: 109 [1250/1251 (100%)]  Loss:  3.383174 (3.6123)  Time: 1.071s,  956.49/s  (1.088s,  941.13/s)  LR: 7.111e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.672 (5.672)  Loss:  0.4968 (0.4968)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.440)  Loss:  0.6461 (1.0527)  Acc@1: 85.1415 (74.9860)  Acc@5: 96.4623 (92.8420)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 74.98600000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 74.7100001147461)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-100.pth.tar', 74.70600006347657)

Train: 110 [   0/1251 (  0%)]  Loss:  3.581390 (3.5814)  Time: 1.113s,  920.43/s  (1.113s,  920.43/s)  LR: 7.063e-04  Data: 0.030 (0.030)
Train: 110 [  50/1251 (  4%)]  Loss:  3.611474 (3.5964)  Time: 1.095s,  934.85/s  (1.096s,  933.96/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 100/1251 (  8%)]  Loss:  3.523949 (3.5723)  Time: 1.079s,  949.35/s  (1.091s,  938.69/s)  LR: 7.063e-04  Data: 0.015 (0.013)
Train: 110 [ 150/1251 ( 12%)]  Loss:  3.451133 (3.5420)  Time: 1.075s,  952.31/s  (1.090s,  939.03/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 200/1251 ( 16%)]  Loss:  3.597033 (3.5530)  Time: 1.078s,  950.15/s  (1.090s,  939.40/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 250/1251 ( 20%)]  Loss:  3.550632 (3.5526)  Time: 1.078s,  949.84/s  (1.090s,  939.26/s)  LR: 7.063e-04  Data: 0.015 (0.013)
Train: 110 [ 300/1251 ( 24%)]  Loss:  3.684357 (3.5714)  Time: 1.083s,  945.35/s  (1.089s,  940.27/s)  LR: 7.063e-04  Data: 0.014 (0.013)
Train: 110 [ 350/1251 ( 28%)]  Loss:  3.591802 (3.5740)  Time: 1.096s,  934.32/s  (1.090s,  939.46/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 400/1251 ( 32%)]  Loss:  3.493836 (3.5651)  Time: 1.087s,  941.95/s  (1.090s,  939.09/s)  LR: 7.063e-04  Data: 0.013 (0.013)
Train: 110 [ 450/1251 ( 36%)]  Loss:  3.487124 (3.5573)  Time: 1.095s,  935.25/s  (1.090s,  939.70/s)  LR: 7.063e-04  Data: 0.013 (0.013)
Train: 110 [ 500/1251 ( 40%)]  Loss:  3.376375 (3.5408)  Time: 1.077s,  950.85/s  (1.089s,  940.11/s)  LR: 7.063e-04  Data: 0.013 (0.013)
Train: 110 [ 550/1251 ( 44%)]  Loss:  3.266860 (3.5180)  Time: 1.077s,  951.05/s  (1.089s,  940.40/s)  LR: 7.063e-04  Data: 0.014 (0.013)
Train: 110 [ 600/1251 ( 48%)]  Loss:  3.550634 (3.5205)  Time: 1.076s,  951.83/s  (1.088s,  941.06/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 650/1251 ( 52%)]  Loss:  3.443476 (3.5150)  Time: 1.077s,  950.71/s  (1.088s,  940.77/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 700/1251 ( 56%)]  Loss:  3.627313 (3.5225)  Time: 1.104s,  927.26/s  (1.088s,  940.77/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 750/1251 ( 60%)]  Loss:  3.430335 (3.5167)  Time: 1.077s,  951.22/s  (1.088s,  940.77/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 800/1251 ( 64%)]  Loss:  3.710793 (3.5281)  Time: 1.079s,  949.01/s  (1.089s,  940.69/s)  LR: 7.063e-04  Data: 0.014 (0.013)
Train: 110 [ 850/1251 ( 68%)]  Loss:  3.497433 (3.5264)  Time: 1.082s,  946.15/s  (1.089s,  940.72/s)  LR: 7.063e-04  Data: 0.017 (0.013)
Train: 110 [ 900/1251 ( 72%)]  Loss:  3.345987 (3.5169)  Time: 1.091s,  938.33/s  (1.089s,  940.70/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [ 950/1251 ( 76%)]  Loss:  3.890027 (3.5356)  Time: 1.077s,  950.68/s  (1.088s,  940.83/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [1000/1251 ( 80%)]  Loss:  3.406716 (3.5295)  Time: 1.077s,  950.55/s  (1.088s,  940.75/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [1050/1251 ( 84%)]  Loss:  3.770847 (3.5404)  Time: 1.082s,  946.61/s  (1.088s,  940.82/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [1100/1251 ( 88%)]  Loss:  3.808060 (3.5521)  Time: 1.094s,  935.85/s  (1.088s,  940.80/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [1150/1251 ( 92%)]  Loss:  3.548008 (3.5519)  Time: 1.106s,  925.59/s  (1.089s,  940.38/s)  LR: 7.063e-04  Data: 0.011 (0.013)
Train: 110 [1200/1251 ( 96%)]  Loss:  3.795025 (3.5616)  Time: 1.094s,  935.93/s  (1.089s,  940.52/s)  LR: 7.063e-04  Data: 0.012 (0.013)
Train: 110 [1250/1251 (100%)]  Loss:  3.712733 (3.5674)  Time: 1.062s,  964.53/s  (1.089s,  940.43/s)  LR: 7.063e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.836 (5.836)  Loss:  0.5143 (0.5143)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.6706 (1.0576)  Acc@1: 84.4340 (75.3160)  Acc@5: 96.6981 (92.8540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 74.98600000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-105.pth.tar', 74.7100001147461)

Train: 111 [   0/1251 (  0%)]  Loss:  3.491262 (3.4913)  Time: 1.100s,  930.95/s  (1.100s,  930.95/s)  LR: 7.016e-04  Data: 0.031 (0.031)
Train: 111 [  50/1251 (  4%)]  Loss:  3.882195 (3.6867)  Time: 1.096s,  934.61/s  (1.093s,  936.60/s)  LR: 7.016e-04  Data: 0.014 (0.013)
Train: 111 [ 100/1251 (  8%)]  Loss:  3.416570 (3.5967)  Time: 1.092s,  937.58/s  (1.097s,  933.32/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 150/1251 ( 12%)]  Loss:  3.283550 (3.5184)  Time: 1.102s,  929.32/s  (1.096s,  934.00/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [ 200/1251 ( 16%)]  Loss:  3.605429 (3.5358)  Time: 1.104s,  927.65/s  (1.095s,  935.33/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 250/1251 ( 20%)]  Loss:  3.650331 (3.5549)  Time: 1.075s,  952.38/s  (1.094s,  935.64/s)  LR: 7.016e-04  Data: 0.013 (0.013)
Train: 111 [ 300/1251 ( 24%)]  Loss:  3.285525 (3.5164)  Time: 1.096s,  934.15/s  (1.093s,  937.00/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 350/1251 ( 28%)]  Loss:  3.453903 (3.5086)  Time: 1.097s,  933.32/s  (1.092s,  937.82/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 400/1251 ( 32%)]  Loss:  3.391494 (3.4956)  Time: 1.086s,  942.80/s  (1.091s,  938.59/s)  LR: 7.016e-04  Data: 0.011 (0.013)
Train: 111 [ 450/1251 ( 36%)]  Loss:  3.549896 (3.5010)  Time: 1.087s,  942.03/s  (1.090s,  939.10/s)  LR: 7.016e-04  Data: 0.013 (0.013)
Train: 111 [ 500/1251 ( 40%)]  Loss:  3.617568 (3.5116)  Time: 1.076s,  951.54/s  (1.090s,  939.72/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 550/1251 ( 44%)]  Loss:  3.702363 (3.5275)  Time: 1.093s,  936.45/s  (1.090s,  939.78/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 600/1251 ( 48%)]  Loss:  3.590621 (3.5324)  Time: 1.170s,  875.58/s  (1.090s,  939.68/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 650/1251 ( 52%)]  Loss:  3.482344 (3.5288)  Time: 1.075s,  952.33/s  (1.090s,  939.75/s)  LR: 7.016e-04  Data: 0.014 (0.013)
Train: 111 [ 700/1251 ( 56%)]  Loss:  3.737437 (3.5427)  Time: 1.142s,  896.70/s  (1.090s,  939.72/s)  LR: 7.016e-04  Data: 0.014 (0.013)
Train: 111 [ 750/1251 ( 60%)]  Loss:  3.465916 (3.5379)  Time: 1.076s,  951.66/s  (1.090s,  939.80/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 800/1251 ( 64%)]  Loss:  3.168590 (3.5162)  Time: 1.083s,  945.86/s  (1.089s,  939.93/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 850/1251 ( 68%)]  Loss:  3.123045 (3.4943)  Time: 1.076s,  952.08/s  (1.089s,  939.99/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 900/1251 ( 72%)]  Loss:  3.274192 (3.4827)  Time: 1.096s,  934.57/s  (1.089s,  939.93/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [ 950/1251 ( 76%)]  Loss:  3.150360 (3.4661)  Time: 1.079s,  949.03/s  (1.089s,  940.14/s)  LR: 7.016e-04  Data: 0.014 (0.013)
Train: 111 [1000/1251 ( 80%)]  Loss:  3.630003 (3.4739)  Time: 1.077s,  950.47/s  (1.089s,  940.33/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [1050/1251 ( 84%)]  Loss:  3.588460 (3.4791)  Time: 1.082s,  946.11/s  (1.089s,  940.54/s)  LR: 7.016e-04  Data: 0.016 (0.013)
Train: 111 [1100/1251 ( 88%)]  Loss:  3.450044 (3.4779)  Time: 1.096s,  934.65/s  (1.089s,  940.57/s)  LR: 7.016e-04  Data: 0.015 (0.013)
Train: 111 [1150/1251 ( 92%)]  Loss:  3.221027 (3.4672)  Time: 1.081s,  947.02/s  (1.089s,  940.49/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [1200/1251 ( 96%)]  Loss:  3.511888 (3.4690)  Time: 1.094s,  936.20/s  (1.089s,  940.42/s)  LR: 7.016e-04  Data: 0.012 (0.013)
Train: 111 [1250/1251 (100%)]  Loss:  3.314868 (3.4630)  Time: 1.063s,  963.63/s  (1.089s,  940.25/s)  LR: 7.016e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.727 (5.727)  Loss:  0.5129 (0.5129)  Acc@1: 89.8438 (89.8438)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.440)  Loss:  0.6819 (1.0852)  Acc@1: 84.7877 (75.3940)  Acc@5: 96.3443 (92.8820)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 74.98600000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-97.pth.tar', 74.92600005859374)

Train: 112 [   0/1251 (  0%)]  Loss:  3.609913 (3.6099)  Time: 1.094s,  935.81/s  (1.094s,  935.81/s)  LR: 6.968e-04  Data: 0.033 (0.033)
Train: 112 [  50/1251 (  4%)]  Loss:  3.576214 (3.5931)  Time: 1.074s,  953.09/s  (1.082s,  946.49/s)  LR: 6.968e-04  Data: 0.013 (0.013)
Train: 112 [ 100/1251 (  8%)]  Loss:  3.723668 (3.6366)  Time: 1.079s,  949.28/s  (1.087s,  942.45/s)  LR: 6.968e-04  Data: 0.012 (0.013)
Train: 112 [ 150/1251 ( 12%)]  Loss:  3.576580 (3.6216)  Time: 1.095s,  934.97/s  (1.091s,  938.96/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [ 200/1251 ( 16%)]  Loss:  3.474403 (3.5922)  Time: 1.083s,  945.26/s  (1.088s,  941.46/s)  LR: 6.968e-04  Data: 0.011 (0.013)
Train: 112 [ 250/1251 ( 20%)]  Loss:  3.476809 (3.5729)  Time: 1.075s,  952.24/s  (1.087s,  942.04/s)  LR: 6.968e-04  Data: 0.012 (0.013)
Train: 112 [ 300/1251 ( 24%)]  Loss:  3.468867 (3.5581)  Time: 1.221s,  839.00/s  (1.089s,  940.23/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 350/1251 ( 28%)]  Loss:  3.434542 (3.5426)  Time: 1.097s,  933.57/s  (1.089s,  940.26/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 400/1251 ( 32%)]  Loss:  3.537875 (3.5421)  Time: 1.075s,  952.71/s  (1.088s,  941.02/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 450/1251 ( 36%)]  Loss:  3.164828 (3.5044)  Time: 1.098s,  932.91/s  (1.089s,  940.49/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 500/1251 ( 40%)]  Loss:  3.518200 (3.5056)  Time: 1.081s,  947.36/s  (1.089s,  939.92/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 550/1251 ( 44%)]  Loss:  3.523487 (3.5071)  Time: 1.086s,  942.60/s  (1.089s,  940.38/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 600/1251 ( 48%)]  Loss:  3.455722 (3.5032)  Time: 1.106s,  925.76/s  (1.089s,  940.48/s)  LR: 6.968e-04  Data: 0.015 (0.012)
Train: 112 [ 650/1251 ( 52%)]  Loss:  3.622186 (3.5117)  Time: 1.183s,  865.72/s  (1.089s,  940.11/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 112 [ 700/1251 ( 56%)]  Loss:  3.386342 (3.5033)  Time: 1.078s,  949.51/s  (1.089s,  939.99/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 750/1251 ( 60%)]  Loss:  3.280391 (3.4894)  Time: 1.103s,  928.27/s  (1.090s,  939.75/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [ 800/1251 ( 64%)]  Loss:  3.337087 (3.4804)  Time: 1.094s,  936.39/s  (1.090s,  939.58/s)  LR: 6.968e-04  Data: 0.013 (0.012)
Train: 112 [ 850/1251 ( 68%)]  Loss:  3.549003 (3.4842)  Time: 1.094s,  935.91/s  (1.090s,  939.81/s)  LR: 6.968e-04  Data: 0.014 (0.012)
Train: 112 [ 900/1251 ( 72%)]  Loss:  3.558300 (3.4881)  Time: 1.192s,  859.23/s  (1.089s,  939.95/s)  LR: 6.968e-04  Data: 0.012 (0.013)
Train: 112 [ 950/1251 ( 76%)]  Loss:  3.324708 (3.4800)  Time: 1.096s,  934.64/s  (1.090s,  939.76/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [1000/1251 ( 80%)]  Loss:  3.541429 (3.4829)  Time: 1.077s,  951.03/s  (1.089s,  939.94/s)  LR: 6.968e-04  Data: 0.011 (0.012)
Train: 112 [1050/1251 ( 84%)]  Loss:  3.745736 (3.4948)  Time: 1.082s,  946.75/s  (1.089s,  940.19/s)  LR: 6.968e-04  Data: 0.011 (0.012)
Train: 112 [1100/1251 ( 88%)]  Loss:  3.502945 (3.4952)  Time: 1.077s,  951.03/s  (1.089s,  940.27/s)  LR: 6.968e-04  Data: 0.013 (0.012)
Train: 112 [1150/1251 ( 92%)]  Loss:  3.490608 (3.4950)  Time: 1.079s,  948.70/s  (1.089s,  939.99/s)  LR: 6.968e-04  Data: 0.011 (0.012)
Train: 112 [1200/1251 ( 96%)]  Loss:  3.746458 (3.5051)  Time: 1.077s,  950.46/s  (1.089s,  940.07/s)  LR: 6.968e-04  Data: 0.012 (0.012)
Train: 112 [1250/1251 (100%)]  Loss:  3.406188 (3.5012)  Time: 1.151s,  889.58/s  (1.090s,  939.88/s)  LR: 6.968e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.793 (5.793)  Loss:  0.5285 (0.5285)  Acc@1: 88.2812 (88.2812)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.6961 (1.0835)  Acc@1: 84.1981 (75.2160)  Acc@5: 96.3443 (93.0360)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-109.pth.tar', 74.98600000976562)

Train: 113 [   0/1251 (  0%)]  Loss:  3.363436 (3.3634)  Time: 1.089s,  939.99/s  (1.089s,  939.99/s)  LR: 6.920e-04  Data: 0.027 (0.027)
Train: 113 [  50/1251 (  4%)]  Loss:  3.399120 (3.3813)  Time: 1.072s,  954.85/s  (1.096s,  934.18/s)  LR: 6.920e-04  Data: 0.011 (0.013)
Train: 113 [ 100/1251 (  8%)]  Loss:  3.155596 (3.3061)  Time: 1.101s,  930.05/s  (1.095s,  935.39/s)  LR: 6.920e-04  Data: 0.013 (0.013)
Train: 113 [ 150/1251 ( 12%)]  Loss:  3.469681 (3.3470)  Time: 1.086s,  942.54/s  (1.092s,  937.85/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [ 200/1251 ( 16%)]  Loss:  3.423469 (3.3623)  Time: 1.095s,  935.48/s  (1.093s,  937.25/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [ 250/1251 ( 20%)]  Loss:  3.405173 (3.3694)  Time: 1.095s,  935.46/s  (1.093s,  937.02/s)  LR: 6.920e-04  Data: 0.012 (0.013)
Train: 113 [ 300/1251 ( 24%)]  Loss:  3.291997 (3.3584)  Time: 1.080s,  947.72/s  (1.092s,  938.08/s)  LR: 6.920e-04  Data: 0.013 (0.013)
Train: 113 [ 350/1251 ( 28%)]  Loss:  3.573972 (3.3853)  Time: 1.099s,  931.62/s  (1.092s,  937.73/s)  LR: 6.920e-04  Data: 0.013 (0.012)
Train: 113 [ 400/1251 ( 32%)]  Loss:  3.474178 (3.3952)  Time: 1.075s,  952.41/s  (1.092s,  937.72/s)  LR: 6.920e-04  Data: 0.011 (0.012)
Train: 113 [ 450/1251 ( 36%)]  Loss:  3.594227 (3.4151)  Time: 1.096s,  934.54/s  (1.092s,  937.32/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 500/1251 ( 40%)]  Loss:  3.462028 (3.4194)  Time: 1.098s,  932.67/s  (1.093s,  936.83/s)  LR: 6.920e-04  Data: 0.013 (0.012)
Train: 113 [ 550/1251 ( 44%)]  Loss:  3.733375 (3.4455)  Time: 1.081s,  947.63/s  (1.093s,  936.58/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 600/1251 ( 48%)]  Loss:  3.650139 (3.4613)  Time: 1.079s,  948.64/s  (1.092s,  937.50/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 650/1251 ( 52%)]  Loss:  3.488831 (3.4632)  Time: 1.088s,  941.07/s  (1.092s,  937.78/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 700/1251 ( 56%)]  Loss:  3.621481 (3.4738)  Time: 1.079s,  949.42/s  (1.092s,  937.93/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 750/1251 ( 60%)]  Loss:  3.658206 (3.4853)  Time: 1.096s,  934.49/s  (1.092s,  937.71/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 800/1251 ( 64%)]  Loss:  3.508268 (3.4867)  Time: 1.098s,  932.80/s  (1.092s,  937.79/s)  LR: 6.920e-04  Data: 0.011 (0.012)
Train: 113 [ 850/1251 ( 68%)]  Loss:  3.866882 (3.5078)  Time: 1.076s,  951.94/s  (1.092s,  937.91/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [ 900/1251 ( 72%)]  Loss:  3.661129 (3.5159)  Time: 1.103s,  928.60/s  (1.092s,  938.01/s)  LR: 6.920e-04  Data: 0.013 (0.012)
Train: 113 [ 950/1251 ( 76%)]  Loss:  3.486416 (3.5144)  Time: 1.082s,  946.41/s  (1.092s,  937.93/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [1000/1251 ( 80%)]  Loss:  3.212646 (3.5000)  Time: 1.091s,  938.60/s  (1.092s,  937.92/s)  LR: 6.920e-04  Data: 0.011 (0.012)
Train: 113 [1050/1251 ( 84%)]  Loss:  3.696937 (3.5090)  Time: 1.095s,  935.46/s  (1.092s,  937.88/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [1100/1251 ( 88%)]  Loss:  3.969280 (3.5290)  Time: 1.096s,  934.51/s  (1.092s,  937.56/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [1150/1251 ( 92%)]  Loss:  3.548678 (3.5298)  Time: 1.095s,  934.78/s  (1.092s,  937.52/s)  LR: 6.920e-04  Data: 0.015 (0.012)
Train: 113 [1200/1251 ( 96%)]  Loss:  3.501878 (3.5287)  Time: 1.081s,  947.56/s  (1.092s,  937.67/s)  LR: 6.920e-04  Data: 0.012 (0.012)
Train: 113 [1250/1251 (100%)]  Loss:  3.770492 (3.5380)  Time: 1.062s,  964.44/s  (1.092s,  937.77/s)  LR: 6.920e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.851 (5.851)  Loss:  0.5556 (0.5556)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6615 (1.0721)  Acc@1: 86.4387 (75.2180)  Acc@5: 97.1698 (93.0060)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-102.pth.tar', 75.0040000366211)

Train: 114 [   0/1251 (  0%)]  Loss:  3.600586 (3.6006)  Time: 1.088s,  941.25/s  (1.088s,  941.25/s)  LR: 6.872e-04  Data: 0.027 (0.027)
Train: 114 [  50/1251 (  4%)]  Loss:  3.502935 (3.5518)  Time: 1.138s,  899.83/s  (1.088s,  941.05/s)  LR: 6.872e-04  Data: 0.012 (0.013)
Train: 114 [ 100/1251 (  8%)]  Loss:  3.228360 (3.4440)  Time: 1.105s,  926.42/s  (1.085s,  943.59/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [ 150/1251 ( 12%)]  Loss:  3.390597 (3.4306)  Time: 1.084s,  944.62/s  (1.086s,  942.71/s)  LR: 6.872e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 114 [ 200/1251 ( 16%)]  Loss:  3.272439 (3.3990)  Time: 1.079s,  949.20/s  (1.086s,  942.76/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 250/1251 ( 20%)]  Loss:  3.460913 (3.4093)  Time: 1.084s,  944.22/s  (1.087s,  941.68/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 300/1251 ( 24%)]  Loss:  3.207569 (3.3805)  Time: 1.097s,  933.85/s  (1.088s,  941.48/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 350/1251 ( 28%)]  Loss:  3.624142 (3.4109)  Time: 1.077s,  950.57/s  (1.088s,  940.91/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 400/1251 ( 32%)]  Loss:  3.528349 (3.4240)  Time: 1.106s,  926.09/s  (1.089s,  940.33/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [ 450/1251 ( 36%)]  Loss:  3.467736 (3.4284)  Time: 1.080s,  947.96/s  (1.090s,  939.82/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [ 500/1251 ( 40%)]  Loss:  3.250806 (3.4122)  Time: 1.080s,  947.96/s  (1.090s,  939.49/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 550/1251 ( 44%)]  Loss:  3.546468 (3.4234)  Time: 1.077s,  950.64/s  (1.089s,  940.18/s)  LR: 6.872e-04  Data: 0.013 (0.012)
Train: 114 [ 600/1251 ( 48%)]  Loss:  3.858776 (3.4569)  Time: 1.078s,  949.85/s  (1.089s,  940.54/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 650/1251 ( 52%)]  Loss:  3.401588 (3.4529)  Time: 1.094s,  936.17/s  (1.089s,  940.32/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 700/1251 ( 56%)]  Loss:  3.496593 (3.4559)  Time: 1.098s,  932.43/s  (1.089s,  940.25/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 750/1251 ( 60%)]  Loss:  3.141639 (3.4362)  Time: 1.094s,  935.83/s  (1.090s,  939.77/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 800/1251 ( 64%)]  Loss:  3.593636 (3.4455)  Time: 1.084s,  944.60/s  (1.090s,  939.72/s)  LR: 6.872e-04  Data: 0.011 (0.012)
Train: 114 [ 850/1251 ( 68%)]  Loss:  3.547559 (3.4511)  Time: 1.131s,  905.21/s  (1.090s,  939.64/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 900/1251 ( 72%)]  Loss:  3.862136 (3.4728)  Time: 1.096s,  933.94/s  (1.090s,  939.73/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [ 950/1251 ( 76%)]  Loss:  3.374210 (3.4679)  Time: 1.077s,  951.11/s  (1.090s,  939.87/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [1000/1251 ( 80%)]  Loss:  3.588666 (3.4736)  Time: 1.095s,  935.28/s  (1.090s,  939.58/s)  LR: 6.872e-04  Data: 0.013 (0.012)
Train: 114 [1050/1251 ( 84%)]  Loss:  3.600297 (3.4794)  Time: 1.094s,  935.78/s  (1.090s,  939.65/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [1100/1251 ( 88%)]  Loss:  3.422677 (3.4769)  Time: 1.095s,  935.38/s  (1.090s,  939.64/s)  LR: 6.872e-04  Data: 0.013 (0.012)
Train: 114 [1150/1251 ( 92%)]  Loss:  3.492915 (3.4776)  Time: 1.075s,  952.12/s  (1.090s,  939.45/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [1200/1251 ( 96%)]  Loss:  3.409393 (3.4748)  Time: 1.098s,  932.19/s  (1.090s,  939.38/s)  LR: 6.872e-04  Data: 0.012 (0.012)
Train: 114 [1250/1251 (100%)]  Loss:  3.161854 (3.4628)  Time: 1.070s,  956.57/s  (1.090s,  939.45/s)  LR: 6.872e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.841 (5.841)  Loss:  0.5407 (0.5407)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6785 (1.0491)  Acc@1: 85.4953 (75.4580)  Acc@5: 96.6981 (93.2700)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-104.pth.tar', 75.02200010498046)

Train: 115 [   0/1251 (  0%)]  Loss:  3.141375 (3.1414)  Time: 1.089s,  940.73/s  (1.089s,  940.73/s)  LR: 6.824e-04  Data: 0.027 (0.027)
Train: 115 [  50/1251 (  4%)]  Loss:  3.379317 (3.2603)  Time: 1.084s,  944.41/s  (1.095s,  935.25/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 100/1251 (  8%)]  Loss:  3.200105 (3.2403)  Time: 1.096s,  934.19/s  (1.094s,  936.07/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 150/1251 ( 12%)]  Loss:  3.435448 (3.2891)  Time: 1.077s,  951.03/s  (1.091s,  938.43/s)  LR: 6.824e-04  Data: 0.015 (0.013)
Train: 115 [ 200/1251 ( 16%)]  Loss:  3.669376 (3.3651)  Time: 1.075s,  952.21/s  (1.089s,  940.19/s)  LR: 6.824e-04  Data: 0.014 (0.013)
Train: 115 [ 250/1251 ( 20%)]  Loss:  3.576181 (3.4003)  Time: 1.095s,  935.31/s  (1.089s,  940.50/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 300/1251 ( 24%)]  Loss:  3.714526 (3.4452)  Time: 1.076s,  951.74/s  (1.089s,  940.17/s)  LR: 6.824e-04  Data: 0.013 (0.013)
Train: 115 [ 350/1251 ( 28%)]  Loss:  3.565211 (3.4602)  Time: 1.082s,  946.81/s  (1.089s,  940.17/s)  LR: 6.824e-04  Data: 0.020 (0.013)
Train: 115 [ 400/1251 ( 32%)]  Loss:  3.642042 (3.4804)  Time: 1.078s,  949.56/s  (1.089s,  940.58/s)  LR: 6.824e-04  Data: 0.013 (0.013)
Train: 115 [ 450/1251 ( 36%)]  Loss:  3.331491 (3.4655)  Time: 1.077s,  950.71/s  (1.088s,  940.86/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 500/1251 ( 40%)]  Loss:  3.544285 (3.4727)  Time: 1.082s,  946.75/s  (1.088s,  941.23/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [ 550/1251 ( 44%)]  Loss:  3.730856 (3.4942)  Time: 1.099s,  931.44/s  (1.088s,  940.79/s)  LR: 6.824e-04  Data: 0.013 (0.013)
Train: 115 [ 600/1251 ( 48%)]  Loss:  3.320395 (3.4808)  Time: 1.095s,  934.93/s  (1.089s,  940.45/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 650/1251 ( 52%)]  Loss:  3.286354 (3.4669)  Time: 1.095s,  935.17/s  (1.090s,  939.85/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 700/1251 ( 56%)]  Loss:  3.480691 (3.4678)  Time: 1.099s,  932.07/s  (1.090s,  939.70/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 750/1251 ( 60%)]  Loss:  3.327308 (3.4591)  Time: 1.079s,  948.98/s  (1.090s,  939.85/s)  LR: 6.824e-04  Data: 0.017 (0.013)
Train: 115 [ 800/1251 ( 64%)]  Loss:  3.511223 (3.4621)  Time: 1.096s,  934.43/s  (1.090s,  939.75/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [ 850/1251 ( 68%)]  Loss:  3.261497 (3.4510)  Time: 1.108s,  924.14/s  (1.090s,  939.75/s)  LR: 6.824e-04  Data: 0.013 (0.013)
Train: 115 [ 900/1251 ( 72%)]  Loss:  3.487920 (3.4529)  Time: 1.095s,  934.86/s  (1.090s,  939.45/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Train: 115 [ 950/1251 ( 76%)]  Loss:  3.364895 (3.4485)  Time: 1.094s,  936.14/s  (1.090s,  939.60/s)  LR: 6.824e-04  Data: 0.013 (0.013)
Train: 115 [1000/1251 ( 80%)]  Loss:  3.446795 (3.4484)  Time: 1.083s,  945.89/s  (1.090s,  939.77/s)  LR: 6.824e-04  Data: 0.014 (0.013)
Train: 115 [1050/1251 ( 84%)]  Loss:  3.256581 (3.4397)  Time: 1.083s,  945.71/s  (1.089s,  940.14/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [1100/1251 ( 88%)]  Loss:  3.548613 (3.4445)  Time: 1.078s,  950.11/s  (1.089s,  940.18/s)  LR: 6.824e-04  Data: 0.012 (0.013)
Train: 115 [1150/1251 ( 92%)]  Loss:  3.708013 (3.4554)  Time: 1.101s,  930.12/s  (1.089s,  939.98/s)  LR: 6.824e-04  Data: 0.013 (0.013)
Train: 115 [1200/1251 ( 96%)]  Loss:  3.658578 (3.4636)  Time: 1.077s,  950.81/s  (1.089s,  939.91/s)  LR: 6.824e-04  Data: 0.011 (0.013)
Train: 115 [1250/1251 (100%)]  Loss:  3.636587 (3.4702)  Time: 1.063s,  963.76/s  (1.089s,  939.96/s)  LR: 6.824e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.835 (5.835)  Loss:  0.5925 (0.5925)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6553 (1.0878)  Acc@1: 85.7311 (75.4720)  Acc@5: 96.5802 (93.1080)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-108.pth.tar', 75.04200008789063)

Train: 116 [   0/1251 (  0%)]  Loss:  3.463554 (3.4636)  Time: 1.088s,  941.42/s  (1.088s,  941.42/s)  LR: 6.775e-04  Data: 0.026 (0.026)
Train: 116 [  50/1251 (  4%)]  Loss:  3.579163 (3.5214)  Time: 1.080s,  948.46/s  (1.090s,  939.76/s)  LR: 6.775e-04  Data: 0.013 (0.013)
Train: 116 [ 100/1251 (  8%)]  Loss:  3.710902 (3.5845)  Time: 1.082s,  946.04/s  (1.090s,  939.05/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 150/1251 ( 12%)]  Loss:  3.378760 (3.5331)  Time: 1.076s,  951.81/s  (1.091s,  938.18/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 200/1251 ( 16%)]  Loss:  3.272705 (3.4810)  Time: 1.082s,  946.76/s  (1.091s,  938.85/s)  LR: 6.775e-04  Data: 0.014 (0.013)
Train: 116 [ 250/1251 ( 20%)]  Loss:  3.277701 (3.4471)  Time: 1.095s,  934.86/s  (1.092s,  937.87/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 300/1251 ( 24%)]  Loss:  3.328319 (3.4302)  Time: 1.105s,  926.56/s  (1.092s,  937.37/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 350/1251 ( 28%)]  Loss:  3.548437 (3.4449)  Time: 1.094s,  935.66/s  (1.093s,  936.82/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 400/1251 ( 32%)]  Loss:  3.474791 (3.4483)  Time: 1.097s,  933.86/s  (1.092s,  937.73/s)  LR: 6.775e-04  Data: 0.013 (0.013)
Train: 116 [ 450/1251 ( 36%)]  Loss:  3.548613 (3.4583)  Time: 1.078s,  950.31/s  (1.092s,  937.95/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 500/1251 ( 40%)]  Loss:  3.482825 (3.4605)  Time: 1.077s,  951.08/s  (1.091s,  938.44/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 550/1251 ( 44%)]  Loss:  3.668083 (3.4778)  Time: 1.078s,  949.87/s  (1.091s,  938.88/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 600/1251 ( 48%)]  Loss:  3.368220 (3.4694)  Time: 1.079s,  948.61/s  (1.091s,  938.74/s)  LR: 6.775e-04  Data: 0.014 (0.013)
Train: 116 [ 650/1251 ( 52%)]  Loss:  3.427419 (3.4664)  Time: 1.090s,  939.67/s  (1.091s,  938.78/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 700/1251 ( 56%)]  Loss:  3.502807 (3.4688)  Time: 1.077s,  950.71/s  (1.090s,  939.17/s)  LR: 6.775e-04  Data: 0.013 (0.013)
Train: 116 [ 750/1251 ( 60%)]  Loss:  3.299911 (3.4583)  Time: 1.096s,  934.41/s  (1.090s,  939.19/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [ 800/1251 ( 64%)]  Loss:  3.497969 (3.4606)  Time: 1.084s,  944.65/s  (1.091s,  938.99/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 850/1251 ( 68%)]  Loss:  3.654767 (3.4714)  Time: 1.095s,  935.33/s  (1.091s,  938.81/s)  LR: 6.775e-04  Data: 0.013 (0.013)
Train: 116 [ 900/1251 ( 72%)]  Loss:  3.665827 (3.4816)  Time: 1.095s,  935.16/s  (1.090s,  939.05/s)  LR: 6.775e-04  Data: 0.011 (0.013)
Train: 116 [ 950/1251 ( 76%)]  Loss:  3.087268 (3.4619)  Time: 1.095s,  935.17/s  (1.091s,  939.01/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [1000/1251 ( 80%)]  Loss:  3.531868 (3.4652)  Time: 1.095s,  934.83/s  (1.091s,  938.98/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [1050/1251 ( 84%)]  Loss:  3.615020 (3.4720)  Time: 1.078s,  949.74/s  (1.090s,  939.18/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [1100/1251 ( 88%)]  Loss:  3.310570 (3.4650)  Time: 1.102s,  929.59/s  (1.090s,  939.08/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [1150/1251 ( 92%)]  Loss:  3.228299 (3.4552)  Time: 1.099s,  931.76/s  (1.090s,  939.22/s)  LR: 6.775e-04  Data: 0.012 (0.013)
Train: 116 [1200/1251 ( 96%)]  Loss:  3.533624 (3.4583)  Time: 1.076s,  951.35/s  (1.090s,  939.24/s)  LR: 6.775e-04  Data: 0.013 (0.013)
Train: 116 [1250/1251 (100%)]  Loss:  3.611727 (3.4642)  Time: 1.063s,  963.49/s  (1.090s,  939.44/s)  LR: 6.775e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.814 (5.814)  Loss:  0.5559 (0.5559)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6459 (1.0742)  Acc@1: 86.4387 (75.3700)  Acc@5: 96.4623 (93.0400)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-107.pth.tar', 75.09999998291016)

Train: 117 [   0/1251 (  0%)]  Loss:  3.578391 (3.5784)  Time: 1.096s,  934.42/s  (1.096s,  934.42/s)  LR: 6.727e-04  Data: 0.034 (0.034)
Train: 117 [  50/1251 (  4%)]  Loss:  3.375669 (3.4770)  Time: 1.076s,  951.43/s  (1.093s,  936.68/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 100/1251 (  8%)]  Loss:  3.326357 (3.4268)  Time: 1.078s,  950.14/s  (1.088s,  941.26/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 150/1251 ( 12%)]  Loss:  3.468458 (3.4372)  Time: 1.095s,  934.93/s  (1.087s,  941.78/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 200/1251 ( 16%)]  Loss:  3.629396 (3.4757)  Time: 1.079s,  949.10/s  (1.088s,  941.58/s)  LR: 6.727e-04  Data: 0.015 (0.013)
Train: 117 [ 250/1251 ( 20%)]  Loss:  3.346890 (3.4542)  Time: 1.096s,  934.44/s  (1.089s,  940.64/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 300/1251 ( 24%)]  Loss:  3.633576 (3.4798)  Time: 1.078s,  950.07/s  (1.089s,  940.21/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 350/1251 ( 28%)]  Loss:  3.318128 (3.4596)  Time: 1.090s,  939.61/s  (1.089s,  940.55/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 117 [ 400/1251 ( 32%)]  Loss:  3.725575 (3.4892)  Time: 1.081s,  947.33/s  (1.088s,  940.79/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 450/1251 ( 36%)]  Loss:  3.640215 (3.5043)  Time: 1.078s,  949.87/s  (1.088s,  940.83/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 500/1251 ( 40%)]  Loss:  3.378160 (3.4928)  Time: 1.073s,  954.25/s  (1.088s,  941.29/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 550/1251 ( 44%)]  Loss:  3.667684 (3.5074)  Time: 1.078s,  949.99/s  (1.088s,  941.41/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 600/1251 ( 48%)]  Loss:  3.597860 (3.5143)  Time: 1.082s,  946.48/s  (1.088s,  941.45/s)  LR: 6.727e-04  Data: 0.014 (0.013)
Train: 117 [ 650/1251 ( 52%)]  Loss:  3.514882 (3.5144)  Time: 1.110s,  922.68/s  (1.088s,  940.91/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 700/1251 ( 56%)]  Loss:  3.705987 (3.5271)  Time: 1.075s,  952.34/s  (1.088s,  941.16/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 750/1251 ( 60%)]  Loss:  3.444490 (3.5220)  Time: 1.072s,  954.86/s  (1.088s,  941.26/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [ 800/1251 ( 64%)]  Loss:  3.426273 (3.5164)  Time: 1.077s,  950.94/s  (1.088s,  940.99/s)  LR: 6.727e-04  Data: 0.013 (0.013)
Train: 117 [ 850/1251 ( 68%)]  Loss:  3.311141 (3.5050)  Time: 1.081s,  947.53/s  (1.088s,  941.22/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 900/1251 ( 72%)]  Loss:  3.544340 (3.5070)  Time: 1.094s,  935.92/s  (1.088s,  941.57/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [ 950/1251 ( 76%)]  Loss:  3.305940 (3.4970)  Time: 1.094s,  936.02/s  (1.088s,  941.28/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [1000/1251 ( 80%)]  Loss:  3.770420 (3.5100)  Time: 1.076s,  951.72/s  (1.088s,  940.96/s)  LR: 6.727e-04  Data: 0.014 (0.013)
Train: 117 [1050/1251 ( 84%)]  Loss:  3.598051 (3.5140)  Time: 1.093s,  936.73/s  (1.088s,  940.93/s)  LR: 6.727e-04  Data: 0.011 (0.013)
Train: 117 [1100/1251 ( 88%)]  Loss:  3.250860 (3.5026)  Time: 1.095s,  935.17/s  (1.089s,  940.74/s)  LR: 6.727e-04  Data: 0.012 (0.013)
Train: 117 [1150/1251 ( 92%)]  Loss:  3.412994 (3.4988)  Time: 1.078s,  949.51/s  (1.088s,  940.75/s)  LR: 6.727e-04  Data: 0.013 (0.013)
Train: 117 [1200/1251 ( 96%)]  Loss:  3.560928 (3.5013)  Time: 1.078s,  949.49/s  (1.089s,  940.59/s)  LR: 6.727e-04  Data: 0.013 (0.013)
Train: 117 [1250/1251 (100%)]  Loss:  3.833291 (3.5141)  Time: 1.080s,  948.18/s  (1.089s,  940.57/s)  LR: 6.727e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.805 (5.805)  Loss:  0.5288 (0.5288)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6150 (1.0545)  Acc@1: 85.8491 (75.5860)  Acc@5: 96.8160 (93.1080)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-103.pth.tar', 75.1120000390625)

Train: 118 [   0/1251 (  0%)]  Loss:  3.421930 (3.4219)  Time: 1.089s,  940.65/s  (1.089s,  940.65/s)  LR: 6.678e-04  Data: 0.027 (0.027)
Train: 118 [  50/1251 (  4%)]  Loss:  3.730314 (3.5761)  Time: 1.076s,  951.31/s  (1.083s,  945.13/s)  LR: 6.678e-04  Data: 0.015 (0.013)
Train: 118 [ 100/1251 (  8%)]  Loss:  3.687337 (3.6132)  Time: 1.094s,  936.17/s  (1.085s,  943.70/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 150/1251 ( 12%)]  Loss:  3.169104 (3.5022)  Time: 1.099s,  931.99/s  (1.088s,  941.42/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 200/1251 ( 16%)]  Loss:  3.641495 (3.5300)  Time: 1.096s,  934.31/s  (1.090s,  939.54/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 250/1251 ( 20%)]  Loss:  3.355391 (3.5009)  Time: 1.104s,  927.51/s  (1.090s,  939.07/s)  LR: 6.678e-04  Data: 0.018 (0.013)
Train: 118 [ 300/1251 ( 24%)]  Loss:  3.317222 (3.4747)  Time: 1.096s,  934.58/s  (1.090s,  939.13/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 350/1251 ( 28%)]  Loss:  3.772701 (3.5119)  Time: 1.094s,  936.12/s  (1.091s,  938.68/s)  LR: 6.678e-04  Data: 0.015 (0.013)
Train: 118 [ 400/1251 ( 32%)]  Loss:  3.583165 (3.5199)  Time: 1.078s,  950.19/s  (1.090s,  939.27/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 450/1251 ( 36%)]  Loss:  3.405151 (3.5084)  Time: 1.076s,  952.02/s  (1.089s,  940.02/s)  LR: 6.678e-04  Data: 0.011 (0.013)
Train: 118 [ 500/1251 ( 40%)]  Loss:  3.299235 (3.4894)  Time: 1.082s,  946.60/s  (1.089s,  940.59/s)  LR: 6.678e-04  Data: 0.013 (0.013)
Train: 118 [ 550/1251 ( 44%)]  Loss:  3.690109 (3.5061)  Time: 1.086s,  942.77/s  (1.088s,  940.83/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 600/1251 ( 48%)]  Loss:  3.520979 (3.5072)  Time: 1.079s,  949.43/s  (1.088s,  940.89/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 650/1251 ( 52%)]  Loss:  3.178385 (3.4838)  Time: 1.103s,  928.24/s  (1.088s,  941.23/s)  LR: 6.678e-04  Data: 0.016 (0.013)
Train: 118 [ 700/1251 ( 56%)]  Loss:  3.396969 (3.4780)  Time: 1.106s,  926.26/s  (1.088s,  941.41/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 750/1251 ( 60%)]  Loss:  3.634358 (3.4877)  Time: 1.076s,  951.87/s  (1.088s,  941.32/s)  LR: 6.678e-04  Data: 0.014 (0.013)
Train: 118 [ 800/1251 ( 64%)]  Loss:  3.468389 (3.4866)  Time: 1.093s,  936.73/s  (1.088s,  940.81/s)  LR: 6.678e-04  Data: 0.013 (0.013)
Train: 118 [ 850/1251 ( 68%)]  Loss:  3.761122 (3.5019)  Time: 1.077s,  950.50/s  (1.088s,  940.76/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 900/1251 ( 72%)]  Loss:  3.561173 (3.5050)  Time: 1.077s,  950.48/s  (1.089s,  940.61/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [ 950/1251 ( 76%)]  Loss:  3.166263 (3.4880)  Time: 1.113s,  920.16/s  (1.089s,  940.15/s)  LR: 6.678e-04  Data: 0.016 (0.013)
Train: 118 [1000/1251 ( 80%)]  Loss:  3.502780 (3.4887)  Time: 1.078s,  950.20/s  (1.089s,  940.03/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [1050/1251 ( 84%)]  Loss:  3.568327 (3.4924)  Time: 1.094s,  935.65/s  (1.089s,  940.09/s)  LR: 6.678e-04  Data: 0.013 (0.013)
Train: 118 [1100/1251 ( 88%)]  Loss:  3.667757 (3.5000)  Time: 1.099s,  932.17/s  (1.090s,  939.85/s)  LR: 6.678e-04  Data: 0.014 (0.013)
Train: 118 [1150/1251 ( 92%)]  Loss:  3.633886 (3.5056)  Time: 1.077s,  950.55/s  (1.090s,  939.80/s)  LR: 6.678e-04  Data: 0.012 (0.013)
Train: 118 [1200/1251 ( 96%)]  Loss:  3.603391 (3.5095)  Time: 1.098s,  932.61/s  (1.090s,  939.81/s)  LR: 6.678e-04  Data: 0.013 (0.013)
Train: 118 [1250/1251 (100%)]  Loss:  3.357456 (3.5036)  Time: 1.080s,  948.01/s  (1.090s,  939.69/s)  LR: 6.678e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.762 (5.762)  Loss:  0.5848 (0.5848)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6806 (1.0738)  Acc@1: 85.4953 (75.7980)  Acc@5: 96.8160 (93.2360)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-112.pth.tar', 75.21599998779297)

Train: 119 [   0/1251 (  0%)]  Loss:  3.596108 (3.5961)  Time: 1.357s,  754.86/s  (1.357s,  754.86/s)  LR: 6.629e-04  Data: 0.157 (0.157)
Train: 119 [  50/1251 (  4%)]  Loss:  3.491314 (3.5437)  Time: 1.075s,  952.18/s  (1.105s,  926.78/s)  LR: 6.629e-04  Data: 0.014 (0.016)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 119 [ 100/1251 (  8%)]  Loss:  3.613613 (3.5670)  Time: 1.076s,  952.00/s  (1.094s,  935.86/s)  LR: 6.629e-04  Data: 0.012 (0.014)
Train: 119 [ 150/1251 ( 12%)]  Loss:  3.558369 (3.5649)  Time: 1.085s,  943.65/s  (1.090s,  939.81/s)  LR: 6.629e-04  Data: 0.012 (0.014)
Train: 119 [ 200/1251 ( 16%)]  Loss:  3.613513 (3.5746)  Time: 1.077s,  951.22/s  (1.089s,  939.91/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 250/1251 ( 20%)]  Loss:  3.654369 (3.5879)  Time: 1.078s,  949.67/s  (1.089s,  940.74/s)  LR: 6.629e-04  Data: 0.014 (0.013)
Train: 119 [ 300/1251 ( 24%)]  Loss:  3.688483 (3.6023)  Time: 1.077s,  950.99/s  (1.087s,  941.83/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 350/1251 ( 28%)]  Loss:  3.605434 (3.6027)  Time: 1.096s,  934.10/s  (1.088s,  941.37/s)  LR: 6.629e-04  Data: 0.013 (0.013)
Train: 119 [ 400/1251 ( 32%)]  Loss:  3.652746 (3.6082)  Time: 1.095s,  934.88/s  (1.089s,  940.27/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 450/1251 ( 36%)]  Loss:  3.151370 (3.5625)  Time: 1.078s,  949.93/s  (1.089s,  939.93/s)  LR: 6.629e-04  Data: 0.014 (0.013)
Train: 119 [ 500/1251 ( 40%)]  Loss:  3.712044 (3.5761)  Time: 1.097s,  933.38/s  (1.090s,  939.77/s)  LR: 6.629e-04  Data: 0.013 (0.013)
Train: 119 [ 550/1251 ( 44%)]  Loss:  3.128678 (3.5388)  Time: 1.085s,  944.19/s  (1.090s,  939.87/s)  LR: 6.629e-04  Data: 0.010 (0.013)
Train: 119 [ 600/1251 ( 48%)]  Loss:  3.159207 (3.5096)  Time: 1.094s,  935.80/s  (1.089s,  939.99/s)  LR: 6.629e-04  Data: 0.014 (0.013)
Train: 119 [ 650/1251 ( 52%)]  Loss:  3.422983 (3.5034)  Time: 1.077s,  951.07/s  (1.089s,  940.42/s)  LR: 6.629e-04  Data: 0.015 (0.013)
Train: 119 [ 700/1251 ( 56%)]  Loss:  3.506405 (3.5036)  Time: 1.077s,  950.42/s  (1.089s,  940.31/s)  LR: 6.629e-04  Data: 0.014 (0.013)
Train: 119 [ 750/1251 ( 60%)]  Loss:  3.393888 (3.4968)  Time: 1.114s,  919.08/s  (1.089s,  940.00/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 800/1251 ( 64%)]  Loss:  3.602959 (3.5030)  Time: 1.076s,  951.94/s  (1.089s,  940.03/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [ 850/1251 ( 68%)]  Loss:  3.627567 (3.5099)  Time: 1.105s,  926.94/s  (1.089s,  940.11/s)  LR: 6.629e-04  Data: 0.014 (0.013)
Train: 119 [ 900/1251 ( 72%)]  Loss:  3.460151 (3.5073)  Time: 1.078s,  949.94/s  (1.089s,  940.04/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [ 950/1251 ( 76%)]  Loss:  3.461082 (3.5050)  Time: 1.095s,  935.48/s  (1.089s,  940.28/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1000/1251 ( 80%)]  Loss:  3.540600 (3.5067)  Time: 1.094s,  935.78/s  (1.089s,  940.06/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [1050/1251 ( 84%)]  Loss:  3.469093 (3.5050)  Time: 1.077s,  950.70/s  (1.089s,  939.92/s)  LR: 6.629e-04  Data: 0.015 (0.013)
Train: 119 [1100/1251 ( 88%)]  Loss:  3.746692 (3.5155)  Time: 1.077s,  950.37/s  (1.089s,  939.91/s)  LR: 6.629e-04  Data: 0.011 (0.013)
Train: 119 [1150/1251 ( 92%)]  Loss:  3.721733 (3.5241)  Time: 1.097s,  933.11/s  (1.090s,  939.62/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [1200/1251 ( 96%)]  Loss:  3.475911 (3.5222)  Time: 1.083s,  945.32/s  (1.090s,  939.34/s)  LR: 6.629e-04  Data: 0.012 (0.013)
Train: 119 [1250/1251 (100%)]  Loss:  3.247892 (3.5116)  Time: 1.060s,  965.81/s  (1.090s,  939.11/s)  LR: 6.629e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.875 (5.875)  Loss:  0.5308 (0.5308)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6085 (1.0526)  Acc@1: 85.9670 (75.6840)  Acc@5: 97.1698 (93.2540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-113.pth.tar', 75.21799992675781)

Train: 120 [   0/1251 (  0%)]  Loss:  3.388770 (3.3888)  Time: 1.088s,  941.39/s  (1.088s,  941.39/s)  LR: 6.580e-04  Data: 0.029 (0.029)
Train: 120 [  50/1251 (  4%)]  Loss:  3.504998 (3.4469)  Time: 1.077s,  950.59/s  (1.099s,  932.00/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 100/1251 (  8%)]  Loss:  3.508662 (3.4675)  Time: 1.072s,  955.23/s  (1.089s,  940.09/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [ 150/1251 ( 12%)]  Loss:  3.491207 (3.4734)  Time: 1.079s,  948.74/s  (1.087s,  941.77/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 200/1251 ( 16%)]  Loss:  3.124947 (3.4037)  Time: 1.093s,  936.80/s  (1.089s,  940.65/s)  LR: 6.580e-04  Data: 0.015 (0.013)
Train: 120 [ 250/1251 ( 20%)]  Loss:  3.483582 (3.4170)  Time: 1.077s,  950.59/s  (1.089s,  940.31/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 300/1251 ( 24%)]  Loss:  3.485605 (3.4268)  Time: 1.078s,  949.61/s  (1.090s,  939.73/s)  LR: 6.580e-04  Data: 0.014 (0.013)
Train: 120 [ 350/1251 ( 28%)]  Loss:  3.654044 (3.4552)  Time: 1.089s,  940.06/s  (1.089s,  940.28/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 400/1251 ( 32%)]  Loss:  3.293107 (3.4372)  Time: 1.102s,  929.45/s  (1.089s,  940.16/s)  LR: 6.580e-04  Data: 0.013 (0.013)
Train: 120 [ 450/1251 ( 36%)]  Loss:  3.530831 (3.4466)  Time: 1.080s,  948.32/s  (1.090s,  939.44/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 500/1251 ( 40%)]  Loss:  3.319917 (3.4351)  Time: 1.079s,  949.04/s  (1.090s,  939.43/s)  LR: 6.580e-04  Data: 0.013 (0.013)
Train: 120 [ 550/1251 ( 44%)]  Loss:  3.807489 (3.4661)  Time: 1.076s,  951.32/s  (1.090s,  939.55/s)  LR: 6.580e-04  Data: 0.013 (0.013)
Train: 120 [ 600/1251 ( 48%)]  Loss:  3.602778 (3.4766)  Time: 1.077s,  950.48/s  (1.090s,  939.79/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 650/1251 ( 52%)]  Loss:  3.621466 (3.4870)  Time: 1.076s,  951.82/s  (1.089s,  940.17/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 700/1251 ( 56%)]  Loss:  3.499979 (3.4878)  Time: 1.095s,  935.34/s  (1.089s,  940.40/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 750/1251 ( 60%)]  Loss:  3.594468 (3.4945)  Time: 1.095s,  935.01/s  (1.089s,  940.42/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 800/1251 ( 64%)]  Loss:  3.690602 (3.5060)  Time: 1.081s,  947.26/s  (1.089s,  940.37/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [ 850/1251 ( 68%)]  Loss:  3.514357 (3.5065)  Time: 1.079s,  948.88/s  (1.089s,  940.31/s)  LR: 6.580e-04  Data: 0.018 (0.013)
Train: 120 [ 900/1251 ( 72%)]  Loss:  3.738950 (3.5187)  Time: 1.079s,  948.75/s  (1.089s,  940.68/s)  LR: 6.580e-04  Data: 0.013 (0.013)
Train: 120 [ 950/1251 ( 76%)]  Loss:  3.522406 (3.5189)  Time: 1.078s,  949.52/s  (1.089s,  940.71/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [1000/1251 ( 80%)]  Loss:  3.718660 (3.5284)  Time: 1.082s,  946.45/s  (1.089s,  940.62/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1050/1251 ( 84%)]  Loss:  3.408407 (3.5230)  Time: 1.078s,  949.97/s  (1.089s,  940.65/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [1100/1251 ( 88%)]  Loss:  3.425580 (3.5187)  Time: 1.093s,  937.28/s  (1.088s,  940.90/s)  LR: 6.580e-04  Data: 0.011 (0.013)
Train: 120 [1150/1251 ( 92%)]  Loss:  3.245486 (3.5073)  Time: 1.102s,  928.84/s  (1.089s,  940.72/s)  LR: 6.580e-04  Data: 0.015 (0.013)
Train: 120 [1200/1251 ( 96%)]  Loss:  3.652583 (3.5132)  Time: 1.079s,  948.69/s  (1.088s,  940.77/s)  LR: 6.580e-04  Data: 0.012 (0.013)
Train: 120 [1250/1251 (100%)]  Loss:  3.358217 (3.5072)  Time: 1.061s,  964.67/s  (1.089s,  940.63/s)  LR: 6.580e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.787 (5.787)  Loss:  0.5366 (0.5366)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6510 (1.0461)  Acc@1: 84.7877 (75.7460)  Acc@5: 97.0519 (93.2260)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-106.pth.tar', 75.28799992919922)

Train: 121 [   0/1251 (  0%)]  Loss:  3.425069 (3.4251)  Time: 1.087s,  942.18/s  (1.087s,  942.18/s)  LR: 6.530e-04  Data: 0.026 (0.026)
Train: 121 [  50/1251 (  4%)]  Loss:  3.316478 (3.3708)  Time: 1.076s,  951.66/s  (1.093s,  937.25/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 100/1251 (  8%)]  Loss:  3.500376 (3.4140)  Time: 1.099s,  931.64/s  (1.089s,  940.69/s)  LR: 6.530e-04  Data: 0.014 (0.013)
Train: 121 [ 150/1251 ( 12%)]  Loss:  3.343704 (3.3964)  Time: 1.095s,  935.07/s  (1.088s,  941.49/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 200/1251 ( 16%)]  Loss:  3.393986 (3.3959)  Time: 1.077s,  950.71/s  (1.087s,  942.17/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 250/1251 ( 20%)]  Loss:  3.575817 (3.4259)  Time: 1.094s,  936.41/s  (1.088s,  940.94/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 300/1251 ( 24%)]  Loss:  3.676616 (3.4617)  Time: 1.082s,  946.76/s  (1.088s,  941.17/s)  LR: 6.530e-04  Data: 0.013 (0.013)
Train: 121 [ 350/1251 ( 28%)]  Loss:  3.501616 (3.4667)  Time: 1.094s,  935.81/s  (1.088s,  940.86/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 400/1251 ( 32%)]  Loss:  3.512577 (3.4718)  Time: 1.077s,  951.10/s  (1.090s,  939.58/s)  LR: 6.530e-04  Data: 0.014 (0.013)
Train: 121 [ 450/1251 ( 36%)]  Loss:  3.147897 (3.4394)  Time: 1.103s,  928.51/s  (1.089s,  940.34/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 500/1251 ( 40%)]  Loss:  3.545643 (3.4491)  Time: 1.094s,  935.61/s  (1.089s,  940.24/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 550/1251 ( 44%)]  Loss:  3.541200 (3.4567)  Time: 1.094s,  935.60/s  (1.090s,  939.53/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 600/1251 ( 48%)]  Loss:  3.210149 (3.4378)  Time: 1.093s,  936.62/s  (1.090s,  939.33/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 650/1251 ( 52%)]  Loss:  3.508104 (3.4428)  Time: 1.077s,  951.23/s  (1.090s,  939.69/s)  LR: 6.530e-04  Data: 0.013 (0.013)
Train: 121 [ 700/1251 ( 56%)]  Loss:  3.924677 (3.4749)  Time: 1.086s,  943.18/s  (1.090s,  939.83/s)  LR: 6.530e-04  Data: 0.013 (0.013)
Train: 121 [ 750/1251 ( 60%)]  Loss:  3.474045 (3.4749)  Time: 1.094s,  935.62/s  (1.089s,  940.13/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [ 800/1251 ( 64%)]  Loss:  3.728447 (3.4898)  Time: 1.095s,  935.48/s  (1.089s,  939.91/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 850/1251 ( 68%)]  Loss:  3.748726 (3.5042)  Time: 1.093s,  937.21/s  (1.090s,  939.74/s)  LR: 6.530e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 121 [ 900/1251 ( 72%)]  Loss:  3.573840 (3.5078)  Time: 1.083s,  945.79/s  (1.090s,  939.82/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [ 950/1251 ( 76%)]  Loss:  3.542065 (3.5096)  Time: 1.103s,  928.31/s  (1.090s,  939.65/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1000/1251 ( 80%)]  Loss:  3.481575 (3.5082)  Time: 1.077s,  950.38/s  (1.090s,  939.65/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1050/1251 ( 84%)]  Loss:  3.606387 (3.5127)  Time: 1.097s,  933.46/s  (1.090s,  939.47/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1100/1251 ( 88%)]  Loss:  3.632315 (3.5179)  Time: 1.104s,  927.76/s  (1.090s,  939.48/s)  LR: 6.530e-04  Data: 0.017 (0.013)
Train: 121 [1150/1251 ( 92%)]  Loss:  3.441175 (3.5147)  Time: 1.081s,  947.18/s  (1.090s,  939.72/s)  LR: 6.530e-04  Data: 0.012 (0.013)
Train: 121 [1200/1251 ( 96%)]  Loss:  3.620345 (3.5189)  Time: 1.096s,  934.20/s  (1.090s,  939.74/s)  LR: 6.530e-04  Data: 0.011 (0.013)
Train: 121 [1250/1251 (100%)]  Loss:  3.761434 (3.5282)  Time: 1.062s,  964.57/s  (1.090s,  939.38/s)  LR: 6.530e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.802 (5.802)  Loss:  0.5718 (0.5718)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.7122 (1.0722)  Acc@1: 84.9057 (75.4320)  Acc@5: 96.8160 (93.3040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 75.43200003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-110.pth.tar', 75.3159999609375)

Train: 122 [   0/1251 (  0%)]  Loss:  3.275435 (3.2754)  Time: 1.087s,  942.16/s  (1.087s,  942.16/s)  LR: 6.481e-04  Data: 0.026 (0.026)
Train: 122 [  50/1251 (  4%)]  Loss:  3.726250 (3.5008)  Time: 1.078s,  949.81/s  (1.087s,  942.00/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 100/1251 (  8%)]  Loss:  3.371426 (3.4577)  Time: 1.083s,  945.72/s  (1.088s,  940.82/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 150/1251 ( 12%)]  Loss:  3.322466 (3.4239)  Time: 1.074s,  953.26/s  (1.088s,  940.97/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 200/1251 ( 16%)]  Loss:  3.612453 (3.4616)  Time: 1.096s,  934.63/s  (1.089s,  940.65/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 250/1251 ( 20%)]  Loss:  3.391750 (3.4500)  Time: 1.094s,  936.22/s  (1.089s,  940.70/s)  LR: 6.481e-04  Data: 0.015 (0.013)
Train: 122 [ 300/1251 ( 24%)]  Loss:  3.796773 (3.4995)  Time: 1.076s,  951.37/s  (1.090s,  939.58/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 350/1251 ( 28%)]  Loss:  3.242330 (3.4674)  Time: 1.079s,  949.21/s  (1.089s,  940.11/s)  LR: 6.481e-04  Data: 0.013 (0.013)
Train: 122 [ 400/1251 ( 32%)]  Loss:  3.744518 (3.4982)  Time: 1.095s,  935.46/s  (1.088s,  940.79/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 450/1251 ( 36%)]  Loss:  3.457156 (3.4941)  Time: 1.094s,  936.44/s  (1.089s,  940.18/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 500/1251 ( 40%)]  Loss:  3.325872 (3.4788)  Time: 1.078s,  949.47/s  (1.090s,  939.72/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 550/1251 ( 44%)]  Loss:  3.830627 (3.5081)  Time: 1.076s,  951.96/s  (1.090s,  939.59/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 600/1251 ( 48%)]  Loss:  3.383320 (3.4985)  Time: 1.093s,  937.19/s  (1.090s,  939.23/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 650/1251 ( 52%)]  Loss:  3.729454 (3.5150)  Time: 1.097s,  933.85/s  (1.091s,  938.73/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 700/1251 ( 56%)]  Loss:  3.654969 (3.5243)  Time: 1.074s,  953.39/s  (1.090s,  939.23/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [ 750/1251 ( 60%)]  Loss:  3.315591 (3.5113)  Time: 1.079s,  948.59/s  (1.090s,  939.27/s)  LR: 6.481e-04  Data: 0.013 (0.013)
Train: 122 [ 800/1251 ( 64%)]  Loss:  3.327280 (3.5005)  Time: 1.096s,  933.99/s  (1.090s,  939.34/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 850/1251 ( 68%)]  Loss:  3.436882 (3.4969)  Time: 1.077s,  950.81/s  (1.090s,  939.51/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [ 900/1251 ( 72%)]  Loss:  3.349915 (3.4892)  Time: 1.080s,  948.23/s  (1.090s,  939.36/s)  LR: 6.481e-04  Data: 0.019 (0.013)
Train: 122 [ 950/1251 ( 76%)]  Loss:  3.444332 (3.4869)  Time: 1.170s,  875.25/s  (1.090s,  939.39/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [1000/1251 ( 80%)]  Loss:  3.384160 (3.4820)  Time: 1.085s,  944.11/s  (1.090s,  939.64/s)  LR: 6.481e-04  Data: 0.015 (0.013)
Train: 122 [1050/1251 ( 84%)]  Loss:  3.284273 (3.4731)  Time: 1.098s,  932.72/s  (1.090s,  939.43/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [1100/1251 ( 88%)]  Loss:  3.336874 (3.4671)  Time: 1.080s,  948.35/s  (1.090s,  939.43/s)  LR: 6.481e-04  Data: 0.012 (0.013)
Train: 122 [1150/1251 ( 92%)]  Loss:  3.116724 (3.4525)  Time: 1.095s,  935.56/s  (1.090s,  939.59/s)  LR: 6.481e-04  Data: 0.013 (0.013)
Train: 122 [1200/1251 ( 96%)]  Loss:  3.588222 (3.4580)  Time: 1.105s,  926.81/s  (1.090s,  939.63/s)  LR: 6.481e-04  Data: 0.011 (0.013)
Train: 122 [1250/1251 (100%)]  Loss:  3.508726 (3.4599)  Time: 1.078s,  950.21/s  (1.090s,  939.48/s)  LR: 6.481e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.854 (5.854)  Loss:  0.5517 (0.5517)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.7018 (1.1032)  Acc@1: 85.8491 (75.6660)  Acc@5: 97.4057 (93.2740)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 75.43200003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-116.pth.tar', 75.37000005615235)

Train: 123 [   0/1251 (  0%)]  Loss:  3.664968 (3.6650)  Time: 1.084s,  944.42/s  (1.084s,  944.42/s)  LR: 6.431e-04  Data: 0.025 (0.025)
Train: 123 [  50/1251 (  4%)]  Loss:  3.472916 (3.5689)  Time: 1.096s,  934.49/s  (1.096s,  934.37/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 100/1251 (  8%)]  Loss:  3.745960 (3.6279)  Time: 1.087s,  942.19/s  (1.089s,  940.26/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 150/1251 ( 12%)]  Loss:  3.419545 (3.5758)  Time: 1.078s,  950.27/s  (1.091s,  938.20/s)  LR: 6.431e-04  Data: 0.014 (0.013)
Train: 123 [ 200/1251 ( 16%)]  Loss:  3.118649 (3.4844)  Time: 1.109s,  923.29/s  (1.091s,  938.33/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 250/1251 ( 20%)]  Loss:  3.481313 (3.4839)  Time: 1.097s,  933.20/s  (1.090s,  939.10/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 300/1251 ( 24%)]  Loss:  3.449982 (3.4790)  Time: 1.095s,  935.19/s  (1.089s,  940.24/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 350/1251 ( 28%)]  Loss:  3.619796 (3.4966)  Time: 1.095s,  934.90/s  (1.091s,  938.51/s)  LR: 6.431e-04  Data: 0.017 (0.013)
Train: 123 [ 400/1251 ( 32%)]  Loss:  3.718918 (3.5213)  Time: 1.102s,  929.01/s  (1.092s,  938.10/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 450/1251 ( 36%)]  Loss:  3.341388 (3.5033)  Time: 1.095s,  935.16/s  (1.091s,  938.63/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 500/1251 ( 40%)]  Loss:  3.650650 (3.5167)  Time: 1.078s,  950.08/s  (1.090s,  939.03/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 550/1251 ( 44%)]  Loss:  3.581954 (3.5222)  Time: 1.087s,  942.10/s  (1.091s,  938.68/s)  LR: 6.431e-04  Data: 0.013 (0.013)
Train: 123 [ 600/1251 ( 48%)]  Loss:  3.567804 (3.5257)  Time: 1.080s,  947.85/s  (1.091s,  939.01/s)  LR: 6.431e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 123 [ 650/1251 ( 52%)]  Loss:  3.540148 (3.5267)  Time: 1.078s,  949.57/s  (1.090s,  939.29/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 700/1251 ( 56%)]  Loss:  3.306161 (3.5120)  Time: 1.075s,  952.48/s  (1.090s,  939.45/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 750/1251 ( 60%)]  Loss:  3.415140 (3.5060)  Time: 1.074s,  953.82/s  (1.090s,  939.67/s)  LR: 6.431e-04  Data: 0.011 (0.013)
Train: 123 [ 800/1251 ( 64%)]  Loss:  3.771510 (3.5216)  Time: 1.078s,  950.14/s  (1.090s,  939.73/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 850/1251 ( 68%)]  Loss:  3.035278 (3.4946)  Time: 1.095s,  934.96/s  (1.090s,  939.77/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 900/1251 ( 72%)]  Loss:  3.289171 (3.4838)  Time: 1.094s,  935.99/s  (1.090s,  939.56/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [ 950/1251 ( 76%)]  Loss:  3.436247 (3.4814)  Time: 1.076s,  951.40/s  (1.089s,  940.00/s)  LR: 6.431e-04  Data: 0.014 (0.013)
Train: 123 [1000/1251 ( 80%)]  Loss:  3.219883 (3.4689)  Time: 1.174s,  872.05/s  (1.090s,  939.79/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [1050/1251 ( 84%)]  Loss:  3.346219 (3.4633)  Time: 1.076s,  951.56/s  (1.089s,  940.25/s)  LR: 6.431e-04  Data: 0.014 (0.013)
Train: 123 [1100/1251 ( 88%)]  Loss:  3.662128 (3.4720)  Time: 1.093s,  936.56/s  (1.089s,  940.18/s)  LR: 6.431e-04  Data: 0.013 (0.013)
Train: 123 [1150/1251 ( 92%)]  Loss:  3.442185 (3.4707)  Time: 1.093s,  936.63/s  (1.089s,  940.10/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [1200/1251 ( 96%)]  Loss:  3.641268 (3.4776)  Time: 1.081s,  947.48/s  (1.089s,  940.09/s)  LR: 6.431e-04  Data: 0.012 (0.013)
Train: 123 [1250/1251 (100%)]  Loss:  3.044951 (3.4609)  Time: 1.080s,  948.51/s  (1.089s,  940.12/s)  LR: 6.431e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.844 (5.844)  Loss:  0.5569 (0.5569)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6157 (1.0481)  Acc@1: 85.0236 (75.7240)  Acc@5: 96.5802 (93.2000)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 75.43200003662109)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-111.pth.tar', 75.39399998535156)

Train: 124 [   0/1251 (  0%)]  Loss:  3.679832 (3.6798)  Time: 1.087s,  941.99/s  (1.087s,  941.99/s)  LR: 6.381e-04  Data: 0.026 (0.026)
Train: 124 [  50/1251 (  4%)]  Loss:  3.457765 (3.5688)  Time: 1.075s,  952.63/s  (1.087s,  941.73/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 100/1251 (  8%)]  Loss:  3.631738 (3.5898)  Time: 1.078s,  949.65/s  (1.089s,  940.04/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 150/1251 ( 12%)]  Loss:  3.359967 (3.5323)  Time: 1.081s,  947.17/s  (1.089s,  940.29/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 200/1251 ( 16%)]  Loss:  3.535018 (3.5329)  Time: 1.079s,  948.79/s  (1.087s,  941.66/s)  LR: 6.381e-04  Data: 0.018 (0.013)
Train: 124 [ 250/1251 ( 20%)]  Loss:  3.566723 (3.5385)  Time: 1.089s,  940.65/s  (1.086s,  942.56/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 300/1251 ( 24%)]  Loss:  3.155793 (3.4838)  Time: 1.156s,  885.64/s  (1.087s,  942.43/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 350/1251 ( 28%)]  Loss:  3.667048 (3.5067)  Time: 1.119s,  914.85/s  (1.086s,  942.59/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 400/1251 ( 32%)]  Loss:  3.601739 (3.5173)  Time: 1.093s,  936.69/s  (1.087s,  942.12/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 450/1251 ( 36%)]  Loss:  3.514228 (3.5170)  Time: 1.076s,  951.76/s  (1.087s,  942.11/s)  LR: 6.381e-04  Data: 0.014 (0.013)
Train: 124 [ 500/1251 ( 40%)]  Loss:  3.198429 (3.4880)  Time: 1.098s,  932.63/s  (1.087s,  941.74/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 550/1251 ( 44%)]  Loss:  3.440685 (3.4841)  Time: 1.096s,  934.07/s  (1.087s,  941.99/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 600/1251 ( 48%)]  Loss:  3.532283 (3.4878)  Time: 1.095s,  935.46/s  (1.087s,  941.76/s)  LR: 6.381e-04  Data: 0.015 (0.013)
Train: 124 [ 650/1251 ( 52%)]  Loss:  3.190034 (3.4665)  Time: 1.080s,  947.82/s  (1.088s,  941.32/s)  LR: 6.381e-04  Data: 0.014 (0.013)
Train: 124 [ 700/1251 ( 56%)]  Loss:  3.180706 (3.4475)  Time: 1.106s,  925.80/s  (1.088s,  941.21/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 750/1251 ( 60%)]  Loss:  3.285030 (3.4373)  Time: 1.076s,  951.92/s  (1.088s,  940.97/s)  LR: 6.381e-04  Data: 0.016 (0.013)
Train: 124 [ 800/1251 ( 64%)]  Loss:  3.437305 (3.4373)  Time: 1.096s,  934.04/s  (1.088s,  941.03/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 850/1251 ( 68%)]  Loss:  3.600481 (3.4464)  Time: 1.077s,  950.54/s  (1.088s,  941.18/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 900/1251 ( 72%)]  Loss:  3.526184 (3.4506)  Time: 1.084s,  944.66/s  (1.088s,  940.85/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [ 950/1251 ( 76%)]  Loss:  3.386250 (3.4474)  Time: 1.076s,  951.62/s  (1.088s,  941.20/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [1000/1251 ( 80%)]  Loss:  3.845985 (3.4663)  Time: 1.098s,  932.99/s  (1.088s,  941.22/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [1050/1251 ( 84%)]  Loss:  3.465486 (3.4663)  Time: 1.080s,  947.73/s  (1.088s,  941.28/s)  LR: 6.381e-04  Data: 0.014 (0.013)
Train: 124 [1100/1251 ( 88%)]  Loss:  3.421828 (3.4644)  Time: 1.097s,  933.75/s  (1.088s,  941.39/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [1150/1251 ( 92%)]  Loss:  3.344939 (3.4594)  Time: 1.094s,  936.43/s  (1.088s,  941.28/s)  LR: 6.381e-04  Data: 0.015 (0.013)
Train: 124 [1200/1251 ( 96%)]  Loss:  3.449487 (3.4590)  Time: 1.076s,  951.52/s  (1.088s,  941.29/s)  LR: 6.381e-04  Data: 0.012 (0.013)
Train: 124 [1250/1251 (100%)]  Loss:  3.346332 (3.4547)  Time: 1.061s,  964.96/s  (1.088s,  940.85/s)  LR: 6.381e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.882 (5.882)  Loss:  0.5646 (0.5646)  Acc@1: 89.7461 (89.7461)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6773 (1.0852)  Acc@1: 85.9670 (75.9240)  Acc@5: 96.8160 (93.3620)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-121.pth.tar', 75.43200003662109)

Train: 125 [   0/1251 (  0%)]  Loss:  3.383949 (3.3839)  Time: 1.087s,  941.81/s  (1.087s,  941.81/s)  LR: 6.331e-04  Data: 0.027 (0.027)
Train: 125 [  50/1251 (  4%)]  Loss:  3.812582 (3.5983)  Time: 1.080s,  948.22/s  (1.083s,  945.21/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 100/1251 (  8%)]  Loss:  3.696526 (3.6310)  Time: 1.096s,  934.32/s  (1.084s,  944.81/s)  LR: 6.331e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 125 [ 150/1251 ( 12%)]  Loss:  3.644212 (3.6343)  Time: 1.094s,  936.23/s  (1.087s,  941.98/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 200/1251 ( 16%)]  Loss:  3.706640 (3.6488)  Time: 1.089s,  940.31/s  (1.087s,  941.85/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 250/1251 ( 20%)]  Loss:  3.611276 (3.6425)  Time: 1.076s,  951.59/s  (1.089s,  940.26/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 300/1251 ( 24%)]  Loss:  3.558267 (3.6305)  Time: 1.099s,  932.08/s  (1.088s,  941.38/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 350/1251 ( 28%)]  Loss:  3.362886 (3.5970)  Time: 1.096s,  934.38/s  (1.088s,  941.12/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 400/1251 ( 32%)]  Loss:  3.539443 (3.5906)  Time: 1.077s,  951.02/s  (1.088s,  941.48/s)  LR: 6.331e-04  Data: 0.013 (0.013)
Train: 125 [ 450/1251 ( 36%)]  Loss:  3.744284 (3.6060)  Time: 1.198s,  854.79/s  (1.088s,  941.48/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [ 500/1251 ( 40%)]  Loss:  3.758959 (3.6199)  Time: 1.092s,  938.01/s  (1.089s,  940.50/s)  LR: 6.331e-04  Data: 0.015 (0.013)
Train: 125 [ 550/1251 ( 44%)]  Loss:  3.515581 (3.6112)  Time: 1.077s,  950.99/s  (1.089s,  940.66/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 600/1251 ( 48%)]  Loss:  3.601927 (3.6105)  Time: 1.079s,  949.40/s  (1.088s,  940.96/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 650/1251 ( 52%)]  Loss:  3.554529 (3.6065)  Time: 1.075s,  952.31/s  (1.088s,  940.79/s)  LR: 6.331e-04  Data: 0.014 (0.013)
Train: 125 [ 700/1251 ( 56%)]  Loss:  3.473777 (3.5977)  Time: 1.075s,  952.54/s  (1.088s,  941.13/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 750/1251 ( 60%)]  Loss:  3.398503 (3.5852)  Time: 1.095s,  934.80/s  (1.088s,  941.29/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 800/1251 ( 64%)]  Loss:  3.799082 (3.5978)  Time: 1.084s,  944.49/s  (1.088s,  941.12/s)  LR: 6.331e-04  Data: 0.014 (0.013)
Train: 125 [ 850/1251 ( 68%)]  Loss:  3.688984 (3.6029)  Time: 1.092s,  938.07/s  (1.088s,  940.94/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 900/1251 ( 72%)]  Loss:  3.540464 (3.5996)  Time: 1.083s,  945.94/s  (1.088s,  940.83/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [ 950/1251 ( 76%)]  Loss:  3.367326 (3.5880)  Time: 1.095s,  935.40/s  (1.089s,  940.70/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [1000/1251 ( 80%)]  Loss:  3.460770 (3.5819)  Time: 1.078s,  950.16/s  (1.089s,  940.10/s)  LR: 6.331e-04  Data: 0.012 (0.013)
Train: 125 [1050/1251 ( 84%)]  Loss:  3.455791 (3.5762)  Time: 1.077s,  950.36/s  (1.089s,  940.18/s)  LR: 6.331e-04  Data: 0.014 (0.013)
Train: 125 [1100/1251 ( 88%)]  Loss:  3.410063 (3.5689)  Time: 1.077s,  950.92/s  (1.089s,  940.38/s)  LR: 6.331e-04  Data: 0.013 (0.013)
Train: 125 [1150/1251 ( 92%)]  Loss:  3.702005 (3.5745)  Time: 1.077s,  950.58/s  (1.089s,  940.42/s)  LR: 6.331e-04  Data: 0.011 (0.013)
Train: 125 [1200/1251 ( 96%)]  Loss:  3.834338 (3.5849)  Time: 1.078s,  949.66/s  (1.089s,  940.30/s)  LR: 6.331e-04  Data: 0.014 (0.013)
Train: 125 [1250/1251 (100%)]  Loss:  3.362773 (3.5763)  Time: 1.147s,  892.74/s  (1.089s,  940.12/s)  LR: 6.331e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.734 (5.734)  Loss:  0.5589 (0.5589)  Acc@1: 89.8438 (89.8438)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6534 (1.0557)  Acc@1: 85.0236 (75.7320)  Acc@5: 96.9340 (93.3400)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-114.pth.tar', 75.45799990478515)

Train: 126 [   0/1251 (  0%)]  Loss:  3.336742 (3.3367)  Time: 1.087s,  942.33/s  (1.087s,  942.33/s)  LR: 6.281e-04  Data: 0.026 (0.026)
Train: 126 [  50/1251 (  4%)]  Loss:  3.349050 (3.3429)  Time: 1.079s,  948.98/s  (1.083s,  945.18/s)  LR: 6.281e-04  Data: 0.013 (0.013)
Train: 126 [ 100/1251 (  8%)]  Loss:  3.423170 (3.3697)  Time: 1.129s,  907.20/s  (1.085s,  944.14/s)  LR: 6.281e-04  Data: 0.013 (0.013)
Train: 126 [ 150/1251 ( 12%)]  Loss:  3.346477 (3.3639)  Time: 1.106s,  926.19/s  (1.087s,  941.91/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 200/1251 ( 16%)]  Loss:  3.512328 (3.3936)  Time: 1.083s,  945.49/s  (1.088s,  941.18/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 250/1251 ( 20%)]  Loss:  3.317150 (3.3808)  Time: 1.079s,  948.98/s  (1.088s,  941.18/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 300/1251 ( 24%)]  Loss:  3.652881 (3.4197)  Time: 1.078s,  949.88/s  (1.088s,  940.82/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 350/1251 ( 28%)]  Loss:  3.333630 (3.4089)  Time: 1.081s,  947.00/s  (1.087s,  941.70/s)  LR: 6.281e-04  Data: 0.013 (0.013)
Train: 126 [ 400/1251 ( 32%)]  Loss:  3.415942 (3.4097)  Time: 1.082s,  946.00/s  (1.087s,  942.08/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 450/1251 ( 36%)]  Loss:  3.684963 (3.4372)  Time: 1.079s,  948.84/s  (1.087s,  941.75/s)  LR: 6.281e-04  Data: 0.014 (0.013)
Train: 126 [ 500/1251 ( 40%)]  Loss:  3.870785 (3.4766)  Time: 1.082s,  946.44/s  (1.087s,  941.83/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 550/1251 ( 44%)]  Loss:  3.508706 (3.4793)  Time: 1.109s,  923.70/s  (1.087s,  942.29/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 600/1251 ( 48%)]  Loss:  3.802366 (3.5042)  Time: 1.095s,  935.12/s  (1.088s,  941.53/s)  LR: 6.281e-04  Data: 0.011 (0.013)
Train: 126 [ 650/1251 ( 52%)]  Loss:  3.548221 (3.5073)  Time: 1.094s,  935.85/s  (1.088s,  940.85/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 700/1251 ( 56%)]  Loss:  3.188278 (3.4860)  Time: 1.097s,  933.85/s  (1.089s,  940.67/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 750/1251 ( 60%)]  Loss:  3.381474 (3.4795)  Time: 1.081s,  947.54/s  (1.088s,  940.75/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 800/1251 ( 64%)]  Loss:  3.471900 (3.4791)  Time: 1.076s,  951.37/s  (1.089s,  940.63/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [ 850/1251 ( 68%)]  Loss:  3.387716 (3.4740)  Time: 1.080s,  948.46/s  (1.089s,  940.66/s)  LR: 6.281e-04  Data: 0.013 (0.013)
Train: 126 [ 900/1251 ( 72%)]  Loss:  3.431628 (3.4718)  Time: 1.083s,  945.85/s  (1.088s,  940.87/s)  LR: 6.281e-04  Data: 0.014 (0.013)
Train: 126 [ 950/1251 ( 76%)]  Loss:  3.365082 (3.4664)  Time: 1.078s,  950.04/s  (1.088s,  941.15/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [1000/1251 ( 80%)]  Loss:  3.226889 (3.4550)  Time: 1.093s,  937.15/s  (1.088s,  940.96/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [1050/1251 ( 84%)]  Loss:  3.360924 (3.4507)  Time: 1.079s,  948.77/s  (1.088s,  940.78/s)  LR: 6.281e-04  Data: 0.015 (0.013)
Train: 126 [1100/1251 ( 88%)]  Loss:  3.584824 (3.4566)  Time: 1.075s,  952.68/s  (1.089s,  940.54/s)  LR: 6.281e-04  Data: 0.015 (0.013)
Train: 126 [1150/1251 ( 92%)]  Loss:  3.682356 (3.4660)  Time: 1.076s,  951.51/s  (1.089s,  940.64/s)  LR: 6.281e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 126 [1200/1251 ( 96%)]  Loss:  3.353005 (3.4615)  Time: 1.079s,  948.70/s  (1.089s,  940.59/s)  LR: 6.281e-04  Data: 0.012 (0.013)
Train: 126 [1250/1251 (100%)]  Loss:  3.476101 (3.4620)  Time: 1.082s,  946.77/s  (1.089s,  940.61/s)  LR: 6.281e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.829 (5.829)  Loss:  0.5227 (0.5227)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6442 (1.0660)  Acc@1: 86.0849 (75.8100)  Acc@5: 96.8160 (93.3200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-115.pth.tar', 75.47200000732421)

Train: 127 [   0/1251 (  0%)]  Loss:  3.526530 (3.5265)  Time: 1.094s,  935.68/s  (1.094s,  935.68/s)  LR: 6.231e-04  Data: 0.033 (0.033)
Train: 127 [  50/1251 (  4%)]  Loss:  3.415140 (3.4708)  Time: 1.083s,  945.57/s  (1.086s,  943.08/s)  LR: 6.231e-04  Data: 0.013 (0.013)
Train: 127 [ 100/1251 (  8%)]  Loss:  3.504086 (3.4819)  Time: 1.083s,  945.11/s  (1.088s,  941.51/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 150/1251 ( 12%)]  Loss:  3.691932 (3.5344)  Time: 1.077s,  950.46/s  (1.088s,  941.16/s)  LR: 6.231e-04  Data: 0.013 (0.013)
Train: 127 [ 200/1251 ( 16%)]  Loss:  3.750044 (3.5775)  Time: 1.095s,  935.43/s  (1.089s,  940.32/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 250/1251 ( 20%)]  Loss:  3.391201 (3.5465)  Time: 1.080s,  948.30/s  (1.088s,  941.02/s)  LR: 6.231e-04  Data: 0.015 (0.013)
Train: 127 [ 300/1251 ( 24%)]  Loss:  3.575530 (3.5506)  Time: 1.075s,  952.52/s  (1.088s,  941.43/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 350/1251 ( 28%)]  Loss:  3.550972 (3.5507)  Time: 1.093s,  937.10/s  (1.088s,  941.10/s)  LR: 6.231e-04  Data: 0.014 (0.013)
Train: 127 [ 400/1251 ( 32%)]  Loss:  3.861868 (3.5853)  Time: 1.075s,  952.48/s  (1.088s,  940.90/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 450/1251 ( 36%)]  Loss:  3.402175 (3.5669)  Time: 1.076s,  951.46/s  (1.088s,  940.88/s)  LR: 6.231e-04  Data: 0.014 (0.013)
Train: 127 [ 500/1251 ( 40%)]  Loss:  3.654252 (3.5749)  Time: 1.079s,  948.86/s  (1.088s,  940.75/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 550/1251 ( 44%)]  Loss:  3.729388 (3.5878)  Time: 1.083s,  945.30/s  (1.088s,  940.90/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [ 600/1251 ( 48%)]  Loss:  3.509079 (3.5817)  Time: 1.096s,  934.13/s  (1.089s,  940.52/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 650/1251 ( 52%)]  Loss:  3.219623 (3.5558)  Time: 1.095s,  935.54/s  (1.089s,  940.21/s)  LR: 6.231e-04  Data: 0.014 (0.013)
Train: 127 [ 700/1251 ( 56%)]  Loss:  3.219146 (3.5334)  Time: 1.074s,  953.83/s  (1.089s,  940.21/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 750/1251 ( 60%)]  Loss:  3.224738 (3.5141)  Time: 1.096s,  933.98/s  (1.089s,  940.05/s)  LR: 6.231e-04  Data: 0.013 (0.013)
Train: 127 [ 800/1251 ( 64%)]  Loss:  3.444141 (3.5100)  Time: 1.077s,  950.95/s  (1.089s,  940.05/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 850/1251 ( 68%)]  Loss:  3.588542 (3.5144)  Time: 1.078s,  950.31/s  (1.089s,  940.14/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 900/1251 ( 72%)]  Loss:  3.408154 (3.5088)  Time: 1.097s,  933.72/s  (1.089s,  940.15/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [ 950/1251 ( 76%)]  Loss:  3.734305 (3.5200)  Time: 1.072s,  955.20/s  (1.089s,  940.01/s)  LR: 6.231e-04  Data: 0.011 (0.013)
Train: 127 [1000/1251 ( 80%)]  Loss:  3.686183 (3.5280)  Time: 1.077s,  950.72/s  (1.089s,  940.37/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [1050/1251 ( 84%)]  Loss:  3.386940 (3.5215)  Time: 1.093s,  937.02/s  (1.089s,  940.51/s)  LR: 6.231e-04  Data: 0.013 (0.013)
Train: 127 [1100/1251 ( 88%)]  Loss:  3.728290 (3.5305)  Time: 1.077s,  950.99/s  (1.089s,  940.41/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [1150/1251 ( 92%)]  Loss:  3.467038 (3.5279)  Time: 1.093s,  936.51/s  (1.089s,  940.51/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [1200/1251 ( 96%)]  Loss:  3.482971 (3.5261)  Time: 1.095s,  935.18/s  (1.089s,  940.29/s)  LR: 6.231e-04  Data: 0.012 (0.013)
Train: 127 [1250/1251 (100%)]  Loss:  3.659177 (3.5312)  Time: 1.060s,  965.63/s  (1.089s,  939.93/s)  LR: 6.231e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.845 (5.845)  Loss:  0.5437 (0.5437)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6064 (1.0497)  Acc@1: 86.4387 (75.9980)  Acc@5: 97.4057 (93.4080)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-117.pth.tar', 75.58600005859375)

Train: 128 [   0/1251 (  0%)]  Loss:  3.471834 (3.4718)  Time: 1.091s,  938.66/s  (1.091s,  938.66/s)  LR: 6.180e-04  Data: 0.030 (0.030)
Train: 128 [  50/1251 (  4%)]  Loss:  3.531336 (3.5016)  Time: 1.095s,  934.76/s  (1.092s,  937.73/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 100/1251 (  8%)]  Loss:  3.442225 (3.4818)  Time: 1.096s,  934.60/s  (1.094s,  936.12/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 150/1251 ( 12%)]  Loss:  3.184943 (3.4076)  Time: 1.095s,  935.00/s  (1.095s,  935.54/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 200/1251 ( 16%)]  Loss:  3.612997 (3.4487)  Time: 1.082s,  946.41/s  (1.092s,  937.87/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 250/1251 ( 20%)]  Loss:  3.061279 (3.3841)  Time: 1.076s,  951.59/s  (1.090s,  939.31/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 300/1251 ( 24%)]  Loss:  3.370014 (3.3821)  Time: 1.094s,  935.61/s  (1.089s,  940.57/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 350/1251 ( 28%)]  Loss:  3.462612 (3.3922)  Time: 1.080s,  948.43/s  (1.089s,  939.96/s)  LR: 6.180e-04  Data: 0.013 (0.013)
Train: 128 [ 400/1251 ( 32%)]  Loss:  3.456134 (3.3993)  Time: 1.079s,  948.76/s  (1.089s,  940.05/s)  LR: 6.180e-04  Data: 0.013 (0.013)
Train: 128 [ 450/1251 ( 36%)]  Loss:  3.495531 (3.4089)  Time: 1.094s,  936.17/s  (1.089s,  939.91/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 500/1251 ( 40%)]  Loss:  3.524493 (3.4194)  Time: 1.096s,  934.14/s  (1.090s,  939.57/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [ 550/1251 ( 44%)]  Loss:  3.480247 (3.4245)  Time: 1.080s,  948.27/s  (1.090s,  939.46/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 600/1251 ( 48%)]  Loss:  3.750619 (3.4496)  Time: 1.080s,  948.02/s  (1.089s,  939.91/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 650/1251 ( 52%)]  Loss:  3.580258 (3.4589)  Time: 1.080s,  948.23/s  (1.090s,  939.66/s)  LR: 6.180e-04  Data: 0.016 (0.013)
Train: 128 [ 700/1251 ( 56%)]  Loss:  3.216068 (3.4427)  Time: 1.093s,  937.15/s  (1.090s,  939.63/s)  LR: 6.180e-04  Data: 0.013 (0.013)
Train: 128 [ 750/1251 ( 60%)]  Loss:  3.413490 (3.4409)  Time: 1.105s,  926.38/s  (1.089s,  939.95/s)  LR: 6.180e-04  Data: 0.011 (0.013)
Train: 128 [ 800/1251 ( 64%)]  Loss:  3.917338 (3.4689)  Time: 1.077s,  951.19/s  (1.090s,  939.74/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 850/1251 ( 68%)]  Loss:  3.537751 (3.4727)  Time: 1.092s,  937.83/s  (1.090s,  939.35/s)  LR: 6.180e-04  Data: 0.013 (0.013)
Train: 128 [ 900/1251 ( 72%)]  Loss:  3.468212 (3.4725)  Time: 1.107s,  925.29/s  (1.090s,  939.56/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [ 950/1251 ( 76%)]  Loss:  3.328959 (3.4653)  Time: 1.076s,  951.94/s  (1.090s,  939.82/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [1000/1251 ( 80%)]  Loss:  3.192350 (3.4523)  Time: 1.078s,  949.56/s  (1.089s,  940.09/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [1050/1251 ( 84%)]  Loss:  3.526999 (3.4557)  Time: 1.083s,  945.55/s  (1.090s,  939.77/s)  LR: 6.180e-04  Data: 0.017 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 128 [1100/1251 ( 88%)]  Loss:  3.636607 (3.4636)  Time: 1.088s,  941.10/s  (1.090s,  939.78/s)  LR: 6.180e-04  Data: 0.013 (0.013)
Train: 128 [1150/1251 ( 92%)]  Loss:  3.367458 (3.4596)  Time: 1.078s,  949.98/s  (1.089s,  940.02/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [1200/1251 ( 96%)]  Loss:  3.691919 (3.4689)  Time: 1.081s,  947.62/s  (1.089s,  940.02/s)  LR: 6.180e-04  Data: 0.012 (0.013)
Train: 128 [1250/1251 (100%)]  Loss:  3.297634 (3.4623)  Time: 1.061s,  965.29/s  (1.089s,  940.11/s)  LR: 6.180e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.855 (5.855)  Loss:  0.4894 (0.4894)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6014 (1.0378)  Acc@1: 86.6745 (75.7260)  Acc@5: 96.8160 (93.3280)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 75.72600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-122.pth.tar', 75.66600005859375)

Train: 129 [   0/1251 (  0%)]  Loss:  3.376252 (3.3763)  Time: 1.087s,  942.34/s  (1.087s,  942.34/s)  LR: 6.130e-04  Data: 0.026 (0.026)
Train: 129 [  50/1251 (  4%)]  Loss:  3.443019 (3.4096)  Time: 1.092s,  937.47/s  (1.085s,  943.80/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 100/1251 (  8%)]  Loss:  3.240587 (3.3533)  Time: 1.173s,  873.24/s  (1.086s,  942.69/s)  LR: 6.130e-04  Data: 0.014 (0.013)
Train: 129 [ 150/1251 ( 12%)]  Loss:  3.521039 (3.3952)  Time: 1.094s,  935.84/s  (1.088s,  940.98/s)  LR: 6.130e-04  Data: 0.013 (0.013)
Train: 129 [ 200/1251 ( 16%)]  Loss:  3.498238 (3.4158)  Time: 1.097s,  933.74/s  (1.089s,  940.25/s)  LR: 6.130e-04  Data: 0.014 (0.013)
Train: 129 [ 250/1251 ( 20%)]  Loss:  3.272695 (3.3920)  Time: 1.170s,  874.98/s  (1.089s,  939.93/s)  LR: 6.130e-04  Data: 0.015 (0.013)
Train: 129 [ 300/1251 ( 24%)]  Loss:  3.596838 (3.4212)  Time: 1.103s,  928.32/s  (1.089s,  939.90/s)  LR: 6.130e-04  Data: 0.014 (0.013)
Train: 129 [ 350/1251 ( 28%)]  Loss:  3.344000 (3.4116)  Time: 1.084s,  944.52/s  (1.089s,  940.03/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 400/1251 ( 32%)]  Loss:  3.575713 (3.4298)  Time: 1.094s,  935.74/s  (1.089s,  940.23/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [ 450/1251 ( 36%)]  Loss:  3.768619 (3.4637)  Time: 1.197s,  855.53/s  (1.089s,  940.05/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 500/1251 ( 40%)]  Loss:  3.386405 (3.4567)  Time: 1.080s,  947.86/s  (1.089s,  939.92/s)  LR: 6.130e-04  Data: 0.014 (0.013)
Train: 129 [ 550/1251 ( 44%)]  Loss:  3.448782 (3.4560)  Time: 1.080s,  948.53/s  (1.090s,  939.69/s)  LR: 6.130e-04  Data: 0.018 (0.013)
Train: 129 [ 600/1251 ( 48%)]  Loss:  3.415986 (3.4529)  Time: 1.075s,  952.35/s  (1.090s,  939.59/s)  LR: 6.130e-04  Data: 0.013 (0.013)
Train: 129 [ 650/1251 ( 52%)]  Loss:  3.711980 (3.4714)  Time: 1.078s,  949.77/s  (1.089s,  940.01/s)  LR: 6.130e-04  Data: 0.013 (0.013)
Train: 129 [ 700/1251 ( 56%)]  Loss:  3.152182 (3.4502)  Time: 1.079s,  949.00/s  (1.089s,  940.26/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 750/1251 ( 60%)]  Loss:  3.641582 (3.4621)  Time: 1.078s,  949.51/s  (1.089s,  940.49/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 800/1251 ( 64%)]  Loss:  3.321672 (3.4539)  Time: 1.083s,  945.94/s  (1.089s,  940.61/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [ 850/1251 ( 68%)]  Loss:  3.405214 (3.4512)  Time: 1.077s,  951.22/s  (1.089s,  940.44/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 900/1251 ( 72%)]  Loss:  3.521213 (3.4548)  Time: 1.105s,  926.29/s  (1.089s,  940.52/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [ 950/1251 ( 76%)]  Loss:  3.842228 (3.4742)  Time: 1.083s,  945.32/s  (1.089s,  940.36/s)  LR: 6.130e-04  Data: 0.015 (0.013)
Train: 129 [1000/1251 ( 80%)]  Loss:  3.225881 (3.4624)  Time: 1.095s,  934.83/s  (1.089s,  940.09/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [1050/1251 ( 84%)]  Loss:  3.457723 (3.4622)  Time: 1.094s,  936.25/s  (1.089s,  939.95/s)  LR: 6.130e-04  Data: 0.011 (0.013)
Train: 129 [1100/1251 ( 88%)]  Loss:  3.367774 (3.4581)  Time: 1.079s,  949.09/s  (1.089s,  939.93/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [1150/1251 ( 92%)]  Loss:  3.621726 (3.4649)  Time: 1.092s,  937.82/s  (1.089s,  939.99/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [1200/1251 ( 96%)]  Loss:  3.283592 (3.4576)  Time: 1.079s,  948.73/s  (1.089s,  940.25/s)  LR: 6.130e-04  Data: 0.012 (0.013)
Train: 129 [1250/1251 (100%)]  Loss:  3.509048 (3.4596)  Time: 1.060s,  966.01/s  (1.089s,  940.44/s)  LR: 6.130e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.775 (5.775)  Loss:  0.5684 (0.5684)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6428 (1.0483)  Acc@1: 85.4953 (75.8720)  Acc@5: 97.5236 (93.2760)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 75.72600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-119.pth.tar', 75.68400010986328)

Train: 130 [   0/1251 (  0%)]  Loss:  3.510840 (3.5108)  Time: 1.088s,  941.07/s  (1.088s,  941.07/s)  LR: 6.079e-04  Data: 0.028 (0.028)
Train: 130 [  50/1251 (  4%)]  Loss:  3.471700 (3.4913)  Time: 1.106s,  925.86/s  (1.086s,  942.83/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 100/1251 (  8%)]  Loss:  3.704240 (3.5623)  Time: 1.078s,  950.00/s  (1.088s,  941.41/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 150/1251 ( 12%)]  Loss:  3.651261 (3.5845)  Time: 1.079s,  949.18/s  (1.086s,  942.79/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 200/1251 ( 16%)]  Loss:  3.542769 (3.5762)  Time: 1.079s,  949.30/s  (1.088s,  941.14/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 250/1251 ( 20%)]  Loss:  3.658019 (3.5898)  Time: 1.082s,  946.12/s  (1.088s,  941.11/s)  LR: 6.079e-04  Data: 0.011 (0.013)
Train: 130 [ 300/1251 ( 24%)]  Loss:  3.374058 (3.5590)  Time: 1.077s,  950.55/s  (1.088s,  940.88/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 350/1251 ( 28%)]  Loss:  3.709971 (3.5779)  Time: 1.077s,  950.65/s  (1.088s,  941.07/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 400/1251 ( 32%)]  Loss:  3.341548 (3.5516)  Time: 1.094s,  935.97/s  (1.089s,  940.74/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 450/1251 ( 36%)]  Loss:  3.743785 (3.5708)  Time: 1.077s,  951.19/s  (1.089s,  940.38/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 500/1251 ( 40%)]  Loss:  3.347122 (3.5505)  Time: 1.077s,  950.35/s  (1.089s,  940.45/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 550/1251 ( 44%)]  Loss:  3.440292 (3.5413)  Time: 1.078s,  949.91/s  (1.088s,  940.85/s)  LR: 6.079e-04  Data: 0.015 (0.013)
Train: 130 [ 600/1251 ( 48%)]  Loss:  3.626532 (3.5479)  Time: 1.077s,  950.60/s  (1.088s,  940.92/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 650/1251 ( 52%)]  Loss:  3.749281 (3.5622)  Time: 1.084s,  944.76/s  (1.088s,  941.18/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 700/1251 ( 56%)]  Loss:  3.267658 (3.5426)  Time: 1.082s,  946.49/s  (1.088s,  941.31/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 750/1251 ( 60%)]  Loss:  3.574651 (3.5446)  Time: 1.097s,  933.76/s  (1.088s,  941.29/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 800/1251 ( 64%)]  Loss:  3.637331 (3.5501)  Time: 1.103s,  928.57/s  (1.088s,  940.80/s)  LR: 6.079e-04  Data: 0.013 (0.013)
Train: 130 [ 850/1251 ( 68%)]  Loss:  3.429436 (3.5434)  Time: 1.177s,  870.26/s  (1.088s,  940.84/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 900/1251 ( 72%)]  Loss:  3.561365 (3.5443)  Time: 1.085s,  943.46/s  (1.088s,  941.01/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [ 950/1251 ( 76%)]  Loss:  3.296217 (3.5319)  Time: 1.095s,  935.11/s  (1.089s,  940.45/s)  LR: 6.079e-04  Data: 0.015 (0.013)
Train: 130 [1000/1251 ( 80%)]  Loss:  3.080090 (3.5104)  Time: 1.076s,  952.04/s  (1.089s,  940.13/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [1050/1251 ( 84%)]  Loss:  3.605696 (3.5147)  Time: 1.076s,  951.80/s  (1.089s,  940.05/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [1100/1251 ( 88%)]  Loss:  3.523299 (3.5151)  Time: 1.079s,  949.09/s  (1.089s,  939.91/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Train: 130 [1150/1251 ( 92%)]  Loss:  3.556244 (3.5168)  Time: 1.077s,  951.12/s  (1.089s,  940.00/s)  LR: 6.079e-04  Data: 0.015 (0.013)
Train: 130 [1200/1251 ( 96%)]  Loss:  3.011756 (3.4966)  Time: 1.098s,  932.56/s  (1.089s,  940.04/s)  LR: 6.079e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 130 [1250/1251 (100%)]  Loss:  3.549483 (3.4986)  Time: 1.103s,  928.55/s  (1.090s,  939.70/s)  LR: 6.079e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.791 (5.791)  Loss:  0.5484 (0.5484)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.5672 (1.0320)  Acc@1: 87.1462 (76.0380)  Acc@5: 97.7594 (93.4100)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 75.72600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-123.pth.tar', 75.7239999584961)

Train: 131 [   0/1251 (  0%)]  Loss:  3.633066 (3.6331)  Time: 1.090s,  939.41/s  (1.090s,  939.41/s)  LR: 6.028e-04  Data: 0.029 (0.029)
Train: 131 [  50/1251 (  4%)]  Loss:  3.193922 (3.4135)  Time: 1.098s,  932.24/s  (1.093s,  936.60/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 100/1251 (  8%)]  Loss:  3.478956 (3.4353)  Time: 1.094s,  936.14/s  (1.095s,  935.41/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 150/1251 ( 12%)]  Loss:  3.241518 (3.3869)  Time: 1.095s,  935.15/s  (1.092s,  937.61/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 200/1251 ( 16%)]  Loss:  3.135811 (3.3367)  Time: 1.077s,  950.58/s  (1.091s,  938.89/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [ 250/1251 ( 20%)]  Loss:  3.170379 (3.3089)  Time: 1.095s,  935.57/s  (1.091s,  938.64/s)  LR: 6.028e-04  Data: 0.014 (0.013)
Train: 131 [ 300/1251 ( 24%)]  Loss:  3.386429 (3.3200)  Time: 1.095s,  934.76/s  (1.090s,  939.41/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 350/1251 ( 28%)]  Loss:  3.344970 (3.3231)  Time: 1.105s,  926.74/s  (1.090s,  939.20/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 400/1251 ( 32%)]  Loss:  3.599530 (3.3538)  Time: 1.137s,  900.60/s  (1.091s,  938.56/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 450/1251 ( 36%)]  Loss:  3.648566 (3.3833)  Time: 1.076s,  951.34/s  (1.090s,  939.11/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 500/1251 ( 40%)]  Loss:  3.480111 (3.3921)  Time: 1.079s,  949.04/s  (1.090s,  939.68/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 550/1251 ( 44%)]  Loss:  3.377712 (3.3909)  Time: 1.095s,  935.20/s  (1.090s,  939.78/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 600/1251 ( 48%)]  Loss:  3.476308 (3.3975)  Time: 1.097s,  933.36/s  (1.089s,  939.89/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 650/1251 ( 52%)]  Loss:  3.811187 (3.4270)  Time: 1.078s,  950.07/s  (1.090s,  939.48/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 700/1251 ( 56%)]  Loss:  3.366109 (3.4230)  Time: 1.078s,  950.28/s  (1.089s,  939.95/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 750/1251 ( 60%)]  Loss:  3.427615 (3.4233)  Time: 1.078s,  950.07/s  (1.089s,  940.19/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [ 800/1251 ( 64%)]  Loss:  3.612106 (3.4344)  Time: 1.110s,  922.56/s  (1.089s,  939.94/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 850/1251 ( 68%)]  Loss:  3.687037 (3.4484)  Time: 1.077s,  950.54/s  (1.089s,  939.98/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 900/1251 ( 72%)]  Loss:  3.119373 (3.4311)  Time: 1.075s,  952.57/s  (1.089s,  940.13/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [ 950/1251 ( 76%)]  Loss:  3.226583 (3.4209)  Time: 1.077s,  950.71/s  (1.089s,  940.19/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [1000/1251 ( 80%)]  Loss:  3.510133 (3.4251)  Time: 1.077s,  951.23/s  (1.089s,  940.24/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [1050/1251 ( 84%)]  Loss:  3.288793 (3.4189)  Time: 1.092s,  937.34/s  (1.089s,  940.01/s)  LR: 6.028e-04  Data: 0.011 (0.013)
Train: 131 [1100/1251 ( 88%)]  Loss:  3.634156 (3.4283)  Time: 1.075s,  952.48/s  (1.089s,  940.17/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [1150/1251 ( 92%)]  Loss:  3.502370 (3.4314)  Time: 1.169s,  876.16/s  (1.089s,  940.00/s)  LR: 6.028e-04  Data: 0.013 (0.013)
Train: 131 [1200/1251 ( 96%)]  Loss:  3.527148 (3.4352)  Time: 1.075s,  952.66/s  (1.089s,  940.06/s)  LR: 6.028e-04  Data: 0.012 (0.013)
Train: 131 [1250/1251 (100%)]  Loss:  3.297285 (3.4299)  Time: 1.063s,  963.75/s  (1.089s,  940.19/s)  LR: 6.028e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.897 (5.897)  Loss:  0.5105 (0.5105)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6433 (1.0211)  Acc@1: 86.3208 (76.3600)  Acc@5: 96.5802 (93.5380)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-128.pth.tar', 75.72600002929687)

Train: 132 [   0/1251 (  0%)]  Loss:  3.380272 (3.3803)  Time: 1.086s,  942.90/s  (1.086s,  942.90/s)  LR: 5.978e-04  Data: 0.026 (0.026)
Train: 132 [  50/1251 (  4%)]  Loss:  3.524110 (3.4522)  Time: 1.077s,  950.99/s  (1.088s,  940.89/s)  LR: 5.978e-04  Data: 0.012 (0.013)
Train: 132 [ 100/1251 (  8%)]  Loss:  3.373226 (3.4259)  Time: 1.074s,  953.30/s  (1.091s,  938.76/s)  LR: 5.978e-04  Data: 0.013 (0.012)
Train: 132 [ 150/1251 ( 12%)]  Loss:  3.284402 (3.3905)  Time: 1.093s,  936.78/s  (1.091s,  938.99/s)  LR: 5.978e-04  Data: 0.011 (0.012)
Train: 132 [ 200/1251 ( 16%)]  Loss:  3.736954 (3.4598)  Time: 1.096s,  934.26/s  (1.091s,  938.75/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 250/1251 ( 20%)]  Loss:  3.142624 (3.4069)  Time: 1.097s,  933.59/s  (1.091s,  938.18/s)  LR: 5.978e-04  Data: 0.011 (0.012)
Train: 132 [ 300/1251 ( 24%)]  Loss:  3.333721 (3.3965)  Time: 1.093s,  936.53/s  (1.092s,  937.76/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 350/1251 ( 28%)]  Loss:  3.408880 (3.3980)  Time: 1.094s,  935.80/s  (1.091s,  938.54/s)  LR: 5.978e-04  Data: 0.011 (0.012)
Train: 132 [ 400/1251 ( 32%)]  Loss:  3.660358 (3.4272)  Time: 1.094s,  935.91/s  (1.091s,  938.69/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 450/1251 ( 36%)]  Loss:  3.551697 (3.4396)  Time: 1.096s,  934.73/s  (1.090s,  939.06/s)  LR: 5.978e-04  Data: 0.013 (0.012)
Train: 132 [ 500/1251 ( 40%)]  Loss:  3.533248 (3.4481)  Time: 1.092s,  937.30/s  (1.090s,  939.40/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 550/1251 ( 44%)]  Loss:  3.660838 (3.4659)  Time: 1.095s,  934.79/s  (1.090s,  939.22/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 600/1251 ( 48%)]  Loss:  3.468142 (3.4660)  Time: 1.106s,  925.63/s  (1.090s,  939.02/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 650/1251 ( 52%)]  Loss:  3.436047 (3.4639)  Time: 1.075s,  952.39/s  (1.091s,  938.91/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 700/1251 ( 56%)]  Loss:  3.239708 (3.4489)  Time: 1.075s,  952.41/s  (1.090s,  939.32/s)  LR: 5.978e-04  Data: 0.011 (0.012)
Train: 132 [ 750/1251 ( 60%)]  Loss:  3.594167 (3.4580)  Time: 1.096s,  934.04/s  (1.090s,  939.56/s)  LR: 5.978e-04  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 132 [ 800/1251 ( 64%)]  Loss:  3.397707 (3.4545)  Time: 1.082s,  946.57/s  (1.090s,  939.26/s)  LR: 5.978e-04  Data: 0.016 (0.012)
Train: 132 [ 850/1251 ( 68%)]  Loss:  3.434488 (3.4534)  Time: 1.097s,  933.73/s  (1.090s,  939.43/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [ 900/1251 ( 72%)]  Loss:  3.664480 (3.4645)  Time: 1.088s,  940.95/s  (1.090s,  939.48/s)  LR: 5.978e-04  Data: 0.014 (0.012)
Train: 132 [ 950/1251 ( 76%)]  Loss:  3.378755 (3.4602)  Time: 1.077s,  950.87/s  (1.090s,  939.69/s)  LR: 5.978e-04  Data: 0.012 (0.013)
Train: 132 [1000/1251 ( 80%)]  Loss:  3.551070 (3.4645)  Time: 1.077s,  950.65/s  (1.090s,  939.70/s)  LR: 5.978e-04  Data: 0.012 (0.013)
Train: 132 [1050/1251 ( 84%)]  Loss:  3.744441 (3.4772)  Time: 1.093s,  936.52/s  (1.090s,  939.75/s)  LR: 5.978e-04  Data: 0.011 (0.013)
Train: 132 [1100/1251 ( 88%)]  Loss:  3.273125 (3.4684)  Time: 1.099s,  932.01/s  (1.089s,  940.01/s)  LR: 5.978e-04  Data: 0.012 (0.013)
Train: 132 [1150/1251 ( 92%)]  Loss:  3.703638 (3.4782)  Time: 1.082s,  946.69/s  (1.089s,  940.04/s)  LR: 5.978e-04  Data: 0.012 (0.012)
Train: 132 [1200/1251 ( 96%)]  Loss:  3.182602 (3.4663)  Time: 1.077s,  951.04/s  (1.089s,  940.37/s)  LR: 5.978e-04  Data: 0.013 (0.013)
Train: 132 [1250/1251 (100%)]  Loss:  3.111312 (3.4527)  Time: 1.070s,  956.97/s  (1.089s,  940.38/s)  LR: 5.978e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.898 (5.898)  Loss:  0.4952 (0.4952)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.6173 (1.0298)  Acc@1: 87.1462 (76.1940)  Acc@5: 97.1698 (93.4160)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-125.pth.tar', 75.73199995849609)

Train: 133 [   0/1251 (  0%)]  Loss:  3.707160 (3.7072)  Time: 1.087s,  942.17/s  (1.087s,  942.17/s)  LR: 5.927e-04  Data: 0.026 (0.026)
Train: 133 [  50/1251 (  4%)]  Loss:  3.621234 (3.6642)  Time: 1.096s,  934.43/s  (1.095s,  935.50/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 100/1251 (  8%)]  Loss:  3.454444 (3.5943)  Time: 1.079s,  949.14/s  (1.092s,  937.97/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 150/1251 ( 12%)]  Loss:  3.168967 (3.4880)  Time: 1.086s,  942.63/s  (1.092s,  937.87/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 200/1251 ( 16%)]  Loss:  3.429792 (3.4763)  Time: 1.092s,  937.35/s  (1.092s,  937.51/s)  LR: 5.927e-04  Data: 0.013 (0.013)
Train: 133 [ 250/1251 ( 20%)]  Loss:  3.519119 (3.4835)  Time: 1.076s,  951.69/s  (1.091s,  938.57/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 300/1251 ( 24%)]  Loss:  3.584122 (3.4978)  Time: 1.077s,  950.80/s  (1.090s,  939.15/s)  LR: 5.927e-04  Data: 0.015 (0.013)
Train: 133 [ 350/1251 ( 28%)]  Loss:  3.172381 (3.4572)  Time: 1.093s,  936.53/s  (1.091s,  938.79/s)  LR: 5.927e-04  Data: 0.015 (0.013)
Train: 133 [ 400/1251 ( 32%)]  Loss:  3.200039 (3.4286)  Time: 1.083s,  945.61/s  (1.090s,  939.14/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 450/1251 ( 36%)]  Loss:  3.448466 (3.4306)  Time: 1.096s,  934.17/s  (1.090s,  939.06/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 500/1251 ( 40%)]  Loss:  3.209891 (3.4105)  Time: 1.097s,  933.70/s  (1.090s,  939.11/s)  LR: 5.927e-04  Data: 0.011 (0.013)
Train: 133 [ 550/1251 ( 44%)]  Loss:  3.449036 (3.4137)  Time: 1.076s,  952.02/s  (1.090s,  939.54/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 600/1251 ( 48%)]  Loss:  3.549697 (3.4242)  Time: 1.097s,  933.41/s  (1.091s,  939.02/s)  LR: 5.927e-04  Data: 0.013 (0.013)
Train: 133 [ 650/1251 ( 52%)]  Loss:  3.242653 (3.4112)  Time: 1.081s,  947.39/s  (1.090s,  939.23/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 700/1251 ( 56%)]  Loss:  3.274344 (3.4021)  Time: 1.077s,  950.43/s  (1.090s,  939.65/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 750/1251 ( 60%)]  Loss:  3.371372 (3.4002)  Time: 1.106s,  926.26/s  (1.090s,  939.29/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 800/1251 ( 64%)]  Loss:  3.569736 (3.4101)  Time: 1.079s,  948.80/s  (1.090s,  939.10/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 850/1251 ( 68%)]  Loss:  3.716195 (3.4271)  Time: 1.077s,  950.65/s  (1.090s,  939.18/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 900/1251 ( 72%)]  Loss:  3.540984 (3.4331)  Time: 1.082s,  946.68/s  (1.091s,  938.96/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [ 950/1251 ( 76%)]  Loss:  3.338187 (3.4284)  Time: 1.094s,  935.63/s  (1.091s,  938.70/s)  LR: 5.927e-04  Data: 0.015 (0.013)
Train: 133 [1000/1251 ( 80%)]  Loss:  3.480949 (3.4309)  Time: 1.077s,  950.62/s  (1.091s,  938.83/s)  LR: 5.927e-04  Data: 0.017 (0.013)
Train: 133 [1050/1251 ( 84%)]  Loss:  3.663850 (3.4415)  Time: 1.078s,  949.54/s  (1.091s,  938.98/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [1100/1251 ( 88%)]  Loss:  3.283692 (3.4346)  Time: 1.080s,  948.56/s  (1.090s,  939.25/s)  LR: 5.927e-04  Data: 0.018 (0.013)
Train: 133 [1150/1251 ( 92%)]  Loss:  3.759453 (3.4482)  Time: 1.082s,  946.37/s  (1.090s,  939.28/s)  LR: 5.927e-04  Data: 0.012 (0.013)
Train: 133 [1200/1251 ( 96%)]  Loss:  3.406926 (3.4465)  Time: 1.079s,  949.46/s  (1.090s,  939.47/s)  LR: 5.927e-04  Data: 0.014 (0.013)
Train: 133 [1250/1251 (100%)]  Loss:  3.900370 (3.4640)  Time: 1.067s,  959.89/s  (1.090s,  939.17/s)  LR: 5.927e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.875 (5.875)  Loss:  0.5576 (0.5576)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6335 (1.0366)  Acc@1: 85.7311 (76.1440)  Acc@5: 96.4623 (93.4720)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-120.pth.tar', 75.74599998535156)

Train: 134 [   0/1251 (  0%)]  Loss:  3.149882 (3.1499)  Time: 1.088s,  941.07/s  (1.088s,  941.07/s)  LR: 5.876e-04  Data: 0.028 (0.028)
Train: 134 [  50/1251 (  4%)]  Loss:  3.386649 (3.2683)  Time: 1.076s,  951.63/s  (1.091s,  938.91/s)  LR: 5.876e-04  Data: 0.015 (0.013)
Train: 134 [ 100/1251 (  8%)]  Loss:  3.489036 (3.3419)  Time: 1.076s,  952.08/s  (1.090s,  939.25/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 150/1251 ( 12%)]  Loss:  3.519540 (3.3863)  Time: 1.096s,  934.49/s  (1.091s,  938.61/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 200/1251 ( 16%)]  Loss:  3.254313 (3.3599)  Time: 1.099s,  931.67/s  (1.089s,  940.08/s)  LR: 5.876e-04  Data: 0.014 (0.013)
Train: 134 [ 250/1251 ( 20%)]  Loss:  3.503520 (3.3838)  Time: 1.078s,  949.80/s  (1.089s,  940.07/s)  LR: 5.876e-04  Data: 0.013 (0.013)
Train: 134 [ 300/1251 ( 24%)]  Loss:  3.424750 (3.3897)  Time: 1.081s,  947.12/s  (1.089s,  940.67/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 350/1251 ( 28%)]  Loss:  3.497763 (3.4032)  Time: 1.093s,  936.83/s  (1.088s,  940.75/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 400/1251 ( 32%)]  Loss:  3.567554 (3.4214)  Time: 1.076s,  951.43/s  (1.089s,  940.66/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 450/1251 ( 36%)]  Loss:  3.712295 (3.4505)  Time: 1.076s,  951.94/s  (1.089s,  940.52/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 500/1251 ( 40%)]  Loss:  2.862037 (3.3970)  Time: 1.073s,  954.36/s  (1.088s,  940.93/s)  LR: 5.876e-04  Data: 0.014 (0.013)
Train: 134 [ 550/1251 ( 44%)]  Loss:  3.005147 (3.3644)  Time: 1.070s,  956.57/s  (1.088s,  941.03/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 600/1251 ( 48%)]  Loss:  3.697353 (3.3900)  Time: 1.077s,  951.11/s  (1.088s,  941.34/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 650/1251 ( 52%)]  Loss:  3.234851 (3.3789)  Time: 1.078s,  950.32/s  (1.088s,  941.17/s)  LR: 5.876e-04  Data: 0.013 (0.013)
Train: 134 [ 700/1251 ( 56%)]  Loss:  3.558968 (3.3909)  Time: 1.095s,  935.56/s  (1.088s,  940.96/s)  LR: 5.876e-04  Data: 0.015 (0.013)
Train: 134 [ 750/1251 ( 60%)]  Loss:  3.266668 (3.3831)  Time: 1.097s,  933.66/s  (1.089s,  940.65/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 800/1251 ( 64%)]  Loss:  3.343942 (3.3808)  Time: 1.075s,  952.20/s  (1.089s,  940.66/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [ 850/1251 ( 68%)]  Loss:  3.583020 (3.3921)  Time: 1.080s,  948.05/s  (1.088s,  940.76/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 900/1251 ( 72%)]  Loss:  3.457279 (3.3955)  Time: 1.095s,  934.77/s  (1.089s,  940.56/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [ 950/1251 ( 76%)]  Loss:  3.765395 (3.4140)  Time: 1.108s,  924.30/s  (1.089s,  940.15/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 134 [1000/1251 ( 80%)]  Loss:  3.800414 (3.4324)  Time: 1.079s,  949.20/s  (1.089s,  940.05/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1050/1251 ( 84%)]  Loss:  3.466464 (3.4339)  Time: 1.093s,  937.07/s  (1.089s,  940.19/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [1100/1251 ( 88%)]  Loss:  3.262807 (3.4265)  Time: 1.102s,  929.01/s  (1.089s,  940.15/s)  LR: 5.876e-04  Data: 0.013 (0.013)
Train: 134 [1150/1251 ( 92%)]  Loss:  3.339151 (3.4229)  Time: 1.078s,  949.99/s  (1.089s,  940.34/s)  LR: 5.876e-04  Data: 0.012 (0.013)
Train: 134 [1200/1251 ( 96%)]  Loss:  3.268557 (3.4167)  Time: 1.097s,  933.09/s  (1.089s,  940.20/s)  LR: 5.876e-04  Data: 0.011 (0.013)
Train: 134 [1250/1251 (100%)]  Loss:  3.253820 (3.4104)  Time: 1.078s,  950.32/s  (1.089s,  939.91/s)  LR: 5.876e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.975 (5.975)  Loss:  0.5037 (0.5037)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.5843 (1.0208)  Acc@1: 86.0849 (76.1300)  Acc@5: 97.0519 (93.4360)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-118.pth.tar', 75.7980000341797)

Train: 135 [   0/1251 (  0%)]  Loss:  3.413240 (3.4132)  Time: 1.099s,  932.07/s  (1.099s,  932.07/s)  LR: 5.824e-04  Data: 0.035 (0.035)
Train: 135 [  50/1251 (  4%)]  Loss:  3.287264 (3.3503)  Time: 1.091s,  938.79/s  (1.089s,  940.55/s)  LR: 5.824e-04  Data: 0.013 (0.013)
Train: 135 [ 100/1251 (  8%)]  Loss:  3.523940 (3.4081)  Time: 1.075s,  952.45/s  (1.091s,  938.94/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 150/1251 ( 12%)]  Loss:  3.555452 (3.4450)  Time: 1.082s,  945.98/s  (1.089s,  940.01/s)  LR: 5.824e-04  Data: 0.014 (0.013)
Train: 135 [ 200/1251 ( 16%)]  Loss:  3.360368 (3.4281)  Time: 1.072s,  955.50/s  (1.090s,  939.77/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [ 250/1251 ( 20%)]  Loss:  3.644638 (3.4642)  Time: 1.097s,  933.58/s  (1.089s,  940.00/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 300/1251 ( 24%)]  Loss:  3.459587 (3.4635)  Time: 1.099s,  932.02/s  (1.089s,  940.14/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 350/1251 ( 28%)]  Loss:  3.718165 (3.4953)  Time: 1.081s,  947.02/s  (1.090s,  939.87/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 400/1251 ( 32%)]  Loss:  3.047017 (3.4455)  Time: 1.082s,  946.06/s  (1.089s,  940.43/s)  LR: 5.824e-04  Data: 0.011 (0.013)
Train: 135 [ 450/1251 ( 36%)]  Loss:  3.602895 (3.4613)  Time: 1.097s,  933.25/s  (1.089s,  940.17/s)  LR: 5.824e-04  Data: 0.014 (0.013)
Train: 135 [ 500/1251 ( 40%)]  Loss:  3.402730 (3.4559)  Time: 1.085s,  943.78/s  (1.089s,  940.71/s)  LR: 5.824e-04  Data: 0.014 (0.013)
Train: 135 [ 550/1251 ( 44%)]  Loss:  3.418745 (3.4528)  Time: 1.094s,  935.97/s  (1.088s,  941.12/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 600/1251 ( 48%)]  Loss:  3.247845 (3.4371)  Time: 1.093s,  937.07/s  (1.088s,  941.23/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 650/1251 ( 52%)]  Loss:  3.486734 (3.4406)  Time: 1.094s,  935.70/s  (1.089s,  940.54/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 700/1251 ( 56%)]  Loss:  3.510794 (3.4453)  Time: 1.076s,  951.43/s  (1.089s,  940.48/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 750/1251 ( 60%)]  Loss:  3.597679 (3.4548)  Time: 1.080s,  948.05/s  (1.089s,  940.61/s)  LR: 5.824e-04  Data: 0.015 (0.013)
Train: 135 [ 800/1251 ( 64%)]  Loss:  3.433439 (3.4536)  Time: 1.096s,  934.26/s  (1.089s,  940.39/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 850/1251 ( 68%)]  Loss:  3.184132 (3.4386)  Time: 1.096s,  933.99/s  (1.089s,  940.39/s)  LR: 5.824e-04  Data: 0.014 (0.013)
Train: 135 [ 900/1251 ( 72%)]  Loss:  3.279299 (3.4302)  Time: 1.093s,  936.87/s  (1.090s,  939.77/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [ 950/1251 ( 76%)]  Loss:  3.516118 (3.4345)  Time: 1.094s,  936.34/s  (1.090s,  939.49/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [1000/1251 ( 80%)]  Loss:  3.468552 (3.4361)  Time: 1.083s,  945.24/s  (1.090s,  939.50/s)  LR: 5.824e-04  Data: 0.013 (0.013)
Train: 135 [1050/1251 ( 84%)]  Loss:  3.266289 (3.4284)  Time: 1.096s,  933.99/s  (1.090s,  939.34/s)  LR: 5.824e-04  Data: 0.013 (0.013)
Train: 135 [1100/1251 ( 88%)]  Loss:  3.305979 (3.4231)  Time: 1.081s,  947.45/s  (1.090s,  939.51/s)  LR: 5.824e-04  Data: 0.015 (0.013)
Train: 135 [1150/1251 ( 92%)]  Loss:  3.939028 (3.4446)  Time: 1.097s,  933.20/s  (1.090s,  939.31/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [1200/1251 ( 96%)]  Loss:  3.398040 (3.4427)  Time: 1.076s,  951.80/s  (1.090s,  939.64/s)  LR: 5.824e-04  Data: 0.012 (0.013)
Train: 135 [1250/1251 (100%)]  Loss:  3.822934 (3.4573)  Time: 1.081s,  947.20/s  (1.090s,  939.52/s)  LR: 5.824e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.825 (5.825)  Loss:  0.5039 (0.5039)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6149 (1.0010)  Acc@1: 86.2028 (76.3540)  Acc@5: 96.5802 (93.4520)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-126.pth.tar', 75.81000016113282)

Train: 136 [   0/1251 (  0%)]  Loss:  3.304082 (3.3041)  Time: 1.089s,  939.94/s  (1.089s,  939.94/s)  LR: 5.773e-04  Data: 0.029 (0.029)
Train: 136 [  50/1251 (  4%)]  Loss:  3.611360 (3.4577)  Time: 1.103s,  928.13/s  (1.086s,  943.07/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 100/1251 (  8%)]  Loss:  3.678655 (3.5314)  Time: 1.095s,  935.58/s  (1.088s,  941.42/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 150/1251 ( 12%)]  Loss:  3.713665 (3.5769)  Time: 1.084s,  944.84/s  (1.088s,  941.01/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 200/1251 ( 16%)]  Loss:  2.974498 (3.4565)  Time: 1.077s,  951.18/s  (1.087s,  941.61/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 250/1251 ( 20%)]  Loss:  3.377334 (3.4433)  Time: 1.078s,  950.23/s  (1.088s,  941.43/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 300/1251 ( 24%)]  Loss:  3.851443 (3.5016)  Time: 1.078s,  949.87/s  (1.088s,  941.36/s)  LR: 5.773e-04  Data: 0.014 (0.013)
Train: 136 [ 350/1251 ( 28%)]  Loss:  3.173955 (3.4606)  Time: 1.096s,  934.08/s  (1.089s,  940.27/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [ 400/1251 ( 32%)]  Loss:  3.530191 (3.4684)  Time: 1.077s,  951.11/s  (1.089s,  939.97/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 450/1251 ( 36%)]  Loss:  3.370679 (3.4586)  Time: 1.079s,  949.14/s  (1.089s,  940.22/s)  LR: 5.773e-04  Data: 0.014 (0.013)
Train: 136 [ 500/1251 ( 40%)]  Loss:  3.373800 (3.4509)  Time: 1.096s,  934.02/s  (1.089s,  940.14/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 550/1251 ( 44%)]  Loss:  3.259411 (3.4349)  Time: 1.092s,  937.41/s  (1.090s,  939.68/s)  LR: 5.773e-04  Data: 0.013 (0.013)
Train: 136 [ 600/1251 ( 48%)]  Loss:  3.699598 (3.4553)  Time: 1.094s,  936.00/s  (1.090s,  939.58/s)  LR: 5.773e-04  Data: 0.015 (0.013)
Train: 136 [ 650/1251 ( 52%)]  Loss:  3.126878 (3.4318)  Time: 1.079s,  949.26/s  (1.090s,  939.64/s)  LR: 5.773e-04  Data: 0.013 (0.013)
Train: 136 [ 700/1251 ( 56%)]  Loss:  3.805009 (3.4567)  Time: 1.084s,  944.69/s  (1.089s,  939.94/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 136 [ 750/1251 ( 60%)]  Loss:  3.599235 (3.4656)  Time: 1.171s,  874.46/s  (1.089s,  940.28/s)  LR: 5.773e-04  Data: 0.013 (0.013)
Train: 136 [ 800/1251 ( 64%)]  Loss:  3.184461 (3.4491)  Time: 1.097s,  933.12/s  (1.089s,  940.51/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 850/1251 ( 68%)]  Loss:  3.450637 (3.4492)  Time: 1.076s,  951.63/s  (1.089s,  940.72/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [ 900/1251 ( 72%)]  Loss:  3.517554 (3.4528)  Time: 1.093s,  936.60/s  (1.088s,  940.78/s)  LR: 5.773e-04  Data: 0.014 (0.013)
Train: 136 [ 950/1251 ( 76%)]  Loss:  3.461558 (3.4532)  Time: 1.079s,  948.88/s  (1.089s,  940.60/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [1000/1251 ( 80%)]  Loss:  3.253346 (3.4437)  Time: 1.080s,  948.24/s  (1.089s,  940.36/s)  LR: 5.773e-04  Data: 0.015 (0.013)
Train: 136 [1050/1251 ( 84%)]  Loss:  3.473960 (3.4451)  Time: 1.085s,  943.93/s  (1.089s,  940.35/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [1100/1251 ( 88%)]  Loss:  3.629928 (3.4531)  Time: 1.096s,  934.20/s  (1.089s,  940.23/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [1150/1251 ( 92%)]  Loss:  3.501340 (3.4551)  Time: 1.073s,  954.27/s  (1.089s,  940.23/s)  LR: 5.773e-04  Data: 0.011 (0.013)
Train: 136 [1200/1251 ( 96%)]  Loss:  3.329047 (3.4501)  Time: 1.078s,  950.27/s  (1.089s,  940.36/s)  LR: 5.773e-04  Data: 0.012 (0.013)
Train: 136 [1250/1251 (100%)]  Loss:  3.731008 (3.4609)  Time: 1.080s,  947.97/s  (1.089s,  940.34/s)  LR: 5.773e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.817 (5.817)  Loss:  0.5301 (0.5301)  Acc@1: 89.3555 (89.3555)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.6318 (1.0334)  Acc@1: 86.3208 (76.3960)  Acc@5: 97.4057 (93.5640)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-129.pth.tar', 75.87200003417969)

Train: 137 [   0/1251 (  0%)]  Loss:  3.538718 (3.5387)  Time: 1.085s,  943.62/s  (1.085s,  943.62/s)  LR: 5.722e-04  Data: 0.025 (0.025)
Train: 137 [  50/1251 (  4%)]  Loss:  3.365833 (3.4523)  Time: 1.102s,  929.07/s  (1.089s,  940.27/s)  LR: 5.722e-04  Data: 0.013 (0.013)
Train: 137 [ 100/1251 (  8%)]  Loss:  3.167329 (3.3573)  Time: 1.078s,  950.18/s  (1.088s,  941.17/s)  LR: 5.722e-04  Data: 0.013 (0.013)
Train: 137 [ 150/1251 ( 12%)]  Loss:  3.758555 (3.4576)  Time: 1.096s,  934.63/s  (1.091s,  938.51/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 200/1251 ( 16%)]  Loss:  3.168277 (3.3997)  Time: 1.096s,  934.53/s  (1.091s,  939.00/s)  LR: 5.722e-04  Data: 0.012 (0.013)
Train: 137 [ 250/1251 ( 20%)]  Loss:  3.403556 (3.4004)  Time: 1.084s,  944.59/s  (1.090s,  939.63/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 300/1251 ( 24%)]  Loss:  3.469708 (3.4103)  Time: 1.078s,  949.74/s  (1.090s,  939.79/s)  LR: 5.722e-04  Data: 0.013 (0.012)
Train: 137 [ 350/1251 ( 28%)]  Loss:  3.241668 (3.3892)  Time: 1.077s,  951.13/s  (1.089s,  939.96/s)  LR: 5.722e-04  Data: 0.012 (0.013)
Train: 137 [ 400/1251 ( 32%)]  Loss:  3.490521 (3.4005)  Time: 1.077s,  950.98/s  (1.090s,  939.28/s)  LR: 5.722e-04  Data: 0.014 (0.012)
Train: 137 [ 450/1251 ( 36%)]  Loss:  3.631749 (3.4236)  Time: 1.076s,  951.73/s  (1.090s,  939.27/s)  LR: 5.722e-04  Data: 0.013 (0.012)
Train: 137 [ 500/1251 ( 40%)]  Loss:  3.246922 (3.4075)  Time: 1.076s,  951.50/s  (1.090s,  939.51/s)  LR: 5.722e-04  Data: 0.013 (0.013)
Train: 137 [ 550/1251 ( 44%)]  Loss:  3.448844 (3.4110)  Time: 1.077s,  950.39/s  (1.089s,  939.99/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 600/1251 ( 48%)]  Loss:  3.705279 (3.4336)  Time: 1.080s,  948.05/s  (1.089s,  940.18/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 650/1251 ( 52%)]  Loss:  3.446839 (3.4346)  Time: 1.085s,  943.74/s  (1.089s,  940.53/s)  LR: 5.722e-04  Data: 0.012 (0.013)
Train: 137 [ 700/1251 ( 56%)]  Loss:  3.164182 (3.4165)  Time: 1.094s,  935.97/s  (1.089s,  940.57/s)  LR: 5.722e-04  Data: 0.017 (0.013)
Train: 137 [ 750/1251 ( 60%)]  Loss:  3.277201 (3.4078)  Time: 1.084s,  944.22/s  (1.089s,  940.15/s)  LR: 5.722e-04  Data: 0.012 (0.013)
Train: 137 [ 800/1251 ( 64%)]  Loss:  3.495296 (3.4130)  Time: 1.078s,  949.47/s  (1.089s,  940.31/s)  LR: 5.722e-04  Data: 0.012 (0.013)
Train: 137 [ 850/1251 ( 68%)]  Loss:  3.542092 (3.4201)  Time: 1.097s,  933.78/s  (1.089s,  940.62/s)  LR: 5.722e-04  Data: 0.011 (0.013)
Train: 137 [ 900/1251 ( 72%)]  Loss:  3.305774 (3.4141)  Time: 1.079s,  948.88/s  (1.089s,  940.58/s)  LR: 5.722e-04  Data: 0.013 (0.012)
Train: 137 [ 950/1251 ( 76%)]  Loss:  3.105077 (3.3987)  Time: 1.098s,  932.92/s  (1.089s,  940.30/s)  LR: 5.722e-04  Data: 0.014 (0.012)
Train: 137 [1000/1251 ( 80%)]  Loss:  3.391138 (3.3983)  Time: 1.090s,  939.05/s  (1.089s,  940.27/s)  LR: 5.722e-04  Data: 0.012 (0.012)
Train: 137 [1050/1251 ( 84%)]  Loss:  3.453382 (3.4008)  Time: 1.078s,  950.02/s  (1.089s,  940.35/s)  LR: 5.722e-04  Data: 0.012 (0.012)
Train: 137 [1100/1251 ( 88%)]  Loss:  3.727026 (3.4150)  Time: 1.085s,  943.78/s  (1.089s,  940.44/s)  LR: 5.722e-04  Data: 0.013 (0.012)
Train: 137 [1150/1251 ( 92%)]  Loss:  3.376144 (3.4134)  Time: 1.077s,  950.91/s  (1.089s,  940.58/s)  LR: 5.722e-04  Data: 0.012 (0.012)
Train: 137 [1200/1251 ( 96%)]  Loss:  3.615254 (3.4215)  Time: 1.077s,  950.70/s  (1.089s,  940.65/s)  LR: 5.722e-04  Data: 0.012 (0.012)
Train: 137 [1250/1251 (100%)]  Loss:  3.719172 (3.4329)  Time: 1.080s,  948.27/s  (1.089s,  940.62/s)  LR: 5.722e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.824 (5.824)  Loss:  0.5233 (0.5233)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6129 (1.0378)  Acc@1: 87.5000 (76.3580)  Acc@5: 97.2877 (93.6380)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-124.pth.tar', 75.92399998046875)

Train: 138 [   0/1251 (  0%)]  Loss:  3.613775 (3.6138)  Time: 1.097s,  933.60/s  (1.097s,  933.60/s)  LR: 5.670e-04  Data: 0.033 (0.033)
Train: 138 [  50/1251 (  4%)]  Loss:  3.468398 (3.5411)  Time: 1.076s,  951.50/s  (1.091s,  938.91/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 100/1251 (  8%)]  Loss:  3.623664 (3.5686)  Time: 1.077s,  950.93/s  (1.090s,  939.56/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 150/1251 ( 12%)]  Loss:  3.299399 (3.5013)  Time: 1.105s,  927.06/s  (1.090s,  939.28/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 200/1251 ( 16%)]  Loss:  3.232723 (3.4476)  Time: 1.097s,  933.85/s  (1.091s,  938.65/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 250/1251 ( 20%)]  Loss:  3.300662 (3.4231)  Time: 1.084s,  944.54/s  (1.092s,  937.56/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 300/1251 ( 24%)]  Loss:  3.734702 (3.4676)  Time: 1.077s,  950.45/s  (1.092s,  937.69/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 350/1251 ( 28%)]  Loss:  3.213176 (3.4358)  Time: 1.103s,  928.44/s  (1.090s,  939.10/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 400/1251 ( 32%)]  Loss:  3.607032 (3.4548)  Time: 1.171s,  874.79/s  (1.090s,  939.66/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 450/1251 ( 36%)]  Loss:  3.554125 (3.4648)  Time: 1.076s,  952.09/s  (1.090s,  939.59/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 500/1251 ( 40%)]  Loss:  3.369019 (3.4561)  Time: 1.083s,  945.60/s  (1.090s,  939.43/s)  LR: 5.670e-04  Data: 0.014 (0.013)
Train: 138 [ 550/1251 ( 44%)]  Loss:  3.377819 (3.4495)  Time: 1.077s,  950.93/s  (1.090s,  939.80/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 600/1251 ( 48%)]  Loss:  3.791643 (3.4759)  Time: 1.075s,  952.21/s  (1.089s,  940.01/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 138 [ 650/1251 ( 52%)]  Loss:  3.172108 (3.4542)  Time: 1.080s,  948.49/s  (1.089s,  940.60/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 700/1251 ( 56%)]  Loss:  3.170869 (3.4353)  Time: 1.080s,  948.09/s  (1.088s,  940.75/s)  LR: 5.670e-04  Data: 0.017 (0.013)
Train: 138 [ 750/1251 ( 60%)]  Loss:  3.495694 (3.4391)  Time: 1.079s,  949.17/s  (1.088s,  940.84/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 800/1251 ( 64%)]  Loss:  3.721386 (3.4557)  Time: 1.102s,  929.51/s  (1.088s,  940.82/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 850/1251 ( 68%)]  Loss:  3.396111 (3.4524)  Time: 1.077s,  950.65/s  (1.088s,  940.77/s)  LR: 5.670e-04  Data: 0.013 (0.013)
Train: 138 [ 900/1251 ( 72%)]  Loss:  3.064746 (3.4319)  Time: 1.077s,  951.20/s  (1.088s,  941.07/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [ 950/1251 ( 76%)]  Loss:  3.349492 (3.4278)  Time: 1.076s,  951.45/s  (1.088s,  941.33/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [1000/1251 ( 80%)]  Loss:  3.419708 (3.4274)  Time: 1.076s,  951.65/s  (1.088s,  941.15/s)  LR: 5.670e-04  Data: 0.013 (0.013)
Train: 138 [1050/1251 ( 84%)]  Loss:  3.259241 (3.4198)  Time: 1.096s,  934.68/s  (1.088s,  941.36/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [1100/1251 ( 88%)]  Loss:  3.098006 (3.4058)  Time: 1.096s,  934.21/s  (1.088s,  941.41/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [1150/1251 ( 92%)]  Loss:  3.295630 (3.4012)  Time: 1.096s,  934.67/s  (1.088s,  941.26/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [1200/1251 ( 96%)]  Loss:  3.439656 (3.4028)  Time: 1.077s,  951.06/s  (1.088s,  940.99/s)  LR: 5.670e-04  Data: 0.012 (0.013)
Train: 138 [1250/1251 (100%)]  Loss:  3.714418 (3.4147)  Time: 1.081s,  947.43/s  (1.089s,  940.73/s)  LR: 5.670e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.704 (5.704)  Loss:  0.5302 (0.5302)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.441)  Loss:  0.6319 (1.0140)  Acc@1: 86.2028 (76.5900)  Acc@5: 97.1698 (93.6280)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-127.pth.tar', 75.99800005615235)

Train: 139 [   0/1251 (  0%)]  Loss:  3.488065 (3.4881)  Time: 1.088s,  941.22/s  (1.088s,  941.22/s)  LR: 5.619e-04  Data: 0.026 (0.026)
Train: 139 [  50/1251 (  4%)]  Loss:  3.133724 (3.3109)  Time: 1.084s,  944.54/s  (1.090s,  939.60/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 100/1251 (  8%)]  Loss:  3.389600 (3.3371)  Time: 1.092s,  937.65/s  (1.089s,  940.72/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 150/1251 ( 12%)]  Loss:  3.310172 (3.3304)  Time: 1.094s,  936.17/s  (1.089s,  940.40/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 200/1251 ( 16%)]  Loss:  3.476532 (3.3596)  Time: 1.078s,  950.09/s  (1.090s,  939.79/s)  LR: 5.619e-04  Data: 0.015 (0.013)
Train: 139 [ 250/1251 ( 20%)]  Loss:  3.613871 (3.4020)  Time: 1.081s,  947.02/s  (1.089s,  940.59/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 300/1251 ( 24%)]  Loss:  3.275547 (3.3839)  Time: 1.095s,  935.48/s  (1.089s,  940.74/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [ 350/1251 ( 28%)]  Loss:  3.431466 (3.3899)  Time: 1.094s,  935.64/s  (1.088s,  940.83/s)  LR: 5.619e-04  Data: 0.013 (0.013)
Train: 139 [ 400/1251 ( 32%)]  Loss:  3.448531 (3.3964)  Time: 1.093s,  937.15/s  (1.090s,  939.58/s)  LR: 5.619e-04  Data: 0.014 (0.013)
Train: 139 [ 450/1251 ( 36%)]  Loss:  3.184757 (3.3752)  Time: 1.077s,  950.91/s  (1.091s,  938.99/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 500/1251 ( 40%)]  Loss:  3.493733 (3.3860)  Time: 1.078s,  949.66/s  (1.091s,  938.87/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 550/1251 ( 44%)]  Loss:  3.127783 (3.3645)  Time: 1.102s,  929.06/s  (1.090s,  939.29/s)  LR: 5.619e-04  Data: 0.014 (0.013)
Train: 139 [ 600/1251 ( 48%)]  Loss:  3.384130 (3.3660)  Time: 1.077s,  950.99/s  (1.090s,  939.02/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 650/1251 ( 52%)]  Loss:  3.898770 (3.4040)  Time: 1.094s,  935.62/s  (1.091s,  938.61/s)  LR: 5.619e-04  Data: 0.014 (0.013)
Train: 139 [ 700/1251 ( 56%)]  Loss:  3.318913 (3.3984)  Time: 1.076s,  951.69/s  (1.091s,  938.78/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 750/1251 ( 60%)]  Loss:  3.660681 (3.4148)  Time: 1.103s,  928.57/s  (1.091s,  938.71/s)  LR: 5.619e-04  Data: 0.021 (0.013)
Train: 139 [ 800/1251 ( 64%)]  Loss:  3.593074 (3.4253)  Time: 1.100s,  931.25/s  (1.091s,  938.29/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 850/1251 ( 68%)]  Loss:  3.048081 (3.4043)  Time: 1.076s,  951.43/s  (1.091s,  938.63/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [ 900/1251 ( 72%)]  Loss:  3.603160 (3.4148)  Time: 1.097s,  933.65/s  (1.091s,  938.76/s)  LR: 5.619e-04  Data: 0.013 (0.013)
Train: 139 [ 950/1251 ( 76%)]  Loss:  3.382790 (3.4132)  Time: 1.079s,  949.42/s  (1.091s,  938.78/s)  LR: 5.619e-04  Data: 0.014 (0.013)
Train: 139 [1000/1251 ( 80%)]  Loss:  3.486544 (3.4167)  Time: 1.201s,  852.71/s  (1.091s,  938.65/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [1050/1251 ( 84%)]  Loss:  3.444271 (3.4179)  Time: 1.078s,  950.25/s  (1.091s,  938.57/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [1100/1251 ( 88%)]  Loss:  3.510974 (3.4220)  Time: 1.095s,  935.55/s  (1.091s,  938.78/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1150/1251 ( 92%)]  Loss:  3.438197 (3.4226)  Time: 1.096s,  933.92/s  (1.090s,  939.05/s)  LR: 5.619e-04  Data: 0.012 (0.013)
Train: 139 [1200/1251 ( 96%)]  Loss:  3.424435 (3.4227)  Time: 1.077s,  950.92/s  (1.090s,  939.11/s)  LR: 5.619e-04  Data: 0.011 (0.013)
Train: 139 [1250/1251 (100%)]  Loss:  3.470921 (3.4246)  Time: 1.062s,  964.17/s  (1.090s,  939.11/s)  LR: 5.619e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.704 (5.704)  Loss:  0.4970 (0.4970)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.230 (0.442)  Loss:  0.6185 (1.0055)  Acc@1: 85.4953 (76.5480)  Acc@5: 97.1698 (93.7760)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-130.pth.tar', 76.03800010498047)

Train: 140 [   0/1251 (  0%)]  Loss:  3.461433 (3.4614)  Time: 1.089s,  940.33/s  (1.089s,  940.33/s)  LR: 5.567e-04  Data: 0.027 (0.027)
Train: 140 [  50/1251 (  4%)]  Loss:  3.527878 (3.4947)  Time: 1.077s,  951.19/s  (1.087s,  942.27/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 100/1251 (  8%)]  Loss:  3.363582 (3.4510)  Time: 1.094s,  935.73/s  (1.089s,  940.66/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [ 150/1251 ( 12%)]  Loss:  3.568575 (3.4804)  Time: 1.078s,  949.87/s  (1.092s,  937.93/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 140 [ 200/1251 ( 16%)]  Loss:  3.341403 (3.4526)  Time: 1.084s,  944.54/s  (1.092s,  937.63/s)  LR: 5.567e-04  Data: 0.013 (0.013)
Train: 140 [ 250/1251 ( 20%)]  Loss:  3.171411 (3.4057)  Time: 1.096s,  934.36/s  (1.093s,  937.02/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 300/1251 ( 24%)]  Loss:  3.406549 (3.4058)  Time: 1.195s,  856.93/s  (1.092s,  937.48/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 350/1251 ( 28%)]  Loss:  3.441647 (3.4103)  Time: 1.078s,  950.17/s  (1.092s,  938.05/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 400/1251 ( 32%)]  Loss:  3.523634 (3.4229)  Time: 1.095s,  935.54/s  (1.091s,  938.54/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 450/1251 ( 36%)]  Loss:  3.370085 (3.4176)  Time: 1.091s,  938.18/s  (1.091s,  938.82/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 500/1251 ( 40%)]  Loss:  3.250814 (3.4025)  Time: 1.096s,  934.54/s  (1.091s,  938.87/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 550/1251 ( 44%)]  Loss:  3.364321 (3.3993)  Time: 1.078s,  949.59/s  (1.091s,  938.25/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 600/1251 ( 48%)]  Loss:  3.584612 (3.4135)  Time: 1.097s,  933.11/s  (1.091s,  938.47/s)  LR: 5.567e-04  Data: 0.015 (0.013)
Train: 140 [ 650/1251 ( 52%)]  Loss:  3.615135 (3.4279)  Time: 1.205s,  849.68/s  (1.091s,  938.53/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 700/1251 ( 56%)]  Loss:  3.441306 (3.4288)  Time: 1.077s,  950.40/s  (1.090s,  939.14/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 750/1251 ( 60%)]  Loss:  3.269589 (3.4189)  Time: 1.095s,  935.06/s  (1.090s,  939.18/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 800/1251 ( 64%)]  Loss:  3.295547 (3.4116)  Time: 1.093s,  936.75/s  (1.091s,  938.93/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 850/1251 ( 68%)]  Loss:  3.671992 (3.4261)  Time: 1.078s,  949.48/s  (1.091s,  938.81/s)  LR: 5.567e-04  Data: 0.011 (0.013)
Train: 140 [ 900/1251 ( 72%)]  Loss:  3.540595 (3.4321)  Time: 1.079s,  949.36/s  (1.091s,  938.86/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [ 950/1251 ( 76%)]  Loss:  3.440638 (3.4325)  Time: 1.095s,  934.73/s  (1.091s,  938.83/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1000/1251 ( 80%)]  Loss:  3.213912 (3.4221)  Time: 1.095s,  935.42/s  (1.091s,  938.68/s)  LR: 5.567e-04  Data: 0.014 (0.013)
Train: 140 [1050/1251 ( 84%)]  Loss:  3.371346 (3.4198)  Time: 1.082s,  946.44/s  (1.091s,  938.63/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1100/1251 ( 88%)]  Loss:  3.701675 (3.4321)  Time: 1.092s,  937.43/s  (1.091s,  938.63/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1150/1251 ( 92%)]  Loss:  3.332378 (3.4279)  Time: 1.079s,  948.94/s  (1.091s,  938.68/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1200/1251 ( 96%)]  Loss:  3.298529 (3.4227)  Time: 1.084s,  945.07/s  (1.091s,  938.77/s)  LR: 5.567e-04  Data: 0.012 (0.013)
Train: 140 [1250/1251 (100%)]  Loss:  3.420813 (3.4227)  Time: 1.079s,  949.27/s  (1.091s,  938.88/s)  LR: 5.567e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.706 (5.706)  Loss:  0.5401 (0.5401)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.229 (0.441)  Loss:  0.5964 (1.0113)  Acc@1: 86.5566 (76.2920)  Acc@5: 97.1698 (93.5580)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 76.29200010742187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-134.pth.tar', 76.13000003173828)

Train: 141 [   0/1251 (  0%)]  Loss:  3.374457 (3.3745)  Time: 1.088s,  941.15/s  (1.088s,  941.15/s)  LR: 5.516e-04  Data: 0.027 (0.027)
Train: 141 [  50/1251 (  4%)]  Loss:  3.518302 (3.4464)  Time: 1.098s,  932.70/s  (1.089s,  940.25/s)  LR: 5.516e-04  Data: 0.013 (0.013)
Train: 141 [ 100/1251 (  8%)]  Loss:  3.386041 (3.4263)  Time: 1.094s,  935.74/s  (1.088s,  940.86/s)  LR: 5.516e-04  Data: 0.014 (0.013)
Train: 141 [ 150/1251 ( 12%)]  Loss:  3.751957 (3.5077)  Time: 1.097s,  933.13/s  (1.087s,  941.96/s)  LR: 5.516e-04  Data: 0.014 (0.013)
Train: 141 [ 200/1251 ( 16%)]  Loss:  3.371417 (3.4804)  Time: 1.079s,  948.91/s  (1.085s,  943.40/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 250/1251 ( 20%)]  Loss:  3.169200 (3.4286)  Time: 1.095s,  935.35/s  (1.087s,  941.76/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 300/1251 ( 24%)]  Loss:  3.545054 (3.4452)  Time: 1.076s,  951.55/s  (1.088s,  941.31/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 350/1251 ( 28%)]  Loss:  3.504814 (3.4527)  Time: 1.095s,  934.80/s  (1.087s,  941.86/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 400/1251 ( 32%)]  Loss:  3.293635 (3.4350)  Time: 1.079s,  948.96/s  (1.087s,  941.80/s)  LR: 5.516e-04  Data: 0.015 (0.013)
Train: 141 [ 450/1251 ( 36%)]  Loss:  3.543201 (3.4458)  Time: 1.086s,  942.58/s  (1.087s,  941.87/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 500/1251 ( 40%)]  Loss:  3.609104 (3.4607)  Time: 1.079s,  948.80/s  (1.087s,  942.10/s)  LR: 5.516e-04  Data: 0.014 (0.013)
Train: 141 [ 550/1251 ( 44%)]  Loss:  3.406738 (3.4562)  Time: 1.079s,  949.42/s  (1.086s,  942.64/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 600/1251 ( 48%)]  Loss:  3.503188 (3.4598)  Time: 1.081s,  946.84/s  (1.087s,  942.33/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 650/1251 ( 52%)]  Loss:  3.284930 (3.4473)  Time: 1.076s,  951.44/s  (1.087s,  942.47/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 700/1251 ( 56%)]  Loss:  3.322422 (3.4390)  Time: 1.095s,  935.14/s  (1.087s,  941.78/s)  LR: 5.516e-04  Data: 0.012 (0.013)
Train: 141 [ 750/1251 ( 60%)]  Loss:  3.284851 (3.4293)  Time: 1.093s,  936.82/s  (1.087s,  941.75/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 800/1251 ( 64%)]  Loss:  3.355553 (3.4250)  Time: 1.105s,  927.09/s  (1.087s,  941.65/s)  LR: 5.516e-04  Data: 0.013 (0.013)
Train: 141 [ 850/1251 ( 68%)]  Loss:  3.644906 (3.4372)  Time: 1.109s,  923.06/s  (1.087s,  941.76/s)  LR: 5.516e-04  Data: 0.015 (0.013)
Train: 141 [ 900/1251 ( 72%)]  Loss:  3.776455 (3.4551)  Time: 1.098s,  932.45/s  (1.088s,  941.14/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [ 950/1251 ( 76%)]  Loss:  3.201313 (3.4424)  Time: 1.095s,  935.42/s  (1.088s,  940.94/s)  LR: 5.516e-04  Data: 0.014 (0.013)
Train: 141 [1000/1251 ( 80%)]  Loss:  3.485965 (3.4445)  Time: 1.080s,  948.50/s  (1.088s,  940.80/s)  LR: 5.516e-04  Data: 0.013 (0.013)
Train: 141 [1050/1251 ( 84%)]  Loss:  3.626317 (3.4527)  Time: 1.083s,  945.70/s  (1.089s,  940.72/s)  LR: 5.516e-04  Data: 0.013 (0.013)
Train: 141 [1100/1251 ( 88%)]  Loss:  3.403272 (3.4506)  Time: 1.083s,  945.15/s  (1.089s,  940.50/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1150/1251 ( 92%)]  Loss:  3.497899 (3.4525)  Time: 1.077s,  950.61/s  (1.089s,  940.30/s)  LR: 5.516e-04  Data: 0.011 (0.013)
Train: 141 [1200/1251 ( 96%)]  Loss:  3.331284 (3.4477)  Time: 1.095s,  935.40/s  (1.089s,  940.30/s)  LR: 5.516e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 141 [1250/1251 (100%)]  Loss:  3.659733 (3.4558)  Time: 1.061s,  964.78/s  (1.089s,  940.42/s)  LR: 5.516e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.814 (5.814)  Loss:  0.5515 (0.5515)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6237 (1.0215)  Acc@1: 86.3208 (76.4560)  Acc@5: 97.2877 (93.7360)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 76.29200010742187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-133.pth.tar', 76.14400000732422)

Train: 142 [   0/1251 (  0%)]  Loss:  3.222985 (3.2230)  Time: 1.088s,  941.59/s  (1.088s,  941.59/s)  LR: 5.464e-04  Data: 0.026 (0.026)
Train: 142 [  50/1251 (  4%)]  Loss:  3.534405 (3.3787)  Time: 1.080s,  948.42/s  (1.086s,  942.91/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 100/1251 (  8%)]  Loss:  3.328796 (3.3621)  Time: 1.098s,  932.43/s  (1.086s,  943.27/s)  LR: 5.464e-04  Data: 0.014 (0.013)
Train: 142 [ 150/1251 ( 12%)]  Loss:  3.845777 (3.4830)  Time: 1.093s,  936.62/s  (1.090s,  939.56/s)  LR: 5.464e-04  Data: 0.015 (0.013)
Train: 142 [ 200/1251 ( 16%)]  Loss:  3.428384 (3.4721)  Time: 1.096s,  934.16/s  (1.091s,  938.81/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 250/1251 ( 20%)]  Loss:  3.506664 (3.4778)  Time: 1.095s,  935.16/s  (1.091s,  938.93/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 300/1251 ( 24%)]  Loss:  3.523966 (3.4844)  Time: 1.094s,  936.01/s  (1.092s,  937.52/s)  LR: 5.464e-04  Data: 0.012 (0.012)
Train: 142 [ 350/1251 ( 28%)]  Loss:  3.169637 (3.4451)  Time: 1.081s,  947.52/s  (1.091s,  938.88/s)  LR: 5.464e-04  Data: 0.014 (0.012)
Train: 142 [ 400/1251 ( 32%)]  Loss:  3.296104 (3.4285)  Time: 1.077s,  950.47/s  (1.090s,  939.64/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 450/1251 ( 36%)]  Loss:  3.622792 (3.4480)  Time: 1.095s,  935.27/s  (1.089s,  940.23/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 500/1251 ( 40%)]  Loss:  3.656041 (3.4669)  Time: 1.086s,  942.99/s  (1.090s,  939.86/s)  LR: 5.464e-04  Data: 0.014 (0.013)
Train: 142 [ 550/1251 ( 44%)]  Loss:  3.770394 (3.4922)  Time: 1.088s,  940.87/s  (1.089s,  940.06/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 600/1251 ( 48%)]  Loss:  3.408392 (3.4857)  Time: 1.082s,  946.32/s  (1.089s,  940.04/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 650/1251 ( 52%)]  Loss:  2.885486 (3.4428)  Time: 1.081s,  946.90/s  (1.090s,  939.64/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 700/1251 ( 56%)]  Loss:  3.564968 (3.4510)  Time: 1.077s,  950.72/s  (1.090s,  939.87/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 750/1251 ( 60%)]  Loss:  3.656610 (3.4638)  Time: 1.200s,  853.22/s  (1.089s,  939.99/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 800/1251 ( 64%)]  Loss:  2.905094 (3.4310)  Time: 1.090s,  939.82/s  (1.090s,  939.73/s)  LR: 5.464e-04  Data: 0.011 (0.013)
Train: 142 [ 850/1251 ( 68%)]  Loss:  3.384281 (3.4284)  Time: 1.094s,  936.31/s  (1.090s,  939.71/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 900/1251 ( 72%)]  Loss:  3.577611 (3.4362)  Time: 1.110s,  922.59/s  (1.090s,  939.88/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [ 950/1251 ( 76%)]  Loss:  3.542633 (3.4416)  Time: 1.095s,  935.09/s  (1.089s,  940.01/s)  LR: 5.464e-04  Data: 0.015 (0.013)
Train: 142 [1000/1251 ( 80%)]  Loss:  3.693398 (3.4535)  Time: 1.078s,  949.89/s  (1.090s,  939.82/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [1050/1251 ( 84%)]  Loss:  3.529674 (3.4570)  Time: 1.079s,  949.28/s  (1.090s,  939.64/s)  LR: 5.464e-04  Data: 0.013 (0.013)
Train: 142 [1100/1251 ( 88%)]  Loss:  3.709596 (3.4680)  Time: 1.075s,  952.34/s  (1.090s,  939.72/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [1150/1251 ( 92%)]  Loss:  3.674778 (3.4766)  Time: 1.080s,  947.73/s  (1.089s,  939.92/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [1200/1251 ( 96%)]  Loss:  3.303455 (3.4697)  Time: 1.076s,  951.91/s  (1.089s,  940.09/s)  LR: 5.464e-04  Data: 0.012 (0.013)
Train: 142 [1250/1251 (100%)]  Loss:  3.210304 (3.4597)  Time: 1.067s,  959.92/s  (1.089s,  940.18/s)  LR: 5.464e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.831 (5.831)  Loss:  0.5069 (0.5069)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6459 (1.0174)  Acc@1: 85.8491 (76.7240)  Acc@5: 96.6981 (93.6240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 76.29200010742187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-132.pth.tar', 76.19399997558594)

Train: 143 [   0/1251 (  0%)]  Loss:  3.198814 (3.1988)  Time: 1.087s,  941.88/s  (1.087s,  941.88/s)  LR: 5.413e-04  Data: 0.026 (0.026)
Train: 143 [  50/1251 (  4%)]  Loss:  3.558598 (3.3787)  Time: 1.079s,  948.79/s  (1.083s,  945.83/s)  LR: 5.413e-04  Data: 0.016 (0.013)
Train: 143 [ 100/1251 (  8%)]  Loss:  3.286356 (3.3479)  Time: 1.076s,  951.97/s  (1.081s,  946.94/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 150/1251 ( 12%)]  Loss:  3.199329 (3.3108)  Time: 1.098s,  932.91/s  (1.086s,  943.19/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 200/1251 ( 16%)]  Loss:  3.099858 (3.2686)  Time: 1.120s,  913.95/s  (1.086s,  943.00/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 250/1251 ( 20%)]  Loss:  3.726565 (3.3449)  Time: 1.094s,  935.64/s  (1.087s,  942.20/s)  LR: 5.413e-04  Data: 0.014 (0.013)
Train: 143 [ 300/1251 ( 24%)]  Loss:  3.315944 (3.3408)  Time: 1.095s,  935.20/s  (1.088s,  940.82/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 350/1251 ( 28%)]  Loss:  3.623370 (3.3761)  Time: 1.095s,  935.34/s  (1.088s,  940.89/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 400/1251 ( 32%)]  Loss:  3.420043 (3.3810)  Time: 1.104s,  927.72/s  (1.088s,  940.78/s)  LR: 5.413e-04  Data: 0.014 (0.013)
Train: 143 [ 450/1251 ( 36%)]  Loss:  3.339794 (3.3769)  Time: 1.097s,  933.66/s  (1.089s,  940.54/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 500/1251 ( 40%)]  Loss:  3.577675 (3.3951)  Time: 1.099s,  932.03/s  (1.089s,  939.93/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 550/1251 ( 44%)]  Loss:  3.435426 (3.3985)  Time: 1.080s,  948.47/s  (1.089s,  940.47/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [ 600/1251 ( 48%)]  Loss:  3.106186 (3.3760)  Time: 1.076s,  951.29/s  (1.089s,  940.67/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 650/1251 ( 52%)]  Loss:  3.242133 (3.3664)  Time: 1.092s,  937.41/s  (1.089s,  940.47/s)  LR: 5.413e-04  Data: 0.017 (0.013)
Train: 143 [ 700/1251 ( 56%)]  Loss:  3.152258 (3.3522)  Time: 1.076s,  952.05/s  (1.089s,  940.55/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 750/1251 ( 60%)]  Loss:  3.481392 (3.3602)  Time: 1.077s,  950.39/s  (1.089s,  940.59/s)  LR: 5.413e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 143 [ 800/1251 ( 64%)]  Loss:  3.505169 (3.3688)  Time: 1.095s,  935.08/s  (1.089s,  940.72/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 850/1251 ( 68%)]  Loss:  3.185929 (3.3586)  Time: 1.080s,  948.41/s  (1.088s,  940.82/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 900/1251 ( 72%)]  Loss:  3.558875 (3.3691)  Time: 1.097s,  933.43/s  (1.089s,  940.51/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [ 950/1251 ( 76%)]  Loss:  3.256895 (3.3635)  Time: 1.080s,  948.01/s  (1.089s,  940.54/s)  LR: 5.413e-04  Data: 0.011 (0.013)
Train: 143 [1000/1251 ( 80%)]  Loss:  3.228602 (3.3571)  Time: 1.084s,  944.66/s  (1.089s,  940.69/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [1050/1251 ( 84%)]  Loss:  3.503017 (3.3637)  Time: 1.106s,  925.66/s  (1.089s,  940.57/s)  LR: 5.413e-04  Data: 0.014 (0.013)
Train: 143 [1100/1251 ( 88%)]  Loss:  3.546659 (3.3717)  Time: 1.097s,  933.16/s  (1.089s,  940.34/s)  LR: 5.413e-04  Data: 0.013 (0.013)
Train: 143 [1150/1251 ( 92%)]  Loss:  3.454861 (3.3752)  Time: 1.096s,  933.93/s  (1.089s,  940.06/s)  LR: 5.413e-04  Data: 0.012 (0.013)
Train: 143 [1200/1251 ( 96%)]  Loss:  3.183980 (3.3675)  Time: 1.076s,  951.60/s  (1.089s,  940.30/s)  LR: 5.413e-04  Data: 0.015 (0.013)
Train: 143 [1250/1251 (100%)]  Loss:  3.279736 (3.3641)  Time: 1.080s,  948.10/s  (1.089s,  940.14/s)  LR: 5.413e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.837 (5.837)  Loss:  0.5287 (0.5287)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6417 (1.0084)  Acc@1: 85.7311 (76.6460)  Acc@5: 97.0519 (93.7260)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-140.pth.tar', 76.29200010742187)

Train: 144 [   0/1251 (  0%)]  Loss:  3.531552 (3.5316)  Time: 1.086s,  942.78/s  (1.086s,  942.78/s)  LR: 5.361e-04  Data: 0.026 (0.026)
Train: 144 [  50/1251 (  4%)]  Loss:  3.475876 (3.5037)  Time: 1.080s,  948.31/s  (1.093s,  937.23/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 100/1251 (  8%)]  Loss:  3.402934 (3.4701)  Time: 1.084s,  944.24/s  (1.089s,  940.60/s)  LR: 5.361e-04  Data: 0.015 (0.013)
Train: 144 [ 150/1251 ( 12%)]  Loss:  3.539795 (3.4875)  Time: 1.078s,  950.18/s  (1.087s,  941.75/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 200/1251 ( 16%)]  Loss:  2.998862 (3.3898)  Time: 1.077s,  950.87/s  (1.088s,  941.44/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 250/1251 ( 20%)]  Loss:  3.420988 (3.3950)  Time: 1.077s,  950.72/s  (1.089s,  940.49/s)  LR: 5.361e-04  Data: 0.013 (0.013)
Train: 144 [ 300/1251 ( 24%)]  Loss:  3.306275 (3.3823)  Time: 1.084s,  944.89/s  (1.088s,  941.20/s)  LR: 5.361e-04  Data: 0.016 (0.013)
Train: 144 [ 350/1251 ( 28%)]  Loss:  3.170228 (3.3558)  Time: 1.089s,  940.62/s  (1.088s,  941.54/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 400/1251 ( 32%)]  Loss:  3.355529 (3.3558)  Time: 1.094s,  935.77/s  (1.087s,  941.72/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 450/1251 ( 36%)]  Loss:  3.446324 (3.3648)  Time: 1.082s,  946.70/s  (1.087s,  942.13/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 500/1251 ( 40%)]  Loss:  3.546640 (3.3814)  Time: 1.097s,  933.85/s  (1.087s,  942.44/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 550/1251 ( 44%)]  Loss:  3.366895 (3.3802)  Time: 1.077s,  950.43/s  (1.087s,  942.43/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 600/1251 ( 48%)]  Loss:  3.432092 (3.3842)  Time: 1.095s,  935.02/s  (1.086s,  942.53/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 650/1251 ( 52%)]  Loss:  3.416007 (3.3864)  Time: 1.076s,  951.28/s  (1.087s,  942.17/s)  LR: 5.361e-04  Data: 0.012 (0.013)
Train: 144 [ 700/1251 ( 56%)]  Loss:  3.515276 (3.3950)  Time: 1.076s,  951.36/s  (1.087s,  942.06/s)  LR: 5.361e-04  Data: 0.013 (0.013)
Train: 144 [ 750/1251 ( 60%)]  Loss:  3.482625 (3.4005)  Time: 1.102s,  929.12/s  (1.087s,  942.09/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [ 800/1251 ( 64%)]  Loss:  3.338703 (3.3969)  Time: 1.094s,  936.02/s  (1.087s,  941.85/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [ 850/1251 ( 68%)]  Loss:  3.444729 (3.3995)  Time: 1.095s,  935.22/s  (1.088s,  941.15/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [ 900/1251 ( 72%)]  Loss:  3.562684 (3.4081)  Time: 1.084s,  944.61/s  (1.088s,  941.17/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [ 950/1251 ( 76%)]  Loss:  3.313652 (3.4034)  Time: 1.075s,  952.52/s  (1.088s,  941.37/s)  LR: 5.361e-04  Data: 0.014 (0.012)
Train: 144 [1000/1251 ( 80%)]  Loss:  3.694339 (3.4172)  Time: 1.105s,  926.77/s  (1.088s,  941.16/s)  LR: 5.361e-04  Data: 0.011 (0.012)
Train: 144 [1050/1251 ( 84%)]  Loss:  3.502588 (3.4211)  Time: 1.075s,  952.38/s  (1.089s,  940.72/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [1100/1251 ( 88%)]  Loss:  3.126808 (3.4083)  Time: 1.087s,  942.02/s  (1.089s,  940.68/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [1150/1251 ( 92%)]  Loss:  3.396604 (3.4078)  Time: 1.076s,  951.44/s  (1.088s,  940.97/s)  LR: 5.361e-04  Data: 0.012 (0.012)
Train: 144 [1200/1251 ( 96%)]  Loss:  3.169077 (3.3983)  Time: 1.076s,  951.59/s  (1.088s,  940.99/s)  LR: 5.361e-04  Data: 0.015 (0.013)
Train: 144 [1250/1251 (100%)]  Loss:  3.313603 (3.3950)  Time: 1.079s,  948.84/s  (1.088s,  940.83/s)  LR: 5.361e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.793 (5.793)  Loss:  0.5025 (0.5025)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5932 (1.0114)  Acc@1: 86.4387 (76.7800)  Acc@5: 97.6415 (93.6320)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-135.pth.tar', 76.35400008300782)

Train: 145 [   0/1251 (  0%)]  Loss:  3.317041 (3.3170)  Time: 1.087s,  941.84/s  (1.087s,  941.84/s)  LR: 5.309e-04  Data: 0.026 (0.026)
Train: 145 [  50/1251 (  4%)]  Loss:  3.086450 (3.2017)  Time: 1.076s,  951.35/s  (1.090s,  939.40/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 100/1251 (  8%)]  Loss:  3.406183 (3.2699)  Time: 1.084s,  945.06/s  (1.086s,  942.65/s)  LR: 5.309e-04  Data: 0.014 (0.013)
Train: 145 [ 150/1251 ( 12%)]  Loss:  3.453244 (3.3157)  Time: 1.077s,  950.90/s  (1.087s,  941.73/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 200/1251 ( 16%)]  Loss:  3.461257 (3.3448)  Time: 1.095s,  935.08/s  (1.087s,  942.40/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 250/1251 ( 20%)]  Loss:  3.454015 (3.3630)  Time: 1.094s,  936.07/s  (1.086s,  942.63/s)  LR: 5.309e-04  Data: 0.013 (0.013)
Train: 145 [ 300/1251 ( 24%)]  Loss:  3.359873 (3.3626)  Time: 1.097s,  933.04/s  (1.088s,  941.45/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 350/1251 ( 28%)]  Loss:  3.501999 (3.3800)  Time: 1.093s,  937.09/s  (1.089s,  940.20/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [ 400/1251 ( 32%)]  Loss:  3.536159 (3.3974)  Time: 1.077s,  950.58/s  (1.089s,  940.03/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 450/1251 ( 36%)]  Loss:  3.585679 (3.4162)  Time: 1.077s,  950.37/s  (1.089s,  940.08/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 500/1251 ( 40%)]  Loss:  3.465874 (3.4207)  Time: 1.104s,  927.15/s  (1.090s,  939.86/s)  LR: 5.309e-04  Data: 0.013 (0.013)
Train: 145 [ 550/1251 ( 44%)]  Loss:  3.229585 (3.4048)  Time: 1.076s,  951.43/s  (1.090s,  939.63/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 600/1251 ( 48%)]  Loss:  3.117739 (3.3827)  Time: 1.075s,  952.78/s  (1.090s,  939.61/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 145 [ 650/1251 ( 52%)]  Loss:  3.426111 (3.3858)  Time: 1.080s,  947.74/s  (1.090s,  939.67/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 700/1251 ( 56%)]  Loss:  3.215652 (3.3745)  Time: 1.079s,  949.18/s  (1.089s,  940.04/s)  LR: 5.309e-04  Data: 0.015 (0.013)
Train: 145 [ 750/1251 ( 60%)]  Loss:  3.563404 (3.3863)  Time: 1.096s,  934.57/s  (1.089s,  939.99/s)  LR: 5.309e-04  Data: 0.015 (0.013)
Train: 145 [ 800/1251 ( 64%)]  Loss:  3.546406 (3.3957)  Time: 1.076s,  951.34/s  (1.089s,  940.15/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [ 850/1251 ( 68%)]  Loss:  3.185859 (3.3840)  Time: 1.094s,  935.81/s  (1.090s,  939.88/s)  LR: 5.309e-04  Data: 0.015 (0.013)
Train: 145 [ 900/1251 ( 72%)]  Loss:  3.479184 (3.3890)  Time: 1.083s,  945.76/s  (1.090s,  939.50/s)  LR: 5.309e-04  Data: 0.015 (0.013)
Train: 145 [ 950/1251 ( 76%)]  Loss:  3.283289 (3.3838)  Time: 1.076s,  951.91/s  (1.090s,  939.86/s)  LR: 5.309e-04  Data: 0.012 (0.013)
Train: 145 [1000/1251 ( 80%)]  Loss:  3.306999 (3.3801)  Time: 1.098s,  932.36/s  (1.089s,  940.21/s)  LR: 5.309e-04  Data: 0.013 (0.013)
Train: 145 [1050/1251 ( 84%)]  Loss:  3.371189 (3.3797)  Time: 1.096s,  934.47/s  (1.089s,  940.10/s)  LR: 5.309e-04  Data: 0.013 (0.013)
Train: 145 [1100/1251 ( 88%)]  Loss:  3.405364 (3.3808)  Time: 1.079s,  949.00/s  (1.089s,  940.10/s)  LR: 5.309e-04  Data: 0.015 (0.013)
Train: 145 [1150/1251 ( 92%)]  Loss:  3.593451 (3.3897)  Time: 1.094s,  935.75/s  (1.089s,  940.24/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1200/1251 ( 96%)]  Loss:  3.482752 (3.3934)  Time: 1.075s,  952.13/s  (1.089s,  940.51/s)  LR: 5.309e-04  Data: 0.011 (0.013)
Train: 145 [1250/1251 (100%)]  Loss:  3.324067 (3.3907)  Time: 1.062s,  964.07/s  (1.089s,  940.33/s)  LR: 5.309e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.854 (5.854)  Loss:  0.5196 (0.5196)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6077 (1.0300)  Acc@1: 86.4387 (76.6240)  Acc@5: 98.1132 (93.7000)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-137.pth.tar', 76.358)

Train: 146 [   0/1251 (  0%)]  Loss:  3.829573 (3.8296)  Time: 1.093s,  936.66/s  (1.093s,  936.66/s)  LR: 5.257e-04  Data: 0.033 (0.033)
Train: 146 [  50/1251 (  4%)]  Loss:  3.656528 (3.7431)  Time: 1.072s,  954.85/s  (1.092s,  937.89/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 100/1251 (  8%)]  Loss:  3.814081 (3.7667)  Time: 1.081s,  947.18/s  (1.088s,  941.09/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 150/1251 ( 12%)]  Loss:  3.687819 (3.7470)  Time: 1.080s,  948.36/s  (1.086s,  942.88/s)  LR: 5.257e-04  Data: 0.013 (0.013)
Train: 146 [ 200/1251 ( 16%)]  Loss:  3.359116 (3.6694)  Time: 1.105s,  926.28/s  (1.088s,  941.58/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 250/1251 ( 20%)]  Loss:  3.546559 (3.6489)  Time: 1.093s,  936.56/s  (1.088s,  941.37/s)  LR: 5.257e-04  Data: 0.014 (0.013)
Train: 146 [ 300/1251 ( 24%)]  Loss:  3.257084 (3.5930)  Time: 1.077s,  950.50/s  (1.088s,  941.44/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 350/1251 ( 28%)]  Loss:  3.518888 (3.5837)  Time: 1.079s,  948.90/s  (1.087s,  941.81/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 400/1251 ( 32%)]  Loss:  3.407591 (3.5641)  Time: 1.096s,  934.47/s  (1.087s,  942.08/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 450/1251 ( 36%)]  Loss:  3.197289 (3.5275)  Time: 1.081s,  947.07/s  (1.087s,  941.77/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 500/1251 ( 40%)]  Loss:  3.456960 (3.5210)  Time: 1.080s,  948.31/s  (1.087s,  941.77/s)  LR: 5.257e-04  Data: 0.014 (0.013)
Train: 146 [ 550/1251 ( 44%)]  Loss:  3.664474 (3.5330)  Time: 1.093s,  936.56/s  (1.087s,  942.00/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 600/1251 ( 48%)]  Loss:  3.659063 (3.5427)  Time: 1.077s,  950.96/s  (1.087s,  942.27/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 650/1251 ( 52%)]  Loss:  3.417533 (3.5338)  Time: 1.093s,  936.56/s  (1.087s,  942.09/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 700/1251 ( 56%)]  Loss:  3.794668 (3.5511)  Time: 1.099s,  931.82/s  (1.087s,  941.75/s)  LR: 5.257e-04  Data: 0.014 (0.013)
Train: 146 [ 750/1251 ( 60%)]  Loss:  3.579388 (3.5529)  Time: 1.094s,  936.00/s  (1.088s,  941.11/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 800/1251 ( 64%)]  Loss:  3.665036 (3.5595)  Time: 1.075s,  952.26/s  (1.089s,  940.68/s)  LR: 5.257e-04  Data: 0.014 (0.013)
Train: 146 [ 850/1251 ( 68%)]  Loss:  3.564317 (3.5598)  Time: 1.076s,  951.31/s  (1.089s,  940.73/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [ 900/1251 ( 72%)]  Loss:  3.469757 (3.5550)  Time: 1.171s,  874.67/s  (1.089s,  940.34/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [ 950/1251 ( 76%)]  Loss:  3.287172 (3.5416)  Time: 1.078s,  949.98/s  (1.089s,  940.73/s)  LR: 5.257e-04  Data: 0.013 (0.013)
Train: 146 [1000/1251 ( 80%)]  Loss:  3.589769 (3.5439)  Time: 1.091s,  938.46/s  (1.088s,  940.78/s)  LR: 5.257e-04  Data: 0.014 (0.013)
Train: 146 [1050/1251 ( 84%)]  Loss:  3.312588 (3.5334)  Time: 1.078s,  949.52/s  (1.088s,  940.93/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [1100/1251 ( 88%)]  Loss:  3.350641 (3.5255)  Time: 1.095s,  934.94/s  (1.088s,  941.04/s)  LR: 5.257e-04  Data: 0.011 (0.013)
Train: 146 [1150/1251 ( 92%)]  Loss:  3.202034 (3.5120)  Time: 1.076s,  951.46/s  (1.088s,  941.03/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [1200/1251 ( 96%)]  Loss:  3.280860 (3.5028)  Time: 1.105s,  927.03/s  (1.088s,  940.95/s)  LR: 5.257e-04  Data: 0.012 (0.013)
Train: 146 [1250/1251 (100%)]  Loss:  3.524045 (3.5036)  Time: 1.079s,  949.30/s  (1.088s,  940.77/s)  LR: 5.257e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.812 (5.812)  Loss:  0.5069 (0.5069)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.6439 (1.0237)  Acc@1: 85.0236 (76.7540)  Acc@5: 97.6415 (93.6980)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-131.pth.tar', 76.36000013427734)

Train: 147 [   0/1251 (  0%)]  Loss:  3.663745 (3.6637)  Time: 1.089s,  940.52/s  (1.089s,  940.52/s)  LR: 5.205e-04  Data: 0.027 (0.027)
Train: 147 [  50/1251 (  4%)]  Loss:  3.342040 (3.5029)  Time: 1.100s,  930.95/s  (1.084s,  944.65/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 100/1251 (  8%)]  Loss:  3.251476 (3.4191)  Time: 1.094s,  935.76/s  (1.088s,  941.11/s)  LR: 5.205e-04  Data: 0.014 (0.013)
Train: 147 [ 150/1251 ( 12%)]  Loss:  3.269871 (3.3818)  Time: 1.084s,  944.62/s  (1.090s,  939.44/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 200/1251 ( 16%)]  Loss:  3.361665 (3.3778)  Time: 1.103s,  928.07/s  (1.090s,  939.25/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 250/1251 ( 20%)]  Loss:  3.335697 (3.3707)  Time: 1.081s,  947.64/s  (1.090s,  939.55/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 147 [ 300/1251 ( 24%)]  Loss:  3.160361 (3.3407)  Time: 1.076s,  951.54/s  (1.089s,  940.41/s)  LR: 5.205e-04  Data: 0.014 (0.013)
Train: 147 [ 350/1251 ( 28%)]  Loss:  3.525131 (3.3637)  Time: 1.106s,  926.19/s  (1.089s,  940.25/s)  LR: 5.205e-04  Data: 0.013 (0.013)
Train: 147 [ 400/1251 ( 32%)]  Loss:  3.483790 (3.3771)  Time: 1.080s,  948.07/s  (1.089s,  940.30/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 450/1251 ( 36%)]  Loss:  3.757991 (3.4152)  Time: 1.078s,  950.35/s  (1.089s,  940.72/s)  LR: 5.205e-04  Data: 0.014 (0.013)
Train: 147 [ 500/1251 ( 40%)]  Loss:  3.487214 (3.4217)  Time: 1.076s,  951.60/s  (1.089s,  940.55/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 550/1251 ( 44%)]  Loss:  3.289472 (3.4107)  Time: 1.096s,  934.55/s  (1.089s,  940.55/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 600/1251 ( 48%)]  Loss:  3.634129 (3.4279)  Time: 1.076s,  951.77/s  (1.089s,  940.65/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 650/1251 ( 52%)]  Loss:  3.420721 (3.4274)  Time: 1.080s,  947.83/s  (1.089s,  940.61/s)  LR: 5.205e-04  Data: 0.015 (0.013)
Train: 147 [ 700/1251 ( 56%)]  Loss:  3.395521 (3.4253)  Time: 1.077s,  950.55/s  (1.088s,  940.96/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 750/1251 ( 60%)]  Loss:  3.209813 (3.4118)  Time: 1.078s,  950.20/s  (1.088s,  941.05/s)  LR: 5.205e-04  Data: 0.014 (0.013)
Train: 147 [ 800/1251 ( 64%)]  Loss:  3.585157 (3.4220)  Time: 1.085s,  943.57/s  (1.088s,  941.20/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [ 850/1251 ( 68%)]  Loss:  3.330384 (3.4169)  Time: 1.096s,  934.00/s  (1.089s,  940.69/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 900/1251 ( 72%)]  Loss:  3.345336 (3.4131)  Time: 1.072s,  955.21/s  (1.089s,  940.41/s)  LR: 5.205e-04  Data: 0.011 (0.013)
Train: 147 [ 950/1251 ( 76%)]  Loss:  3.250602 (3.4050)  Time: 1.075s,  952.79/s  (1.089s,  940.63/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [1000/1251 ( 80%)]  Loss:  3.503718 (3.4097)  Time: 1.095s,  934.90/s  (1.089s,  940.61/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [1050/1251 ( 84%)]  Loss:  3.679314 (3.4220)  Time: 1.080s,  947.90/s  (1.089s,  940.50/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [1100/1251 ( 88%)]  Loss:  3.369001 (3.4197)  Time: 1.081s,  947.40/s  (1.089s,  940.49/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [1150/1251 ( 92%)]  Loss:  3.238976 (3.4121)  Time: 1.077s,  950.69/s  (1.089s,  940.71/s)  LR: 5.205e-04  Data: 0.014 (0.013)
Train: 147 [1200/1251 ( 96%)]  Loss:  3.426797 (3.4127)  Time: 1.123s,  912.06/s  (1.089s,  940.63/s)  LR: 5.205e-04  Data: 0.012 (0.013)
Train: 147 [1250/1251 (100%)]  Loss:  3.643340 (3.4216)  Time: 1.062s,  964.25/s  (1.089s,  940.21/s)  LR: 5.205e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.773 (5.773)  Loss:  0.5464 (0.5464)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.6125 (1.0488)  Acc@1: 86.3208 (76.4800)  Acc@5: 97.6415 (93.6880)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 76.48000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-136.pth.tar', 76.39600000488281)

Train: 148 [   0/1251 (  0%)]  Loss:  3.589317 (3.5893)  Time: 1.090s,  939.77/s  (1.090s,  939.77/s)  LR: 5.154e-04  Data: 0.028 (0.028)
Train: 148 [  50/1251 (  4%)]  Loss:  3.628061 (3.6087)  Time: 1.108s,  924.14/s  (1.086s,  943.18/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 100/1251 (  8%)]  Loss:  3.391610 (3.5363)  Time: 1.092s,  937.34/s  (1.086s,  943.05/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 150/1251 ( 12%)]  Loss:  3.447804 (3.5142)  Time: 1.103s,  928.09/s  (1.087s,  941.99/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 200/1251 ( 16%)]  Loss:  3.409427 (3.4932)  Time: 1.095s,  935.43/s  (1.090s,  939.74/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 250/1251 ( 20%)]  Loss:  3.426724 (3.4822)  Time: 1.078s,  949.47/s  (1.091s,  939.02/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 300/1251 ( 24%)]  Loss:  3.264822 (3.4511)  Time: 1.080s,  948.10/s  (1.090s,  939.63/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 350/1251 ( 28%)]  Loss:  3.446378 (3.4505)  Time: 1.095s,  934.79/s  (1.089s,  940.18/s)  LR: 5.154e-04  Data: 0.012 (0.013)
Train: 148 [ 400/1251 ( 32%)]  Loss:  3.491148 (3.4550)  Time: 1.077s,  950.49/s  (1.089s,  940.25/s)  LR: 5.154e-04  Data: 0.013 (0.013)
Train: 148 [ 450/1251 ( 36%)]  Loss:  3.391535 (3.4487)  Time: 1.078s,  950.16/s  (1.089s,  940.56/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [ 500/1251 ( 40%)]  Loss:  3.468883 (3.4505)  Time: 1.078s,  950.24/s  (1.088s,  940.81/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [ 550/1251 ( 44%)]  Loss:  3.557748 (3.4595)  Time: 1.073s,  954.51/s  (1.089s,  940.57/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 148 [ 600/1251 ( 48%)]  Loss:  3.333183 (3.4497)  Time: 1.103s,  928.42/s  (1.089s,  940.00/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 148 [ 650/1251 ( 52%)]  Loss:  3.216422 (3.4331)  Time: 1.094s,  936.34/s  (1.089s,  940.22/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [ 700/1251 ( 56%)]  Loss:  3.148985 (3.4141)  Time: 1.086s,  942.75/s  (1.089s,  940.31/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 148 [ 750/1251 ( 60%)]  Loss:  3.276533 (3.4055)  Time: 1.084s,  944.71/s  (1.089s,  940.63/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [ 800/1251 ( 64%)]  Loss:  3.081334 (3.3865)  Time: 1.077s,  950.74/s  (1.088s,  940.92/s)  LR: 5.154e-04  Data: 0.014 (0.012)
Train: 148 [ 850/1251 ( 68%)]  Loss:  3.881948 (3.4140)  Time: 1.080s,  947.94/s  (1.088s,  941.17/s)  LR: 5.154e-04  Data: 0.011 (0.012)
Train: 148 [ 900/1251 ( 72%)]  Loss:  3.687255 (3.4284)  Time: 1.075s,  952.45/s  (1.088s,  941.02/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [ 950/1251 ( 76%)]  Loss:  3.512254 (3.4326)  Time: 1.087s,  942.36/s  (1.088s,  941.26/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [1000/1251 ( 80%)]  Loss:  3.285467 (3.4256)  Time: 1.084s,  944.84/s  (1.088s,  941.05/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [1050/1251 ( 84%)]  Loss:  3.550937 (3.4313)  Time: 1.076s,  951.56/s  (1.088s,  941.08/s)  LR: 5.154e-04  Data: 0.012 (0.012)
Train: 148 [1100/1251 ( 88%)]  Loss:  3.577785 (3.4376)  Time: 1.102s,  929.36/s  (1.088s,  941.21/s)  LR: 5.154e-04  Data: 0.014 (0.012)
Train: 148 [1150/1251 ( 92%)]  Loss:  3.476237 (3.4392)  Time: 1.077s,  951.13/s  (1.088s,  941.40/s)  LR: 5.154e-04  Data: 0.014 (0.012)
Train: 148 [1200/1251 ( 96%)]  Loss:  3.531873 (3.4429)  Time: 1.078s,  949.66/s  (1.088s,  941.43/s)  LR: 5.154e-04  Data: 0.016 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 148 [1250/1251 (100%)]  Loss:  3.301701 (3.4375)  Time: 1.082s,  946.76/s  (1.088s,  941.52/s)  LR: 5.154e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 6.184 (6.184)  Loss:  0.4851 (0.4851)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6013 (0.9820)  Acc@1: 85.4953 (76.8840)  Acc@5: 97.6415 (93.9240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 76.48000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-141.pth.tar', 76.45600000488281)

Train: 149 [   0/1251 (  0%)]  Loss:  3.348521 (3.3485)  Time: 1.095s,  934.97/s  (1.095s,  934.97/s)  LR: 5.102e-04  Data: 0.029 (0.029)
Train: 149 [  50/1251 (  4%)]  Loss:  3.599944 (3.4742)  Time: 1.097s,  933.43/s  (1.091s,  938.65/s)  LR: 5.102e-04  Data: 0.014 (0.013)
Train: 149 [ 100/1251 (  8%)]  Loss:  3.606850 (3.5184)  Time: 1.093s,  936.55/s  (1.092s,  937.92/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 150/1251 ( 12%)]  Loss:  3.295244 (3.4626)  Time: 1.081s,  947.13/s  (1.091s,  938.84/s)  LR: 5.102e-04  Data: 0.019 (0.013)
Train: 149 [ 200/1251 ( 16%)]  Loss:  3.505167 (3.4711)  Time: 1.078s,  950.34/s  (1.089s,  940.08/s)  LR: 5.102e-04  Data: 0.013 (0.013)
Train: 149 [ 250/1251 ( 20%)]  Loss:  3.235526 (3.4319)  Time: 1.079s,  948.94/s  (1.089s,  940.32/s)  LR: 5.102e-04  Data: 0.011 (0.013)
Train: 149 [ 300/1251 ( 24%)]  Loss:  3.548609 (3.4486)  Time: 1.173s,  873.09/s  (1.089s,  940.60/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 350/1251 ( 28%)]  Loss:  3.522321 (3.4578)  Time: 1.095s,  934.92/s  (1.089s,  939.99/s)  LR: 5.102e-04  Data: 0.015 (0.013)
Train: 149 [ 400/1251 ( 32%)]  Loss:  3.416219 (3.4532)  Time: 1.106s,  925.76/s  (1.089s,  940.11/s)  LR: 5.102e-04  Data: 0.013 (0.013)
Train: 149 [ 450/1251 ( 36%)]  Loss:  3.499263 (3.4578)  Time: 1.100s,  930.87/s  (1.089s,  940.14/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 500/1251 ( 40%)]  Loss:  3.650609 (3.4753)  Time: 1.084s,  944.88/s  (1.089s,  939.98/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 550/1251 ( 44%)]  Loss:  3.271890 (3.4583)  Time: 1.077s,  950.48/s  (1.089s,  940.09/s)  LR: 5.102e-04  Data: 0.015 (0.013)
Train: 149 [ 600/1251 ( 48%)]  Loss:  3.490798 (3.4608)  Time: 1.078s,  949.99/s  (1.089s,  940.33/s)  LR: 5.102e-04  Data: 0.016 (0.013)
Train: 149 [ 650/1251 ( 52%)]  Loss:  3.183367 (3.4410)  Time: 1.096s,  934.07/s  (1.089s,  940.15/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 700/1251 ( 56%)]  Loss:  3.339222 (3.4342)  Time: 1.095s,  934.93/s  (1.089s,  940.16/s)  LR: 5.102e-04  Data: 0.014 (0.013)
Train: 149 [ 750/1251 ( 60%)]  Loss:  3.688834 (3.4501)  Time: 1.079s,  948.93/s  (1.089s,  940.15/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 800/1251 ( 64%)]  Loss:  3.375443 (3.4458)  Time: 1.094s,  936.33/s  (1.089s,  940.36/s)  LR: 5.102e-04  Data: 0.013 (0.013)
Train: 149 [ 850/1251 ( 68%)]  Loss:  3.388091 (3.4426)  Time: 1.095s,  935.45/s  (1.089s,  940.15/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 900/1251 ( 72%)]  Loss:  3.502245 (3.4457)  Time: 1.094s,  935.84/s  (1.089s,  940.51/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [ 950/1251 ( 76%)]  Loss:  3.306914 (3.4388)  Time: 1.076s,  951.47/s  (1.089s,  940.58/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [1000/1251 ( 80%)]  Loss:  3.515636 (3.4424)  Time: 1.081s,  947.45/s  (1.089s,  940.68/s)  LR: 5.102e-04  Data: 0.013 (0.013)
Train: 149 [1050/1251 ( 84%)]  Loss:  3.711028 (3.4546)  Time: 1.097s,  933.64/s  (1.089s,  940.52/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [1100/1251 ( 88%)]  Loss:  3.328884 (3.4492)  Time: 1.075s,  952.38/s  (1.089s,  940.68/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [1150/1251 ( 92%)]  Loss:  3.284868 (3.4423)  Time: 1.094s,  935.59/s  (1.089s,  940.55/s)  LR: 5.102e-04  Data: 0.012 (0.013)
Train: 149 [1200/1251 ( 96%)]  Loss:  3.760368 (3.4550)  Time: 1.103s,  928.57/s  (1.089s,  940.63/s)  LR: 5.102e-04  Data: 0.015 (0.013)
Train: 149 [1250/1251 (100%)]  Loss:  3.146826 (3.4432)  Time: 1.063s,  963.61/s  (1.089s,  940.46/s)  LR: 5.102e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.797 (5.797)  Loss:  0.4883 (0.4883)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6261 (0.9965)  Acc@1: 85.6132 (76.7000)  Acc@5: 97.0519 (93.8400)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 76.70000008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-147.pth.tar', 76.48000000488281)

Train: 150 [   0/1251 (  0%)]  Loss:  3.577354 (3.5774)  Time: 1.092s,  937.87/s  (1.092s,  937.87/s)  LR: 5.050e-04  Data: 0.030 (0.030)
Train: 150 [  50/1251 (  4%)]  Loss:  3.452918 (3.5151)  Time: 1.078s,  949.79/s  (1.096s,  934.01/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 100/1251 (  8%)]  Loss:  3.447122 (3.4925)  Time: 1.093s,  936.89/s  (1.093s,  936.48/s)  LR: 5.050e-04  Data: 0.016 (0.013)
Train: 150 [ 150/1251 ( 12%)]  Loss:  3.482105 (3.4899)  Time: 1.082s,  946.05/s  (1.091s,  938.81/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 200/1251 ( 16%)]  Loss:  3.444149 (3.4807)  Time: 1.076s,  951.64/s  (1.090s,  939.86/s)  LR: 5.050e-04  Data: 0.013 (0.013)
Train: 150 [ 250/1251 ( 20%)]  Loss:  3.230759 (3.4391)  Time: 1.106s,  926.15/s  (1.090s,  939.53/s)  LR: 5.050e-04  Data: 0.015 (0.013)
Train: 150 [ 300/1251 ( 24%)]  Loss:  3.367013 (3.4288)  Time: 1.082s,  946.03/s  (1.090s,  939.81/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 350/1251 ( 28%)]  Loss:  3.343683 (3.4181)  Time: 1.082s,  946.66/s  (1.089s,  940.56/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 400/1251 ( 32%)]  Loss:  3.339331 (3.4094)  Time: 1.079s,  948.62/s  (1.088s,  940.86/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 450/1251 ( 36%)]  Loss:  3.682587 (3.4367)  Time: 1.076s,  951.39/s  (1.088s,  941.21/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 500/1251 ( 40%)]  Loss:  3.611538 (3.4526)  Time: 1.095s,  935.23/s  (1.088s,  941.54/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 550/1251 ( 44%)]  Loss:  3.464285 (3.4536)  Time: 1.096s,  934.07/s  (1.088s,  940.80/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 600/1251 ( 48%)]  Loss:  3.239517 (3.4371)  Time: 1.097s,  933.05/s  (1.089s,  940.67/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 650/1251 ( 52%)]  Loss:  3.292000 (3.4267)  Time: 1.077s,  951.18/s  (1.089s,  940.72/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 700/1251 ( 56%)]  Loss:  3.422902 (3.4265)  Time: 1.095s,  935.50/s  (1.088s,  940.98/s)  LR: 5.050e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 150 [ 750/1251 ( 60%)]  Loss:  3.353097 (3.4219)  Time: 1.077s,  950.84/s  (1.088s,  941.06/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 800/1251 ( 64%)]  Loss:  3.267117 (3.4128)  Time: 1.094s,  935.73/s  (1.089s,  940.70/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 850/1251 ( 68%)]  Loss:  3.388920 (3.4115)  Time: 1.098s,  933.01/s  (1.089s,  940.62/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 900/1251 ( 72%)]  Loss:  3.492688 (3.4157)  Time: 1.105s,  926.57/s  (1.088s,  940.87/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [ 950/1251 ( 76%)]  Loss:  3.259508 (3.4079)  Time: 1.100s,  931.10/s  (1.088s,  941.02/s)  LR: 5.050e-04  Data: 0.014 (0.013)
Train: 150 [1000/1251 ( 80%)]  Loss:  3.634494 (3.4187)  Time: 1.082s,  946.03/s  (1.088s,  940.80/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1050/1251 ( 84%)]  Loss:  3.367124 (3.4164)  Time: 1.096s,  934.66/s  (1.089s,  940.68/s)  LR: 5.050e-04  Data: 0.013 (0.013)
Train: 150 [1100/1251 ( 88%)]  Loss:  3.534758 (3.4215)  Time: 1.107s,  924.84/s  (1.088s,  940.78/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [1150/1251 ( 92%)]  Loss:  3.739154 (3.4348)  Time: 1.106s,  925.65/s  (1.089s,  940.70/s)  LR: 5.050e-04  Data: 0.012 (0.013)
Train: 150 [1200/1251 ( 96%)]  Loss:  3.312877 (3.4299)  Time: 1.095s,  935.55/s  (1.089s,  940.60/s)  LR: 5.050e-04  Data: 0.011 (0.013)
Train: 150 [1250/1251 (100%)]  Loss:  3.227015 (3.4221)  Time: 1.155s,  886.83/s  (1.089s,  940.28/s)  LR: 5.050e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.852 (5.852)  Loss:  0.5117 (0.5117)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6624 (1.0257)  Acc@1: 86.2028 (77.0700)  Acc@5: 96.6981 (93.9820)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 76.70000008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-139.pth.tar', 76.5480000341797)

Train: 151 [   0/1251 (  0%)]  Loss:  3.025328 (3.0253)  Time: 1.088s,  941.13/s  (1.088s,  941.13/s)  LR: 4.998e-04  Data: 0.025 (0.025)
Train: 151 [  50/1251 (  4%)]  Loss:  3.492679 (3.2590)  Time: 1.104s,  927.39/s  (1.082s,  946.20/s)  LR: 4.998e-04  Data: 0.013 (0.013)
Train: 151 [ 100/1251 (  8%)]  Loss:  3.153961 (3.2240)  Time: 1.083s,  945.94/s  (1.087s,  942.30/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 150/1251 ( 12%)]  Loss:  3.212120 (3.2210)  Time: 1.094s,  936.13/s  (1.088s,  941.40/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 200/1251 ( 16%)]  Loss:  3.580453 (3.2929)  Time: 1.082s,  946.68/s  (1.087s,  942.19/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 250/1251 ( 20%)]  Loss:  3.150626 (3.2692)  Time: 1.097s,  933.55/s  (1.088s,  941.58/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 300/1251 ( 24%)]  Loss:  3.494456 (3.3014)  Time: 1.084s,  944.53/s  (1.088s,  940.91/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 350/1251 ( 28%)]  Loss:  3.456003 (3.3207)  Time: 1.076s,  951.42/s  (1.087s,  941.82/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 400/1251 ( 32%)]  Loss:  3.614585 (3.3534)  Time: 1.094s,  935.95/s  (1.087s,  941.67/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 450/1251 ( 36%)]  Loss:  3.496881 (3.3677)  Time: 1.076s,  951.31/s  (1.088s,  941.30/s)  LR: 4.998e-04  Data: 0.014 (0.013)
Train: 151 [ 500/1251 ( 40%)]  Loss:  3.430089 (3.3734)  Time: 1.075s,  952.51/s  (1.087s,  941.82/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 550/1251 ( 44%)]  Loss:  3.488496 (3.3830)  Time: 1.078s,  949.76/s  (1.087s,  941.80/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 600/1251 ( 48%)]  Loss:  3.252876 (3.3730)  Time: 1.075s,  952.89/s  (1.087s,  941.86/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 650/1251 ( 52%)]  Loss:  3.665984 (3.3939)  Time: 1.083s,  945.63/s  (1.087s,  941.81/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 700/1251 ( 56%)]  Loss:  3.447463 (3.3975)  Time: 1.083s,  945.32/s  (1.087s,  942.06/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 750/1251 ( 60%)]  Loss:  3.200462 (3.3852)  Time: 1.093s,  936.44/s  (1.087s,  942.08/s)  LR: 4.998e-04  Data: 0.015 (0.013)
Train: 151 [ 800/1251 ( 64%)]  Loss:  3.198737 (3.3742)  Time: 1.076s,  951.91/s  (1.087s,  941.97/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [ 850/1251 ( 68%)]  Loss:  3.369069 (3.3739)  Time: 1.123s,  911.73/s  (1.087s,  941.64/s)  LR: 4.998e-04  Data: 0.014 (0.013)
Train: 151 [ 900/1251 ( 72%)]  Loss:  3.367289 (3.3736)  Time: 1.084s,  944.42/s  (1.088s,  941.33/s)  LR: 4.998e-04  Data: 0.011 (0.013)
Train: 151 [ 950/1251 ( 76%)]  Loss:  3.557099 (3.3827)  Time: 1.079s,  949.30/s  (1.088s,  941.03/s)  LR: 4.998e-04  Data: 0.015 (0.013)
Train: 151 [1000/1251 ( 80%)]  Loss:  3.262571 (3.3770)  Time: 1.078s,  949.69/s  (1.088s,  941.37/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [1050/1251 ( 84%)]  Loss:  3.615674 (3.3879)  Time: 1.108s,  923.87/s  (1.088s,  941.02/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [1100/1251 ( 88%)]  Loss:  3.424400 (3.3894)  Time: 1.076s,  951.34/s  (1.089s,  940.63/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [1150/1251 ( 92%)]  Loss:  3.452804 (3.3921)  Time: 1.099s,  931.85/s  (1.089s,  940.60/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [1200/1251 ( 96%)]  Loss:  3.550144 (3.3984)  Time: 1.077s,  950.40/s  (1.088s,  940.86/s)  LR: 4.998e-04  Data: 0.012 (0.013)
Train: 151 [1250/1251 (100%)]  Loss:  3.482869 (3.4017)  Time: 1.079s,  949.38/s  (1.088s,  940.81/s)  LR: 4.998e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.828 (5.828)  Loss:  0.4689 (0.4689)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6388 (0.9907)  Acc@1: 85.2594 (77.0400)  Acc@5: 97.4057 (93.9620)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 76.70000008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-138.pth.tar', 76.58999995361329)

Train: 152 [   0/1251 (  0%)]  Loss:  3.143663 (3.1437)  Time: 1.086s,  943.03/s  (1.086s,  943.03/s)  LR: 4.946e-04  Data: 0.026 (0.026)
Train: 152 [  50/1251 (  4%)]  Loss:  3.628979 (3.3863)  Time: 1.082s,  946.01/s  (1.088s,  941.46/s)  LR: 4.946e-04  Data: 0.017 (0.012)
Train: 152 [ 100/1251 (  8%)]  Loss:  3.311579 (3.3614)  Time: 1.079s,  949.38/s  (1.087s,  941.87/s)  LR: 4.946e-04  Data: 0.013 (0.012)
Train: 152 [ 150/1251 ( 12%)]  Loss:  3.428205 (3.3781)  Time: 1.097s,  933.15/s  (1.086s,  942.90/s)  LR: 4.946e-04  Data: 0.011 (0.012)
Train: 152 [ 200/1251 ( 16%)]  Loss:  3.225126 (3.3475)  Time: 1.077s,  951.18/s  (1.086s,  942.53/s)  LR: 4.946e-04  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 152 [ 250/1251 ( 20%)]  Loss:  3.453572 (3.3652)  Time: 1.077s,  950.67/s  (1.087s,  941.80/s)  LR: 4.946e-04  Data: 0.015 (0.012)
Train: 152 [ 300/1251 ( 24%)]  Loss:  3.430058 (3.3745)  Time: 1.077s,  950.52/s  (1.087s,  941.86/s)  LR: 4.946e-04  Data: 0.013 (0.012)
Train: 152 [ 350/1251 ( 28%)]  Loss:  3.451579 (3.3841)  Time: 1.095s,  935.13/s  (1.087s,  941.63/s)  LR: 4.946e-04  Data: 0.012 (0.012)
Train: 152 [ 400/1251 ( 32%)]  Loss:  3.372097 (3.3828)  Time: 1.078s,  949.90/s  (1.088s,  941.36/s)  LR: 4.946e-04  Data: 0.015 (0.013)
Train: 152 [ 450/1251 ( 36%)]  Loss:  3.628505 (3.4073)  Time: 1.096s,  933.92/s  (1.088s,  941.56/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 500/1251 ( 40%)]  Loss:  3.725838 (3.4363)  Time: 1.077s,  950.74/s  (1.088s,  941.49/s)  LR: 4.946e-04  Data: 0.013 (0.013)
Train: 152 [ 550/1251 ( 44%)]  Loss:  3.042061 (3.4034)  Time: 1.171s,  874.49/s  (1.088s,  941.23/s)  LR: 4.946e-04  Data: 0.014 (0.013)
Train: 152 [ 600/1251 ( 48%)]  Loss:  3.537698 (3.4138)  Time: 1.076s,  951.34/s  (1.088s,  941.60/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 650/1251 ( 52%)]  Loss:  3.135586 (3.3939)  Time: 1.076s,  951.41/s  (1.087s,  941.77/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 700/1251 ( 56%)]  Loss:  3.588669 (3.4069)  Time: 1.076s,  951.44/s  (1.087s,  941.70/s)  LR: 4.946e-04  Data: 0.013 (0.013)
Train: 152 [ 750/1251 ( 60%)]  Loss:  3.091659 (3.3872)  Time: 1.083s,  945.61/s  (1.087s,  941.95/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 800/1251 ( 64%)]  Loss:  3.488034 (3.3931)  Time: 1.077s,  950.66/s  (1.087s,  941.80/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 850/1251 ( 68%)]  Loss:  3.411450 (3.3941)  Time: 1.098s,  932.49/s  (1.087s,  941.66/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [ 900/1251 ( 72%)]  Loss:  3.368449 (3.3928)  Time: 1.081s,  947.41/s  (1.087s,  941.74/s)  LR: 4.946e-04  Data: 0.013 (0.013)
Train: 152 [ 950/1251 ( 76%)]  Loss:  3.275356 (3.3869)  Time: 1.078s,  949.57/s  (1.087s,  941.78/s)  LR: 4.946e-04  Data: 0.013 (0.013)
Train: 152 [1000/1251 ( 80%)]  Loss:  3.406695 (3.3879)  Time: 1.076s,  951.27/s  (1.087s,  941.94/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [1050/1251 ( 84%)]  Loss:  3.646889 (3.3996)  Time: 1.077s,  950.80/s  (1.087s,  942.09/s)  LR: 4.946e-04  Data: 0.012 (0.013)
Train: 152 [1100/1251 ( 88%)]  Loss:  3.521352 (3.4049)  Time: 1.077s,  951.11/s  (1.087s,  942.09/s)  LR: 4.946e-04  Data: 0.014 (0.013)
Train: 152 [1150/1251 ( 92%)]  Loss:  3.303915 (3.4007)  Time: 1.078s,  949.58/s  (1.087s,  941.97/s)  LR: 4.946e-04  Data: 0.014 (0.013)
Train: 152 [1200/1251 ( 96%)]  Loss:  3.400925 (3.4007)  Time: 1.079s,  949.16/s  (1.087s,  941.98/s)  LR: 4.946e-04  Data: 0.012 (0.012)
Train: 152 [1250/1251 (100%)]  Loss:  3.660084 (3.4107)  Time: 1.145s,  893.97/s  (1.087s,  941.91/s)  LR: 4.946e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.854 (5.854)  Loss:  0.5053 (0.5053)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.232 (0.447)  Loss:  0.6325 (1.0010)  Acc@1: 85.4953 (76.9980)  Acc@5: 97.0519 (93.9880)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 76.70000008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-145.pth.tar', 76.62400005615234)

Train: 153 [   0/1251 (  0%)]  Loss:  3.560712 (3.5607)  Time: 1.089s,  940.33/s  (1.089s,  940.33/s)  LR: 4.895e-04  Data: 0.027 (0.027)
Train: 153 [  50/1251 (  4%)]  Loss:  3.289957 (3.4253)  Time: 1.081s,  947.03/s  (1.080s,  948.12/s)  LR: 4.895e-04  Data: 0.015 (0.013)
Train: 153 [ 100/1251 (  8%)]  Loss:  3.446977 (3.4325)  Time: 1.193s,  858.48/s  (1.088s,  940.75/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 150/1251 ( 12%)]  Loss:  3.608652 (3.4766)  Time: 1.079s,  949.37/s  (1.090s,  939.44/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 200/1251 ( 16%)]  Loss:  3.251248 (3.4315)  Time: 1.097s,  933.83/s  (1.092s,  938.14/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 250/1251 ( 20%)]  Loss:  3.310324 (3.4113)  Time: 1.078s,  949.97/s  (1.091s,  938.57/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 300/1251 ( 24%)]  Loss:  3.305055 (3.3961)  Time: 1.082s,  946.65/s  (1.091s,  938.47/s)  LR: 4.895e-04  Data: 0.016 (0.013)
Train: 153 [ 350/1251 ( 28%)]  Loss:  3.109487 (3.3603)  Time: 1.073s,  953.97/s  (1.089s,  940.09/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 400/1251 ( 32%)]  Loss:  3.334751 (3.3575)  Time: 1.077s,  951.05/s  (1.089s,  940.17/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 450/1251 ( 36%)]  Loss:  3.199263 (3.3416)  Time: 1.076s,  951.79/s  (1.089s,  940.46/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Train: 153 [ 500/1251 ( 40%)]  Loss:  3.289009 (3.3369)  Time: 1.094s,  936.21/s  (1.089s,  940.56/s)  LR: 4.895e-04  Data: 0.013 (0.013)
Train: 153 [ 550/1251 ( 44%)]  Loss:  2.891752 (3.2998)  Time: 1.088s,  941.49/s  (1.089s,  940.70/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 600/1251 ( 48%)]  Loss:  3.389227 (3.3066)  Time: 1.095s,  934.86/s  (1.089s,  940.45/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 650/1251 ( 52%)]  Loss:  3.189128 (3.2983)  Time: 1.078s,  950.13/s  (1.089s,  940.18/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 700/1251 ( 56%)]  Loss:  3.600709 (3.3184)  Time: 1.077s,  950.41/s  (1.089s,  940.30/s)  LR: 4.895e-04  Data: 0.014 (0.013)
Train: 153 [ 750/1251 ( 60%)]  Loss:  3.180941 (3.3098)  Time: 1.078s,  949.68/s  (1.089s,  940.44/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 800/1251 ( 64%)]  Loss:  3.273906 (3.3077)  Time: 1.079s,  948.80/s  (1.089s,  940.04/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 850/1251 ( 68%)]  Loss:  3.021043 (3.2918)  Time: 1.095s,  935.21/s  (1.089s,  940.05/s)  LR: 4.895e-04  Data: 0.012 (0.013)
Train: 153 [ 900/1251 ( 72%)]  Loss:  3.422643 (3.2987)  Time: 1.107s,  925.21/s  (1.089s,  939.99/s)  LR: 4.895e-04  Data: 0.013 (0.013)
Train: 153 [ 950/1251 ( 76%)]  Loss:  3.445357 (3.3060)  Time: 1.096s,  934.68/s  (1.090s,  939.71/s)  LR: 4.895e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 153 [1000/1251 ( 80%)]  Loss:  3.144858 (3.2983)  Time: 1.080s,  948.04/s  (1.090s,  939.46/s)  LR: 4.895e-04  Data: 0.011 (0.012)
Train: 153 [1050/1251 ( 84%)]  Loss:  3.427882 (3.3042)  Time: 1.094s,  935.88/s  (1.090s,  939.40/s)  LR: 4.895e-04  Data: 0.012 (0.012)
Train: 153 [1100/1251 ( 88%)]  Loss:  3.630400 (3.3184)  Time: 1.079s,  948.76/s  (1.090s,  939.46/s)  LR: 4.895e-04  Data: 0.015 (0.012)
Train: 153 [1150/1251 ( 92%)]  Loss:  3.681353 (3.3335)  Time: 1.099s,  931.85/s  (1.090s,  939.56/s)  LR: 4.895e-04  Data: 0.011 (0.012)
Train: 153 [1200/1251 ( 96%)]  Loss:  3.430009 (3.3374)  Time: 1.093s,  937.16/s  (1.090s,  939.58/s)  LR: 4.895e-04  Data: 0.014 (0.012)
Train: 153 [1250/1251 (100%)]  Loss:  3.541529 (3.3452)  Time: 1.062s,  964.24/s  (1.090s,  939.64/s)  LR: 4.895e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.963 (5.963)  Loss:  0.4770 (0.4770)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6213 (0.9985)  Acc@1: 85.4953 (77.2000)  Acc@5: 96.9340 (93.9540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 76.70000008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-143.pth.tar', 76.64600000732422)

Train: 154 [   0/1251 (  0%)]  Loss:  3.142019 (3.1420)  Time: 1.090s,  939.69/s  (1.090s,  939.69/s)  LR: 4.843e-04  Data: 0.027 (0.027)
Train: 154 [  50/1251 (  4%)]  Loss:  3.176861 (3.1594)  Time: 1.094s,  935.90/s  (1.100s,  931.02/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 100/1251 (  8%)]  Loss:  3.707928 (3.3423)  Time: 1.076s,  951.36/s  (1.098s,  932.97/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 150/1251 ( 12%)]  Loss:  3.263579 (3.3226)  Time: 1.123s,  911.75/s  (1.094s,  935.88/s)  LR: 4.843e-04  Data: 0.016 (0.013)
Train: 154 [ 200/1251 ( 16%)]  Loss:  3.434187 (3.3449)  Time: 1.076s,  951.58/s  (1.093s,  937.25/s)  LR: 4.843e-04  Data: 0.012 (0.012)
Train: 154 [ 250/1251 ( 20%)]  Loss:  3.372063 (3.3494)  Time: 1.099s,  931.66/s  (1.091s,  938.29/s)  LR: 4.843e-04  Data: 0.012 (0.012)
Train: 154 [ 300/1251 ( 24%)]  Loss:  3.490071 (3.3695)  Time: 1.076s,  951.99/s  (1.091s,  938.75/s)  LR: 4.843e-04  Data: 0.013 (0.012)
Train: 154 [ 350/1251 ( 28%)]  Loss:  3.195833 (3.3478)  Time: 1.081s,  947.14/s  (1.090s,  939.57/s)  LR: 4.843e-04  Data: 0.013 (0.013)
Train: 154 [ 400/1251 ( 32%)]  Loss:  3.290928 (3.3415)  Time: 1.101s,  930.31/s  (1.091s,  938.93/s)  LR: 4.843e-04  Data: 0.012 (0.012)
Train: 154 [ 450/1251 ( 36%)]  Loss:  3.349751 (3.3423)  Time: 1.084s,  944.47/s  (1.092s,  938.09/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 500/1251 ( 40%)]  Loss:  3.642753 (3.3696)  Time: 1.095s,  934.85/s  (1.092s,  937.38/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 550/1251 ( 44%)]  Loss:  3.179171 (3.3538)  Time: 1.096s,  934.01/s  (1.093s,  937.03/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 600/1251 ( 48%)]  Loss:  3.506747 (3.3655)  Time: 1.080s,  947.82/s  (1.092s,  937.51/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 650/1251 ( 52%)]  Loss:  3.133955 (3.3490)  Time: 1.078s,  949.86/s  (1.092s,  937.68/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 700/1251 ( 56%)]  Loss:  3.551383 (3.3625)  Time: 1.077s,  951.04/s  (1.092s,  937.72/s)  LR: 4.843e-04  Data: 0.011 (0.013)
Train: 154 [ 750/1251 ( 60%)]  Loss:  3.503417 (3.3713)  Time: 1.079s,  949.10/s  (1.092s,  937.78/s)  LR: 4.843e-04  Data: 0.015 (0.013)
Train: 154 [ 800/1251 ( 64%)]  Loss:  3.391460 (3.3725)  Time: 1.086s,  943.08/s  (1.092s,  937.88/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 850/1251 ( 68%)]  Loss:  3.371134 (3.3724)  Time: 1.099s,  932.18/s  (1.092s,  938.04/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 900/1251 ( 72%)]  Loss:  3.385484 (3.3731)  Time: 1.096s,  934.53/s  (1.092s,  937.84/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [ 950/1251 ( 76%)]  Loss:  3.612375 (3.3851)  Time: 1.077s,  951.08/s  (1.091s,  938.26/s)  LR: 4.843e-04  Data: 0.013 (0.013)
Train: 154 [1000/1251 ( 80%)]  Loss:  3.466829 (3.3889)  Time: 1.077s,  951.15/s  (1.091s,  938.34/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1050/1251 ( 84%)]  Loss:  3.003670 (3.3714)  Time: 1.078s,  949.51/s  (1.091s,  938.36/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1100/1251 ( 88%)]  Loss:  3.482552 (3.3763)  Time: 1.078s,  950.21/s  (1.091s,  938.59/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1150/1251 ( 92%)]  Loss:  3.516973 (3.3821)  Time: 1.079s,  949.44/s  (1.091s,  938.61/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1200/1251 ( 96%)]  Loss:  3.566271 (3.3895)  Time: 1.076s,  951.53/s  (1.091s,  938.93/s)  LR: 4.843e-04  Data: 0.012 (0.013)
Train: 154 [1250/1251 (100%)]  Loss:  3.293764 (3.3858)  Time: 1.061s,  964.82/s  (1.090s,  939.13/s)  LR: 4.843e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.811 (5.811)  Loss:  0.5040 (0.5040)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.6580 (0.9999)  Acc@1: 85.3774 (77.0780)  Acc@5: 97.1698 (93.9440)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-149.pth.tar', 76.70000008544922)

Train: 155 [   0/1251 (  0%)]  Loss:  3.448101 (3.4481)  Time: 1.085s,  943.60/s  (1.085s,  943.60/s)  LR: 4.791e-04  Data: 0.025 (0.025)
Train: 155 [  50/1251 (  4%)]  Loss:  3.442233 (3.4452)  Time: 1.077s,  951.16/s  (1.083s,  945.87/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 100/1251 (  8%)]  Loss:  3.491774 (3.4607)  Time: 1.073s,  954.41/s  (1.084s,  944.52/s)  LR: 4.791e-04  Data: 0.011 (0.012)
Train: 155 [ 150/1251 ( 12%)]  Loss:  3.418815 (3.4502)  Time: 1.099s,  932.09/s  (1.085s,  944.07/s)  LR: 4.791e-04  Data: 0.013 (0.012)
Train: 155 [ 200/1251 ( 16%)]  Loss:  3.578025 (3.4758)  Time: 1.078s,  950.01/s  (1.085s,  944.18/s)  LR: 4.791e-04  Data: 0.013 (0.012)
Train: 155 [ 250/1251 ( 20%)]  Loss:  3.083117 (3.4103)  Time: 1.095s,  934.74/s  (1.087s,  942.31/s)  LR: 4.791e-04  Data: 0.017 (0.012)
Train: 155 [ 300/1251 ( 24%)]  Loss:  3.490875 (3.4218)  Time: 1.079s,  949.24/s  (1.088s,  941.14/s)  LR: 4.791e-04  Data: 0.013 (0.013)
Train: 155 [ 350/1251 ( 28%)]  Loss:  3.314194 (3.4084)  Time: 1.076s,  951.80/s  (1.087s,  941.88/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 400/1251 ( 32%)]  Loss:  3.403292 (3.4078)  Time: 1.083s,  945.91/s  (1.087s,  941.87/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 450/1251 ( 36%)]  Loss:  3.521948 (3.4192)  Time: 1.076s,  951.35/s  (1.088s,  941.23/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 500/1251 ( 40%)]  Loss:  3.409299 (3.4183)  Time: 1.079s,  948.84/s  (1.087s,  941.67/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 155 [ 550/1251 ( 44%)]  Loss:  3.231286 (3.4027)  Time: 1.075s,  952.55/s  (1.088s,  941.60/s)  LR: 4.791e-04  Data: 0.013 (0.013)
Train: 155 [ 600/1251 ( 48%)]  Loss:  3.170210 (3.3849)  Time: 1.077s,  950.60/s  (1.087s,  942.12/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 650/1251 ( 52%)]  Loss:  3.440487 (3.3888)  Time: 1.099s,  931.88/s  (1.087s,  941.99/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 700/1251 ( 56%)]  Loss:  3.396914 (3.3894)  Time: 1.077s,  950.44/s  (1.087s,  941.92/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 750/1251 ( 60%)]  Loss:  3.516826 (3.3973)  Time: 1.093s,  936.58/s  (1.087s,  941.64/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 800/1251 ( 64%)]  Loss:  3.372006 (3.3958)  Time: 1.079s,  948.70/s  (1.087s,  941.78/s)  LR: 4.791e-04  Data: 0.018 (0.013)
Train: 155 [ 850/1251 ( 68%)]  Loss:  3.308285 (3.3910)  Time: 1.076s,  951.74/s  (1.087s,  941.85/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 900/1251 ( 72%)]  Loss:  3.470705 (3.3952)  Time: 1.078s,  950.02/s  (1.087s,  942.00/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [ 950/1251 ( 76%)]  Loss:  3.175653 (3.3842)  Time: 1.103s,  928.36/s  (1.087s,  941.92/s)  LR: 4.791e-04  Data: 0.013 (0.013)
Train: 155 [1000/1251 ( 80%)]  Loss:  3.281343 (3.3793)  Time: 1.075s,  952.66/s  (1.087s,  941.92/s)  LR: 4.791e-04  Data: 0.013 (0.013)
Train: 155 [1050/1251 ( 84%)]  Loss:  3.310357 (3.3762)  Time: 1.079s,  949.22/s  (1.087s,  941.93/s)  LR: 4.791e-04  Data: 0.011 (0.013)
Train: 155 [1100/1251 ( 88%)]  Loss:  3.519330 (3.3824)  Time: 1.093s,  936.50/s  (1.087s,  941.94/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [1150/1251 ( 92%)]  Loss:  3.158094 (3.3730)  Time: 1.079s,  949.16/s  (1.087s,  941.68/s)  LR: 4.791e-04  Data: 0.017 (0.013)
Train: 155 [1200/1251 ( 96%)]  Loss:  3.633391 (3.3835)  Time: 1.076s,  951.97/s  (1.087s,  941.79/s)  LR: 4.791e-04  Data: 0.012 (0.013)
Train: 155 [1250/1251 (100%)]  Loss:  3.458099 (3.3863)  Time: 1.062s,  964.67/s  (1.087s,  941.77/s)  LR: 4.791e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.784 (5.784)  Loss:  0.5242 (0.5242)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6345 (1.0081)  Acc@1: 84.6698 (77.0240)  Acc@5: 96.4623 (93.8800)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 77.02399993408203)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-142.pth.tar', 76.72400005859375)

Train: 156 [   0/1251 (  0%)]  Loss:  3.408948 (3.4089)  Time: 1.086s,  943.11/s  (1.086s,  943.11/s)  LR: 4.739e-04  Data: 0.025 (0.025)
Train: 156 [  50/1251 (  4%)]  Loss:  3.267241 (3.3381)  Time: 1.086s,  943.29/s  (1.085s,  943.59/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 100/1251 (  8%)]  Loss:  3.257202 (3.3111)  Time: 1.077s,  951.15/s  (1.091s,  938.45/s)  LR: 4.739e-04  Data: 0.014 (0.013)
Train: 156 [ 150/1251 ( 12%)]  Loss:  3.318772 (3.3130)  Time: 1.096s,  934.67/s  (1.092s,  937.88/s)  LR: 4.739e-04  Data: 0.014 (0.013)
Train: 156 [ 200/1251 ( 16%)]  Loss:  3.440745 (3.3386)  Time: 1.079s,  949.22/s  (1.090s,  939.47/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 250/1251 ( 20%)]  Loss:  3.166445 (3.3099)  Time: 1.079s,  949.25/s  (1.089s,  940.53/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 300/1251 ( 24%)]  Loss:  3.351690 (3.3159)  Time: 1.093s,  936.63/s  (1.089s,  940.45/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 350/1251 ( 28%)]  Loss:  3.205200 (3.3020)  Time: 1.105s,  927.00/s  (1.090s,  939.42/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 400/1251 ( 32%)]  Loss:  3.092788 (3.2788)  Time: 1.103s,  927.97/s  (1.090s,  939.83/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 450/1251 ( 36%)]  Loss:  3.015537 (3.2525)  Time: 1.077s,  951.14/s  (1.090s,  939.44/s)  LR: 4.739e-04  Data: 0.014 (0.013)
Train: 156 [ 500/1251 ( 40%)]  Loss:  3.644153 (3.2881)  Time: 1.081s,  946.88/s  (1.090s,  939.46/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 550/1251 ( 44%)]  Loss:  3.411040 (3.2983)  Time: 1.084s,  944.31/s  (1.090s,  939.78/s)  LR: 4.739e-04  Data: 0.017 (0.013)
Train: 156 [ 600/1251 ( 48%)]  Loss:  3.380061 (3.3046)  Time: 1.084s,  944.48/s  (1.090s,  939.75/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 650/1251 ( 52%)]  Loss:  3.148018 (3.2934)  Time: 1.103s,  928.34/s  (1.090s,  939.55/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [ 700/1251 ( 56%)]  Loss:  3.754999 (3.3242)  Time: 1.093s,  937.08/s  (1.090s,  939.22/s)  LR: 4.739e-04  Data: 0.011 (0.013)
Train: 156 [ 750/1251 ( 60%)]  Loss:  3.151121 (3.3134)  Time: 1.076s,  951.91/s  (1.090s,  939.37/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 800/1251 ( 64%)]  Loss:  3.424786 (3.3199)  Time: 1.078s,  950.15/s  (1.090s,  939.41/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 850/1251 ( 68%)]  Loss:  3.392269 (3.3239)  Time: 1.076s,  952.09/s  (1.089s,  939.99/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 900/1251 ( 72%)]  Loss:  3.112642 (3.3128)  Time: 1.086s,  943.18/s  (1.089s,  940.31/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [ 950/1251 ( 76%)]  Loss:  3.367516 (3.3156)  Time: 1.077s,  951.16/s  (1.089s,  940.22/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [1000/1251 ( 80%)]  Loss:  2.921439 (3.2968)  Time: 1.092s,  937.62/s  (1.089s,  940.20/s)  LR: 4.739e-04  Data: 0.013 (0.013)
Train: 156 [1050/1251 ( 84%)]  Loss:  3.516029 (3.3068)  Time: 1.099s,  932.18/s  (1.089s,  940.15/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [1100/1251 ( 88%)]  Loss:  3.509629 (3.3156)  Time: 1.081s,  946.92/s  (1.089s,  940.36/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [1150/1251 ( 92%)]  Loss:  3.141489 (3.3083)  Time: 1.095s,  935.57/s  (1.089s,  940.46/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [1200/1251 ( 96%)]  Loss:  3.209195 (3.3044)  Time: 1.080s,  947.97/s  (1.089s,  940.30/s)  LR: 4.739e-04  Data: 0.012 (0.013)
Train: 156 [1250/1251 (100%)]  Loss:  3.296606 (3.3041)  Time: 1.062s,  964.12/s  (1.089s,  940.45/s)  LR: 4.739e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.994 (5.994)  Loss:  0.5161 (0.5161)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6323 (1.0067)  Acc@1: 85.6132 (77.2340)  Acc@5: 97.1698 (93.9000)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 77.02399993408203)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-146.pth.tar', 76.7539999584961)

Train: 157 [   0/1251 (  0%)]  Loss:  3.378439 (3.3784)  Time: 1.089s,  940.50/s  (1.089s,  940.50/s)  LR: 4.687e-04  Data: 0.027 (0.027)
Train: 157 [  50/1251 (  4%)]  Loss:  3.309526 (3.3440)  Time: 1.096s,  934.67/s  (1.087s,  941.72/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 157 [ 100/1251 (  8%)]  Loss:  3.111687 (3.2666)  Time: 1.076s,  951.81/s  (1.088s,  941.17/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 150/1251 ( 12%)]  Loss:  3.286083 (3.2714)  Time: 1.094s,  936.05/s  (1.089s,  940.19/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 200/1251 ( 16%)]  Loss:  3.193956 (3.2559)  Time: 1.078s,  950.06/s  (1.090s,  939.50/s)  LR: 4.687e-04  Data: 0.013 (0.013)
Train: 157 [ 250/1251 ( 20%)]  Loss:  3.546298 (3.3043)  Time: 1.089s,  940.37/s  (1.090s,  939.22/s)  LR: 4.687e-04  Data: 0.016 (0.013)
Train: 157 [ 300/1251 ( 24%)]  Loss:  3.537320 (3.3376)  Time: 1.100s,  931.31/s  (1.089s,  940.07/s)  LR: 4.687e-04  Data: 0.019 (0.013)
Train: 157 [ 350/1251 ( 28%)]  Loss:  3.451006 (3.3518)  Time: 1.082s,  946.19/s  (1.090s,  939.48/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 400/1251 ( 32%)]  Loss:  3.281007 (3.3439)  Time: 1.170s,  874.87/s  (1.090s,  939.17/s)  LR: 4.687e-04  Data: 0.013 (0.013)
Train: 157 [ 450/1251 ( 36%)]  Loss:  3.521540 (3.3617)  Time: 1.079s,  949.22/s  (1.090s,  939.79/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 500/1251 ( 40%)]  Loss:  3.435551 (3.3684)  Time: 1.077s,  950.40/s  (1.089s,  940.02/s)  LR: 4.687e-04  Data: 0.014 (0.013)
Train: 157 [ 550/1251 ( 44%)]  Loss:  3.399656 (3.3710)  Time: 1.083s,  945.42/s  (1.089s,  940.56/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 600/1251 ( 48%)]  Loss:  3.337188 (3.3684)  Time: 1.076s,  951.28/s  (1.089s,  940.60/s)  LR: 4.687e-04  Data: 0.015 (0.013)
Train: 157 [ 650/1251 ( 52%)]  Loss:  3.292901 (3.3630)  Time: 1.075s,  952.76/s  (1.089s,  940.67/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 700/1251 ( 56%)]  Loss:  3.351148 (3.3622)  Time: 1.094s,  936.38/s  (1.089s,  940.29/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 750/1251 ( 60%)]  Loss:  2.898810 (3.3333)  Time: 1.074s,  953.56/s  (1.089s,  940.33/s)  LR: 4.687e-04  Data: 0.011 (0.013)
Train: 157 [ 800/1251 ( 64%)]  Loss:  3.127211 (3.3211)  Time: 1.095s,  935.34/s  (1.089s,  940.66/s)  LR: 4.687e-04  Data: 0.014 (0.013)
Train: 157 [ 850/1251 ( 68%)]  Loss:  3.352648 (3.3229)  Time: 1.083s,  945.75/s  (1.089s,  940.72/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 900/1251 ( 72%)]  Loss:  3.385919 (3.3262)  Time: 1.078s,  950.18/s  (1.089s,  940.41/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [ 950/1251 ( 76%)]  Loss:  3.420046 (3.3309)  Time: 1.095s,  934.74/s  (1.089s,  940.12/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [1000/1251 ( 80%)]  Loss:  3.570779 (3.3423)  Time: 1.095s,  934.90/s  (1.090s,  939.81/s)  LR: 4.687e-04  Data: 0.013 (0.013)
Train: 157 [1050/1251 ( 84%)]  Loss:  3.302001 (3.3405)  Time: 1.096s,  934.69/s  (1.090s,  939.54/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [1100/1251 ( 88%)]  Loss:  3.492132 (3.3471)  Time: 1.171s,  874.68/s  (1.090s,  939.19/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [1150/1251 ( 92%)]  Loss:  3.347715 (3.3471)  Time: 1.096s,  934.35/s  (1.090s,  939.13/s)  LR: 4.687e-04  Data: 0.012 (0.013)
Train: 157 [1200/1251 ( 96%)]  Loss:  3.553204 (3.3554)  Time: 1.097s,  933.83/s  (1.090s,  939.16/s)  LR: 4.687e-04  Data: 0.014 (0.013)
Train: 157 [1250/1251 (100%)]  Loss:  3.742878 (3.3703)  Time: 1.062s,  964.20/s  (1.090s,  939.19/s)  LR: 4.687e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.833 (5.833)  Loss:  0.5415 (0.5415)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6643 (1.0134)  Acc@1: 86.2028 (77.3000)  Acc@5: 97.4057 (93.9140)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 77.02399993408203)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-144.pth.tar', 76.77999992675781)

Train: 158 [   0/1251 (  0%)]  Loss:  3.564343 (3.5643)  Time: 1.096s,  934.05/s  (1.096s,  934.05/s)  LR: 4.636e-04  Data: 0.034 (0.034)
Train: 158 [  50/1251 (  4%)]  Loss:  3.304395 (3.4344)  Time: 1.076s,  951.62/s  (1.082s,  946.35/s)  LR: 4.636e-04  Data: 0.013 (0.013)
Train: 158 [ 100/1251 (  8%)]  Loss:  3.149076 (3.3393)  Time: 1.101s,  930.20/s  (1.085s,  943.85/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 150/1251 ( 12%)]  Loss:  3.027440 (3.2613)  Time: 1.085s,  943.97/s  (1.089s,  940.23/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 200/1251 ( 16%)]  Loss:  3.428789 (3.2948)  Time: 1.080s,  948.09/s  (1.089s,  940.53/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 250/1251 ( 20%)]  Loss:  3.246951 (3.2868)  Time: 1.095s,  935.07/s  (1.090s,  939.74/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 300/1251 ( 24%)]  Loss:  3.378325 (3.2999)  Time: 1.076s,  951.65/s  (1.089s,  940.25/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 350/1251 ( 28%)]  Loss:  2.912294 (3.2515)  Time: 1.077s,  950.48/s  (1.089s,  940.37/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 400/1251 ( 32%)]  Loss:  3.197526 (3.2455)  Time: 1.104s,  927.32/s  (1.089s,  940.23/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 450/1251 ( 36%)]  Loss:  3.273734 (3.2483)  Time: 1.083s,  945.68/s  (1.089s,  940.29/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 500/1251 ( 40%)]  Loss:  3.057933 (3.2310)  Time: 1.077s,  950.54/s  (1.089s,  940.35/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 550/1251 ( 44%)]  Loss:  3.581298 (3.2602)  Time: 1.094s,  935.90/s  (1.090s,  939.86/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 600/1251 ( 48%)]  Loss:  3.251376 (3.2595)  Time: 1.092s,  937.36/s  (1.089s,  940.04/s)  LR: 4.636e-04  Data: 0.011 (0.013)
Train: 158 [ 650/1251 ( 52%)]  Loss:  3.224186 (3.2570)  Time: 1.077s,  950.98/s  (1.089s,  939.94/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 700/1251 ( 56%)]  Loss:  3.415211 (3.2675)  Time: 1.076s,  951.35/s  (1.089s,  940.24/s)  LR: 4.636e-04  Data: 0.013 (0.013)
Train: 158 [ 750/1251 ( 60%)]  Loss:  3.645011 (3.2911)  Time: 1.095s,  934.85/s  (1.090s,  939.80/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 800/1251 ( 64%)]  Loss:  3.426516 (3.2991)  Time: 1.101s,  929.99/s  (1.090s,  939.36/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [ 850/1251 ( 68%)]  Loss:  3.436228 (3.3067)  Time: 1.091s,  938.17/s  (1.091s,  938.78/s)  LR: 4.636e-04  Data: 0.013 (0.013)
Train: 158 [ 900/1251 ( 72%)]  Loss:  3.070469 (3.2943)  Time: 1.172s,  874.07/s  (1.091s,  938.74/s)  LR: 4.636e-04  Data: 0.015 (0.013)
Train: 158 [ 950/1251 ( 76%)]  Loss:  3.567844 (3.3079)  Time: 1.093s,  936.59/s  (1.091s,  938.99/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Train: 158 [1000/1251 ( 80%)]  Loss:  2.947423 (3.2908)  Time: 1.078s,  950.07/s  (1.090s,  939.08/s)  LR: 4.636e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 158 [1050/1251 ( 84%)]  Loss:  3.330304 (3.2926)  Time: 1.077s,  950.59/s  (1.090s,  939.14/s)  LR: 4.636e-04  Data: 0.015 (0.013)
Train: 158 [1100/1251 ( 88%)]  Loss:  3.011352 (3.2803)  Time: 1.076s,  951.98/s  (1.090s,  939.14/s)  LR: 4.636e-04  Data: 0.013 (0.013)
Train: 158 [1150/1251 ( 92%)]  Loss:  3.398471 (3.2853)  Time: 1.090s,  939.56/s  (1.090s,  939.06/s)  LR: 4.636e-04  Data: 0.015 (0.013)
Train: 158 [1200/1251 ( 96%)]  Loss:  3.155748 (3.2801)  Time: 1.083s,  945.92/s  (1.090s,  939.26/s)  LR: 4.636e-04  Data: 0.014 (0.013)
Train: 158 [1250/1251 (100%)]  Loss:  3.530753 (3.2897)  Time: 1.080s,  948.52/s  (1.090s,  939.38/s)  LR: 4.636e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.871 (5.871)  Loss:  0.4886 (0.4886)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6875 (0.9892)  Acc@1: 85.0236 (77.4960)  Acc@5: 96.8160 (94.0580)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 77.02399993408203)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-148.pth.tar', 76.88400016357421)

Train: 159 [   0/1251 (  0%)]  Loss:  3.228460 (3.2285)  Time: 1.090s,  939.34/s  (1.090s,  939.34/s)  LR: 4.584e-04  Data: 0.027 (0.027)
Train: 159 [  50/1251 (  4%)]  Loss:  3.400073 (3.3143)  Time: 1.084s,  944.24/s  (1.088s,  941.56/s)  LR: 4.584e-04  Data: 0.013 (0.013)
Train: 159 [ 100/1251 (  8%)]  Loss:  3.361811 (3.3301)  Time: 1.080s,  947.84/s  (1.088s,  941.24/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 150/1251 ( 12%)]  Loss:  3.364377 (3.3387)  Time: 1.107s,  925.26/s  (1.088s,  941.17/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 200/1251 ( 16%)]  Loss:  3.235447 (3.3180)  Time: 1.077s,  950.94/s  (1.088s,  940.82/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 250/1251 ( 20%)]  Loss:  3.422025 (3.3354)  Time: 1.079s,  948.61/s  (1.089s,  939.93/s)  LR: 4.584e-04  Data: 0.014 (0.013)
Train: 159 [ 300/1251 ( 24%)]  Loss:  3.325833 (3.3340)  Time: 1.096s,  934.69/s  (1.089s,  940.24/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 350/1251 ( 28%)]  Loss:  3.093081 (3.3039)  Time: 1.080s,  948.02/s  (1.089s,  940.41/s)  LR: 4.584e-04  Data: 0.018 (0.013)
Train: 159 [ 400/1251 ( 32%)]  Loss:  3.359611 (3.3101)  Time: 1.080s,  947.71/s  (1.089s,  940.73/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 450/1251 ( 36%)]  Loss:  3.091305 (3.2882)  Time: 1.077s,  950.64/s  (1.089s,  940.66/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 500/1251 ( 40%)]  Loss:  3.394495 (3.2979)  Time: 1.104s,  927.35/s  (1.088s,  941.07/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 550/1251 ( 44%)]  Loss:  3.389972 (3.3055)  Time: 1.077s,  950.81/s  (1.088s,  941.19/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 600/1251 ( 48%)]  Loss:  3.359675 (3.3097)  Time: 1.098s,  932.76/s  (1.088s,  941.52/s)  LR: 4.584e-04  Data: 0.011 (0.013)
Train: 159 [ 650/1251 ( 52%)]  Loss:  3.336757 (3.3116)  Time: 1.076s,  951.41/s  (1.088s,  941.00/s)  LR: 4.584e-04  Data: 0.015 (0.013)
Train: 159 [ 700/1251 ( 56%)]  Loss:  3.498893 (3.3241)  Time: 1.079s,  948.67/s  (1.088s,  941.32/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 750/1251 ( 60%)]  Loss:  3.334310 (3.3248)  Time: 1.096s,  934.11/s  (1.088s,  941.35/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 800/1251 ( 64%)]  Loss:  3.319879 (3.3245)  Time: 1.095s,  934.96/s  (1.089s,  940.70/s)  LR: 4.584e-04  Data: 0.013 (0.013)
Train: 159 [ 850/1251 ( 68%)]  Loss:  3.537637 (3.3363)  Time: 1.077s,  950.72/s  (1.089s,  940.27/s)  LR: 4.584e-04  Data: 0.013 (0.013)
Train: 159 [ 900/1251 ( 72%)]  Loss:  3.249317 (3.3317)  Time: 1.081s,  947.05/s  (1.089s,  940.54/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [ 950/1251 ( 76%)]  Loss:  3.264309 (3.3284)  Time: 1.081s,  946.89/s  (1.088s,  940.80/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [1000/1251 ( 80%)]  Loss:  3.320153 (3.3280)  Time: 1.084s,  944.94/s  (1.089s,  940.62/s)  LR: 4.584e-04  Data: 0.013 (0.013)
Train: 159 [1050/1251 ( 84%)]  Loss:  3.735302 (3.3465)  Time: 1.105s,  926.66/s  (1.089s,  940.45/s)  LR: 4.584e-04  Data: 0.014 (0.013)
Train: 159 [1100/1251 ( 88%)]  Loss:  3.368441 (3.3474)  Time: 1.079s,  948.59/s  (1.089s,  940.52/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [1150/1251 ( 92%)]  Loss:  3.735561 (3.3636)  Time: 1.105s,  926.36/s  (1.089s,  940.38/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [1200/1251 ( 96%)]  Loss:  3.100204 (3.3531)  Time: 1.094s,  936.18/s  (1.089s,  940.27/s)  LR: 4.584e-04  Data: 0.012 (0.013)
Train: 159 [1250/1251 (100%)]  Loss:  3.378493 (3.3541)  Time: 1.067s,  959.80/s  (1.089s,  940.15/s)  LR: 4.584e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.903 (5.903)  Loss:  0.5068 (0.5068)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6210 (0.9922)  Acc@1: 86.4387 (77.5200)  Acc@5: 96.9340 (94.1000)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 77.02399993408203)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-152.pth.tar', 76.99800003417968)

Train: 160 [   0/1251 (  0%)]  Loss:  2.995814 (2.9958)  Time: 1.094s,  936.35/s  (1.094s,  936.35/s)  LR: 4.533e-04  Data: 0.025 (0.025)
Train: 160 [  50/1251 (  4%)]  Loss:  3.251198 (3.1235)  Time: 1.075s,  952.34/s  (1.093s,  936.47/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 100/1251 (  8%)]  Loss:  3.415375 (3.2208)  Time: 1.114s,  919.62/s  (1.093s,  936.88/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 150/1251 ( 12%)]  Loss:  3.460113 (3.2806)  Time: 1.079s,  949.12/s  (1.093s,  936.98/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 200/1251 ( 16%)]  Loss:  3.427212 (3.3099)  Time: 1.081s,  947.18/s  (1.091s,  938.65/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 250/1251 ( 20%)]  Loss:  3.409656 (3.3266)  Time: 1.093s,  936.55/s  (1.091s,  938.33/s)  LR: 4.533e-04  Data: 0.014 (0.013)
Train: 160 [ 300/1251 ( 24%)]  Loss:  3.146951 (3.3009)  Time: 1.094s,  935.88/s  (1.092s,  937.75/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 350/1251 ( 28%)]  Loss:  3.218713 (3.2906)  Time: 1.097s,  933.05/s  (1.091s,  938.25/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 400/1251 ( 32%)]  Loss:  3.154344 (3.2755)  Time: 1.094s,  936.18/s  (1.092s,  937.96/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 450/1251 ( 36%)]  Loss:  3.384863 (3.2864)  Time: 1.093s,  936.70/s  (1.092s,  937.57/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 500/1251 ( 40%)]  Loss:  3.288880 (3.2866)  Time: 1.079s,  948.92/s  (1.092s,  937.54/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 550/1251 ( 44%)]  Loss:  3.368583 (3.2935)  Time: 1.086s,  943.20/s  (1.092s,  937.74/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 600/1251 ( 48%)]  Loss:  3.388703 (3.3008)  Time: 1.106s,  926.22/s  (1.092s,  938.02/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 160 [ 650/1251 ( 52%)]  Loss:  3.415770 (3.3090)  Time: 1.079s,  948.79/s  (1.092s,  937.75/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 700/1251 ( 56%)]  Loss:  3.559112 (3.3257)  Time: 1.075s,  952.12/s  (1.092s,  938.11/s)  LR: 4.533e-04  Data: 0.013 (0.013)
Train: 160 [ 750/1251 ( 60%)]  Loss:  3.179549 (3.3166)  Time: 1.079s,  948.75/s  (1.091s,  938.59/s)  LR: 4.533e-04  Data: 0.013 (0.013)
Train: 160 [ 800/1251 ( 64%)]  Loss:  3.566843 (3.3313)  Time: 1.096s,  934.56/s  (1.091s,  938.75/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 850/1251 ( 68%)]  Loss:  3.331178 (3.3313)  Time: 1.078s,  949.64/s  (1.091s,  938.57/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 900/1251 ( 72%)]  Loss:  3.062428 (3.3171)  Time: 1.098s,  932.63/s  (1.091s,  938.39/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [ 950/1251 ( 76%)]  Loss:  3.461289 (3.3243)  Time: 1.077s,  951.00/s  (1.091s,  938.44/s)  LR: 4.533e-04  Data: 0.013 (0.013)
Train: 160 [1000/1251 ( 80%)]  Loss:  3.481406 (3.3318)  Time: 1.078s,  949.81/s  (1.091s,  938.65/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [1050/1251 ( 84%)]  Loss:  3.578609 (3.3430)  Time: 1.095s,  935.17/s  (1.091s,  938.75/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [1100/1251 ( 88%)]  Loss:  3.709640 (3.3590)  Time: 1.089s,  940.50/s  (1.091s,  938.65/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [1150/1251 ( 92%)]  Loss:  3.606653 (3.3693)  Time: 1.105s,  926.96/s  (1.091s,  938.77/s)  LR: 4.533e-04  Data: 0.013 (0.013)
Train: 160 [1200/1251 ( 96%)]  Loss:  3.682865 (3.3818)  Time: 1.094s,  935.72/s  (1.091s,  938.60/s)  LR: 4.533e-04  Data: 0.012 (0.013)
Train: 160 [1250/1251 (100%)]  Loss:  3.307521 (3.3790)  Time: 1.178s,  868.94/s  (1.091s,  938.43/s)  LR: 4.533e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.830 (5.830)  Loss:  0.5144 (0.5144)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6425 (1.0051)  Acc@1: 86.9104 (77.4060)  Acc@5: 97.1698 (94.1320)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-155.pth.tar', 77.02399993408203)

Train: 161 [   0/1251 (  0%)]  Loss:  3.335588 (3.3356)  Time: 1.086s,  942.94/s  (1.086s,  942.94/s)  LR: 4.481e-04  Data: 0.025 (0.025)
Train: 161 [  50/1251 (  4%)]  Loss:  3.061952 (3.1988)  Time: 1.093s,  936.60/s  (1.095s,  935.31/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [ 100/1251 (  8%)]  Loss:  3.209086 (3.2022)  Time: 1.077s,  950.52/s  (1.093s,  936.57/s)  LR: 4.481e-04  Data: 0.014 (0.012)
Train: 161 [ 150/1251 ( 12%)]  Loss:  3.515273 (3.2805)  Time: 1.078s,  949.92/s  (1.091s,  938.20/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [ 200/1251 ( 16%)]  Loss:  3.651539 (3.3547)  Time: 1.171s,  874.59/s  (1.091s,  938.43/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [ 250/1251 ( 20%)]  Loss:  3.466393 (3.3733)  Time: 1.077s,  950.79/s  (1.090s,  939.81/s)  LR: 4.481e-04  Data: 0.015 (0.013)
Train: 161 [ 300/1251 ( 24%)]  Loss:  3.195137 (3.3479)  Time: 1.081s,  947.47/s  (1.089s,  939.94/s)  LR: 4.481e-04  Data: 0.013 (0.013)
Train: 161 [ 350/1251 ( 28%)]  Loss:  3.307925 (3.3429)  Time: 1.083s,  945.10/s  (1.090s,  939.27/s)  LR: 4.481e-04  Data: 0.014 (0.013)
Train: 161 [ 400/1251 ( 32%)]  Loss:  3.549016 (3.3658)  Time: 1.076s,  951.39/s  (1.090s,  939.74/s)  LR: 4.481e-04  Data: 0.014 (0.013)
Train: 161 [ 450/1251 ( 36%)]  Loss:  3.394078 (3.3686)  Time: 1.096s,  934.53/s  (1.090s,  939.24/s)  LR: 4.481e-04  Data: 0.013 (0.013)
Train: 161 [ 500/1251 ( 40%)]  Loss:  3.495165 (3.3801)  Time: 1.104s,  927.28/s  (1.091s,  938.77/s)  LR: 4.481e-04  Data: 0.016 (0.013)
Train: 161 [ 550/1251 ( 44%)]  Loss:  3.423814 (3.3837)  Time: 1.080s,  948.35/s  (1.091s,  938.98/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [ 600/1251 ( 48%)]  Loss:  3.095806 (3.3616)  Time: 1.099s,  932.06/s  (1.091s,  938.70/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [ 650/1251 ( 52%)]  Loss:  3.118181 (3.3442)  Time: 1.095s,  935.43/s  (1.091s,  938.67/s)  LR: 4.481e-04  Data: 0.012 (0.013)
Train: 161 [ 700/1251 ( 56%)]  Loss:  3.401041 (3.3480)  Time: 1.081s,  947.34/s  (1.091s,  938.59/s)  LR: 4.481e-04  Data: 0.013 (0.012)
Train: 161 [ 750/1251 ( 60%)]  Loss:  3.407223 (3.3517)  Time: 1.075s,  952.12/s  (1.090s,  939.10/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [ 800/1251 ( 64%)]  Loss:  3.109349 (3.3374)  Time: 1.076s,  951.69/s  (1.090s,  939.08/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [ 850/1251 ( 68%)]  Loss:  3.480625 (3.3454)  Time: 1.076s,  951.72/s  (1.091s,  938.91/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [ 900/1251 ( 72%)]  Loss:  3.244326 (3.3401)  Time: 1.085s,  943.74/s  (1.090s,  939.11/s)  LR: 4.481e-04  Data: 0.014 (0.012)
Train: 161 [ 950/1251 ( 76%)]  Loss:  3.361656 (3.3412)  Time: 1.097s,  933.41/s  (1.090s,  939.31/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [1000/1251 ( 80%)]  Loss:  3.501201 (3.3488)  Time: 1.095s,  935.32/s  (1.090s,  939.09/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [1050/1251 ( 84%)]  Loss:  3.345276 (3.3486)  Time: 1.099s,  931.97/s  (1.090s,  939.26/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [1100/1251 ( 88%)]  Loss:  3.375531 (3.3498)  Time: 1.078s,  950.06/s  (1.090s,  939.30/s)  LR: 4.481e-04  Data: 0.013 (0.012)
Train: 161 [1150/1251 ( 92%)]  Loss:  3.411248 (3.3524)  Time: 1.078s,  950.15/s  (1.090s,  939.07/s)  LR: 4.481e-04  Data: 0.012 (0.012)
Train: 161 [1200/1251 ( 96%)]  Loss:  3.266360 (3.3489)  Time: 1.095s,  935.36/s  (1.090s,  939.14/s)  LR: 4.481e-04  Data: 0.011 (0.012)
Train: 161 [1250/1251 (100%)]  Loss:  3.275144 (3.3461)  Time: 1.062s,  964.24/s  (1.090s,  939.09/s)  LR: 4.481e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.882 (5.882)  Loss:  0.4852 (0.4852)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5949 (0.9737)  Acc@1: 86.9104 (77.4280)  Acc@5: 97.2877 (94.0700)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-151.pth.tar', 77.04000006103516)

Train: 162 [   0/1251 (  0%)]  Loss:  3.477248 (3.4772)  Time: 1.088s,  941.19/s  (1.088s,  941.19/s)  LR: 4.430e-04  Data: 0.027 (0.027)
Train: 162 [  50/1251 (  4%)]  Loss:  3.025825 (3.2515)  Time: 1.173s,  872.79/s  (1.089s,  940.15/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 100/1251 (  8%)]  Loss:  3.296668 (3.2666)  Time: 1.175s,  871.54/s  (1.091s,  938.20/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 150/1251 ( 12%)]  Loss:  3.476645 (3.3191)  Time: 1.077s,  950.97/s  (1.089s,  940.38/s)  LR: 4.430e-04  Data: 0.013 (0.013)
Train: 162 [ 200/1251 ( 16%)]  Loss:  3.248523 (3.3050)  Time: 1.079s,  949.07/s  (1.088s,  940.86/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 250/1251 ( 20%)]  Loss:  3.583587 (3.3514)  Time: 1.103s,  928.72/s  (1.090s,  939.05/s)  LR: 4.430e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 162 [ 300/1251 ( 24%)]  Loss:  3.074190 (3.3118)  Time: 1.081s,  946.87/s  (1.090s,  939.31/s)  LR: 4.430e-04  Data: 0.012 (0.012)
Train: 162 [ 350/1251 ( 28%)]  Loss:  3.555055 (3.3422)  Time: 1.078s,  949.93/s  (1.089s,  940.10/s)  LR: 4.430e-04  Data: 0.012 (0.012)
Train: 162 [ 400/1251 ( 32%)]  Loss:  3.198097 (3.3262)  Time: 1.094s,  935.83/s  (1.089s,  940.50/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 450/1251 ( 36%)]  Loss:  3.724986 (3.3661)  Time: 1.087s,  942.20/s  (1.089s,  940.19/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 500/1251 ( 40%)]  Loss:  3.361593 (3.3657)  Time: 1.097s,  933.68/s  (1.089s,  940.45/s)  LR: 4.430e-04  Data: 0.011 (0.012)
Train: 162 [ 550/1251 ( 44%)]  Loss:  3.463698 (3.3738)  Time: 1.077s,  950.48/s  (1.089s,  940.64/s)  LR: 4.430e-04  Data: 0.013 (0.013)
Train: 162 [ 600/1251 ( 48%)]  Loss:  3.240965 (3.3636)  Time: 1.082s,  946.76/s  (1.089s,  940.25/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 650/1251 ( 52%)]  Loss:  3.237099 (3.3546)  Time: 1.093s,  936.56/s  (1.090s,  939.66/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 700/1251 ( 56%)]  Loss:  3.438498 (3.3602)  Time: 1.096s,  934.19/s  (1.090s,  939.10/s)  LR: 4.430e-04  Data: 0.015 (0.013)
Train: 162 [ 750/1251 ( 60%)]  Loss:  3.302529 (3.3566)  Time: 1.093s,  936.50/s  (1.091s,  938.71/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 800/1251 ( 64%)]  Loss:  2.998200 (3.3355)  Time: 1.082s,  946.04/s  (1.091s,  938.59/s)  LR: 4.430e-04  Data: 0.013 (0.013)
Train: 162 [ 850/1251 ( 68%)]  Loss:  3.256209 (3.3311)  Time: 1.096s,  934.47/s  (1.091s,  938.57/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 900/1251 ( 72%)]  Loss:  3.358535 (3.3325)  Time: 1.093s,  936.54/s  (1.091s,  938.43/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [ 950/1251 ( 76%)]  Loss:  3.198518 (3.3258)  Time: 1.084s,  944.60/s  (1.091s,  938.23/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1000/1251 ( 80%)]  Loss:  3.216226 (3.3206)  Time: 1.106s,  925.85/s  (1.091s,  938.42/s)  LR: 4.430e-04  Data: 0.014 (0.013)
Train: 162 [1050/1251 ( 84%)]  Loss:  3.392565 (3.3239)  Time: 1.078s,  949.88/s  (1.091s,  938.65/s)  LR: 4.430e-04  Data: 0.012 (0.013)
Train: 162 [1100/1251 ( 88%)]  Loss:  3.197007 (3.3184)  Time: 1.101s,  929.97/s  (1.091s,  938.49/s)  LR: 4.430e-04  Data: 0.014 (0.013)
Train: 162 [1150/1251 ( 92%)]  Loss:  3.507087 (3.3262)  Time: 1.077s,  950.51/s  (1.091s,  938.48/s)  LR: 4.430e-04  Data: 0.013 (0.013)
Train: 162 [1200/1251 ( 96%)]  Loss:  3.353259 (3.3273)  Time: 1.072s,  954.88/s  (1.091s,  938.57/s)  LR: 4.430e-04  Data: 0.011 (0.013)
Train: 162 [1250/1251 (100%)]  Loss:  3.352136 (3.3283)  Time: 1.061s,  964.68/s  (1.091s,  938.44/s)  LR: 4.430e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.732 (5.732)  Loss:  0.5101 (0.5101)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.6036 (0.9826)  Acc@1: 86.0849 (77.5200)  Acc@5: 97.4057 (94.0660)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-150.pth.tar', 77.07000008300781)

Train: 163 [   0/1251 (  0%)]  Loss:  3.371837 (3.3718)  Time: 1.088s,  941.00/s  (1.088s,  941.00/s)  LR: 4.378e-04  Data: 0.027 (0.027)
Train: 163 [  50/1251 (  4%)]  Loss:  3.376265 (3.3741)  Time: 1.079s,  948.60/s  (1.089s,  940.56/s)  LR: 4.378e-04  Data: 0.012 (0.013)
Train: 163 [ 100/1251 (  8%)]  Loss:  3.476981 (3.4084)  Time: 1.093s,  936.52/s  (1.087s,  941.81/s)  LR: 4.378e-04  Data: 0.013 (0.013)
Train: 163 [ 150/1251 ( 12%)]  Loss:  3.344991 (3.3925)  Time: 1.096s,  934.48/s  (1.090s,  939.26/s)  LR: 4.378e-04  Data: 0.013 (0.013)
Train: 163 [ 200/1251 ( 16%)]  Loss:  3.365369 (3.3871)  Time: 1.125s,  910.02/s  (1.091s,  938.57/s)  LR: 4.378e-04  Data: 0.013 (0.012)
Train: 163 [ 250/1251 ( 20%)]  Loss:  3.212475 (3.3580)  Time: 1.082s,  946.25/s  (1.091s,  938.75/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 300/1251 ( 24%)]  Loss:  3.289672 (3.3482)  Time: 1.093s,  937.06/s  (1.090s,  939.22/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 350/1251 ( 28%)]  Loss:  3.245703 (3.3354)  Time: 1.109s,  923.09/s  (1.091s,  938.30/s)  LR: 4.378e-04  Data: 0.014 (0.012)
Train: 163 [ 400/1251 ( 32%)]  Loss:  3.398643 (3.3424)  Time: 1.086s,  943.30/s  (1.090s,  939.09/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 450/1251 ( 36%)]  Loss:  3.098711 (3.3181)  Time: 1.096s,  933.95/s  (1.091s,  938.97/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 163 [ 500/1251 ( 40%)]  Loss:  3.281433 (3.3147)  Time: 1.092s,  937.54/s  (1.090s,  939.51/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 550/1251 ( 44%)]  Loss:  3.415845 (3.3232)  Time: 1.096s,  934.58/s  (1.090s,  939.65/s)  LR: 4.378e-04  Data: 0.013 (0.012)
Train: 163 [ 600/1251 ( 48%)]  Loss:  2.947155 (3.2942)  Time: 1.094s,  935.70/s  (1.091s,  938.94/s)  LR: 4.378e-04  Data: 0.014 (0.012)
Train: 163 [ 650/1251 ( 52%)]  Loss:  3.232445 (3.2898)  Time: 1.076s,  951.33/s  (1.091s,  938.91/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 700/1251 ( 56%)]  Loss:  3.436754 (3.2996)  Time: 1.098s,  932.47/s  (1.090s,  939.35/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 750/1251 ( 60%)]  Loss:  3.144367 (3.2899)  Time: 1.077s,  950.94/s  (1.090s,  939.17/s)  LR: 4.378e-04  Data: 0.014 (0.012)
Train: 163 [ 800/1251 ( 64%)]  Loss:  3.283282 (3.2895)  Time: 1.077s,  951.15/s  (1.090s,  939.64/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 850/1251 ( 68%)]  Loss:  3.497149 (3.3011)  Time: 1.094s,  936.33/s  (1.090s,  939.69/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 900/1251 ( 72%)]  Loss:  3.637431 (3.3188)  Time: 1.094s,  936.24/s  (1.090s,  939.78/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [ 950/1251 ( 76%)]  Loss:  3.287892 (3.3172)  Time: 1.095s,  934.88/s  (1.090s,  939.61/s)  LR: 4.378e-04  Data: 0.013 (0.012)
Train: 163 [1000/1251 ( 80%)]  Loss:  3.463199 (3.3242)  Time: 1.078s,  949.56/s  (1.090s,  939.72/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [1050/1251 ( 84%)]  Loss:  3.437628 (3.3293)  Time: 1.072s,  955.27/s  (1.089s,  940.06/s)  LR: 4.378e-04  Data: 0.011 (0.012)
Train: 163 [1100/1251 ( 88%)]  Loss:  3.283931 (3.3274)  Time: 1.094s,  935.67/s  (1.089s,  939.90/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [1150/1251 ( 92%)]  Loss:  3.476968 (3.3336)  Time: 1.081s,  947.70/s  (1.090s,  939.80/s)  LR: 4.378e-04  Data: 0.014 (0.012)
Train: 163 [1200/1251 ( 96%)]  Loss:  3.272751 (3.3312)  Time: 1.076s,  951.87/s  (1.089s,  940.08/s)  LR: 4.378e-04  Data: 0.012 (0.012)
Train: 163 [1250/1251 (100%)]  Loss:  3.138835 (3.3238)  Time: 1.080s,  948.01/s  (1.090s,  939.86/s)  LR: 4.378e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.842 (5.842)  Loss:  0.4758 (0.4758)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.5898 (0.9813)  Acc@1: 86.6745 (77.4860)  Acc@5: 97.6415 (94.1580)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-154.pth.tar', 77.07799998291016)

Train: 164 [   0/1251 (  0%)]  Loss:  3.092621 (3.0926)  Time: 1.091s,  938.24/s  (1.091s,  938.24/s)  LR: 4.327e-04  Data: 0.030 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 164 [  50/1251 (  4%)]  Loss:  2.980873 (3.0367)  Time: 1.075s,  952.78/s  (1.088s,  941.57/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 100/1251 (  8%)]  Loss:  3.198776 (3.0908)  Time: 1.094s,  935.69/s  (1.088s,  941.06/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 150/1251 ( 12%)]  Loss:  3.396743 (3.1673)  Time: 1.101s,  930.25/s  (1.089s,  940.16/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 200/1251 ( 16%)]  Loss:  3.362348 (3.2063)  Time: 1.077s,  951.06/s  (1.089s,  940.50/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 250/1251 ( 20%)]  Loss:  3.303774 (3.2225)  Time: 1.076s,  951.48/s  (1.087s,  941.75/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 300/1251 ( 24%)]  Loss:  3.212836 (3.2211)  Time: 1.095s,  935.12/s  (1.088s,  941.58/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 350/1251 ( 28%)]  Loss:  3.681500 (3.2787)  Time: 1.094s,  936.13/s  (1.089s,  940.27/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 400/1251 ( 32%)]  Loss:  3.566399 (3.3107)  Time: 1.097s,  933.63/s  (1.089s,  940.29/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 450/1251 ( 36%)]  Loss:  3.241736 (3.3038)  Time: 1.095s,  935.21/s  (1.089s,  940.55/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 500/1251 ( 40%)]  Loss:  3.609125 (3.3315)  Time: 1.094s,  935.95/s  (1.089s,  940.15/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 550/1251 ( 44%)]  Loss:  3.410625 (3.3381)  Time: 1.097s,  933.82/s  (1.089s,  940.49/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 600/1251 ( 48%)]  Loss:  3.653978 (3.3624)  Time: 1.096s,  934.50/s  (1.089s,  940.09/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 650/1251 ( 52%)]  Loss:  3.574603 (3.3776)  Time: 1.084s,  944.58/s  (1.090s,  939.66/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [ 700/1251 ( 56%)]  Loss:  3.565578 (3.3901)  Time: 1.087s,  942.27/s  (1.090s,  939.70/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 750/1251 ( 60%)]  Loss:  3.131104 (3.3739)  Time: 1.097s,  933.08/s  (1.090s,  939.54/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 800/1251 ( 64%)]  Loss:  3.447486 (3.3782)  Time: 1.085s,  943.74/s  (1.089s,  940.02/s)  LR: 4.327e-04  Data: 0.013 (0.013)
Train: 164 [ 850/1251 ( 68%)]  Loss:  3.190117 (3.3678)  Time: 1.093s,  936.78/s  (1.089s,  939.93/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [ 900/1251 ( 72%)]  Loss:  3.421411 (3.3706)  Time: 1.080s,  947.75/s  (1.090s,  939.86/s)  LR: 4.327e-04  Data: 0.013 (0.013)
Train: 164 [ 950/1251 ( 76%)]  Loss:  3.651497 (3.3847)  Time: 1.095s,  935.16/s  (1.090s,  939.82/s)  LR: 4.327e-04  Data: 0.018 (0.013)
Train: 164 [1000/1251 ( 80%)]  Loss:  3.435598 (3.3871)  Time: 1.078s,  949.70/s  (1.090s,  939.88/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1050/1251 ( 84%)]  Loss:  3.767055 (3.4044)  Time: 1.097s,  933.85/s  (1.090s,  939.82/s)  LR: 4.327e-04  Data: 0.011 (0.013)
Train: 164 [1100/1251 ( 88%)]  Loss:  3.397258 (3.4040)  Time: 1.084s,  944.33/s  (1.089s,  939.95/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1150/1251 ( 92%)]  Loss:  3.576914 (3.4112)  Time: 1.079s,  949.30/s  (1.089s,  940.03/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1200/1251 ( 96%)]  Loss:  3.635636 (3.4202)  Time: 1.079s,  949.40/s  (1.089s,  940.19/s)  LR: 4.327e-04  Data: 0.012 (0.013)
Train: 164 [1250/1251 (100%)]  Loss:  2.760171 (3.3948)  Time: 1.061s,  964.89/s  (1.089s,  940.18/s)  LR: 4.327e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.859 (5.859)  Loss:  0.5130 (0.5130)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6324 (0.9732)  Acc@1: 86.3208 (77.4960)  Acc@5: 97.1698 (94.2660)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-153.pth.tar', 77.20000003417968)

Train: 165 [   0/1251 (  0%)]  Loss:  3.399154 (3.3992)  Time: 1.083s,  945.40/s  (1.083s,  945.40/s)  LR: 4.276e-04  Data: 0.023 (0.023)
Train: 165 [  50/1251 (  4%)]  Loss:  3.279853 (3.3395)  Time: 1.171s,  874.64/s  (1.094s,  936.18/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [ 100/1251 (  8%)]  Loss:  3.662896 (3.4473)  Time: 1.096s,  934.46/s  (1.091s,  938.20/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 150/1251 ( 12%)]  Loss:  3.203657 (3.3864)  Time: 1.077s,  950.52/s  (1.093s,  937.26/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 200/1251 ( 16%)]  Loss:  3.262215 (3.3616)  Time: 1.081s,  946.91/s  (1.091s,  938.25/s)  LR: 4.276e-04  Data: 0.014 (0.013)
Train: 165 [ 250/1251 ( 20%)]  Loss:  3.676557 (3.4141)  Time: 1.100s,  930.96/s  (1.091s,  938.35/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 300/1251 ( 24%)]  Loss:  2.712903 (3.3139)  Time: 1.094s,  935.71/s  (1.091s,  938.49/s)  LR: 4.276e-04  Data: 0.014 (0.013)
Train: 165 [ 350/1251 ( 28%)]  Loss:  3.479096 (3.3345)  Time: 1.075s,  952.25/s  (1.090s,  939.05/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 400/1251 ( 32%)]  Loss:  3.479872 (3.3507)  Time: 1.080s,  948.58/s  (1.090s,  939.26/s)  LR: 4.276e-04  Data: 0.015 (0.013)
Train: 165 [ 450/1251 ( 36%)]  Loss:  3.314900 (3.3471)  Time: 1.075s,  952.40/s  (1.090s,  939.64/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [ 500/1251 ( 40%)]  Loss:  2.848877 (3.3018)  Time: 1.078s,  949.79/s  (1.090s,  939.82/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 550/1251 ( 44%)]  Loss:  3.323382 (3.3036)  Time: 1.101s,  929.93/s  (1.089s,  939.94/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 600/1251 ( 48%)]  Loss:  3.394231 (3.3106)  Time: 1.079s,  949.27/s  (1.089s,  939.96/s)  LR: 4.276e-04  Data: 0.014 (0.013)
Train: 165 [ 650/1251 ( 52%)]  Loss:  3.372803 (3.3150)  Time: 1.081s,  947.25/s  (1.089s,  940.21/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 700/1251 ( 56%)]  Loss:  3.299751 (3.3140)  Time: 1.100s,  930.59/s  (1.090s,  939.77/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [ 750/1251 ( 60%)]  Loss:  3.322727 (3.3146)  Time: 1.096s,  934.69/s  (1.090s,  939.43/s)  LR: 4.276e-04  Data: 0.015 (0.013)
Train: 165 [ 800/1251 ( 64%)]  Loss:  3.283210 (3.3127)  Time: 1.086s,  942.74/s  (1.090s,  939.13/s)  LR: 4.276e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 165 [ 850/1251 ( 68%)]  Loss:  3.399815 (3.3175)  Time: 1.097s,  933.65/s  (1.090s,  939.10/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [ 900/1251 ( 72%)]  Loss:  3.475132 (3.3258)  Time: 1.102s,  928.95/s  (1.090s,  939.38/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [ 950/1251 ( 76%)]  Loss:  3.483605 (3.3337)  Time: 1.094s,  935.87/s  (1.090s,  939.64/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [1000/1251 ( 80%)]  Loss:  3.310890 (3.3326)  Time: 1.076s,  951.58/s  (1.090s,  939.50/s)  LR: 4.276e-04  Data: 0.014 (0.012)
Train: 165 [1050/1251 ( 84%)]  Loss:  3.563266 (3.3431)  Time: 1.078s,  950.19/s  (1.090s,  939.76/s)  LR: 4.276e-04  Data: 0.012 (0.013)
Train: 165 [1100/1251 ( 88%)]  Loss:  3.648313 (3.3564)  Time: 1.083s,  945.37/s  (1.089s,  940.17/s)  LR: 4.276e-04  Data: 0.011 (0.013)
Train: 165 [1150/1251 ( 92%)]  Loss:  3.230647 (3.3512)  Time: 1.076s,  951.47/s  (1.089s,  940.19/s)  LR: 4.276e-04  Data: 0.012 (0.012)
Train: 165 [1200/1251 ( 96%)]  Loss:  2.957640 (3.3354)  Time: 1.101s,  930.11/s  (1.089s,  940.26/s)  LR: 4.276e-04  Data: 0.012 (0.012)
Train: 165 [1250/1251 (100%)]  Loss:  3.410357 (3.3383)  Time: 1.143s,  895.77/s  (1.089s,  940.23/s)  LR: 4.276e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.858 (5.858)  Loss:  0.4847 (0.4847)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6034 (0.9523)  Acc@1: 86.3208 (77.4880)  Acc@5: 97.6415 (94.1840)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-156.pth.tar', 77.23400008544922)

Train: 166 [   0/1251 (  0%)]  Loss:  3.354928 (3.3549)  Time: 1.085s,  944.15/s  (1.085s,  944.15/s)  LR: 4.224e-04  Data: 0.022 (0.022)
Train: 166 [  50/1251 (  4%)]  Loss:  3.346849 (3.3509)  Time: 1.095s,  935.35/s  (1.089s,  940.12/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 100/1251 (  8%)]  Loss:  3.690249 (3.4640)  Time: 1.096s,  934.29/s  (1.090s,  939.39/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 150/1251 ( 12%)]  Loss:  3.220564 (3.4031)  Time: 1.169s,  876.19/s  (1.091s,  938.24/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 200/1251 ( 16%)]  Loss:  3.508514 (3.4242)  Time: 1.095s,  934.99/s  (1.091s,  938.34/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 250/1251 ( 20%)]  Loss:  3.476366 (3.4329)  Time: 1.096s,  934.57/s  (1.092s,  937.87/s)  LR: 4.224e-04  Data: 0.015 (0.013)
Train: 166 [ 300/1251 ( 24%)]  Loss:  3.471672 (3.4384)  Time: 1.104s,  927.39/s  (1.092s,  938.06/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 350/1251 ( 28%)]  Loss:  3.307897 (3.4221)  Time: 1.091s,  938.26/s  (1.093s,  937.23/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 400/1251 ( 32%)]  Loss:  3.158423 (3.3928)  Time: 1.076s,  951.55/s  (1.092s,  937.43/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 450/1251 ( 36%)]  Loss:  3.426803 (3.3962)  Time: 1.081s,  947.43/s  (1.092s,  937.92/s)  LR: 4.224e-04  Data: 0.014 (0.013)
Train: 166 [ 500/1251 ( 40%)]  Loss:  3.297588 (3.3873)  Time: 1.082s,  946.17/s  (1.092s,  937.84/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 550/1251 ( 44%)]  Loss:  3.127404 (3.3656)  Time: 1.174s,  872.21/s  (1.092s,  937.88/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 600/1251 ( 48%)]  Loss:  3.116431 (3.3464)  Time: 1.080s,  948.33/s  (1.092s,  937.86/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 650/1251 ( 52%)]  Loss:  3.312906 (3.3440)  Time: 1.111s,  921.54/s  (1.091s,  938.60/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 700/1251 ( 56%)]  Loss:  3.129649 (3.3297)  Time: 1.084s,  944.62/s  (1.091s,  938.39/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 750/1251 ( 60%)]  Loss:  3.265934 (3.3258)  Time: 1.093s,  937.00/s  (1.092s,  938.05/s)  LR: 4.224e-04  Data: 0.012 (0.012)
Train: 166 [ 800/1251 ( 64%)]  Loss:  3.165035 (3.3163)  Time: 1.104s,  927.41/s  (1.091s,  938.25/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 850/1251 ( 68%)]  Loss:  3.359121 (3.3187)  Time: 1.081s,  946.88/s  (1.091s,  938.67/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [ 900/1251 ( 72%)]  Loss:  3.145178 (3.3096)  Time: 1.094s,  935.65/s  (1.091s,  938.86/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [ 950/1251 ( 76%)]  Loss:  3.376094 (3.3129)  Time: 1.075s,  952.20/s  (1.090s,  939.25/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [1000/1251 ( 80%)]  Loss:  3.510136 (3.3223)  Time: 1.080s,  948.40/s  (1.090s,  939.45/s)  LR: 4.224e-04  Data: 0.018 (0.013)
Train: 166 [1050/1251 ( 84%)]  Loss:  3.483189 (3.3296)  Time: 1.189s,  861.21/s  (1.090s,  939.58/s)  LR: 4.224e-04  Data: 0.014 (0.013)
Train: 166 [1100/1251 ( 88%)]  Loss:  3.171906 (3.3227)  Time: 1.106s,  925.72/s  (1.090s,  939.71/s)  LR: 4.224e-04  Data: 0.015 (0.013)
Train: 166 [1150/1251 ( 92%)]  Loss:  3.119315 (3.3143)  Time: 1.083s,  945.76/s  (1.090s,  939.53/s)  LR: 4.224e-04  Data: 0.012 (0.013)
Train: 166 [1200/1251 ( 96%)]  Loss:  3.223623 (3.3106)  Time: 1.081s,  947.61/s  (1.090s,  939.75/s)  LR: 4.224e-04  Data: 0.011 (0.013)
Train: 166 [1250/1251 (100%)]  Loss:  3.048018 (3.3005)  Time: 1.164s,  879.37/s  (1.090s,  939.63/s)  LR: 4.224e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.817 (5.817)  Loss:  0.4890 (0.4890)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5902 (0.9751)  Acc@1: 87.1462 (77.4480)  Acc@5: 97.4057 (94.1600)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 77.44799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-157.pth.tar', 77.30000008300782)

Train: 167 [   0/1251 (  0%)]  Loss:  3.183466 (3.1835)  Time: 1.085s,  943.37/s  (1.085s,  943.37/s)  LR: 4.173e-04  Data: 0.022 (0.022)
Train: 167 [  50/1251 (  4%)]  Loss:  3.338077 (3.2608)  Time: 1.094s,  936.22/s  (1.092s,  938.13/s)  LR: 4.173e-04  Data: 0.016 (0.013)
Train: 167 [ 100/1251 (  8%)]  Loss:  2.870680 (3.1307)  Time: 1.077s,  950.68/s  (1.091s,  938.57/s)  LR: 4.173e-04  Data: 0.013 (0.013)
Train: 167 [ 150/1251 ( 12%)]  Loss:  3.355300 (3.1869)  Time: 1.095s,  935.24/s  (1.091s,  938.59/s)  LR: 4.173e-04  Data: 0.013 (0.013)
Train: 167 [ 200/1251 ( 16%)]  Loss:  2.935001 (3.1365)  Time: 1.076s,  951.78/s  (1.089s,  940.09/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 250/1251 ( 20%)]  Loss:  3.538304 (3.2035)  Time: 1.171s,  874.16/s  (1.089s,  940.40/s)  LR: 4.173e-04  Data: 0.013 (0.013)
Train: 167 [ 300/1251 ( 24%)]  Loss:  3.497533 (3.2455)  Time: 1.075s,  952.59/s  (1.089s,  940.23/s)  LR: 4.173e-04  Data: 0.013 (0.013)
Train: 167 [ 350/1251 ( 28%)]  Loss:  3.292925 (3.2514)  Time: 1.094s,  935.62/s  (1.089s,  940.32/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 400/1251 ( 32%)]  Loss:  3.343488 (3.2616)  Time: 1.087s,  941.61/s  (1.089s,  939.99/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 450/1251 ( 36%)]  Loss:  3.300620 (3.2655)  Time: 1.090s,  939.07/s  (1.090s,  939.88/s)  LR: 4.173e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 167 [ 500/1251 ( 40%)]  Loss:  3.175613 (3.2574)  Time: 1.096s,  934.43/s  (1.089s,  940.24/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 550/1251 ( 44%)]  Loss:  3.115743 (3.2456)  Time: 1.095s,  935.17/s  (1.089s,  939.90/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 600/1251 ( 48%)]  Loss:  3.302207 (3.2499)  Time: 1.093s,  936.86/s  (1.090s,  939.32/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 650/1251 ( 52%)]  Loss:  3.321566 (3.2550)  Time: 1.096s,  934.67/s  (1.090s,  939.14/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 700/1251 ( 56%)]  Loss:  3.291342 (3.2575)  Time: 1.075s,  952.21/s  (1.090s,  939.44/s)  LR: 4.173e-04  Data: 0.011 (0.013)
Train: 167 [ 750/1251 ( 60%)]  Loss:  3.491630 (3.2721)  Time: 1.094s,  936.32/s  (1.090s,  939.64/s)  LR: 4.173e-04  Data: 0.014 (0.013)
Train: 167 [ 800/1251 ( 64%)]  Loss:  3.090349 (3.2614)  Time: 1.095s,  935.22/s  (1.089s,  939.93/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 850/1251 ( 68%)]  Loss:  3.202083 (3.2581)  Time: 1.082s,  946.22/s  (1.090s,  939.76/s)  LR: 4.173e-04  Data: 0.014 (0.013)
Train: 167 [ 900/1251 ( 72%)]  Loss:  3.241353 (3.2572)  Time: 1.081s,  947.25/s  (1.089s,  940.02/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [ 950/1251 ( 76%)]  Loss:  3.528663 (3.2708)  Time: 1.094s,  935.61/s  (1.090s,  939.85/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1000/1251 ( 80%)]  Loss:  3.411510 (3.2775)  Time: 1.076s,  951.47/s  (1.089s,  939.96/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1050/1251 ( 84%)]  Loss:  3.240710 (3.2758)  Time: 1.098s,  932.99/s  (1.090s,  939.79/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1100/1251 ( 88%)]  Loss:  3.504328 (3.2858)  Time: 1.077s,  950.98/s  (1.089s,  939.92/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1150/1251 ( 92%)]  Loss:  3.459339 (3.2930)  Time: 1.093s,  936.73/s  (1.089s,  939.96/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1200/1251 ( 96%)]  Loss:  3.287957 (3.2928)  Time: 1.080s,  947.87/s  (1.090s,  939.85/s)  LR: 4.173e-04  Data: 0.012 (0.013)
Train: 167 [1250/1251 (100%)]  Loss:  3.274436 (3.2921)  Time: 1.080s,  948.53/s  (1.090s,  939.67/s)  LR: 4.173e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.842 (5.842)  Loss:  0.4781 (0.4781)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.6064 (0.9715)  Acc@1: 86.2028 (77.7340)  Acc@5: 97.6415 (94.1940)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 77.44799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-160.pth.tar', 77.40600013183594)

Train: 168 [   0/1251 (  0%)]  Loss:  3.216953 (3.2170)  Time: 1.085s,  943.53/s  (1.085s,  943.53/s)  LR: 4.122e-04  Data: 0.025 (0.025)
Train: 168 [  50/1251 (  4%)]  Loss:  3.271847 (3.2444)  Time: 1.107s,  925.40/s  (1.084s,  944.23/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 100/1251 (  8%)]  Loss:  3.372705 (3.2872)  Time: 1.079s,  948.98/s  (1.086s,  942.87/s)  LR: 4.122e-04  Data: 0.015 (0.013)
Train: 168 [ 150/1251 ( 12%)]  Loss:  3.519600 (3.3453)  Time: 1.078s,  950.10/s  (1.087s,  942.05/s)  LR: 4.122e-04  Data: 0.015 (0.013)
Train: 168 [ 200/1251 ( 16%)]  Loss:  3.371949 (3.3506)  Time: 1.079s,  949.19/s  (1.088s,  941.46/s)  LR: 4.122e-04  Data: 0.015 (0.013)
Train: 168 [ 250/1251 ( 20%)]  Loss:  3.098067 (3.3085)  Time: 1.104s,  927.14/s  (1.089s,  940.53/s)  LR: 4.122e-04  Data: 0.013 (0.013)
Train: 168 [ 300/1251 ( 24%)]  Loss:  3.515388 (3.3381)  Time: 1.103s,  928.31/s  (1.090s,  939.22/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 350/1251 ( 28%)]  Loss:  3.195813 (3.3203)  Time: 1.077s,  950.90/s  (1.091s,  938.81/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 400/1251 ( 32%)]  Loss:  3.184231 (3.3052)  Time: 1.098s,  933.01/s  (1.091s,  938.98/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 450/1251 ( 36%)]  Loss:  3.051232 (3.2798)  Time: 1.080s,  947.92/s  (1.090s,  939.47/s)  LR: 4.122e-04  Data: 0.013 (0.013)
Train: 168 [ 500/1251 ( 40%)]  Loss:  3.137316 (3.2668)  Time: 1.096s,  933.98/s  (1.091s,  938.98/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 550/1251 ( 44%)]  Loss:  3.314565 (3.2708)  Time: 1.099s,  931.40/s  (1.090s,  939.19/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 600/1251 ( 48%)]  Loss:  3.548645 (3.2922)  Time: 1.088s,  941.41/s  (1.090s,  939.55/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 650/1251 ( 52%)]  Loss:  3.375496 (3.2981)  Time: 1.095s,  935.58/s  (1.090s,  939.55/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 700/1251 ( 56%)]  Loss:  3.346283 (3.3013)  Time: 1.078s,  949.89/s  (1.091s,  938.99/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 750/1251 ( 60%)]  Loss:  3.596161 (3.3198)  Time: 1.096s,  934.33/s  (1.091s,  938.91/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 800/1251 ( 64%)]  Loss:  3.442271 (3.3270)  Time: 1.094s,  936.27/s  (1.091s,  938.62/s)  LR: 4.122e-04  Data: 0.014 (0.013)
Train: 168 [ 850/1251 ( 68%)]  Loss:  3.506378 (3.3369)  Time: 1.082s,  946.61/s  (1.091s,  938.61/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [ 900/1251 ( 72%)]  Loss:  3.312876 (3.3357)  Time: 1.077s,  950.51/s  (1.091s,  938.90/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [ 950/1251 ( 76%)]  Loss:  3.201333 (3.3290)  Time: 1.075s,  952.54/s  (1.091s,  938.98/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1000/1251 ( 80%)]  Loss:  2.786502 (3.3031)  Time: 1.094s,  935.91/s  (1.090s,  939.16/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1050/1251 ( 84%)]  Loss:  3.469205 (3.3107)  Time: 1.080s,  947.91/s  (1.090s,  939.29/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1100/1251 ( 88%)]  Loss:  3.337258 (3.3118)  Time: 1.096s,  934.33/s  (1.090s,  939.25/s)  LR: 4.122e-04  Data: 0.011 (0.013)
Train: 168 [1150/1251 ( 92%)]  Loss:  3.021585 (3.2997)  Time: 1.108s,  924.34/s  (1.090s,  939.40/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1200/1251 ( 96%)]  Loss:  3.258141 (3.2981)  Time: 1.081s,  946.97/s  (1.090s,  939.29/s)  LR: 4.122e-04  Data: 0.012 (0.013)
Train: 168 [1250/1251 (100%)]  Loss:  2.958801 (3.2850)  Time: 1.062s,  963.87/s  (1.090s,  939.33/s)  LR: 4.122e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.846 (5.846)  Loss:  0.5109 (0.5109)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.6089 (0.9863)  Acc@1: 87.3821 (77.7740)  Acc@5: 97.9953 (94.2000)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 77.44799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-161.pth.tar', 77.42800000244141)

Train: 169 [   0/1251 (  0%)]  Loss:  3.219259 (3.2193)  Time: 1.090s,  939.51/s  (1.090s,  939.51/s)  LR: 4.072e-04  Data: 0.023 (0.023)
Train: 169 [  50/1251 (  4%)]  Loss:  3.163414 (3.1913)  Time: 1.098s,  932.50/s  (1.097s,  933.28/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 169 [ 100/1251 (  8%)]  Loss:  3.591623 (3.3248)  Time: 1.066s,  960.49/s  (1.095s,  935.25/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 150/1251 ( 12%)]  Loss:  3.371658 (3.3365)  Time: 1.097s,  933.42/s  (1.091s,  938.19/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 200/1251 ( 16%)]  Loss:  3.320603 (3.3333)  Time: 1.081s,  947.62/s  (1.091s,  938.89/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 250/1251 ( 20%)]  Loss:  3.255980 (3.3204)  Time: 1.095s,  935.30/s  (1.091s,  938.82/s)  LR: 4.072e-04  Data: 0.011 (0.013)
Train: 169 [ 300/1251 ( 24%)]  Loss:  3.349419 (3.3246)  Time: 1.171s,  874.62/s  (1.092s,  937.57/s)  LR: 4.072e-04  Data: 0.014 (0.013)
Train: 169 [ 350/1251 ( 28%)]  Loss:  3.180234 (3.3065)  Time: 1.095s,  934.84/s  (1.092s,  937.92/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 400/1251 ( 32%)]  Loss:  3.283436 (3.3040)  Time: 1.076s,  952.07/s  (1.092s,  937.94/s)  LR: 4.072e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 169 [ 450/1251 ( 36%)]  Loss:  3.470142 (3.3206)  Time: 1.078s,  950.05/s  (1.091s,  938.59/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 500/1251 ( 40%)]  Loss:  3.463067 (3.3335)  Time: 1.096s,  934.00/s  (1.091s,  938.84/s)  LR: 4.072e-04  Data: 0.013 (0.013)
Train: 169 [ 550/1251 ( 44%)]  Loss:  3.385246 (3.3378)  Time: 1.076s,  951.72/s  (1.091s,  938.82/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 600/1251 ( 48%)]  Loss:  3.370081 (3.3403)  Time: 1.081s,  947.21/s  (1.090s,  939.13/s)  LR: 4.072e-04  Data: 0.013 (0.013)
Train: 169 [ 650/1251 ( 52%)]  Loss:  3.292834 (3.3369)  Time: 1.105s,  927.09/s  (1.090s,  939.19/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 700/1251 ( 56%)]  Loss:  3.242883 (3.3307)  Time: 1.095s,  934.88/s  (1.091s,  938.80/s)  LR: 4.072e-04  Data: 0.013 (0.013)
Train: 169 [ 750/1251 ( 60%)]  Loss:  3.445805 (3.3379)  Time: 1.079s,  948.83/s  (1.090s,  939.03/s)  LR: 4.072e-04  Data: 0.017 (0.013)
Train: 169 [ 800/1251 ( 64%)]  Loss:  3.250014 (3.3327)  Time: 1.095s,  934.99/s  (1.091s,  938.87/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 850/1251 ( 68%)]  Loss:  3.383834 (3.3355)  Time: 1.171s,  874.37/s  (1.091s,  938.93/s)  LR: 4.072e-04  Data: 0.017 (0.013)
Train: 169 [ 900/1251 ( 72%)]  Loss:  3.122351 (3.3243)  Time: 1.096s,  934.69/s  (1.090s,  939.37/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [ 950/1251 ( 76%)]  Loss:  3.583326 (3.3373)  Time: 1.079s,  949.13/s  (1.090s,  939.22/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [1000/1251 ( 80%)]  Loss:  3.552836 (3.3475)  Time: 1.092s,  937.32/s  (1.090s,  939.48/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [1050/1251 ( 84%)]  Loss:  3.216245 (3.3416)  Time: 1.098s,  932.31/s  (1.090s,  939.40/s)  LR: 4.072e-04  Data: 0.014 (0.013)
Train: 169 [1100/1251 ( 88%)]  Loss:  3.094708 (3.3308)  Time: 1.079s,  949.09/s  (1.090s,  939.39/s)  LR: 4.072e-04  Data: 0.014 (0.013)
Train: 169 [1150/1251 ( 92%)]  Loss:  3.334129 (3.3310)  Time: 1.140s,  897.92/s  (1.090s,  939.32/s)  LR: 4.072e-04  Data: 0.012 (0.013)
Train: 169 [1200/1251 ( 96%)]  Loss:  3.281738 (3.3290)  Time: 1.076s,  951.81/s  (1.090s,  939.46/s)  LR: 4.072e-04  Data: 0.013 (0.013)
Train: 169 [1250/1251 (100%)]  Loss:  2.935563 (3.3139)  Time: 1.158s,  884.34/s  (1.090s,  939.32/s)  LR: 4.072e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.884 (5.884)  Loss:  0.4892 (0.4892)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.6103 (0.9762)  Acc@1: 86.3208 (77.6400)  Acc@5: 97.7594 (94.2820)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-166.pth.tar', 77.44799997558594)

Train: 170 [   0/1251 (  0%)]  Loss:  3.375036 (3.3750)  Time: 1.099s,  932.03/s  (1.099s,  932.03/s)  LR: 4.021e-04  Data: 0.028 (0.028)
Train: 170 [  50/1251 (  4%)]  Loss:  3.300025 (3.3375)  Time: 1.076s,  952.05/s  (1.087s,  941.63/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 100/1251 (  8%)]  Loss:  3.561252 (3.4121)  Time: 1.077s,  950.85/s  (1.087s,  942.40/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 150/1251 ( 12%)]  Loss:  3.009736 (3.3115)  Time: 1.098s,  932.54/s  (1.086s,  943.17/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 200/1251 ( 16%)]  Loss:  3.135256 (3.2763)  Time: 1.077s,  950.40/s  (1.086s,  942.92/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 250/1251 ( 20%)]  Loss:  3.342859 (3.2874)  Time: 1.097s,  933.72/s  (1.087s,  942.24/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 300/1251 ( 24%)]  Loss:  3.137385 (3.2659)  Time: 1.084s,  944.48/s  (1.087s,  941.92/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 350/1251 ( 28%)]  Loss:  3.513914 (3.2969)  Time: 1.078s,  950.15/s  (1.087s,  941.85/s)  LR: 4.021e-04  Data: 0.013 (0.013)
Train: 170 [ 400/1251 ( 32%)]  Loss:  2.995226 (3.2634)  Time: 1.078s,  949.73/s  (1.087s,  942.24/s)  LR: 4.021e-04  Data: 0.013 (0.013)
Train: 170 [ 450/1251 ( 36%)]  Loss:  3.492627 (3.2863)  Time: 1.076s,  951.72/s  (1.087s,  942.19/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 500/1251 ( 40%)]  Loss:  3.203344 (3.2788)  Time: 1.077s,  950.77/s  (1.087s,  941.96/s)  LR: 4.021e-04  Data: 0.011 (0.013)
Train: 170 [ 550/1251 ( 44%)]  Loss:  3.581561 (3.3040)  Time: 1.079s,  949.06/s  (1.087s,  941.78/s)  LR: 4.021e-04  Data: 0.013 (0.013)
Train: 170 [ 600/1251 ( 48%)]  Loss:  3.568258 (3.3243)  Time: 1.173s,  873.08/s  (1.088s,  941.37/s)  LR: 4.021e-04  Data: 0.016 (0.013)
Train: 170 [ 650/1251 ( 52%)]  Loss:  3.559403 (3.3411)  Time: 1.106s,  925.73/s  (1.088s,  941.08/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 700/1251 ( 56%)]  Loss:  3.469773 (3.3497)  Time: 1.076s,  951.45/s  (1.088s,  941.16/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [ 750/1251 ( 60%)]  Loss:  3.266169 (3.3445)  Time: 1.076s,  951.58/s  (1.088s,  941.48/s)  LR: 4.021e-04  Data: 0.013 (0.012)
Train: 170 [ 800/1251 ( 64%)]  Loss:  3.239608 (3.3383)  Time: 1.088s,  941.41/s  (1.088s,  941.43/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 170 [ 850/1251 ( 68%)]  Loss:  3.326588 (3.3377)  Time: 1.077s,  951.04/s  (1.088s,  941.50/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 170 [ 900/1251 ( 72%)]  Loss:  3.151172 (3.3279)  Time: 1.096s,  934.10/s  (1.088s,  941.07/s)  LR: 4.021e-04  Data: 0.012 (0.012)
Train: 170 [ 950/1251 ( 76%)]  Loss:  3.353259 (3.3291)  Time: 1.097s,  933.53/s  (1.089s,  940.68/s)  LR: 4.021e-04  Data: 0.017 (0.012)
Train: 170 [1000/1251 ( 80%)]  Loss:  3.428284 (3.3338)  Time: 1.104s,  927.14/s  (1.089s,  940.57/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [1050/1251 ( 84%)]  Loss:  3.410111 (3.3373)  Time: 1.098s,  932.90/s  (1.088s,  940.75/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [1100/1251 ( 88%)]  Loss:  3.598515 (3.3487)  Time: 1.077s,  950.77/s  (1.088s,  940.80/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [1150/1251 ( 92%)]  Loss:  3.204716 (3.3427)  Time: 1.096s,  934.22/s  (1.088s,  940.85/s)  LR: 4.021e-04  Data: 0.017 (0.013)
Train: 170 [1200/1251 ( 96%)]  Loss:  2.944136 (3.3267)  Time: 1.080s,  948.33/s  (1.088s,  940.84/s)  LR: 4.021e-04  Data: 0.012 (0.013)
Train: 170 [1250/1251 (100%)]  Loss:  3.466934 (3.3321)  Time: 1.081s,  947.60/s  (1.088s,  940.84/s)  LR: 4.021e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.834 (5.834)  Loss:  0.5032 (0.5032)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6469 (0.9896)  Acc@1: 85.1415 (78.0100)  Acc@5: 97.4057 (94.3500)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-163.pth.tar', 77.48600002929687)

Train: 171 [   0/1251 (  0%)]  Loss:  3.357531 (3.3575)  Time: 1.084s,  944.24/s  (1.084s,  944.24/s)  LR: 3.970e-04  Data: 0.022 (0.022)
Train: 171 [  50/1251 (  4%)]  Loss:  3.135081 (3.2463)  Time: 1.097s,  933.56/s  (1.091s,  938.76/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 100/1251 (  8%)]  Loss:  3.418686 (3.3038)  Time: 1.078s,  949.97/s  (1.091s,  938.60/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 150/1251 ( 12%)]  Loss:  3.038240 (3.2374)  Time: 1.079s,  949.24/s  (1.089s,  940.18/s)  LR: 3.970e-04  Data: 0.016 (0.013)
Train: 171 [ 200/1251 ( 16%)]  Loss:  3.235798 (3.2371)  Time: 1.077s,  950.83/s  (1.089s,  939.91/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 250/1251 ( 20%)]  Loss:  3.303791 (3.2482)  Time: 1.077s,  950.70/s  (1.091s,  938.34/s)  LR: 3.970e-04  Data: 0.012 (0.013)
Train: 171 [ 300/1251 ( 24%)]  Loss:  3.233392 (3.2461)  Time: 1.094s,  936.09/s  (1.090s,  939.17/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 350/1251 ( 28%)]  Loss:  3.442823 (3.2707)  Time: 1.173s,  873.29/s  (1.090s,  939.73/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 400/1251 ( 32%)]  Loss:  3.328365 (3.2771)  Time: 1.099s,  932.08/s  (1.089s,  940.05/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 450/1251 ( 36%)]  Loss:  3.404586 (3.2898)  Time: 1.078s,  949.52/s  (1.089s,  940.63/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 500/1251 ( 40%)]  Loss:  3.465014 (3.3058)  Time: 1.076s,  951.42/s  (1.088s,  940.91/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 550/1251 ( 44%)]  Loss:  3.432655 (3.3163)  Time: 1.077s,  950.83/s  (1.088s,  941.09/s)  LR: 3.970e-04  Data: 0.014 (0.012)
Train: 171 [ 600/1251 ( 48%)]  Loss:  3.233903 (3.3100)  Time: 1.081s,  946.87/s  (1.088s,  941.16/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 650/1251 ( 52%)]  Loss:  3.438332 (3.3192)  Time: 1.077s,  950.93/s  (1.088s,  941.48/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 700/1251 ( 56%)]  Loss:  3.533628 (3.3335)  Time: 1.075s,  952.30/s  (1.087s,  941.96/s)  LR: 3.970e-04  Data: 0.013 (0.012)
Train: 171 [ 750/1251 ( 60%)]  Loss:  3.622515 (3.3515)  Time: 1.101s,  929.90/s  (1.087s,  941.70/s)  LR: 3.970e-04  Data: 0.013 (0.012)
Train: 171 [ 800/1251 ( 64%)]  Loss:  3.506978 (3.3607)  Time: 1.073s,  954.63/s  (1.088s,  941.19/s)  LR: 3.970e-04  Data: 0.011 (0.012)
Train: 171 [ 850/1251 ( 68%)]  Loss:  3.529693 (3.3701)  Time: 1.077s,  950.88/s  (1.088s,  941.56/s)  LR: 3.970e-04  Data: 0.014 (0.012)
Train: 171 [ 900/1251 ( 72%)]  Loss:  3.165792 (3.3593)  Time: 1.094s,  936.02/s  (1.088s,  941.30/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [ 950/1251 ( 76%)]  Loss:  3.223088 (3.3525)  Time: 1.105s,  926.69/s  (1.088s,  940.78/s)  LR: 3.970e-04  Data: 0.013 (0.012)
Train: 171 [1000/1251 ( 80%)]  Loss:  3.280215 (3.3491)  Time: 1.105s,  926.47/s  (1.089s,  940.66/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [1050/1251 ( 84%)]  Loss:  3.256851 (3.3449)  Time: 1.096s,  934.19/s  (1.088s,  940.81/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [1100/1251 ( 88%)]  Loss:  3.205117 (3.3388)  Time: 1.077s,  950.93/s  (1.089s,  940.39/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [1150/1251 ( 92%)]  Loss:  3.291493 (3.3368)  Time: 1.081s,  947.56/s  (1.089s,  940.35/s)  LR: 3.970e-04  Data: 0.012 (0.012)
Train: 171 [1200/1251 ( 96%)]  Loss:  3.431856 (3.3406)  Time: 1.096s,  934.32/s  (1.089s,  940.25/s)  LR: 3.970e-04  Data: 0.014 (0.012)
Train: 171 [1250/1251 (100%)]  Loss:  3.542778 (3.3484)  Time: 1.061s,  964.84/s  (1.089s,  940.41/s)  LR: 3.970e-04  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.758 (5.758)  Loss:  0.4937 (0.4937)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5975 (0.9601)  Acc@1: 86.0849 (77.7620)  Acc@5: 97.5236 (94.2860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-165.pth.tar', 77.48800013427734)

Train: 172 [   0/1251 (  0%)]  Loss:  3.139958 (3.1400)  Time: 1.090s,  939.84/s  (1.090s,  939.84/s)  LR: 3.920e-04  Data: 0.027 (0.027)
Train: 172 [  50/1251 (  4%)]  Loss:  3.493642 (3.3168)  Time: 1.079s,  949.38/s  (1.088s,  941.08/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 100/1251 (  8%)]  Loss:  3.272535 (3.3020)  Time: 1.077s,  951.12/s  (1.091s,  938.65/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 150/1251 ( 12%)]  Loss:  3.192059 (3.2745)  Time: 1.096s,  934.65/s  (1.089s,  940.50/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 200/1251 ( 16%)]  Loss:  3.530064 (3.3257)  Time: 1.083s,  945.17/s  (1.091s,  938.89/s)  LR: 3.920e-04  Data: 0.013 (0.013)
Train: 172 [ 250/1251 ( 20%)]  Loss:  3.203537 (3.3053)  Time: 1.096s,  934.73/s  (1.090s,  939.48/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 300/1251 ( 24%)]  Loss:  3.561456 (3.3419)  Time: 1.078s,  950.08/s  (1.090s,  939.45/s)  LR: 3.920e-04  Data: 0.014 (0.013)
Train: 172 [ 350/1251 ( 28%)]  Loss:  3.434433 (3.3535)  Time: 1.079s,  949.07/s  (1.090s,  939.55/s)  LR: 3.920e-04  Data: 0.015 (0.013)
Train: 172 [ 400/1251 ( 32%)]  Loss:  3.559592 (3.3764)  Time: 1.077s,  951.06/s  (1.089s,  939.94/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 450/1251 ( 36%)]  Loss:  3.264536 (3.3652)  Time: 1.076s,  951.34/s  (1.089s,  940.56/s)  LR: 3.920e-04  Data: 0.014 (0.013)
Train: 172 [ 500/1251 ( 40%)]  Loss:  3.379541 (3.3665)  Time: 1.079s,  948.76/s  (1.088s,  940.75/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 550/1251 ( 44%)]  Loss:  3.095839 (3.3439)  Time: 1.095s,  935.10/s  (1.090s,  939.73/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [ 600/1251 ( 48%)]  Loss:  3.370803 (3.3460)  Time: 1.079s,  948.64/s  (1.089s,  940.11/s)  LR: 3.920e-04  Data: 0.013 (0.013)
Train: 172 [ 650/1251 ( 52%)]  Loss:  3.361369 (3.3471)  Time: 1.077s,  951.20/s  (1.089s,  940.24/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 172 [ 700/1251 ( 56%)]  Loss:  3.342105 (3.3468)  Time: 1.080s,  948.52/s  (1.089s,  940.40/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 750/1251 ( 60%)]  Loss:  3.358017 (3.3475)  Time: 1.094s,  935.90/s  (1.089s,  940.58/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [ 800/1251 ( 64%)]  Loss:  3.146156 (3.3356)  Time: 1.098s,  932.44/s  (1.089s,  940.46/s)  LR: 3.920e-04  Data: 0.015 (0.013)
Train: 172 [ 850/1251 ( 68%)]  Loss:  3.352052 (3.3365)  Time: 1.076s,  951.43/s  (1.089s,  940.68/s)  LR: 3.920e-04  Data: 0.013 (0.013)
Train: 172 [ 900/1251 ( 72%)]  Loss:  3.255183 (3.3323)  Time: 1.094s,  936.25/s  (1.089s,  940.59/s)  LR: 3.920e-04  Data: 0.014 (0.013)
Train: 172 [ 950/1251 ( 76%)]  Loss:  2.884616 (3.3099)  Time: 1.095s,  934.93/s  (1.089s,  940.41/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [1000/1251 ( 80%)]  Loss:  3.329079 (3.3108)  Time: 1.096s,  934.28/s  (1.089s,  940.04/s)  LR: 3.920e-04  Data: 0.013 (0.013)
Train: 172 [1050/1251 ( 84%)]  Loss:  3.717617 (3.3293)  Time: 1.102s,  929.20/s  (1.090s,  939.81/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [1100/1251 ( 88%)]  Loss:  3.028978 (3.3162)  Time: 1.081s,  946.95/s  (1.090s,  939.87/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1150/1251 ( 92%)]  Loss:  3.320395 (3.3164)  Time: 1.086s,  942.82/s  (1.090s,  939.78/s)  LR: 3.920e-04  Data: 0.011 (0.013)
Train: 172 [1200/1251 ( 96%)]  Loss:  3.565325 (3.3264)  Time: 1.073s,  953.91/s  (1.089s,  939.94/s)  LR: 3.920e-04  Data: 0.012 (0.013)
Train: 172 [1250/1251 (100%)]  Loss:  3.597427 (3.3368)  Time: 1.071s,  956.19/s  (1.090s,  939.68/s)  LR: 3.920e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.831 (5.831)  Loss:  0.4782 (0.4782)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.232 (0.445)  Loss:  0.6131 (0.9625)  Acc@1: 86.3208 (78.0020)  Acc@5: 97.4057 (94.3400)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-164.pth.tar', 77.49600000488282)

Train: 173 [   0/1251 (  0%)]  Loss:  3.348119 (3.3481)  Time: 1.083s,  945.30/s  (1.083s,  945.30/s)  LR: 3.869e-04  Data: 0.023 (0.023)
Train: 173 [  50/1251 (  4%)]  Loss:  3.436109 (3.3921)  Time: 1.094s,  935.59/s  (1.087s,  942.21/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 100/1251 (  8%)]  Loss:  3.640260 (3.4748)  Time: 1.078s,  950.04/s  (1.088s,  941.60/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 150/1251 ( 12%)]  Loss:  3.658991 (3.5209)  Time: 1.091s,  938.87/s  (1.087s,  942.17/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 200/1251 ( 16%)]  Loss:  3.297174 (3.4761)  Time: 1.097s,  933.12/s  (1.090s,  939.50/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 250/1251 ( 20%)]  Loss:  3.349178 (3.4550)  Time: 1.079s,  948.68/s  (1.091s,  938.41/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 300/1251 ( 24%)]  Loss:  3.811712 (3.5059)  Time: 1.106s,  925.78/s  (1.090s,  939.74/s)  LR: 3.869e-04  Data: 0.011 (0.013)
Train: 173 [ 350/1251 ( 28%)]  Loss:  3.250842 (3.4740)  Time: 1.078s,  949.82/s  (1.090s,  939.84/s)  LR: 3.869e-04  Data: 0.017 (0.013)
Train: 173 [ 400/1251 ( 32%)]  Loss:  3.265086 (3.4508)  Time: 1.106s,  926.17/s  (1.089s,  939.97/s)  LR: 3.869e-04  Data: 0.013 (0.013)
Train: 173 [ 450/1251 ( 36%)]  Loss:  3.431278 (3.4489)  Time: 1.107s,  925.03/s  (1.090s,  939.39/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 500/1251 ( 40%)]  Loss:  3.577416 (3.4606)  Time: 1.082s,  946.29/s  (1.090s,  939.46/s)  LR: 3.869e-04  Data: 0.019 (0.013)
Train: 173 [ 550/1251 ( 44%)]  Loss:  2.811867 (3.4065)  Time: 1.094s,  935.94/s  (1.091s,  938.98/s)  LR: 3.869e-04  Data: 0.014 (0.013)
Train: 173 [ 600/1251 ( 48%)]  Loss:  3.618487 (3.4228)  Time: 1.077s,  951.02/s  (1.090s,  939.38/s)  LR: 3.869e-04  Data: 0.013 (0.013)
Train: 173 [ 650/1251 ( 52%)]  Loss:  3.557880 (3.4325)  Time: 1.096s,  934.11/s  (1.090s,  939.49/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 700/1251 ( 56%)]  Loss:  3.600726 (3.4437)  Time: 1.076s,  951.72/s  (1.090s,  939.14/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 750/1251 ( 60%)]  Loss:  3.382753 (3.4399)  Time: 1.076s,  951.64/s  (1.090s,  939.18/s)  LR: 3.869e-04  Data: 0.013 (0.013)
Train: 173 [ 800/1251 ( 64%)]  Loss:  3.121988 (3.4212)  Time: 1.104s,  927.16/s  (1.090s,  939.13/s)  LR: 3.869e-04  Data: 0.014 (0.013)
Train: 173 [ 850/1251 ( 68%)]  Loss:  3.429319 (3.4216)  Time: 1.082s,  946.37/s  (1.090s,  939.05/s)  LR: 3.869e-04  Data: 0.013 (0.013)
Train: 173 [ 900/1251 ( 72%)]  Loss:  3.391955 (3.4201)  Time: 1.081s,  947.56/s  (1.090s,  939.04/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [ 950/1251 ( 76%)]  Loss:  3.303113 (3.4142)  Time: 1.077s,  950.71/s  (1.091s,  938.98/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [1000/1251 ( 80%)]  Loss:  3.308834 (3.4092)  Time: 1.076s,  951.51/s  (1.090s,  939.18/s)  LR: 3.869e-04  Data: 0.013 (0.013)
Train: 173 [1050/1251 ( 84%)]  Loss:  3.635913 (3.4195)  Time: 1.096s,  934.72/s  (1.090s,  939.12/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [1100/1251 ( 88%)]  Loss:  3.236396 (3.4115)  Time: 1.095s,  934.96/s  (1.091s,  938.91/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [1150/1251 ( 92%)]  Loss:  3.039997 (3.3961)  Time: 1.096s,  934.15/s  (1.090s,  939.12/s)  LR: 3.869e-04  Data: 0.012 (0.013)
Train: 173 [1200/1251 ( 96%)]  Loss:  3.338784 (3.3938)  Time: 1.106s,  925.52/s  (1.091s,  938.96/s)  LR: 3.869e-04  Data: 0.014 (0.013)
Train: 173 [1250/1251 (100%)]  Loss:  3.499692 (3.3978)  Time: 1.078s,  950.04/s  (1.091s,  938.94/s)  LR: 3.869e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.699 (5.699)  Loss:  0.4862 (0.4862)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.6492 (0.9743)  Acc@1: 85.0236 (77.9560)  Acc@5: 97.0519 (94.3860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-158.pth.tar', 77.49600008789062)

Train: 174 [   0/1251 (  0%)]  Loss:  3.146010 (3.1460)  Time: 1.082s,  946.03/s  (1.082s,  946.03/s)  LR: 3.819e-04  Data: 0.021 (0.021)
Train: 174 [  50/1251 (  4%)]  Loss:  3.455580 (3.3008)  Time: 1.097s,  933.37/s  (1.084s,  944.26/s)  LR: 3.819e-04  Data: 0.013 (0.013)
Train: 174 [ 100/1251 (  8%)]  Loss:  2.856516 (3.1527)  Time: 1.076s,  951.87/s  (1.089s,  940.70/s)  LR: 3.819e-04  Data: 0.014 (0.013)
Train: 174 [ 150/1251 ( 12%)]  Loss:  3.236896 (3.1738)  Time: 1.089s,  940.03/s  (1.089s,  940.46/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [ 200/1251 ( 16%)]  Loss:  3.409937 (3.2210)  Time: 1.076s,  951.37/s  (1.090s,  939.76/s)  LR: 3.819e-04  Data: 0.013 (0.013)
Train: 174 [ 250/1251 ( 20%)]  Loss:  3.279205 (3.2307)  Time: 1.076s,  951.41/s  (1.089s,  939.99/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 300/1251 ( 24%)]  Loss:  3.269415 (3.2362)  Time: 1.083s,  945.94/s  (1.090s,  939.42/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 350/1251 ( 28%)]  Loss:  3.599613 (3.2816)  Time: 1.097s,  933.65/s  (1.090s,  939.36/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 400/1251 ( 32%)]  Loss:  3.505044 (3.3065)  Time: 1.075s,  952.59/s  (1.090s,  939.37/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 174 [ 450/1251 ( 36%)]  Loss:  3.322041 (3.3080)  Time: 1.092s,  937.50/s  (1.090s,  939.78/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 500/1251 ( 40%)]  Loss:  3.380552 (3.3146)  Time: 1.079s,  949.19/s  (1.090s,  939.47/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 550/1251 ( 44%)]  Loss:  3.436396 (3.3248)  Time: 1.094s,  935.88/s  (1.090s,  939.52/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 600/1251 ( 48%)]  Loss:  3.535324 (3.3410)  Time: 1.095s,  935.34/s  (1.090s,  939.19/s)  LR: 3.819e-04  Data: 0.014 (0.013)
Train: 174 [ 650/1251 ( 52%)]  Loss:  3.496942 (3.3521)  Time: 1.076s,  951.83/s  (1.090s,  939.39/s)  LR: 3.819e-04  Data: 0.013 (0.013)
Train: 174 [ 700/1251 ( 56%)]  Loss:  3.321946 (3.3501)  Time: 1.076s,  951.97/s  (1.090s,  939.61/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 750/1251 ( 60%)]  Loss:  3.505014 (3.3598)  Time: 1.083s,  945.40/s  (1.090s,  939.71/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 800/1251 ( 64%)]  Loss:  3.140895 (3.3469)  Time: 1.096s,  934.52/s  (1.090s,  939.59/s)  LR: 3.819e-04  Data: 0.011 (0.013)
Train: 174 [ 850/1251 ( 68%)]  Loss:  3.505829 (3.3557)  Time: 1.094s,  936.07/s  (1.090s,  939.52/s)  LR: 3.819e-04  Data: 0.013 (0.013)
Train: 174 [ 900/1251 ( 72%)]  Loss:  3.234526 (3.3494)  Time: 1.076s,  951.27/s  (1.090s,  939.47/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [ 950/1251 ( 76%)]  Loss:  3.134597 (3.3386)  Time: 1.079s,  949.15/s  (1.090s,  939.59/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [1000/1251 ( 80%)]  Loss:  3.223264 (3.3331)  Time: 1.079s,  948.87/s  (1.090s,  939.84/s)  LR: 3.819e-04  Data: 0.012 (0.013)
Train: 174 [1050/1251 ( 84%)]  Loss:  3.450725 (3.3385)  Time: 1.077s,  950.46/s  (1.089s,  940.00/s)  LR: 3.819e-04  Data: 0.013 (0.013)
Train: 174 [1100/1251 ( 88%)]  Loss:  3.392690 (3.3408)  Time: 1.078s,  950.20/s  (1.089s,  940.35/s)  LR: 3.819e-04  Data: 0.016 (0.012)
Train: 174 [1150/1251 ( 92%)]  Loss:  3.529004 (3.3487)  Time: 1.096s,  934.72/s  (1.089s,  940.15/s)  LR: 3.819e-04  Data: 0.012 (0.012)
Train: 174 [1200/1251 ( 96%)]  Loss:  2.945984 (3.3326)  Time: 1.078s,  949.75/s  (1.089s,  939.94/s)  LR: 3.819e-04  Data: 0.014 (0.013)
Train: 174 [1250/1251 (100%)]  Loss:  3.238909 (3.3290)  Time: 1.081s,  947.58/s  (1.090s,  939.76/s)  LR: 3.819e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.841 (5.841)  Loss:  0.4547 (0.4547)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5955 (0.9512)  Acc@1: 86.2028 (78.0900)  Acc@5: 97.5236 (94.4880)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-159.pth.tar', 77.51999992675782)

Train: 175 [   0/1251 (  0%)]  Loss:  3.143498 (3.1435)  Time: 1.107s,  925.03/s  (1.107s,  925.03/s)  LR: 3.769e-04  Data: 0.026 (0.026)
Train: 175 [  50/1251 (  4%)]  Loss:  3.225517 (3.1845)  Time: 1.078s,  949.93/s  (1.084s,  944.76/s)  LR: 3.769e-04  Data: 0.013 (0.014)
Train: 175 [ 100/1251 (  8%)]  Loss:  3.310129 (3.2264)  Time: 1.095s,  935.25/s  (1.092s,  937.57/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 150/1251 ( 12%)]  Loss:  3.094939 (3.1935)  Time: 1.078s,  949.57/s  (1.090s,  939.75/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 200/1251 ( 16%)]  Loss:  3.237660 (3.2023)  Time: 1.093s,  936.57/s  (1.089s,  940.51/s)  LR: 3.769e-04  Data: 0.016 (0.013)
Train: 175 [ 250/1251 ( 20%)]  Loss:  3.662337 (3.2790)  Time: 1.081s,  947.30/s  (1.088s,  940.85/s)  LR: 3.769e-04  Data: 0.019 (0.013)
Train: 175 [ 300/1251 ( 24%)]  Loss:  3.260659 (3.2764)  Time: 1.108s,  924.10/s  (1.090s,  939.31/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 350/1251 ( 28%)]  Loss:  3.195460 (3.2663)  Time: 1.078s,  949.53/s  (1.090s,  939.39/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 400/1251 ( 32%)]  Loss:  2.942849 (3.2303)  Time: 1.089s,  940.60/s  (1.090s,  939.56/s)  LR: 3.769e-04  Data: 0.014 (0.013)
Train: 175 [ 450/1251 ( 36%)]  Loss:  3.547132 (3.2620)  Time: 1.079s,  948.76/s  (1.091s,  938.96/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 500/1251 ( 40%)]  Loss:  3.588748 (3.2917)  Time: 1.104s,  927.27/s  (1.091s,  938.33/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 550/1251 ( 44%)]  Loss:  3.163404 (3.2810)  Time: 1.077s,  950.91/s  (1.092s,  938.09/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 600/1251 ( 48%)]  Loss:  3.224930 (3.2767)  Time: 1.077s,  950.76/s  (1.091s,  938.37/s)  LR: 3.769e-04  Data: 0.013 (0.013)
Train: 175 [ 650/1251 ( 52%)]  Loss:  3.568445 (3.2976)  Time: 1.093s,  936.49/s  (1.091s,  938.18/s)  LR: 3.769e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 175 [ 700/1251 ( 56%)]  Loss:  3.379000 (3.3030)  Time: 1.077s,  950.98/s  (1.091s,  938.61/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 750/1251 ( 60%)]  Loss:  3.478952 (3.3140)  Time: 1.078s,  950.03/s  (1.090s,  939.03/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 800/1251 ( 64%)]  Loss:  3.637151 (3.3330)  Time: 1.105s,  926.78/s  (1.090s,  939.08/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [ 850/1251 ( 68%)]  Loss:  3.227135 (3.3271)  Time: 1.077s,  950.72/s  (1.090s,  939.11/s)  LR: 3.769e-04  Data: 0.013 (0.013)
Train: 175 [ 900/1251 ( 72%)]  Loss:  3.035594 (3.3118)  Time: 1.080s,  948.40/s  (1.090s,  939.49/s)  LR: 3.769e-04  Data: 0.013 (0.013)
Train: 175 [ 950/1251 ( 76%)]  Loss:  3.305223 (3.3114)  Time: 1.085s,  943.96/s  (1.090s,  939.80/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [1000/1251 ( 80%)]  Loss:  3.196867 (3.3060)  Time: 1.076s,  951.74/s  (1.090s,  939.64/s)  LR: 3.769e-04  Data: 0.014 (0.013)
Train: 175 [1050/1251 ( 84%)]  Loss:  3.228380 (3.3025)  Time: 1.103s,  928.22/s  (1.090s,  939.69/s)  LR: 3.769e-04  Data: 0.014 (0.013)
Train: 175 [1100/1251 ( 88%)]  Loss:  3.232705 (3.2994)  Time: 1.079s,  949.22/s  (1.090s,  939.65/s)  LR: 3.769e-04  Data: 0.018 (0.013)
Train: 175 [1150/1251 ( 92%)]  Loss:  2.981381 (3.2862)  Time: 1.076s,  951.91/s  (1.090s,  939.64/s)  LR: 3.769e-04  Data: 0.012 (0.013)
Train: 175 [1200/1251 ( 96%)]  Loss:  3.221346 (3.2836)  Time: 1.094s,  935.64/s  (1.090s,  939.44/s)  LR: 3.769e-04  Data: 0.013 (0.013)
Train: 175 [1250/1251 (100%)]  Loss:  3.665298 (3.2983)  Time: 1.062s,  964.36/s  (1.090s,  939.42/s)  LR: 3.769e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.909 (5.909)  Loss:  0.4901 (0.4901)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6050 (0.9600)  Acc@1: 86.5566 (78.1120)  Acc@5: 97.0519 (94.4900)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-162.pth.tar', 77.52000003173828)

Train: 176 [   0/1251 (  0%)]  Loss:  3.236211 (3.2362)  Time: 1.084s,  944.70/s  (1.084s,  944.70/s)  LR: 3.719e-04  Data: 0.023 (0.023)
Train: 176 [  50/1251 (  4%)]  Loss:  3.362820 (3.2995)  Time: 1.076s,  951.24/s  (1.089s,  940.39/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 100/1251 (  8%)]  Loss:  3.317942 (3.3057)  Time: 1.094s,  935.88/s  (1.090s,  939.34/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 150/1251 ( 12%)]  Loss:  3.200107 (3.2793)  Time: 1.080s,  948.15/s  (1.089s,  940.42/s)  LR: 3.719e-04  Data: 0.017 (0.013)
Train: 176 [ 200/1251 ( 16%)]  Loss:  3.434772 (3.3104)  Time: 1.077s,  950.84/s  (1.089s,  940.38/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 250/1251 ( 20%)]  Loss:  2.921109 (3.2455)  Time: 1.078s,  950.24/s  (1.088s,  941.13/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 300/1251 ( 24%)]  Loss:  3.152589 (3.2322)  Time: 1.080s,  947.94/s  (1.088s,  941.02/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 350/1251 ( 28%)]  Loss:  3.711702 (3.2922)  Time: 1.082s,  946.59/s  (1.087s,  941.65/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 400/1251 ( 32%)]  Loss:  3.010121 (3.2608)  Time: 1.102s,  928.98/s  (1.087s,  941.83/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 450/1251 ( 36%)]  Loss:  3.211514 (3.2559)  Time: 1.082s,  946.12/s  (1.088s,  941.45/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 500/1251 ( 40%)]  Loss:  3.010521 (3.2336)  Time: 1.077s,  950.57/s  (1.088s,  941.50/s)  LR: 3.719e-04  Data: 0.014 (0.013)
Train: 176 [ 550/1251 ( 44%)]  Loss:  3.223885 (3.2328)  Time: 1.093s,  936.79/s  (1.088s,  941.19/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 600/1251 ( 48%)]  Loss:  2.836768 (3.2023)  Time: 1.099s,  932.14/s  (1.088s,  941.30/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 650/1251 ( 52%)]  Loss:  3.312256 (3.2102)  Time: 1.106s,  925.62/s  (1.088s,  941.44/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 700/1251 ( 56%)]  Loss:  3.399366 (3.2228)  Time: 1.097s,  933.86/s  (1.088s,  941.25/s)  LR: 3.719e-04  Data: 0.013 (0.013)
Train: 176 [ 750/1251 ( 60%)]  Loss:  2.917141 (3.2037)  Time: 1.103s,  928.25/s  (1.088s,  940.81/s)  LR: 3.719e-04  Data: 0.013 (0.013)
Train: 176 [ 800/1251 ( 64%)]  Loss:  3.309836 (3.2099)  Time: 1.103s,  928.69/s  (1.089s,  940.51/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 850/1251 ( 68%)]  Loss:  3.310990 (3.2155)  Time: 1.076s,  951.96/s  (1.089s,  940.50/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [ 900/1251 ( 72%)]  Loss:  3.413966 (3.2260)  Time: 1.175s,  871.32/s  (1.089s,  940.54/s)  LR: 3.719e-04  Data: 0.011 (0.013)
Train: 176 [ 950/1251 ( 76%)]  Loss:  3.386989 (3.2340)  Time: 1.096s,  934.21/s  (1.089s,  940.62/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1000/1251 ( 80%)]  Loss:  3.178039 (3.2314)  Time: 1.077s,  951.01/s  (1.089s,  940.48/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1050/1251 ( 84%)]  Loss:  3.191329 (3.2295)  Time: 1.081s,  947.41/s  (1.089s,  940.36/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1100/1251 ( 88%)]  Loss:  3.427735 (3.2382)  Time: 1.099s,  931.35/s  (1.089s,  940.38/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1150/1251 ( 92%)]  Loss:  3.113034 (3.2329)  Time: 1.088s,  940.86/s  (1.089s,  940.41/s)  LR: 3.719e-04  Data: 0.014 (0.013)
Train: 176 [1200/1251 ( 96%)]  Loss:  3.516210 (3.2443)  Time: 1.077s,  950.45/s  (1.089s,  940.42/s)  LR: 3.719e-04  Data: 0.012 (0.013)
Train: 176 [1250/1251 (100%)]  Loss:  3.323384 (3.2473)  Time: 1.080s,  948.23/s  (1.089s,  940.26/s)  LR: 3.719e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.797 (5.797)  Loss:  0.4704 (0.4704)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5631 (0.9654)  Acc@1: 87.5000 (77.8980)  Acc@5: 97.5236 (94.2880)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 77.89800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-169.pth.tar', 77.64000000488281)

Train: 177 [   0/1251 (  0%)]  Loss:  3.161279 (3.1613)  Time: 1.083s,  945.16/s  (1.083s,  945.16/s)  LR: 3.669e-04  Data: 0.023 (0.023)
Train: 177 [  50/1251 (  4%)]  Loss:  3.396961 (3.2791)  Time: 1.077s,  950.55/s  (1.094s,  935.69/s)  LR: 3.669e-04  Data: 0.013 (0.013)
Train: 177 [ 100/1251 (  8%)]  Loss:  3.365647 (3.3080)  Time: 1.106s,  925.60/s  (1.094s,  936.44/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 150/1251 ( 12%)]  Loss:  3.125646 (3.2624)  Time: 1.076s,  951.99/s  (1.092s,  937.31/s)  LR: 3.669e-04  Data: 0.014 (0.013)
Train: 177 [ 200/1251 ( 16%)]  Loss:  3.173776 (3.2447)  Time: 1.078s,  949.99/s  (1.090s,  939.63/s)  LR: 3.669e-04  Data: 0.017 (0.013)
Train: 177 [ 250/1251 ( 20%)]  Loss:  3.335253 (3.2598)  Time: 1.093s,  936.98/s  (1.090s,  939.57/s)  LR: 3.669e-04  Data: 0.014 (0.013)
Train: 177 [ 300/1251 ( 24%)]  Loss:  3.592602 (3.3073)  Time: 1.096s,  934.10/s  (1.090s,  939.62/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 350/1251 ( 28%)]  Loss:  3.331229 (3.3103)  Time: 1.078s,  949.99/s  (1.090s,  939.28/s)  LR: 3.669e-04  Data: 0.013 (0.013)
Train: 177 [ 400/1251 ( 32%)]  Loss:  3.226233 (3.3010)  Time: 1.077s,  951.09/s  (1.089s,  940.52/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 450/1251 ( 36%)]  Loss:  3.547210 (3.3256)  Time: 1.105s,  926.99/s  (1.089s,  940.43/s)  LR: 3.669e-04  Data: 0.015 (0.013)
Train: 177 [ 500/1251 ( 40%)]  Loss:  3.405651 (3.3329)  Time: 1.095s,  935.57/s  (1.089s,  940.27/s)  LR: 3.669e-04  Data: 0.013 (0.013)
Train: 177 [ 550/1251 ( 44%)]  Loss:  3.496889 (3.3465)  Time: 1.084s,  944.26/s  (1.089s,  940.57/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 600/1251 ( 48%)]  Loss:  3.595161 (3.3657)  Time: 1.097s,  933.23/s  (1.089s,  940.49/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 650/1251 ( 52%)]  Loss:  3.288107 (3.3601)  Time: 1.097s,  933.64/s  (1.089s,  940.42/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 700/1251 ( 56%)]  Loss:  3.535699 (3.3718)  Time: 1.101s,  930.42/s  (1.089s,  940.29/s)  LR: 3.669e-04  Data: 0.013 (0.013)
Train: 177 [ 750/1251 ( 60%)]  Loss:  3.087117 (3.3540)  Time: 1.095s,  935.43/s  (1.089s,  940.31/s)  LR: 3.669e-04  Data: 0.015 (0.013)
Train: 177 [ 800/1251 ( 64%)]  Loss:  3.402183 (3.3569)  Time: 1.095s,  934.89/s  (1.089s,  940.29/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [ 850/1251 ( 68%)]  Loss:  3.535107 (3.3668)  Time: 1.080s,  947.76/s  (1.089s,  940.25/s)  LR: 3.669e-04  Data: 0.018 (0.013)
Train: 177 [ 900/1251 ( 72%)]  Loss:  3.325191 (3.3646)  Time: 1.082s,  946.21/s  (1.089s,  940.32/s)  LR: 3.669e-04  Data: 0.014 (0.013)
Train: 177 [ 950/1251 ( 76%)]  Loss:  3.534978 (3.3731)  Time: 1.095s,  934.80/s  (1.089s,  940.23/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [1000/1251 ( 80%)]  Loss:  3.293351 (3.3693)  Time: 1.077s,  950.39/s  (1.089s,  940.34/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [1050/1251 ( 84%)]  Loss:  3.318912 (3.3670)  Time: 1.080s,  948.03/s  (1.089s,  940.22/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [1100/1251 ( 88%)]  Loss:  3.403436 (3.3686)  Time: 1.098s,  932.69/s  (1.089s,  940.21/s)  LR: 3.669e-04  Data: 0.011 (0.013)
Train: 177 [1150/1251 ( 92%)]  Loss:  3.500087 (3.3741)  Time: 1.076s,  951.26/s  (1.089s,  940.24/s)  LR: 3.669e-04  Data: 0.013 (0.013)
Train: 177 [1200/1251 ( 96%)]  Loss:  3.323724 (3.3721)  Time: 1.077s,  950.61/s  (1.089s,  940.15/s)  LR: 3.669e-04  Data: 0.012 (0.013)
Train: 177 [1250/1251 (100%)]  Loss:  3.451870 (3.3751)  Time: 1.062s,  964.10/s  (1.089s,  940.15/s)  LR: 3.669e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.826 (5.826)  Loss:  0.4982 (0.4982)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5963 (0.9606)  Acc@1: 86.5566 (78.2200)  Acc@5: 97.7594 (94.3320)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 77.89800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-167.pth.tar', 77.73400021240235)

Train: 178 [   0/1251 (  0%)]  Loss:  3.280094 (3.2801)  Time: 1.084s,  944.82/s  (1.084s,  944.82/s)  LR: 3.619e-04  Data: 0.022 (0.022)
Train: 178 [  50/1251 (  4%)]  Loss:  3.123787 (3.2019)  Time: 1.077s,  950.63/s  (1.083s,  945.15/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 100/1251 (  8%)]  Loss:  3.210353 (3.2047)  Time: 1.079s,  948.79/s  (1.087s,  942.28/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 150/1251 ( 12%)]  Loss:  3.125622 (3.1850)  Time: 1.088s,  941.34/s  (1.089s,  939.92/s)  LR: 3.619e-04  Data: 0.014 (0.013)
Train: 178 [ 200/1251 ( 16%)]  Loss:  3.406356 (3.2292)  Time: 1.099s,  931.82/s  (1.089s,  940.65/s)  LR: 3.619e-04  Data: 0.016 (0.013)
Train: 178 [ 250/1251 ( 20%)]  Loss:  3.307078 (3.2422)  Time: 1.076s,  951.32/s  (1.089s,  940.66/s)  LR: 3.619e-04  Data: 0.015 (0.013)
Train: 178 [ 300/1251 ( 24%)]  Loss:  3.637663 (3.2987)  Time: 1.096s,  934.68/s  (1.087s,  941.82/s)  LR: 3.619e-04  Data: 0.015 (0.013)
Train: 178 [ 350/1251 ( 28%)]  Loss:  3.299881 (3.2989)  Time: 1.086s,  943.19/s  (1.087s,  941.77/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 400/1251 ( 32%)]  Loss:  3.441672 (3.3147)  Time: 1.094s,  935.77/s  (1.087s,  942.20/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 450/1251 ( 36%)]  Loss:  3.363963 (3.3196)  Time: 1.077s,  950.83/s  (1.087s,  942.20/s)  LR: 3.619e-04  Data: 0.013 (0.013)
Train: 178 [ 500/1251 ( 40%)]  Loss:  3.337093 (3.3212)  Time: 1.109s,  923.71/s  (1.087s,  941.95/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 550/1251 ( 44%)]  Loss:  3.115710 (3.3041)  Time: 1.094s,  935.88/s  (1.088s,  941.27/s)  LR: 3.619e-04  Data: 0.015 (0.013)
Train: 178 [ 600/1251 ( 48%)]  Loss:  2.970455 (3.2784)  Time: 1.078s,  950.17/s  (1.088s,  941.57/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 650/1251 ( 52%)]  Loss:  3.372864 (3.2852)  Time: 1.079s,  949.41/s  (1.088s,  941.58/s)  LR: 3.619e-04  Data: 0.013 (0.013)
Train: 178 [ 700/1251 ( 56%)]  Loss:  3.654331 (3.3098)  Time: 1.082s,  946.08/s  (1.087s,  941.86/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 750/1251 ( 60%)]  Loss:  3.489173 (3.3210)  Time: 1.076s,  951.31/s  (1.087s,  942.25/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 800/1251 ( 64%)]  Loss:  3.309850 (3.3203)  Time: 1.084s,  944.59/s  (1.087s,  941.93/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [ 850/1251 ( 68%)]  Loss:  3.286702 (3.3185)  Time: 1.074s,  953.81/s  (1.088s,  941.54/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [ 900/1251 ( 72%)]  Loss:  3.305640 (3.3178)  Time: 1.076s,  952.07/s  (1.087s,  941.67/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 178 [ 950/1251 ( 76%)]  Loss:  3.170491 (3.3104)  Time: 1.045s,  979.52/s  (1.087s,  941.65/s)  LR: 3.619e-04  Data: 0.013 (0.013)
Train: 178 [1000/1251 ( 80%)]  Loss:  3.160467 (3.3033)  Time: 1.106s,  926.00/s  (1.088s,  941.37/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [1050/1251 ( 84%)]  Loss:  3.168156 (3.2972)  Time: 1.076s,  951.51/s  (1.088s,  941.09/s)  LR: 3.619e-04  Data: 0.015 (0.013)
Train: 178 [1100/1251 ( 88%)]  Loss:  3.245988 (3.2949)  Time: 1.076s,  951.33/s  (1.088s,  940.95/s)  LR: 3.619e-04  Data: 0.014 (0.013)
Train: 178 [1150/1251 ( 92%)]  Loss:  3.094692 (3.2866)  Time: 1.078s,  950.30/s  (1.088s,  940.95/s)  LR: 3.619e-04  Data: 0.011 (0.013)
Train: 178 [1200/1251 ( 96%)]  Loss:  3.426800 (3.2922)  Time: 1.076s,  951.88/s  (1.088s,  940.84/s)  LR: 3.619e-04  Data: 0.012 (0.013)
Train: 178 [1250/1251 (100%)]  Loss:  2.929508 (3.2782)  Time: 1.068s,  958.71/s  (1.088s,  940.98/s)  LR: 3.619e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.937 (5.937)  Loss:  0.4742 (0.4742)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5850 (0.9458)  Acc@1: 86.4387 (78.1200)  Acc@5: 97.5236 (94.3900)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 77.89800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-171.pth.tar', 77.76200016113282)

Train: 179 [   0/1251 (  0%)]  Loss:  3.058156 (3.0582)  Time: 1.085s,  944.21/s  (1.085s,  944.21/s)  LR: 3.570e-04  Data: 0.023 (0.023)
Train: 179 [  50/1251 (  4%)]  Loss:  3.206755 (3.1325)  Time: 1.079s,  948.97/s  (1.088s,  940.86/s)  LR: 3.570e-04  Data: 0.012 (0.013)
Train: 179 [ 100/1251 (  8%)]  Loss:  3.215381 (3.1601)  Time: 1.080s,  948.29/s  (1.090s,  939.07/s)  LR: 3.570e-04  Data: 0.012 (0.013)
Train: 179 [ 150/1251 ( 12%)]  Loss:  3.233297 (3.1784)  Time: 1.097s,  933.51/s  (1.090s,  939.31/s)  LR: 3.570e-04  Data: 0.012 (0.012)
Train: 179 [ 200/1251 ( 16%)]  Loss:  3.396463 (3.2220)  Time: 1.076s,  951.58/s  (1.089s,  940.33/s)  LR: 3.570e-04  Data: 0.012 (0.012)
Train: 179 [ 250/1251 ( 20%)]  Loss:  2.950883 (3.1768)  Time: 1.078s,  950.27/s  (1.088s,  941.22/s)  LR: 3.570e-04  Data: 0.013 (0.012)
Train: 179 [ 300/1251 ( 24%)]  Loss:  3.201687 (3.1804)  Time: 1.106s,  926.16/s  (1.088s,  940.88/s)  LR: 3.570e-04  Data: 0.012 (0.012)
Train: 179 [ 350/1251 ( 28%)]  Loss:  3.261590 (3.1905)  Time: 1.080s,  948.30/s  (1.088s,  941.27/s)  LR: 3.570e-04  Data: 0.014 (0.013)
Train: 179 [ 400/1251 ( 32%)]  Loss:  3.556853 (3.2312)  Time: 1.082s,  946.26/s  (1.088s,  940.88/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 450/1251 ( 36%)]  Loss:  2.955861 (3.2037)  Time: 1.077s,  950.45/s  (1.089s,  940.65/s)  LR: 3.570e-04  Data: 0.012 (0.013)
Train: 179 [ 500/1251 ( 40%)]  Loss:  3.205706 (3.2039)  Time: 1.103s,  928.34/s  (1.089s,  940.67/s)  LR: 3.570e-04  Data: 0.015 (0.012)
Train: 179 [ 550/1251 ( 44%)]  Loss:  3.505037 (3.2290)  Time: 1.082s,  946.68/s  (1.089s,  940.50/s)  LR: 3.570e-04  Data: 0.011 (0.013)
Train: 179 [ 600/1251 ( 48%)]  Loss:  2.911092 (3.2045)  Time: 1.096s,  934.32/s  (1.089s,  940.11/s)  LR: 3.570e-04  Data: 0.012 (0.013)
Train: 179 [ 650/1251 ( 52%)]  Loss:  3.171773 (3.2022)  Time: 1.096s,  934.30/s  (1.090s,  939.87/s)  LR: 3.570e-04  Data: 0.015 (0.013)
Train: 179 [ 700/1251 ( 56%)]  Loss:  3.228285 (3.2039)  Time: 1.094s,  936.15/s  (1.090s,  939.88/s)  LR: 3.570e-04  Data: 0.012 (0.012)
Train: 179 [ 750/1251 ( 60%)]  Loss:  3.604608 (3.2290)  Time: 1.082s,  946.69/s  (1.090s,  939.79/s)  LR: 3.570e-04  Data: 0.014 (0.012)
Train: 179 [ 800/1251 ( 64%)]  Loss:  3.172802 (3.2257)  Time: 1.175s,  871.52/s  (1.090s,  939.16/s)  LR: 3.570e-04  Data: 0.012 (0.012)
Train: 179 [ 850/1251 ( 68%)]  Loss:  3.493005 (3.2405)  Time: 1.095s,  935.55/s  (1.090s,  939.15/s)  LR: 3.570e-04  Data: 0.012 (0.012)
Train: 179 [ 900/1251 ( 72%)]  Loss:  3.599419 (3.2594)  Time: 1.104s,  927.13/s  (1.090s,  939.19/s)  LR: 3.570e-04  Data: 0.014 (0.012)
Train: 179 [ 950/1251 ( 76%)]  Loss:  3.481281 (3.2705)  Time: 1.102s,  928.84/s  (1.090s,  939.19/s)  LR: 3.570e-04  Data: 0.013 (0.013)
Train: 179 [1000/1251 ( 80%)]  Loss:  3.212586 (3.2677)  Time: 1.096s,  934.37/s  (1.090s,  939.09/s)  LR: 3.570e-04  Data: 0.014 (0.013)
Train: 179 [1050/1251 ( 84%)]  Loss:  3.090693 (3.2597)  Time: 1.076s,  951.61/s  (1.090s,  939.24/s)  LR: 3.570e-04  Data: 0.012 (0.013)
Train: 179 [1100/1251 ( 88%)]  Loss:  3.482283 (3.2694)  Time: 1.081s,  947.51/s  (1.090s,  939.42/s)  LR: 3.570e-04  Data: 0.013 (0.013)
Train: 179 [1150/1251 ( 92%)]  Loss:  3.270916 (3.2694)  Time: 1.104s,  927.25/s  (1.090s,  939.26/s)  LR: 3.570e-04  Data: 0.014 (0.013)
Train: 179 [1200/1251 ( 96%)]  Loss:  3.060951 (3.2611)  Time: 1.077s,  951.09/s  (1.090s,  939.29/s)  LR: 3.570e-04  Data: 0.013 (0.013)
Train: 179 [1250/1251 (100%)]  Loss:  3.339255 (3.2641)  Time: 1.078s,  949.64/s  (1.090s,  939.09/s)  LR: 3.570e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.869 (5.869)  Loss:  0.4682 (0.4682)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5828 (0.9500)  Acc@1: 85.7311 (78.0500)  Acc@5: 97.6415 (94.4440)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 78.05000013671875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 77.89800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-168.pth.tar', 77.774000078125)

Train: 180 [   0/1251 (  0%)]  Loss:  3.178356 (3.1784)  Time: 1.087s,  942.02/s  (1.087s,  942.02/s)  LR: 3.520e-04  Data: 0.023 (0.023)
Train: 180 [  50/1251 (  4%)]  Loss:  3.392609 (3.2855)  Time: 1.084s,  944.97/s  (1.086s,  943.13/s)  LR: 3.520e-04  Data: 0.013 (0.013)
Train: 180 [ 100/1251 (  8%)]  Loss:  3.475510 (3.3488)  Time: 1.076s,  951.23/s  (1.090s,  939.42/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [ 150/1251 ( 12%)]  Loss:  3.120002 (3.2916)  Time: 1.082s,  946.16/s  (1.087s,  942.02/s)  LR: 3.520e-04  Data: 0.011 (0.013)
Train: 180 [ 200/1251 ( 16%)]  Loss:  3.085931 (3.2505)  Time: 1.078s,  949.67/s  (1.087s,  942.37/s)  LR: 3.520e-04  Data: 0.012 (0.012)
Train: 180 [ 250/1251 ( 20%)]  Loss:  3.095733 (3.2247)  Time: 1.094s,  936.26/s  (1.087s,  941.69/s)  LR: 3.520e-04  Data: 0.013 (0.012)
Train: 180 [ 300/1251 ( 24%)]  Loss:  2.987576 (3.1908)  Time: 1.078s,  949.53/s  (1.089s,  940.50/s)  LR: 3.520e-04  Data: 0.012 (0.012)
Train: 180 [ 350/1251 ( 28%)]  Loss:  3.462938 (3.2248)  Time: 1.080s,  947.83/s  (1.089s,  940.10/s)  LR: 3.520e-04  Data: 0.012 (0.012)
Train: 180 [ 400/1251 ( 32%)]  Loss:  3.471464 (3.2522)  Time: 1.079s,  949.04/s  (1.088s,  941.13/s)  LR: 3.520e-04  Data: 0.014 (0.012)
Train: 180 [ 450/1251 ( 36%)]  Loss:  3.326948 (3.2597)  Time: 1.101s,  930.42/s  (1.088s,  941.44/s)  LR: 3.520e-04  Data: 0.014 (0.012)
Train: 180 [ 500/1251 ( 40%)]  Loss:  2.864867 (3.2238)  Time: 1.075s,  952.22/s  (1.087s,  941.92/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 180 [ 550/1251 ( 44%)]  Loss:  3.365604 (3.2356)  Time: 1.078s,  950.25/s  (1.087s,  941.73/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [ 600/1251 ( 48%)]  Loss:  3.473647 (3.2539)  Time: 1.076s,  951.31/s  (1.087s,  941.96/s)  LR: 3.520e-04  Data: 0.013 (0.013)
Train: 180 [ 650/1251 ( 52%)]  Loss:  3.220313 (3.2515)  Time: 1.075s,  952.87/s  (1.087s,  941.94/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [ 700/1251 ( 56%)]  Loss:  3.703030 (3.2816)  Time: 1.081s,  947.70/s  (1.087s,  942.17/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 180 [ 750/1251 ( 60%)]  Loss:  3.211151 (3.2772)  Time: 1.095s,  935.52/s  (1.087s,  942.44/s)  LR: 3.520e-04  Data: 0.015 (0.013)
Train: 180 [ 800/1251 ( 64%)]  Loss:  3.399505 (3.2844)  Time: 1.095s,  935.39/s  (1.086s,  942.52/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [ 850/1251 ( 68%)]  Loss:  3.276245 (3.2840)  Time: 1.078s,  949.95/s  (1.087s,  942.42/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [ 900/1251 ( 72%)]  Loss:  3.189824 (3.2790)  Time: 1.077s,  951.15/s  (1.086s,  942.59/s)  LR: 3.520e-04  Data: 0.015 (0.013)
Train: 180 [ 950/1251 ( 76%)]  Loss:  3.044632 (3.2673)  Time: 1.075s,  952.81/s  (1.086s,  942.75/s)  LR: 3.520e-04  Data: 0.010 (0.013)
Train: 180 [1000/1251 ( 80%)]  Loss:  3.303082 (3.2690)  Time: 1.075s,  952.17/s  (1.086s,  942.70/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [1050/1251 ( 84%)]  Loss:  3.159690 (3.2640)  Time: 1.074s,  953.86/s  (1.086s,  942.49/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [1100/1251 ( 88%)]  Loss:  3.438989 (3.2716)  Time: 1.096s,  933.90/s  (1.086s,  942.65/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [1150/1251 ( 92%)]  Loss:  3.127868 (3.2656)  Time: 1.097s,  933.03/s  (1.087s,  942.42/s)  LR: 3.520e-04  Data: 0.012 (0.013)
Train: 180 [1200/1251 ( 96%)]  Loss:  3.048521 (3.2570)  Time: 1.094s,  935.70/s  (1.087s,  942.15/s)  LR: 3.520e-04  Data: 0.015 (0.013)
Train: 180 [1250/1251 (100%)]  Loss:  3.006358 (3.2473)  Time: 1.079s,  948.85/s  (1.087s,  941.84/s)  LR: 3.520e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.803 (5.803)  Loss:  0.4685 (0.4685)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5593 (0.9414)  Acc@1: 86.0849 (78.3260)  Acc@5: 97.7594 (94.4220)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 78.05000013671875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-176.pth.tar', 77.89800012939453)

Train: 181 [   0/1251 (  0%)]  Loss:  3.138940 (3.1389)  Time: 1.109s,  923.75/s  (1.109s,  923.75/s)  LR: 3.471e-04  Data: 0.030 (0.030)
Train: 181 [  50/1251 (  4%)]  Loss:  3.336478 (3.2377)  Time: 1.077s,  950.73/s  (1.087s,  942.04/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 100/1251 (  8%)]  Loss:  3.273300 (3.2496)  Time: 1.094s,  935.81/s  (1.092s,  938.06/s)  LR: 3.471e-04  Data: 0.011 (0.013)
Train: 181 [ 150/1251 ( 12%)]  Loss:  3.305129 (3.2635)  Time: 1.075s,  952.21/s  (1.090s,  939.59/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 200/1251 ( 16%)]  Loss:  3.331598 (3.2771)  Time: 1.076s,  951.67/s  (1.090s,  939.68/s)  LR: 3.471e-04  Data: 0.014 (0.013)
Train: 181 [ 250/1251 ( 20%)]  Loss:  3.346210 (3.2886)  Time: 1.078s,  949.71/s  (1.089s,  940.19/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 300/1251 ( 24%)]  Loss:  2.975224 (3.2438)  Time: 1.079s,  948.84/s  (1.089s,  940.44/s)  LR: 3.471e-04  Data: 0.013 (0.013)
Train: 181 [ 350/1251 ( 28%)]  Loss:  3.417310 (3.2655)  Time: 1.095s,  935.40/s  (1.090s,  939.53/s)  LR: 3.471e-04  Data: 0.014 (0.013)
Train: 181 [ 400/1251 ( 32%)]  Loss:  3.160390 (3.2538)  Time: 1.096s,  933.90/s  (1.090s,  939.08/s)  LR: 3.471e-04  Data: 0.013 (0.013)
Train: 181 [ 450/1251 ( 36%)]  Loss:  3.522205 (3.2807)  Time: 1.078s,  950.34/s  (1.091s,  938.75/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 500/1251 ( 40%)]  Loss:  3.139390 (3.2678)  Time: 1.171s,  874.75/s  (1.091s,  938.76/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 550/1251 ( 44%)]  Loss:  3.199898 (3.2622)  Time: 1.074s,  953.01/s  (1.091s,  938.83/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 600/1251 ( 48%)]  Loss:  3.172990 (3.2553)  Time: 1.084s,  944.87/s  (1.091s,  938.96/s)  LR: 3.471e-04  Data: 0.014 (0.013)
Train: 181 [ 650/1251 ( 52%)]  Loss:  3.458454 (3.2698)  Time: 1.094s,  935.68/s  (1.091s,  938.89/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 700/1251 ( 56%)]  Loss:  3.325024 (3.2735)  Time: 1.076s,  951.44/s  (1.091s,  938.97/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 181 [ 750/1251 ( 60%)]  Loss:  3.192488 (3.2684)  Time: 1.078s,  949.99/s  (1.090s,  939.27/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 181 [ 800/1251 ( 64%)]  Loss:  3.295069 (3.2700)  Time: 1.096s,  934.67/s  (1.090s,  939.26/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 181 [ 850/1251 ( 68%)]  Loss:  3.202212 (3.2662)  Time: 1.081s,  947.13/s  (1.090s,  939.09/s)  LR: 3.471e-04  Data: 0.012 (0.013)
Train: 181 [ 900/1251 ( 72%)]  Loss:  3.107547 (3.2579)  Time: 1.076s,  951.83/s  (1.090s,  939.24/s)  LR: 3.471e-04  Data: 0.014 (0.012)
Train: 181 [ 950/1251 ( 76%)]  Loss:  3.246143 (3.2573)  Time: 1.082s,  946.02/s  (1.090s,  939.35/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 181 [1000/1251 ( 80%)]  Loss:  3.322469 (3.2604)  Time: 1.103s,  928.51/s  (1.090s,  939.26/s)  LR: 3.471e-04  Data: 0.014 (0.012)
Train: 181 [1050/1251 ( 84%)]  Loss:  3.516036 (3.2720)  Time: 1.081s,  946.95/s  (1.090s,  939.34/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 181 [1100/1251 ( 88%)]  Loss:  3.459769 (3.2802)  Time: 1.076s,  951.86/s  (1.090s,  939.63/s)  LR: 3.471e-04  Data: 0.012 (0.012)
Train: 181 [1150/1251 ( 92%)]  Loss:  3.088255 (3.2722)  Time: 1.194s,  857.94/s  (1.090s,  939.63/s)  LR: 3.471e-04  Data: 0.013 (0.012)
Train: 181 [1200/1251 ( 96%)]  Loss:  3.197096 (3.2692)  Time: 1.095s,  934.94/s  (1.090s,  939.29/s)  LR: 3.471e-04  Data: 0.013 (0.012)
Train: 181 [1250/1251 (100%)]  Loss:  3.065882 (3.2614)  Time: 1.080s,  947.73/s  (1.091s,  939.01/s)  LR: 3.471e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.855 (5.855)  Loss:  0.5302 (0.5302)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5855 (0.9555)  Acc@1: 85.9670 (78.2820)  Acc@5: 97.7594 (94.3200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 78.05000013671875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-173.pth.tar', 77.9559999584961)

Train: 182 [   0/1251 (  0%)]  Loss:  3.227409 (3.2274)  Time: 1.083s,  945.34/s  (1.083s,  945.34/s)  LR: 3.422e-04  Data: 0.023 (0.023)
Train: 182 [  50/1251 (  4%)]  Loss:  3.204480 (3.2159)  Time: 1.081s,  947.20/s  (1.094s,  936.41/s)  LR: 3.422e-04  Data: 0.016 (0.013)
Train: 182 [ 100/1251 (  8%)]  Loss:  3.046078 (3.1593)  Time: 1.096s,  934.39/s  (1.093s,  936.46/s)  LR: 3.422e-04  Data: 0.012 (0.012)
Train: 182 [ 150/1251 ( 12%)]  Loss:  3.328143 (3.2015)  Time: 1.095s,  935.18/s  (1.092s,  937.84/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 200/1251 ( 16%)]  Loss:  3.276453 (3.2165)  Time: 1.075s,  952.55/s  (1.092s,  937.80/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 250/1251 ( 20%)]  Loss:  3.386703 (3.2449)  Time: 1.096s,  934.69/s  (1.091s,  938.64/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 300/1251 ( 24%)]  Loss:  3.036430 (3.2151)  Time: 1.079s,  948.86/s  (1.092s,  937.95/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 350/1251 ( 28%)]  Loss:  3.002851 (3.1886)  Time: 1.097s,  933.36/s  (1.090s,  939.13/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 400/1251 ( 32%)]  Loss:  3.215713 (3.1916)  Time: 1.080s,  948.42/s  (1.091s,  938.95/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 450/1251 ( 36%)]  Loss:  3.130297 (3.1855)  Time: 1.096s,  933.88/s  (1.091s,  938.70/s)  LR: 3.422e-04  Data: 0.012 (0.012)
Train: 182 [ 500/1251 ( 40%)]  Loss:  3.511389 (3.2151)  Time: 1.094s,  936.35/s  (1.091s,  938.76/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 550/1251 ( 44%)]  Loss:  3.221217 (3.2156)  Time: 1.079s,  948.69/s  (1.090s,  939.33/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [ 600/1251 ( 48%)]  Loss:  3.438889 (3.2328)  Time: 1.075s,  952.43/s  (1.090s,  939.65/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 650/1251 ( 52%)]  Loss:  3.340945 (3.2405)  Time: 1.099s,  931.79/s  (1.090s,  939.37/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 700/1251 ( 56%)]  Loss:  3.477346 (3.2563)  Time: 1.078s,  949.75/s  (1.090s,  939.61/s)  LR: 3.422e-04  Data: 0.015 (0.013)
Train: 182 [ 750/1251 ( 60%)]  Loss:  3.337557 (3.2614)  Time: 1.094s,  936.24/s  (1.090s,  939.17/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 800/1251 ( 64%)]  Loss:  3.321192 (3.2649)  Time: 1.095s,  935.14/s  (1.090s,  939.32/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 850/1251 ( 68%)]  Loss:  2.674334 (3.2321)  Time: 1.102s,  929.60/s  (1.091s,  938.81/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [ 900/1251 ( 72%)]  Loss:  2.891243 (3.2141)  Time: 1.071s,  955.71/s  (1.091s,  938.99/s)  LR: 3.422e-04  Data: 0.010 (0.013)
Train: 182 [ 950/1251 ( 76%)]  Loss:  2.984485 (3.2027)  Time: 1.076s,  951.41/s  (1.090s,  939.20/s)  LR: 3.422e-04  Data: 0.013 (0.013)
Train: 182 [1000/1251 ( 80%)]  Loss:  3.130099 (3.1992)  Time: 1.084s,  944.66/s  (1.090s,  939.14/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [1050/1251 ( 84%)]  Loss:  3.018764 (3.1910)  Time: 1.093s,  936.62/s  (1.090s,  939.13/s)  LR: 3.422e-04  Data: 0.013 (0.013)
Train: 182 [1100/1251 ( 88%)]  Loss:  3.124540 (3.1881)  Time: 1.095s,  934.74/s  (1.090s,  939.25/s)  LR: 3.422e-04  Data: 0.017 (0.013)
Train: 182 [1150/1251 ( 92%)]  Loss:  3.467512 (3.1998)  Time: 1.092s,  937.43/s  (1.090s,  939.34/s)  LR: 3.422e-04  Data: 0.011 (0.013)
Train: 182 [1200/1251 ( 96%)]  Loss:  3.144683 (3.1976)  Time: 1.105s,  926.75/s  (1.090s,  939.24/s)  LR: 3.422e-04  Data: 0.012 (0.013)
Train: 182 [1250/1251 (100%)]  Loss:  3.318606 (3.2022)  Time: 1.078s,  950.22/s  (1.091s,  938.99/s)  LR: 3.422e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.836 (5.836)  Loss:  0.4561 (0.4561)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6004 (0.9352)  Acc@1: 85.1415 (78.4540)  Acc@5: 97.5236 (94.5300)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 78.05000013671875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-172.pth.tar', 78.00200000488282)

Train: 183 [   0/1251 (  0%)]  Loss:  3.449639 (3.4496)  Time: 1.084s,  944.70/s  (1.084s,  944.70/s)  LR: 3.373e-04  Data: 0.022 (0.022)
Train: 183 [  50/1251 (  4%)]  Loss:  3.354096 (3.4019)  Time: 1.083s,  945.70/s  (1.087s,  941.96/s)  LR: 3.373e-04  Data: 0.014 (0.013)
Train: 183 [ 100/1251 (  8%)]  Loss:  3.084244 (3.2960)  Time: 1.086s,  942.49/s  (1.092s,  937.50/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [ 150/1251 ( 12%)]  Loss:  3.118776 (3.2517)  Time: 1.085s,  943.46/s  (1.094s,  935.81/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [ 200/1251 ( 16%)]  Loss:  2.729560 (3.1473)  Time: 1.094s,  936.20/s  (1.093s,  936.82/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 250/1251 ( 20%)]  Loss:  3.422428 (3.1931)  Time: 1.075s,  952.22/s  (1.091s,  938.20/s)  LR: 3.373e-04  Data: 0.013 (0.013)
Train: 183 [ 300/1251 ( 24%)]  Loss:  3.085842 (3.1778)  Time: 1.077s,  950.96/s  (1.091s,  938.54/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 350/1251 ( 28%)]  Loss:  3.257598 (3.1878)  Time: 1.077s,  950.75/s  (1.090s,  939.68/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 400/1251 ( 32%)]  Loss:  3.358523 (3.2067)  Time: 1.076s,  951.26/s  (1.089s,  940.02/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 450/1251 ( 36%)]  Loss:  3.264546 (3.2125)  Time: 1.083s,  945.91/s  (1.089s,  940.11/s)  LR: 3.373e-04  Data: 0.013 (0.013)
Train: 183 [ 500/1251 ( 40%)]  Loss:  3.297851 (3.2203)  Time: 1.093s,  936.47/s  (1.089s,  940.18/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 550/1251 ( 44%)]  Loss:  3.480730 (3.2420)  Time: 1.164s,  879.74/s  (1.089s,  940.45/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 600/1251 ( 48%)]  Loss:  3.308964 (3.2471)  Time: 1.078s,  950.26/s  (1.089s,  940.67/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 650/1251 ( 52%)]  Loss:  3.324071 (3.2526)  Time: 1.095s,  935.42/s  (1.089s,  940.64/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 700/1251 ( 56%)]  Loss:  3.423386 (3.2640)  Time: 1.094s,  935.64/s  (1.089s,  940.58/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 750/1251 ( 60%)]  Loss:  3.073628 (3.2521)  Time: 1.076s,  951.80/s  (1.089s,  940.72/s)  LR: 3.373e-04  Data: 0.011 (0.013)
Train: 183 [ 800/1251 ( 64%)]  Loss:  3.326568 (3.2565)  Time: 1.096s,  934.70/s  (1.089s,  940.60/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 850/1251 ( 68%)]  Loss:  3.416124 (3.2654)  Time: 1.077s,  950.77/s  (1.089s,  940.21/s)  LR: 3.373e-04  Data: 0.014 (0.013)
Train: 183 [ 900/1251 ( 72%)]  Loss:  2.941396 (3.2483)  Time: 1.077s,  950.93/s  (1.089s,  940.12/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [ 950/1251 ( 76%)]  Loss:  3.117765 (3.2418)  Time: 1.096s,  934.31/s  (1.089s,  940.38/s)  LR: 3.373e-04  Data: 0.018 (0.013)
Train: 183 [1000/1251 ( 80%)]  Loss:  3.345292 (3.2467)  Time: 1.075s,  952.38/s  (1.089s,  940.23/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [1050/1251 ( 84%)]  Loss:  2.884753 (3.2303)  Time: 1.079s,  949.21/s  (1.089s,  940.35/s)  LR: 3.373e-04  Data: 0.015 (0.013)
Train: 183 [1100/1251 ( 88%)]  Loss:  3.291656 (3.2329)  Time: 1.096s,  934.15/s  (1.089s,  940.47/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 183 [1150/1251 ( 92%)]  Loss:  3.308877 (3.2361)  Time: 1.094s,  936.37/s  (1.089s,  940.39/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [1200/1251 ( 96%)]  Loss:  3.402959 (3.2428)  Time: 1.076s,  951.87/s  (1.089s,  940.43/s)  LR: 3.373e-04  Data: 0.012 (0.013)
Train: 183 [1250/1251 (100%)]  Loss:  3.139462 (3.2388)  Time: 1.062s,  964.35/s  (1.089s,  940.60/s)  LR: 3.373e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.824 (5.824)  Loss:  0.5124 (0.5124)  Acc@1: 89.7461 (89.7461)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6582 (0.9499)  Acc@1: 84.5519 (78.5480)  Acc@5: 97.6415 (94.4940)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 78.05000013671875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-170.pth.tar', 78.01000000976562)

Train: 184 [   0/1251 (  0%)]  Loss:  3.266023 (3.2660)  Time: 1.086s,  943.18/s  (1.086s,  943.18/s)  LR: 3.325e-04  Data: 0.025 (0.025)
Train: 184 [  50/1251 (  4%)]  Loss:  2.943928 (3.1050)  Time: 1.076s,  951.45/s  (1.085s,  943.97/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [ 100/1251 (  8%)]  Loss:  3.298512 (3.1695)  Time: 1.095s,  934.97/s  (1.088s,  940.81/s)  LR: 3.325e-04  Data: 0.013 (0.013)
Train: 184 [ 150/1251 ( 12%)]  Loss:  3.568041 (3.2691)  Time: 1.094s,  936.30/s  (1.089s,  940.58/s)  LR: 3.325e-04  Data: 0.012 (0.012)
Train: 184 [ 200/1251 ( 16%)]  Loss:  3.410608 (3.2974)  Time: 1.086s,  943.28/s  (1.088s,  940.91/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [ 250/1251 ( 20%)]  Loss:  3.369489 (3.3094)  Time: 1.084s,  944.48/s  (1.088s,  940.83/s)  LR: 3.325e-04  Data: 0.012 (0.012)
Train: 184 [ 300/1251 ( 24%)]  Loss:  3.084061 (3.2772)  Time: 1.093s,  937.24/s  (1.088s,  940.91/s)  LR: 3.325e-04  Data: 0.012 (0.012)
Train: 184 [ 350/1251 ( 28%)]  Loss:  3.359876 (3.2876)  Time: 1.082s,  946.76/s  (1.088s,  941.04/s)  LR: 3.325e-04  Data: 0.012 (0.012)
Train: 184 [ 400/1251 ( 32%)]  Loss:  3.312140 (3.2903)  Time: 1.076s,  951.30/s  (1.088s,  941.27/s)  LR: 3.325e-04  Data: 0.013 (0.012)
Train: 184 [ 450/1251 ( 36%)]  Loss:  2.951140 (3.2564)  Time: 1.076s,  951.98/s  (1.088s,  941.11/s)  LR: 3.325e-04  Data: 0.013 (0.012)
Train: 184 [ 500/1251 ( 40%)]  Loss:  3.242275 (3.2551)  Time: 1.079s,  949.16/s  (1.088s,  941.36/s)  LR: 3.325e-04  Data: 0.016 (0.012)
Train: 184 [ 550/1251 ( 44%)]  Loss:  3.247068 (3.2544)  Time: 1.076s,  951.35/s  (1.087s,  941.71/s)  LR: 3.325e-04  Data: 0.014 (0.012)
Train: 184 [ 600/1251 ( 48%)]  Loss:  3.348744 (3.2617)  Time: 1.076s,  951.38/s  (1.088s,  941.58/s)  LR: 3.325e-04  Data: 0.012 (0.012)
Train: 184 [ 650/1251 ( 52%)]  Loss:  3.222089 (3.2589)  Time: 1.093s,  936.63/s  (1.087s,  941.62/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [ 700/1251 ( 56%)]  Loss:  3.401835 (3.2684)  Time: 1.096s,  934.58/s  (1.088s,  941.51/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [ 750/1251 ( 60%)]  Loss:  3.038845 (3.2540)  Time: 1.095s,  934.90/s  (1.088s,  941.33/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [ 800/1251 ( 64%)]  Loss:  3.052372 (3.2422)  Time: 1.082s,  946.59/s  (1.088s,  941.09/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [ 850/1251 ( 68%)]  Loss:  3.106293 (3.2346)  Time: 1.079s,  948.79/s  (1.088s,  941.24/s)  LR: 3.325e-04  Data: 0.013 (0.013)
Train: 184 [ 900/1251 ( 72%)]  Loss:  3.433893 (3.2451)  Time: 1.078s,  950.05/s  (1.088s,  940.92/s)  LR: 3.325e-04  Data: 0.013 (0.013)
Train: 184 [ 950/1251 ( 76%)]  Loss:  3.317456 (3.2487)  Time: 1.094s,  935.84/s  (1.088s,  940.99/s)  LR: 3.325e-04  Data: 0.013 (0.013)
Train: 184 [1000/1251 ( 80%)]  Loss:  3.509153 (3.2611)  Time: 1.080s,  948.00/s  (1.088s,  940.99/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1050/1251 ( 84%)]  Loss:  3.471112 (3.2707)  Time: 1.094s,  935.94/s  (1.088s,  941.12/s)  LR: 3.325e-04  Data: 0.013 (0.013)
Train: 184 [1100/1251 ( 88%)]  Loss:  3.590047 (3.2846)  Time: 1.078s,  949.93/s  (1.088s,  941.12/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [1150/1251 ( 92%)]  Loss:  3.389532 (3.2889)  Time: 1.082s,  946.36/s  (1.088s,  941.19/s)  LR: 3.325e-04  Data: 0.011 (0.013)
Train: 184 [1200/1251 ( 96%)]  Loss:  3.486140 (3.2968)  Time: 1.076s,  951.72/s  (1.088s,  941.30/s)  LR: 3.325e-04  Data: 0.012 (0.013)
Train: 184 [1250/1251 (100%)]  Loss:  3.105467 (3.2895)  Time: 1.081s,  946.95/s  (1.088s,  941.02/s)  LR: 3.325e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.816 (5.816)  Loss:  0.4635 (0.4635)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.5726 (0.9342)  Acc@1: 86.2028 (78.4440)  Acc@5: 97.8774 (94.5140)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-179.pth.tar', 78.05000013671875)

Train: 185 [   0/1251 (  0%)]  Loss:  3.371889 (3.3719)  Time: 1.086s,  943.03/s  (1.086s,  943.03/s)  LR: 3.276e-04  Data: 0.023 (0.023)
Train: 185 [  50/1251 (  4%)]  Loss:  3.237006 (3.3044)  Time: 1.076s,  951.84/s  (1.086s,  942.79/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 100/1251 (  8%)]  Loss:  3.382264 (3.3304)  Time: 1.078s,  949.96/s  (1.088s,  941.42/s)  LR: 3.276e-04  Data: 0.015 (0.013)
Train: 185 [ 150/1251 ( 12%)]  Loss:  3.243301 (3.3086)  Time: 1.078s,  950.25/s  (1.087s,  942.06/s)  LR: 3.276e-04  Data: 0.014 (0.013)
Train: 185 [ 200/1251 ( 16%)]  Loss:  3.406274 (3.3281)  Time: 1.077s,  950.80/s  (1.087s,  941.88/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 250/1251 ( 20%)]  Loss:  3.090425 (3.2885)  Time: 1.094s,  936.35/s  (1.087s,  941.85/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 300/1251 ( 24%)]  Loss:  3.323768 (3.2936)  Time: 1.078s,  949.66/s  (1.087s,  941.65/s)  LR: 3.276e-04  Data: 0.014 (0.013)
Train: 185 [ 350/1251 ( 28%)]  Loss:  3.032518 (3.2609)  Time: 1.077s,  951.08/s  (1.087s,  942.11/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 400/1251 ( 32%)]  Loss:  3.153658 (3.2490)  Time: 1.078s,  950.21/s  (1.087s,  942.23/s)  LR: 3.276e-04  Data: 0.015 (0.013)
Train: 185 [ 450/1251 ( 36%)]  Loss:  2.830632 (3.2072)  Time: 1.079s,  949.31/s  (1.087s,  941.97/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 500/1251 ( 40%)]  Loss:  2.969463 (3.1856)  Time: 1.077s,  950.37/s  (1.087s,  941.77/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 550/1251 ( 44%)]  Loss:  3.298660 (3.1950)  Time: 1.095s,  935.25/s  (1.087s,  941.76/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 600/1251 ( 48%)]  Loss:  3.379536 (3.2092)  Time: 1.078s,  950.10/s  (1.087s,  941.73/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 650/1251 ( 52%)]  Loss:  3.165211 (3.2060)  Time: 1.096s,  934.17/s  (1.088s,  941.24/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 185 [ 700/1251 ( 56%)]  Loss:  3.187384 (3.2048)  Time: 1.078s,  949.78/s  (1.088s,  941.30/s)  LR: 3.276e-04  Data: 0.017 (0.013)
Train: 185 [ 750/1251 ( 60%)]  Loss:  3.110389 (3.1989)  Time: 1.094s,  935.89/s  (1.088s,  941.35/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 800/1251 ( 64%)]  Loss:  2.921098 (3.1826)  Time: 1.077s,  951.07/s  (1.088s,  941.21/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 850/1251 ( 68%)]  Loss:  3.383855 (3.1937)  Time: 1.093s,  936.67/s  (1.088s,  941.15/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [ 900/1251 ( 72%)]  Loss:  3.303272 (3.1995)  Time: 1.073s,  954.74/s  (1.088s,  941.02/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [ 950/1251 ( 76%)]  Loss:  2.883594 (3.1837)  Time: 1.095s,  934.97/s  (1.088s,  941.02/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [1000/1251 ( 80%)]  Loss:  3.091088 (3.1793)  Time: 1.105s,  926.69/s  (1.088s,  940.82/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [1050/1251 ( 84%)]  Loss:  3.209391 (3.1807)  Time: 1.096s,  934.33/s  (1.088s,  940.79/s)  LR: 3.276e-04  Data: 0.012 (0.013)
Train: 185 [1100/1251 ( 88%)]  Loss:  3.313059 (3.1864)  Time: 1.097s,  933.59/s  (1.088s,  941.12/s)  LR: 3.276e-04  Data: 0.014 (0.013)
Train: 185 [1150/1251 ( 92%)]  Loss:  3.209380 (3.1874)  Time: 1.105s,  926.90/s  (1.088s,  940.90/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1200/1251 ( 96%)]  Loss:  3.438123 (3.1974)  Time: 1.170s,  874.88/s  (1.089s,  940.68/s)  LR: 3.276e-04  Data: 0.011 (0.013)
Train: 185 [1250/1251 (100%)]  Loss:  3.083446 (3.1930)  Time: 1.062s,  963.88/s  (1.089s,  940.68/s)  LR: 3.276e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.814 (5.814)  Loss:  0.4591 (0.4591)  Acc@1: 91.0156 (91.0156)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.5838 (0.9442)  Acc@1: 86.5566 (78.3820)  Acc@5: 98.2311 (94.5760)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-174.pth.tar', 78.09000008300781)

Train: 186 [   0/1251 (  0%)]  Loss:  3.438172 (3.4382)  Time: 1.089s,  940.36/s  (1.089s,  940.36/s)  LR: 3.228e-04  Data: 0.026 (0.026)
Train: 186 [  50/1251 (  4%)]  Loss:  3.631195 (3.5347)  Time: 1.075s,  952.14/s  (1.087s,  942.23/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 100/1251 (  8%)]  Loss:  3.077882 (3.3824)  Time: 1.076s,  951.35/s  (1.089s,  940.66/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 150/1251 ( 12%)]  Loss:  3.141708 (3.3222)  Time: 1.105s,  927.09/s  (1.088s,  941.45/s)  LR: 3.228e-04  Data: 0.014 (0.013)
Train: 186 [ 200/1251 ( 16%)]  Loss:  2.922079 (3.2422)  Time: 1.095s,  934.80/s  (1.087s,  942.15/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 250/1251 ( 20%)]  Loss:  2.988623 (3.1999)  Time: 1.075s,  952.78/s  (1.088s,  941.47/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 300/1251 ( 24%)]  Loss:  3.470864 (3.2386)  Time: 1.094s,  936.26/s  (1.087s,  941.77/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 350/1251 ( 28%)]  Loss:  3.076184 (3.2183)  Time: 1.081s,  947.47/s  (1.088s,  941.06/s)  LR: 3.228e-04  Data: 0.013 (0.013)
Train: 186 [ 400/1251 ( 32%)]  Loss:  3.066452 (3.2015)  Time: 1.081s,  946.92/s  (1.088s,  940.82/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 450/1251 ( 36%)]  Loss:  3.121850 (3.1935)  Time: 1.080s,  948.25/s  (1.089s,  940.67/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 500/1251 ( 40%)]  Loss:  3.448520 (3.2167)  Time: 1.081s,  947.19/s  (1.088s,  940.77/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 550/1251 ( 44%)]  Loss:  3.186232 (3.2141)  Time: 1.076s,  951.29/s  (1.088s,  940.86/s)  LR: 3.228e-04  Data: 0.014 (0.013)
Train: 186 [ 600/1251 ( 48%)]  Loss:  3.412511 (3.2294)  Time: 1.077s,  950.67/s  (1.088s,  941.17/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 650/1251 ( 52%)]  Loss:  3.190575 (3.2266)  Time: 1.080s,  948.55/s  (1.088s,  941.44/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 700/1251 ( 56%)]  Loss:  3.381158 (3.2369)  Time: 1.096s,  934.71/s  (1.088s,  941.07/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [ 750/1251 ( 60%)]  Loss:  3.325571 (3.2425)  Time: 1.077s,  950.96/s  (1.089s,  940.55/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 800/1251 ( 64%)]  Loss:  3.562281 (3.2613)  Time: 1.087s,  941.78/s  (1.088s,  940.82/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 850/1251 ( 68%)]  Loss:  3.201894 (3.2580)  Time: 1.080s,  948.41/s  (1.089s,  940.62/s)  LR: 3.228e-04  Data: 0.014 (0.013)
Train: 186 [ 900/1251 ( 72%)]  Loss:  3.154424 (3.2525)  Time: 1.087s,  942.18/s  (1.089s,  940.45/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [ 950/1251 ( 76%)]  Loss:  3.239895 (3.2519)  Time: 1.097s,  933.80/s  (1.089s,  940.32/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [1000/1251 ( 80%)]  Loss:  3.144393 (3.2468)  Time: 1.074s,  953.17/s  (1.089s,  940.21/s)  LR: 3.228e-04  Data: 0.013 (0.013)
Train: 186 [1050/1251 ( 84%)]  Loss:  3.332791 (3.2507)  Time: 1.095s,  935.27/s  (1.089s,  940.32/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1100/1251 ( 88%)]  Loss:  3.332327 (3.2542)  Time: 1.183s,  865.57/s  (1.089s,  940.13/s)  LR: 3.228e-04  Data: 0.011 (0.013)
Train: 186 [1150/1251 ( 92%)]  Loss:  2.864532 (3.2380)  Time: 1.097s,  933.09/s  (1.089s,  940.31/s)  LR: 3.228e-04  Data: 0.013 (0.013)
Train: 186 [1200/1251 ( 96%)]  Loss:  3.032053 (3.2298)  Time: 1.079s,  948.86/s  (1.089s,  940.13/s)  LR: 3.228e-04  Data: 0.012 (0.013)
Train: 186 [1250/1251 (100%)]  Loss:  3.129663 (3.2259)  Time: 1.095s,  934.81/s  (1.089s,  940.02/s)  LR: 3.228e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.849 (5.849)  Loss:  0.4725 (0.4725)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5642 (0.9257)  Acc@1: 86.3208 (78.4760)  Acc@5: 98.1132 (94.6600)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-175.pth.tar', 78.11200010742188)

Train: 187 [   0/1251 (  0%)]  Loss:  3.204583 (3.2046)  Time: 1.091s,  938.31/s  (1.091s,  938.31/s)  LR: 3.180e-04  Data: 0.027 (0.027)
Train: 187 [  50/1251 (  4%)]  Loss:  3.346309 (3.2754)  Time: 1.081s,  947.41/s  (1.092s,  937.86/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [ 100/1251 (  8%)]  Loss:  3.033002 (3.1946)  Time: 1.079s,  948.71/s  (1.089s,  940.06/s)  LR: 3.180e-04  Data: 0.013 (0.013)
Train: 187 [ 150/1251 ( 12%)]  Loss:  3.063104 (3.1617)  Time: 1.081s,  946.97/s  (1.089s,  940.07/s)  LR: 3.180e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 187 [ 200/1251 ( 16%)]  Loss:  3.134044 (3.1562)  Time: 1.107s,  925.09/s  (1.087s,  941.63/s)  LR: 3.180e-04  Data: 0.013 (0.013)
Train: 187 [ 250/1251 ( 20%)]  Loss:  3.250859 (3.1720)  Time: 1.096s,  934.23/s  (1.089s,  940.31/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 300/1251 ( 24%)]  Loss:  3.509293 (3.2202)  Time: 1.097s,  933.13/s  (1.091s,  939.01/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 350/1251 ( 28%)]  Loss:  3.347741 (3.2361)  Time: 1.095s,  935.40/s  (1.091s,  938.25/s)  LR: 3.180e-04  Data: 0.015 (0.013)
Train: 187 [ 400/1251 ( 32%)]  Loss:  3.073376 (3.2180)  Time: 1.077s,  950.90/s  (1.091s,  938.60/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 450/1251 ( 36%)]  Loss:  3.231818 (3.2194)  Time: 1.095s,  934.94/s  (1.091s,  938.91/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [ 500/1251 ( 40%)]  Loss:  3.081275 (3.2069)  Time: 1.075s,  952.70/s  (1.090s,  939.37/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 550/1251 ( 44%)]  Loss:  3.344596 (3.2183)  Time: 1.104s,  927.46/s  (1.089s,  939.89/s)  LR: 3.180e-04  Data: 0.013 (0.013)
Train: 187 [ 600/1251 ( 48%)]  Loss:  3.299760 (3.2246)  Time: 1.077s,  951.20/s  (1.090s,  939.79/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 650/1251 ( 52%)]  Loss:  3.206676 (3.2233)  Time: 1.094s,  935.72/s  (1.089s,  939.92/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 700/1251 ( 56%)]  Loss:  3.369676 (3.2331)  Time: 1.092s,  937.37/s  (1.090s,  939.65/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 750/1251 ( 60%)]  Loss:  3.138206 (3.2271)  Time: 1.095s,  935.46/s  (1.089s,  939.94/s)  LR: 3.180e-04  Data: 0.015 (0.013)
Train: 187 [ 800/1251 ( 64%)]  Loss:  3.367489 (3.2354)  Time: 1.177s,  870.05/s  (1.089s,  940.05/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 850/1251 ( 68%)]  Loss:  2.928031 (3.2183)  Time: 1.075s,  952.37/s  (1.089s,  940.26/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [ 900/1251 ( 72%)]  Loss:  3.224321 (3.2186)  Time: 1.093s,  936.71/s  (1.089s,  940.27/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [ 950/1251 ( 76%)]  Loss:  3.359224 (3.2257)  Time: 1.098s,  932.26/s  (1.089s,  940.13/s)  LR: 3.180e-04  Data: 0.013 (0.013)
Train: 187 [1000/1251 ( 80%)]  Loss:  2.940416 (3.2121)  Time: 1.079s,  949.32/s  (1.090s,  939.83/s)  LR: 3.180e-04  Data: 0.015 (0.013)
Train: 187 [1050/1251 ( 84%)]  Loss:  3.164561 (3.2099)  Time: 1.095s,  934.76/s  (1.089s,  940.03/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [1100/1251 ( 88%)]  Loss:  3.550091 (3.2247)  Time: 1.095s,  935.49/s  (1.090s,  939.84/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [1150/1251 ( 92%)]  Loss:  3.142455 (3.2213)  Time: 1.096s,  934.48/s  (1.090s,  939.69/s)  LR: 3.180e-04  Data: 0.011 (0.013)
Train: 187 [1200/1251 ( 96%)]  Loss:  2.964452 (3.2110)  Time: 1.076s,  951.53/s  (1.090s,  939.79/s)  LR: 3.180e-04  Data: 0.012 (0.013)
Train: 187 [1250/1251 (100%)]  Loss:  3.293470 (3.2142)  Time: 1.079s,  948.90/s  (1.090s,  939.77/s)  LR: 3.180e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.881 (5.881)  Loss:  0.4793 (0.4793)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6261 (0.9326)  Acc@1: 85.6132 (78.5540)  Acc@5: 97.4057 (94.6220)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-178.pth.tar', 78.12000005615235)

Train: 188 [   0/1251 (  0%)]  Loss:  3.185366 (3.1854)  Time: 1.088s,  941.40/s  (1.088s,  941.40/s)  LR: 3.132e-04  Data: 0.026 (0.026)
Train: 188 [  50/1251 (  4%)]  Loss:  3.024249 (3.1048)  Time: 1.077s,  950.37/s  (1.085s,  943.65/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 100/1251 (  8%)]  Loss:  3.127318 (3.1123)  Time: 1.097s,  933.24/s  (1.089s,  939.94/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 150/1251 ( 12%)]  Loss:  3.168452 (3.1263)  Time: 1.094s,  936.27/s  (1.089s,  940.03/s)  LR: 3.132e-04  Data: 0.014 (0.013)
Train: 188 [ 200/1251 ( 16%)]  Loss:  3.665578 (3.2342)  Time: 1.079s,  949.03/s  (1.091s,  938.79/s)  LR: 3.132e-04  Data: 0.016 (0.013)
Train: 188 [ 250/1251 ( 20%)]  Loss:  3.074063 (3.2075)  Time: 1.078s,  950.30/s  (1.091s,  938.66/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 300/1251 ( 24%)]  Loss:  3.294280 (3.2199)  Time: 1.075s,  952.62/s  (1.090s,  939.70/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [ 350/1251 ( 28%)]  Loss:  3.265495 (3.2256)  Time: 1.097s,  933.09/s  (1.090s,  939.74/s)  LR: 3.132e-04  Data: 0.014 (0.013)
Train: 188 [ 400/1251 ( 32%)]  Loss:  3.162867 (3.2186)  Time: 1.095s,  935.57/s  (1.090s,  939.49/s)  LR: 3.132e-04  Data: 0.014 (0.013)
Train: 188 [ 450/1251 ( 36%)]  Loss:  2.878182 (3.1846)  Time: 1.104s,  927.64/s  (1.090s,  939.65/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 500/1251 ( 40%)]  Loss:  3.186698 (3.1848)  Time: 1.085s,  944.17/s  (1.090s,  939.82/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 550/1251 ( 44%)]  Loss:  3.232507 (3.1888)  Time: 1.076s,  951.44/s  (1.089s,  940.09/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 600/1251 ( 48%)]  Loss:  3.224877 (3.1915)  Time: 1.076s,  951.49/s  (1.090s,  939.54/s)  LR: 3.132e-04  Data: 0.013 (0.013)
Train: 188 [ 650/1251 ( 52%)]  Loss:  3.403218 (3.2067)  Time: 1.079s,  948.65/s  (1.090s,  939.36/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 700/1251 ( 56%)]  Loss:  3.373322 (3.2178)  Time: 1.092s,  937.34/s  (1.090s,  939.18/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 750/1251 ( 60%)]  Loss:  3.092330 (3.2099)  Time: 1.096s,  934.44/s  (1.090s,  939.39/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 800/1251 ( 64%)]  Loss:  3.218706 (3.2104)  Time: 1.103s,  928.65/s  (1.090s,  939.07/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [ 850/1251 ( 68%)]  Loss:  3.022894 (3.2000)  Time: 1.081s,  947.66/s  (1.091s,  938.88/s)  LR: 3.132e-04  Data: 0.011 (0.013)
Train: 188 [ 900/1251 ( 72%)]  Loss:  3.431284 (3.2122)  Time: 1.084s,  944.53/s  (1.090s,  939.33/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 188 [ 950/1251 ( 76%)]  Loss:  3.071307 (3.2051)  Time: 1.105s,  926.30/s  (1.090s,  939.38/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [1000/1251 ( 80%)]  Loss:  2.935910 (3.1923)  Time: 1.095s,  935.00/s  (1.091s,  939.00/s)  LR: 3.132e-04  Data: 0.015 (0.013)
Train: 188 [1050/1251 ( 84%)]  Loss:  2.909289 (3.1795)  Time: 1.080s,  947.98/s  (1.091s,  938.92/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [1100/1251 ( 88%)]  Loss:  3.254342 (3.1827)  Time: 1.094s,  935.67/s  (1.090s,  939.06/s)  LR: 3.132e-04  Data: 0.014 (0.013)
Train: 188 [1150/1251 ( 92%)]  Loss:  3.333553 (3.1890)  Time: 1.077s,  950.71/s  (1.091s,  939.00/s)  LR: 3.132e-04  Data: 0.015 (0.013)
Train: 188 [1200/1251 ( 96%)]  Loss:  3.276525 (3.1925)  Time: 1.078s,  949.88/s  (1.091s,  938.69/s)  LR: 3.132e-04  Data: 0.012 (0.013)
Train: 188 [1250/1251 (100%)]  Loss:  3.037327 (3.1865)  Time: 1.081s,  947.51/s  (1.091s,  938.57/s)  LR: 3.132e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.859 (5.859)  Loss:  0.4498 (0.4498)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5821 (0.9261)  Acc@1: 86.2028 (78.6260)  Acc@5: 97.7594 (94.5740)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-177.pth.tar', 78.22000010742188)

Train: 189 [   0/1251 (  0%)]  Loss:  3.106663 (3.1067)  Time: 1.085s,  944.17/s  (1.085s,  944.17/s)  LR: 3.084e-04  Data: 0.024 (0.024)
Train: 189 [  50/1251 (  4%)]  Loss:  3.560563 (3.3336)  Time: 1.076s,  951.26/s  (1.083s,  945.50/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 100/1251 (  8%)]  Loss:  3.124872 (3.2640)  Time: 1.082s,  946.23/s  (1.086s,  943.32/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 150/1251 ( 12%)]  Loss:  2.853856 (3.1615)  Time: 1.085s,  944.20/s  (1.087s,  941.62/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 200/1251 ( 16%)]  Loss:  3.299552 (3.1891)  Time: 1.091s,  938.31/s  (1.087s,  941.74/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 250/1251 ( 20%)]  Loss:  3.326880 (3.2121)  Time: 1.077s,  951.13/s  (1.087s,  942.05/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 300/1251 ( 24%)]  Loss:  3.283727 (3.2223)  Time: 1.079s,  948.85/s  (1.087s,  941.77/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 350/1251 ( 28%)]  Loss:  3.184865 (3.2176)  Time: 1.095s,  934.93/s  (1.087s,  942.27/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 400/1251 ( 32%)]  Loss:  3.236980 (3.2198)  Time: 1.106s,  926.27/s  (1.087s,  942.44/s)  LR: 3.084e-04  Data: 0.014 (0.013)
Train: 189 [ 450/1251 ( 36%)]  Loss:  3.334911 (3.2313)  Time: 1.095s,  934.82/s  (1.087s,  941.65/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 500/1251 ( 40%)]  Loss:  3.369021 (3.2438)  Time: 1.094s,  935.92/s  (1.089s,  940.74/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 550/1251 ( 44%)]  Loss:  3.441251 (3.2603)  Time: 1.093s,  936.70/s  (1.089s,  940.11/s)  LR: 3.084e-04  Data: 0.015 (0.013)
Train: 189 [ 600/1251 ( 48%)]  Loss:  2.943978 (3.2359)  Time: 1.076s,  951.90/s  (1.089s,  939.99/s)  LR: 3.084e-04  Data: 0.013 (0.013)
Train: 189 [ 650/1251 ( 52%)]  Loss:  3.189677 (3.2326)  Time: 1.079s,  948.96/s  (1.090s,  939.82/s)  LR: 3.084e-04  Data: 0.014 (0.013)
Train: 189 [ 700/1251 ( 56%)]  Loss:  3.376855 (3.2422)  Time: 1.083s,  945.46/s  (1.089s,  940.26/s)  LR: 3.084e-04  Data: 0.015 (0.013)
Train: 189 [ 750/1251 ( 60%)]  Loss:  2.971062 (3.2253)  Time: 1.093s,  937.12/s  (1.089s,  940.28/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [ 800/1251 ( 64%)]  Loss:  3.278684 (3.2284)  Time: 1.082s,  946.34/s  (1.089s,  940.52/s)  LR: 3.084e-04  Data: 0.014 (0.013)
Train: 189 [ 850/1251 ( 68%)]  Loss:  3.195676 (3.2266)  Time: 1.078s,  949.55/s  (1.089s,  940.34/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 900/1251 ( 72%)]  Loss:  3.564126 (3.2444)  Time: 1.091s,  938.36/s  (1.089s,  940.40/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [ 950/1251 ( 76%)]  Loss:  2.994053 (3.2319)  Time: 1.083s,  945.87/s  (1.089s,  940.55/s)  LR: 3.084e-04  Data: 0.011 (0.013)
Train: 189 [1000/1251 ( 80%)]  Loss:  3.264277 (3.2334)  Time: 1.076s,  951.97/s  (1.089s,  940.44/s)  LR: 3.084e-04  Data: 0.013 (0.013)
Train: 189 [1050/1251 ( 84%)]  Loss:  3.483251 (3.2448)  Time: 1.076s,  951.68/s  (1.089s,  940.50/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [1100/1251 ( 88%)]  Loss:  3.083541 (3.2378)  Time: 1.100s,  931.12/s  (1.089s,  940.51/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [1150/1251 ( 92%)]  Loss:  3.346339 (3.2423)  Time: 1.079s,  949.28/s  (1.089s,  940.29/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [1200/1251 ( 96%)]  Loss:  3.171299 (3.2394)  Time: 1.075s,  952.92/s  (1.089s,  940.49/s)  LR: 3.084e-04  Data: 0.012 (0.013)
Train: 189 [1250/1251 (100%)]  Loss:  3.379183 (3.2448)  Time: 1.080s,  948.31/s  (1.089s,  940.39/s)  LR: 3.084e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.782 (5.782)  Loss:  0.4551 (0.4551)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5331 (0.9243)  Acc@1: 87.8538 (78.6680)  Acc@5: 97.8774 (94.6200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-181.pth.tar', 78.28200010986328)

Train: 190 [   0/1251 (  0%)]  Loss:  3.709133 (3.7091)  Time: 1.086s,  942.66/s  (1.086s,  942.66/s)  LR: 3.037e-04  Data: 0.023 (0.023)
Train: 190 [  50/1251 (  4%)]  Loss:  2.933616 (3.3214)  Time: 1.078s,  950.23/s  (1.087s,  941.85/s)  LR: 3.037e-04  Data: 0.015 (0.013)
Train: 190 [ 100/1251 (  8%)]  Loss:  3.077850 (3.2402)  Time: 1.083s,  945.39/s  (1.086s,  942.94/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 150/1251 ( 12%)]  Loss:  3.467100 (3.2969)  Time: 1.075s,  952.20/s  (1.086s,  942.48/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 200/1251 ( 16%)]  Loss:  3.160835 (3.2697)  Time: 1.077s,  950.95/s  (1.086s,  942.91/s)  LR: 3.037e-04  Data: 0.015 (0.013)
Train: 190 [ 250/1251 ( 20%)]  Loss:  2.960843 (3.2182)  Time: 1.076s,  952.01/s  (1.086s,  943.28/s)  LR: 3.037e-04  Data: 0.014 (0.013)
Train: 190 [ 300/1251 ( 24%)]  Loss:  3.489669 (3.2570)  Time: 1.083s,  945.60/s  (1.086s,  943.20/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 350/1251 ( 28%)]  Loss:  3.231134 (3.2538)  Time: 1.083s,  945.44/s  (1.085s,  943.45/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 400/1251 ( 32%)]  Loss:  3.301578 (3.2591)  Time: 1.080s,  947.99/s  (1.085s,  943.97/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Train: 190 [ 450/1251 ( 36%)]  Loss:  3.494845 (3.2827)  Time: 1.078s,  949.65/s  (1.085s,  944.11/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 500/1251 ( 40%)]  Loss:  3.257092 (3.2803)  Time: 1.103s,  928.62/s  (1.086s,  943.15/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 550/1251 ( 44%)]  Loss:  3.104595 (3.2657)  Time: 1.083s,  945.94/s  (1.086s,  942.87/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 600/1251 ( 48%)]  Loss:  3.408226 (3.2767)  Time: 1.173s,  873.20/s  (1.087s,  942.41/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 650/1251 ( 52%)]  Loss:  3.269900 (3.2762)  Time: 1.076s,  951.98/s  (1.087s,  942.43/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 700/1251 ( 56%)]  Loss:  3.345727 (3.2808)  Time: 1.095s,  934.84/s  (1.086s,  942.71/s)  LR: 3.037e-04  Data: 0.011 (0.013)
Train: 190 [ 750/1251 ( 60%)]  Loss:  3.332592 (3.2840)  Time: 1.084s,  944.48/s  (1.086s,  942.95/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 800/1251 ( 64%)]  Loss:  3.172494 (3.2775)  Time: 1.082s,  946.49/s  (1.086s,  943.02/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 850/1251 ( 68%)]  Loss:  3.578767 (3.2942)  Time: 1.079s,  948.94/s  (1.086s,  942.99/s)  LR: 3.037e-04  Data: 0.013 (0.013)
Train: 190 [ 900/1251 ( 72%)]  Loss:  3.361819 (3.2978)  Time: 1.098s,  932.65/s  (1.086s,  943.05/s)  LR: 3.037e-04  Data: 0.012 (0.013)
Train: 190 [ 950/1251 ( 76%)]  Loss:  3.193345 (3.2926)  Time: 1.080s,  948.40/s  (1.086s,  942.55/s)  LR: 3.037e-04  Data: 0.012 (0.012)
Train: 190 [1000/1251 ( 80%)]  Loss:  2.847854 (3.2714)  Time: 1.077s,  950.66/s  (1.087s,  942.36/s)  LR: 3.037e-04  Data: 0.012 (0.012)
Train: 190 [1050/1251 ( 84%)]  Loss:  3.293514 (3.2724)  Time: 1.076s,  951.76/s  (1.087s,  942.09/s)  LR: 3.037e-04  Data: 0.012 (0.012)
Train: 190 [1100/1251 ( 88%)]  Loss:  3.337605 (3.2752)  Time: 1.082s,  946.45/s  (1.087s,  941.98/s)  LR: 3.037e-04  Data: 0.011 (0.012)
Train: 190 [1150/1251 ( 92%)]  Loss:  3.579749 (3.2879)  Time: 1.073s,  954.00/s  (1.087s,  941.89/s)  LR: 3.037e-04  Data: 0.011 (0.012)
Train: 190 [1200/1251 ( 96%)]  Loss:  3.198762 (3.2843)  Time: 1.095s,  935.22/s  (1.087s,  941.75/s)  LR: 3.037e-04  Data: 0.012 (0.012)
Train: 190 [1250/1251 (100%)]  Loss:  3.249120 (3.2830)  Time: 1.070s,  956.91/s  (1.087s,  941.88/s)  LR: 3.037e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.804 (5.804)  Loss:  0.4624 (0.4624)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6159 (0.9430)  Acc@1: 86.5566 (78.6320)  Acc@5: 97.2877 (94.5860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-180.pth.tar', 78.32600003173827)

Train: 191 [   0/1251 (  0%)]  Loss:  3.334700 (3.3347)  Time: 1.082s,  946.13/s  (1.082s,  946.13/s)  LR: 2.989e-04  Data: 0.022 (0.022)
Train: 191 [  50/1251 (  4%)]  Loss:  3.058661 (3.1967)  Time: 1.094s,  935.68/s  (1.091s,  938.79/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 191 [ 100/1251 (  8%)]  Loss:  3.242849 (3.2121)  Time: 1.078s,  949.80/s  (1.090s,  939.24/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 150/1251 ( 12%)]  Loss:  3.337983 (3.2435)  Time: 1.104s,  927.28/s  (1.091s,  938.49/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 200/1251 ( 16%)]  Loss:  2.966262 (3.1881)  Time: 1.081s,  947.10/s  (1.091s,  938.62/s)  LR: 2.989e-04  Data: 0.015 (0.013)
Train: 191 [ 250/1251 ( 20%)]  Loss:  2.931521 (3.1453)  Time: 1.094s,  936.39/s  (1.093s,  937.29/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 191 [ 300/1251 ( 24%)]  Loss:  3.279330 (3.1645)  Time: 1.108s,  924.46/s  (1.091s,  938.54/s)  LR: 2.989e-04  Data: 0.014 (0.013)
Train: 191 [ 350/1251 ( 28%)]  Loss:  3.461014 (3.2015)  Time: 1.095s,  935.32/s  (1.091s,  938.43/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 400/1251 ( 32%)]  Loss:  3.258342 (3.2079)  Time: 1.076s,  952.05/s  (1.092s,  938.02/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 450/1251 ( 36%)]  Loss:  3.197515 (3.2068)  Time: 1.076s,  951.24/s  (1.091s,  938.82/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 191 [ 500/1251 ( 40%)]  Loss:  3.118733 (3.1988)  Time: 1.099s,  932.08/s  (1.091s,  938.90/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 550/1251 ( 44%)]  Loss:  3.340959 (3.2107)  Time: 1.082s,  946.14/s  (1.091s,  938.70/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 600/1251 ( 48%)]  Loss:  3.270750 (3.2153)  Time: 1.078s,  949.49/s  (1.091s,  938.84/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 650/1251 ( 52%)]  Loss:  3.397654 (3.2283)  Time: 1.099s,  931.45/s  (1.090s,  939.42/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 191 [ 700/1251 ( 56%)]  Loss:  3.134435 (3.2220)  Time: 1.080s,  947.92/s  (1.090s,  939.56/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 750/1251 ( 60%)]  Loss:  3.519456 (3.2406)  Time: 1.075s,  952.24/s  (1.090s,  939.82/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 800/1251 ( 64%)]  Loss:  3.119195 (3.2335)  Time: 1.079s,  948.74/s  (1.089s,  939.90/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [ 850/1251 ( 68%)]  Loss:  3.426867 (3.2442)  Time: 1.077s,  950.91/s  (1.089s,  940.30/s)  LR: 2.989e-04  Data: 0.014 (0.013)
Train: 191 [ 900/1251 ( 72%)]  Loss:  3.486563 (3.2570)  Time: 1.099s,  931.35/s  (1.089s,  940.39/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [ 950/1251 ( 76%)]  Loss:  2.977657 (3.2430)  Time: 1.093s,  937.25/s  (1.089s,  940.27/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1000/1251 ( 80%)]  Loss:  3.497187 (3.2551)  Time: 1.084s,  944.52/s  (1.089s,  940.43/s)  LR: 2.989e-04  Data: 0.011 (0.013)
Train: 191 [1050/1251 ( 84%)]  Loss:  3.375684 (3.2606)  Time: 1.074s,  953.76/s  (1.089s,  940.51/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1100/1251 ( 88%)]  Loss:  3.136614 (3.2552)  Time: 1.079s,  949.23/s  (1.089s,  940.66/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1150/1251 ( 92%)]  Loss:  3.404259 (3.2614)  Time: 1.096s,  934.63/s  (1.089s,  940.69/s)  LR: 2.989e-04  Data: 0.012 (0.013)
Train: 191 [1200/1251 ( 96%)]  Loss:  3.187859 (3.2585)  Time: 1.076s,  951.57/s  (1.089s,  940.64/s)  LR: 2.989e-04  Data: 0.013 (0.013)
Train: 191 [1250/1251 (100%)]  Loss:  3.145535 (3.2541)  Time: 1.068s,  958.93/s  (1.089s,  940.66/s)  LR: 2.989e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.826 (5.826)  Loss:  0.4900 (0.4900)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5990 (0.9378)  Acc@1: 86.4387 (78.6080)  Acc@5: 97.5236 (94.6120)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-185.pth.tar', 78.38199997802734)

Train: 192 [   0/1251 (  0%)]  Loss:  3.387720 (3.3877)  Time: 1.083s,  945.42/s  (1.083s,  945.42/s)  LR: 2.942e-04  Data: 0.021 (0.021)
Train: 192 [  50/1251 (  4%)]  Loss:  3.171075 (3.2794)  Time: 1.095s,  935.38/s  (1.090s,  939.23/s)  LR: 2.942e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 192 [ 100/1251 (  8%)]  Loss:  3.325202 (3.2947)  Time: 1.110s,  922.74/s  (1.090s,  939.71/s)  LR: 2.942e-04  Data: 0.013 (0.013)
Train: 192 [ 150/1251 ( 12%)]  Loss:  3.348336 (3.3081)  Time: 1.078s,  949.54/s  (1.090s,  939.69/s)  LR: 2.942e-04  Data: 0.013 (0.013)
Train: 192 [ 200/1251 ( 16%)]  Loss:  3.000393 (3.2465)  Time: 1.096s,  934.60/s  (1.090s,  939.79/s)  LR: 2.942e-04  Data: 0.012 (0.013)
Train: 192 [ 250/1251 ( 20%)]  Loss:  3.180919 (3.2356)  Time: 1.096s,  934.10/s  (1.088s,  940.81/s)  LR: 2.942e-04  Data: 0.012 (0.013)
Train: 192 [ 300/1251 ( 24%)]  Loss:  3.344563 (3.2512)  Time: 1.079s,  949.37/s  (1.088s,  940.93/s)  LR: 2.942e-04  Data: 0.012 (0.013)
Train: 192 [ 350/1251 ( 28%)]  Loss:  3.278673 (3.2546)  Time: 1.095s,  934.82/s  (1.088s,  940.76/s)  LR: 2.942e-04  Data: 0.012 (0.013)
Train: 192 [ 400/1251 ( 32%)]  Loss:  2.991861 (3.2254)  Time: 1.097s,  933.46/s  (1.089s,  940.03/s)  LR: 2.942e-04  Data: 0.012 (0.013)
Train: 192 [ 450/1251 ( 36%)]  Loss:  3.347191 (3.2376)  Time: 1.077s,  950.97/s  (1.090s,  939.41/s)  LR: 2.942e-04  Data: 0.013 (0.013)
Train: 192 [ 500/1251 ( 40%)]  Loss:  3.167859 (3.2313)  Time: 1.095s,  935.49/s  (1.091s,  938.79/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 192 [ 550/1251 ( 44%)]  Loss:  3.285329 (3.2358)  Time: 1.096s,  934.32/s  (1.090s,  939.06/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 192 [ 600/1251 ( 48%)]  Loss:  3.145713 (3.2288)  Time: 1.093s,  936.81/s  (1.090s,  939.29/s)  LR: 2.942e-04  Data: 0.013 (0.012)
Train: 192 [ 650/1251 ( 52%)]  Loss:  3.337014 (3.2366)  Time: 1.095s,  935.39/s  (1.090s,  939.06/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 192 [ 700/1251 ( 56%)]  Loss:  3.070583 (3.2255)  Time: 1.103s,  928.28/s  (1.090s,  939.04/s)  LR: 2.942e-04  Data: 0.013 (0.012)
Train: 192 [ 750/1251 ( 60%)]  Loss:  3.365702 (3.2343)  Time: 1.095s,  934.82/s  (1.090s,  939.04/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 192 [ 800/1251 ( 64%)]  Loss:  3.070314 (3.2246)  Time: 1.171s,  874.39/s  (1.090s,  939.10/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 192 [ 850/1251 ( 68%)]  Loss:  3.138873 (3.2199)  Time: 1.097s,  933.05/s  (1.090s,  939.39/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 192 [ 900/1251 ( 72%)]  Loss:  3.302939 (3.2242)  Time: 1.078s,  950.24/s  (1.090s,  939.25/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 192 [ 950/1251 ( 76%)]  Loss:  3.264549 (3.2262)  Time: 1.171s,  874.82/s  (1.090s,  939.19/s)  LR: 2.942e-04  Data: 0.014 (0.012)
Train: 192 [1000/1251 ( 80%)]  Loss:  3.493283 (3.2390)  Time: 1.081s,  947.45/s  (1.090s,  939.40/s)  LR: 2.942e-04  Data: 0.011 (0.012)
Train: 192 [1050/1251 ( 84%)]  Loss:  3.481333 (3.2500)  Time: 1.104s,  927.75/s  (1.090s,  939.28/s)  LR: 2.942e-04  Data: 0.012 (0.012)
Train: 192 [1100/1251 ( 88%)]  Loss:  3.289332 (3.2517)  Time: 1.102s,  928.88/s  (1.091s,  938.97/s)  LR: 2.942e-04  Data: 0.013 (0.012)
Train: 192 [1150/1251 ( 92%)]  Loss:  3.339329 (3.2553)  Time: 1.096s,  934.18/s  (1.091s,  938.71/s)  LR: 2.942e-04  Data: 0.013 (0.012)
Train: 192 [1200/1251 ( 96%)]  Loss:  3.567097 (3.2678)  Time: 1.077s,  950.96/s  (1.091s,  938.55/s)  LR: 2.942e-04  Data: 0.013 (0.012)
Train: 192 [1250/1251 (100%)]  Loss:  2.925381 (3.2546)  Time: 1.070s,  957.37/s  (1.091s,  938.39/s)  LR: 2.942e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.848 (5.848)  Loss:  0.4867 (0.4867)  Acc@1: 91.4062 (91.4062)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5957 (0.9442)  Acc@1: 86.5566 (78.5760)  Acc@5: 97.4057 (94.6200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 78.57599997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-184.pth.tar', 78.44400008300781)

Train: 193 [   0/1251 (  0%)]  Loss:  3.210655 (3.2107)  Time: 1.084s,  944.24/s  (1.084s,  944.24/s)  LR: 2.896e-04  Data: 0.022 (0.022)
Train: 193 [  50/1251 (  4%)]  Loss:  2.812868 (3.0118)  Time: 1.148s,  892.36/s  (1.090s,  939.35/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 100/1251 (  8%)]  Loss:  2.826387 (2.9500)  Time: 1.096s,  934.39/s  (1.089s,  939.91/s)  LR: 2.896e-04  Data: 0.014 (0.013)
Train: 193 [ 150/1251 ( 12%)]  Loss:  3.086576 (2.9841)  Time: 1.181s,  866.77/s  (1.090s,  939.79/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 200/1251 ( 16%)]  Loss:  3.301480 (3.0476)  Time: 1.100s,  930.51/s  (1.088s,  941.11/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Train: 193 [ 250/1251 ( 20%)]  Loss:  3.033283 (3.0452)  Time: 1.078s,  949.63/s  (1.088s,  941.02/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 300/1251 ( 24%)]  Loss:  3.462987 (3.1049)  Time: 1.097s,  933.64/s  (1.088s,  940.96/s)  LR: 2.896e-04  Data: 0.013 (0.013)
Train: 193 [ 350/1251 ( 28%)]  Loss:  3.170587 (3.1131)  Time: 1.095s,  934.94/s  (1.088s,  941.20/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 400/1251 ( 32%)]  Loss:  3.036264 (3.1046)  Time: 1.078s,  949.47/s  (1.089s,  940.45/s)  LR: 2.896e-04  Data: 0.012 (0.012)
Train: 193 [ 450/1251 ( 36%)]  Loss:  3.088040 (3.1029)  Time: 1.075s,  952.49/s  (1.089s,  940.34/s)  LR: 2.896e-04  Data: 0.012 (0.012)
Train: 193 [ 500/1251 ( 40%)]  Loss:  3.337509 (3.1242)  Time: 1.106s,  926.27/s  (1.089s,  940.58/s)  LR: 2.896e-04  Data: 0.013 (0.013)
Train: 193 [ 550/1251 ( 44%)]  Loss:  3.281551 (3.1373)  Time: 1.078s,  950.29/s  (1.088s,  940.84/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 600/1251 ( 48%)]  Loss:  3.155992 (3.1388)  Time: 1.096s,  934.19/s  (1.089s,  940.36/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 650/1251 ( 52%)]  Loss:  3.144936 (3.1392)  Time: 1.092s,  937.31/s  (1.089s,  940.20/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 700/1251 ( 56%)]  Loss:  3.147986 (3.1398)  Time: 1.096s,  934.05/s  (1.089s,  940.12/s)  LR: 2.896e-04  Data: 0.013 (0.013)
Train: 193 [ 750/1251 ( 60%)]  Loss:  3.434491 (3.1582)  Time: 1.078s,  949.66/s  (1.089s,  939.91/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 800/1251 ( 64%)]  Loss:  3.338129 (3.1688)  Time: 1.094s,  936.19/s  (1.090s,  939.80/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [ 850/1251 ( 68%)]  Loss:  3.078778 (3.1638)  Time: 1.093s,  936.58/s  (1.089s,  940.02/s)  LR: 2.896e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 193 [ 900/1251 ( 72%)]  Loss:  3.141793 (3.1626)  Time: 1.077s,  951.08/s  (1.089s,  940.42/s)  LR: 2.896e-04  Data: 0.014 (0.013)
Train: 193 [ 950/1251 ( 76%)]  Loss:  3.222064 (3.1656)  Time: 1.083s,  945.45/s  (1.089s,  940.68/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [1000/1251 ( 80%)]  Loss:  3.199260 (3.1672)  Time: 1.076s,  952.01/s  (1.089s,  940.68/s)  LR: 2.896e-04  Data: 0.014 (0.013)
Train: 193 [1050/1251 ( 84%)]  Loss:  3.244151 (3.1707)  Time: 1.082s,  946.14/s  (1.089s,  940.67/s)  LR: 2.896e-04  Data: 0.013 (0.013)
Train: 193 [1100/1251 ( 88%)]  Loss:  2.817253 (3.1553)  Time: 1.101s,  930.35/s  (1.089s,  940.72/s)  LR: 2.896e-04  Data: 0.014 (0.013)
Train: 193 [1150/1251 ( 92%)]  Loss:  3.223248 (3.1582)  Time: 1.081s,  946.89/s  (1.089s,  940.53/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [1200/1251 ( 96%)]  Loss:  3.422113 (3.1687)  Time: 1.077s,  950.60/s  (1.089s,  940.63/s)  LR: 2.896e-04  Data: 0.012 (0.013)
Train: 193 [1250/1251 (100%)]  Loss:  3.047506 (3.1641)  Time: 1.068s,  958.63/s  (1.089s,  940.39/s)  LR: 2.896e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.927 (5.927)  Loss:  0.4716 (0.4716)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.5964 (0.9162)  Acc@1: 86.3208 (78.7520)  Acc@5: 97.7594 (94.8540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 78.57599997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-182.pth.tar', 78.45400000976562)

Train: 194 [   0/1251 (  0%)]  Loss:  3.164272 (3.1643)  Time: 1.086s,  943.26/s  (1.086s,  943.26/s)  LR: 2.849e-04  Data: 0.025 (0.025)
Train: 194 [  50/1251 (  4%)]  Loss:  3.015172 (3.0897)  Time: 1.170s,  875.26/s  (1.083s,  945.91/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 100/1251 (  8%)]  Loss:  3.407595 (3.1957)  Time: 1.098s,  932.95/s  (1.086s,  942.55/s)  LR: 2.849e-04  Data: 0.012 (0.012)
Train: 194 [ 150/1251 ( 12%)]  Loss:  3.192506 (3.1949)  Time: 1.095s,  935.07/s  (1.088s,  940.91/s)  LR: 2.849e-04  Data: 0.013 (0.012)
Train: 194 [ 200/1251 ( 16%)]  Loss:  3.404373 (3.2368)  Time: 1.073s,  954.28/s  (1.090s,  939.81/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 250/1251 ( 20%)]  Loss:  3.028160 (3.2020)  Time: 1.082s,  946.02/s  (1.090s,  939.72/s)  LR: 2.849e-04  Data: 0.011 (0.012)
Train: 194 [ 300/1251 ( 24%)]  Loss:  3.271811 (3.2120)  Time: 1.094s,  936.08/s  (1.089s,  940.34/s)  LR: 2.849e-04  Data: 0.014 (0.012)
Train: 194 [ 350/1251 ( 28%)]  Loss:  3.240511 (3.2156)  Time: 1.090s,  939.16/s  (1.090s,  939.63/s)  LR: 2.849e-04  Data: 0.011 (0.012)
Train: 194 [ 400/1251 ( 32%)]  Loss:  3.161929 (3.2096)  Time: 1.095s,  935.34/s  (1.090s,  939.55/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 450/1251 ( 36%)]  Loss:  3.016458 (3.1903)  Time: 1.082s,  946.71/s  (1.090s,  939.27/s)  LR: 2.849e-04  Data: 0.013 (0.013)
Train: 194 [ 500/1251 ( 40%)]  Loss:  3.351594 (3.2049)  Time: 1.096s,  934.21/s  (1.090s,  939.40/s)  LR: 2.849e-04  Data: 0.018 (0.013)
Train: 194 [ 550/1251 ( 44%)]  Loss:  3.481302 (3.2280)  Time: 1.086s,  942.90/s  (1.090s,  939.45/s)  LR: 2.849e-04  Data: 0.015 (0.013)
Train: 194 [ 600/1251 ( 48%)]  Loss:  3.408076 (3.2418)  Time: 1.079s,  949.13/s  (1.090s,  939.81/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 650/1251 ( 52%)]  Loss:  3.425020 (3.2549)  Time: 1.078s,  950.33/s  (1.089s,  940.40/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 700/1251 ( 56%)]  Loss:  2.939060 (3.2339)  Time: 1.077s,  950.64/s  (1.089s,  940.57/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 750/1251 ( 60%)]  Loss:  3.501077 (3.2506)  Time: 1.099s,  931.48/s  (1.089s,  940.10/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 800/1251 ( 64%)]  Loss:  3.215742 (3.2485)  Time: 1.079s,  949.01/s  (1.090s,  939.85/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [ 850/1251 ( 68%)]  Loss:  3.308630 (3.2518)  Time: 1.074s,  953.63/s  (1.090s,  939.35/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 900/1251 ( 72%)]  Loss:  3.120459 (3.2449)  Time: 1.087s,  941.84/s  (1.090s,  939.19/s)  LR: 2.849e-04  Data: 0.011 (0.013)
Train: 194 [ 950/1251 ( 76%)]  Loss:  3.259635 (3.2457)  Time: 1.077s,  950.37/s  (1.090s,  939.22/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [1000/1251 ( 80%)]  Loss:  3.173904 (3.2423)  Time: 1.173s,  872.79/s  (1.090s,  939.41/s)  LR: 2.849e-04  Data: 0.013 (0.013)
Train: 194 [1050/1251 ( 84%)]  Loss:  3.182369 (3.2395)  Time: 1.076s,  951.28/s  (1.090s,  939.26/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [1100/1251 ( 88%)]  Loss:  3.142103 (3.2353)  Time: 1.173s,  873.16/s  (1.090s,  939.15/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [1150/1251 ( 92%)]  Loss:  3.089076 (3.2292)  Time: 1.078s,  949.66/s  (1.090s,  939.37/s)  LR: 2.849e-04  Data: 0.013 (0.013)
Train: 194 [1200/1251 ( 96%)]  Loss:  3.031874 (3.2213)  Time: 1.096s,  934.62/s  (1.090s,  939.44/s)  LR: 2.849e-04  Data: 0.012 (0.013)
Train: 194 [1250/1251 (100%)]  Loss:  3.449318 (3.2301)  Time: 1.062s,  964.36/s  (1.090s,  939.54/s)  LR: 2.849e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.847 (5.847)  Loss:  0.4414 (0.4414)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.6064 (0.9156)  Acc@1: 86.4387 (78.7560)  Acc@5: 97.4057 (94.7800)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 78.57599997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-186.pth.tar', 78.4760000048828)

Train: 195 [   0/1251 (  0%)]  Loss:  2.830213 (2.8302)  Time: 1.086s,  942.73/s  (1.086s,  942.73/s)  LR: 2.803e-04  Data: 0.022 (0.022)
Train: 195 [  50/1251 (  4%)]  Loss:  3.466230 (3.1482)  Time: 1.077s,  950.71/s  (1.089s,  940.69/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [ 100/1251 (  8%)]  Loss:  3.445458 (3.2473)  Time: 1.077s,  950.68/s  (1.086s,  942.53/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [ 150/1251 ( 12%)]  Loss:  3.472875 (3.3037)  Time: 1.077s,  950.39/s  (1.088s,  941.56/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [ 200/1251 ( 16%)]  Loss:  3.130184 (3.2690)  Time: 1.086s,  942.62/s  (1.087s,  941.79/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [ 250/1251 ( 20%)]  Loss:  3.453653 (3.2998)  Time: 1.112s,  921.15/s  (1.088s,  940.81/s)  LR: 2.803e-04  Data: 0.012 (0.012)
Train: 195 [ 300/1251 ( 24%)]  Loss:  2.728741 (3.2182)  Time: 1.077s,  951.09/s  (1.089s,  940.41/s)  LR: 2.803e-04  Data: 0.012 (0.012)
Train: 195 [ 350/1251 ( 28%)]  Loss:  3.158433 (3.2107)  Time: 1.077s,  950.54/s  (1.089s,  940.33/s)  LR: 2.803e-04  Data: 0.014 (0.012)
Train: 195 [ 400/1251 ( 32%)]  Loss:  3.359542 (3.2273)  Time: 1.098s,  932.76/s  (1.089s,  940.38/s)  LR: 2.803e-04  Data: 0.014 (0.012)
Train: 195 [ 450/1251 ( 36%)]  Loss:  3.599764 (3.2645)  Time: 1.094s,  935.74/s  (1.090s,  939.87/s)  LR: 2.803e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 195 [ 500/1251 ( 40%)]  Loss:  3.345694 (3.2719)  Time: 1.083s,  945.89/s  (1.090s,  939.41/s)  LR: 2.803e-04  Data: 0.013 (0.012)
Train: 195 [ 550/1251 ( 44%)]  Loss:  3.184976 (3.2646)  Time: 1.082s,  946.05/s  (1.090s,  939.28/s)  LR: 2.803e-04  Data: 0.012 (0.012)
Train: 195 [ 600/1251 ( 48%)]  Loss:  3.227435 (3.2618)  Time: 1.077s,  950.73/s  (1.090s,  939.78/s)  LR: 2.803e-04  Data: 0.012 (0.012)
Train: 195 [ 650/1251 ( 52%)]  Loss:  3.271761 (3.2625)  Time: 1.095s,  935.08/s  (1.090s,  939.54/s)  LR: 2.803e-04  Data: 0.011 (0.012)
Train: 195 [ 700/1251 ( 56%)]  Loss:  3.245208 (3.2613)  Time: 1.096s,  934.01/s  (1.090s,  939.80/s)  LR: 2.803e-04  Data: 0.011 (0.012)
Train: 195 [ 750/1251 ( 60%)]  Loss:  3.142294 (3.2539)  Time: 1.104s,  927.20/s  (1.090s,  939.86/s)  LR: 2.803e-04  Data: 0.014 (0.012)
Train: 195 [ 800/1251 ( 64%)]  Loss:  3.263950 (3.2545)  Time: 1.082s,  946.38/s  (1.090s,  939.46/s)  LR: 2.803e-04  Data: 0.014 (0.013)
Train: 195 [ 850/1251 ( 68%)]  Loss:  3.097214 (3.2458)  Time: 1.095s,  935.08/s  (1.090s,  939.60/s)  LR: 2.803e-04  Data: 0.015 (0.013)
Train: 195 [ 900/1251 ( 72%)]  Loss:  3.354460 (3.2515)  Time: 1.081s,  947.37/s  (1.090s,  939.68/s)  LR: 2.803e-04  Data: 0.013 (0.013)
Train: 195 [ 950/1251 ( 76%)]  Loss:  3.173512 (3.2476)  Time: 1.105s,  926.60/s  (1.090s,  939.88/s)  LR: 2.803e-04  Data: 0.013 (0.013)
Train: 195 [1000/1251 ( 80%)]  Loss:  3.140566 (3.2425)  Time: 1.095s,  934.93/s  (1.089s,  939.94/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1050/1251 ( 84%)]  Loss:  2.929137 (3.2282)  Time: 1.102s,  928.86/s  (1.089s,  939.92/s)  LR: 2.803e-04  Data: 0.013 (0.013)
Train: 195 [1100/1251 ( 88%)]  Loss:  3.337937 (3.2330)  Time: 1.097s,  933.58/s  (1.089s,  939.98/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1150/1251 ( 92%)]  Loss:  3.043949 (3.2251)  Time: 1.104s,  927.81/s  (1.089s,  940.06/s)  LR: 2.803e-04  Data: 0.013 (0.013)
Train: 195 [1200/1251 ( 96%)]  Loss:  3.557517 (3.2384)  Time: 1.104s,  927.53/s  (1.089s,  940.15/s)  LR: 2.803e-04  Data: 0.012 (0.013)
Train: 195 [1250/1251 (100%)]  Loss:  2.955238 (3.2275)  Time: 1.097s,  933.35/s  (1.090s,  939.75/s)  LR: 2.803e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.901 (5.901)  Loss:  0.4549 (0.4549)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.6136 (0.9381)  Acc@1: 86.4387 (78.8360)  Acc@5: 97.8774 (94.6540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 78.57599997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-183.pth.tar', 78.54800001220703)

Train: 196 [   0/1251 (  0%)]  Loss:  3.415047 (3.4150)  Time: 1.084s,  945.03/s  (1.084s,  945.03/s)  LR: 2.757e-04  Data: 0.022 (0.022)
Train: 196 [  50/1251 (  4%)]  Loss:  2.862883 (3.1390)  Time: 1.084s,  944.61/s  (1.081s,  946.96/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 100/1251 (  8%)]  Loss:  3.408432 (3.2288)  Time: 1.073s,  954.44/s  (1.081s,  947.55/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 150/1251 ( 12%)]  Loss:  3.223673 (3.2275)  Time: 1.094s,  935.70/s  (1.087s,  942.34/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 200/1251 ( 16%)]  Loss:  3.056991 (3.1934)  Time: 1.091s,  939.00/s  (1.086s,  942.57/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 250/1251 ( 20%)]  Loss:  2.825569 (3.1321)  Time: 1.079s,  949.36/s  (1.085s,  943.60/s)  LR: 2.757e-04  Data: 0.015 (0.013)
Train: 196 [ 300/1251 ( 24%)]  Loss:  3.209078 (3.1431)  Time: 1.081s,  947.65/s  (1.086s,  942.62/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 350/1251 ( 28%)]  Loss:  2.936054 (3.1172)  Time: 1.077s,  950.80/s  (1.086s,  942.95/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 400/1251 ( 32%)]  Loss:  3.356971 (3.1439)  Time: 1.095s,  935.48/s  (1.086s,  943.03/s)  LR: 2.757e-04  Data: 0.013 (0.013)
Train: 196 [ 450/1251 ( 36%)]  Loss:  3.482634 (3.1777)  Time: 1.079s,  949.31/s  (1.087s,  941.94/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 500/1251 ( 40%)]  Loss:  3.208105 (3.1805)  Time: 1.071s,  956.00/s  (1.087s,  942.27/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 550/1251 ( 44%)]  Loss:  3.030179 (3.1680)  Time: 1.080s,  947.87/s  (1.087s,  941.87/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 600/1251 ( 48%)]  Loss:  3.273182 (3.1761)  Time: 1.076s,  951.61/s  (1.088s,  941.43/s)  LR: 2.757e-04  Data: 0.011 (0.013)
Train: 196 [ 650/1251 ( 52%)]  Loss:  3.315331 (3.1860)  Time: 1.094s,  935.86/s  (1.088s,  940.91/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 700/1251 ( 56%)]  Loss:  3.354490 (3.1972)  Time: 1.105s,  926.98/s  (1.089s,  940.39/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 750/1251 ( 60%)]  Loss:  3.396974 (3.2097)  Time: 1.078s,  950.31/s  (1.089s,  940.15/s)  LR: 2.757e-04  Data: 0.013 (0.013)
Train: 196 [ 800/1251 ( 64%)]  Loss:  3.301546 (3.2151)  Time: 1.077s,  951.09/s  (1.089s,  939.97/s)  LR: 2.757e-04  Data: 0.015 (0.013)
Train: 196 [ 850/1251 ( 68%)]  Loss:  2.905279 (3.1979)  Time: 1.080s,  948.40/s  (1.090s,  939.88/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 900/1251 ( 72%)]  Loss:  3.082973 (3.1919)  Time: 1.078s,  950.23/s  (1.089s,  940.10/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [ 950/1251 ( 76%)]  Loss:  2.748339 (3.1697)  Time: 1.078s,  950.15/s  (1.090s,  939.65/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [1000/1251 ( 80%)]  Loss:  3.362358 (3.1789)  Time: 1.084s,  944.58/s  (1.090s,  939.71/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [1050/1251 ( 84%)]  Loss:  3.319932 (3.1853)  Time: 1.096s,  934.60/s  (1.090s,  939.77/s)  LR: 2.757e-04  Data: 0.014 (0.013)
Train: 196 [1100/1251 ( 88%)]  Loss:  3.141465 (3.1834)  Time: 1.107s,  925.40/s  (1.090s,  939.66/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [1150/1251 ( 92%)]  Loss:  3.147397 (3.1819)  Time: 1.099s,  931.74/s  (1.090s,  939.60/s)  LR: 2.757e-04  Data: 0.012 (0.013)
Train: 196 [1200/1251 ( 96%)]  Loss:  3.352819 (3.1887)  Time: 1.095s,  935.24/s  (1.090s,  939.48/s)  LR: 2.757e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 196 [1250/1251 (100%)]  Loss:  3.509530 (3.2010)  Time: 1.073s,  954.68/s  (1.090s,  939.62/s)  LR: 2.757e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.798 (5.798)  Loss:  0.4610 (0.4610)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5888 (0.9226)  Acc@1: 86.7925 (78.9860)  Acc@5: 98.1132 (94.7600)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 78.57599997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-187.pth.tar', 78.55400008544922)

Train: 197 [   0/1251 (  0%)]  Loss:  2.972315 (2.9723)  Time: 1.085s,  943.57/s  (1.085s,  943.57/s)  LR: 2.711e-04  Data: 0.023 (0.023)
Train: 197 [  50/1251 (  4%)]  Loss:  3.209109 (3.0907)  Time: 1.105s,  927.03/s  (1.087s,  941.71/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 100/1251 (  8%)]  Loss:  3.186746 (3.1227)  Time: 1.079s,  949.36/s  (1.088s,  941.19/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 150/1251 ( 12%)]  Loss:  3.399312 (3.1919)  Time: 1.077s,  951.11/s  (1.087s,  942.35/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 200/1251 ( 16%)]  Loss:  3.161807 (3.1859)  Time: 1.103s,  928.19/s  (1.088s,  941.52/s)  LR: 2.711e-04  Data: 0.014 (0.013)
Train: 197 [ 250/1251 ( 20%)]  Loss:  3.459299 (3.2314)  Time: 1.097s,  933.41/s  (1.089s,  940.35/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 300/1251 ( 24%)]  Loss:  3.312320 (3.2430)  Time: 1.075s,  952.48/s  (1.089s,  940.73/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 350/1251 ( 28%)]  Loss:  3.317043 (3.2522)  Time: 1.095s,  935.42/s  (1.088s,  941.02/s)  LR: 2.711e-04  Data: 0.015 (0.013)
Train: 197 [ 400/1251 ( 32%)]  Loss:  3.582414 (3.2889)  Time: 1.095s,  934.92/s  (1.090s,  939.60/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 450/1251 ( 36%)]  Loss:  3.469473 (3.3070)  Time: 1.095s,  934.80/s  (1.090s,  939.31/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 500/1251 ( 40%)]  Loss:  3.103160 (3.2885)  Time: 1.094s,  936.10/s  (1.090s,  939.83/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 550/1251 ( 44%)]  Loss:  3.040161 (3.2678)  Time: 1.094s,  936.23/s  (1.091s,  938.94/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 600/1251 ( 48%)]  Loss:  3.206664 (3.2631)  Time: 1.103s,  928.18/s  (1.091s,  938.46/s)  LR: 2.711e-04  Data: 0.013 (0.013)
Train: 197 [ 650/1251 ( 52%)]  Loss:  3.111351 (3.2522)  Time: 1.086s,  942.96/s  (1.091s,  938.25/s)  LR: 2.711e-04  Data: 0.011 (0.013)
Train: 197 [ 700/1251 ( 56%)]  Loss:  3.172348 (3.2469)  Time: 1.077s,  951.11/s  (1.091s,  938.34/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 750/1251 ( 60%)]  Loss:  3.323261 (3.2517)  Time: 1.094s,  936.34/s  (1.092s,  937.66/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 800/1251 ( 64%)]  Loss:  3.169891 (3.2469)  Time: 1.109s,  923.57/s  (1.092s,  937.98/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 850/1251 ( 68%)]  Loss:  3.408312 (3.2558)  Time: 1.097s,  933.63/s  (1.092s,  937.82/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [ 900/1251 ( 72%)]  Loss:  3.519331 (3.2697)  Time: 1.077s,  950.83/s  (1.092s,  937.87/s)  LR: 2.711e-04  Data: 0.015 (0.013)
Train: 197 [ 950/1251 ( 76%)]  Loss:  3.303154 (3.2714)  Time: 1.106s,  925.83/s  (1.091s,  938.20/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1000/1251 ( 80%)]  Loss:  3.422894 (3.2786)  Time: 1.174s,  872.47/s  (1.092s,  938.07/s)  LR: 2.711e-04  Data: 0.016 (0.013)
Train: 197 [1050/1251 ( 84%)]  Loss:  3.088574 (3.2700)  Time: 1.078s,  950.19/s  (1.091s,  938.33/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1100/1251 ( 88%)]  Loss:  3.314275 (3.2719)  Time: 1.095s,  935.20/s  (1.091s,  938.26/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1150/1251 ( 92%)]  Loss:  3.319347 (3.2739)  Time: 1.077s,  950.63/s  (1.091s,  938.26/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1200/1251 ( 96%)]  Loss:  3.425007 (3.2799)  Time: 1.079s,  948.64/s  (1.091s,  938.48/s)  LR: 2.711e-04  Data: 0.012 (0.013)
Train: 197 [1250/1251 (100%)]  Loss:  3.151468 (3.2750)  Time: 1.079s,  949.00/s  (1.091s,  938.38/s)  LR: 2.711e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.810 (5.810)  Loss:  0.4709 (0.4709)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5666 (0.9130)  Acc@1: 87.6179 (79.0440)  Acc@5: 97.6415 (94.8460)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-192.pth.tar', 78.57599997802734)

Train: 198 [   0/1251 (  0%)]  Loss:  3.324325 (3.3243)  Time: 1.085s,  943.67/s  (1.085s,  943.67/s)  LR: 2.665e-04  Data: 0.023 (0.023)
Train: 198 [  50/1251 (  4%)]  Loss:  3.216744 (3.2705)  Time: 1.078s,  950.11/s  (1.087s,  941.97/s)  LR: 2.665e-04  Data: 0.013 (0.013)
Train: 198 [ 100/1251 (  8%)]  Loss:  3.341294 (3.2941)  Time: 1.106s,  925.81/s  (1.091s,  938.81/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [ 150/1251 ( 12%)]  Loss:  3.214361 (3.2742)  Time: 1.113s,  919.65/s  (1.092s,  937.96/s)  LR: 2.665e-04  Data: 0.014 (0.013)
Train: 198 [ 200/1251 ( 16%)]  Loss:  3.228991 (3.2651)  Time: 1.078s,  950.34/s  (1.090s,  939.26/s)  LR: 2.665e-04  Data: 0.013 (0.013)
Train: 198 [ 250/1251 ( 20%)]  Loss:  3.036472 (3.2270)  Time: 1.074s,  953.03/s  (1.090s,  939.54/s)  LR: 2.665e-04  Data: 0.013 (0.013)
Train: 198 [ 300/1251 ( 24%)]  Loss:  2.968970 (3.1902)  Time: 1.099s,  931.40/s  (1.090s,  939.77/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [ 350/1251 ( 28%)]  Loss:  3.099566 (3.1788)  Time: 1.102s,  928.99/s  (1.089s,  939.99/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 400/1251 ( 32%)]  Loss:  3.285657 (3.1907)  Time: 1.097s,  933.15/s  (1.091s,  938.87/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [ 450/1251 ( 36%)]  Loss:  3.107533 (3.1824)  Time: 1.093s,  936.58/s  (1.091s,  938.43/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 500/1251 ( 40%)]  Loss:  3.161067 (3.1805)  Time: 1.075s,  952.56/s  (1.091s,  938.39/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 550/1251 ( 44%)]  Loss:  3.177588 (3.1802)  Time: 1.078s,  949.64/s  (1.091s,  938.29/s)  LR: 2.665e-04  Data: 0.013 (0.013)
Train: 198 [ 600/1251 ( 48%)]  Loss:  3.435534 (3.1999)  Time: 1.083s,  945.79/s  (1.091s,  938.83/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [ 650/1251 ( 52%)]  Loss:  3.181733 (3.1986)  Time: 1.082s,  946.50/s  (1.090s,  939.06/s)  LR: 2.665e-04  Data: 0.013 (0.013)
Train: 198 [ 700/1251 ( 56%)]  Loss:  3.526439 (3.2204)  Time: 1.077s,  950.79/s  (1.090s,  939.21/s)  LR: 2.665e-04  Data: 0.013 (0.013)
Train: 198 [ 750/1251 ( 60%)]  Loss:  3.134011 (3.2150)  Time: 1.093s,  936.61/s  (1.091s,  938.75/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 198 [ 800/1251 ( 64%)]  Loss:  2.910969 (3.1971)  Time: 1.079s,  948.67/s  (1.091s,  938.75/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [ 850/1251 ( 68%)]  Loss:  3.215257 (3.1981)  Time: 1.077s,  950.43/s  (1.091s,  938.74/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [ 900/1251 ( 72%)]  Loss:  3.210355 (3.1988)  Time: 1.094s,  935.82/s  (1.091s,  938.50/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [ 950/1251 ( 76%)]  Loss:  3.190490 (3.1984)  Time: 1.079s,  948.73/s  (1.091s,  938.61/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [1000/1251 ( 80%)]  Loss:  3.460798 (3.2109)  Time: 1.105s,  927.11/s  (1.091s,  938.87/s)  LR: 2.665e-04  Data: 0.012 (0.013)
Train: 198 [1050/1251 ( 84%)]  Loss:  3.063163 (3.2042)  Time: 1.095s,  934.75/s  (1.091s,  938.86/s)  LR: 2.665e-04  Data: 0.014 (0.013)
Train: 198 [1100/1251 ( 88%)]  Loss:  3.254115 (3.2063)  Time: 1.085s,  943.41/s  (1.090s,  939.03/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1150/1251 ( 92%)]  Loss:  3.061408 (3.2003)  Time: 1.076s,  951.46/s  (1.091s,  938.97/s)  LR: 2.665e-04  Data: 0.014 (0.013)
Train: 198 [1200/1251 ( 96%)]  Loss:  3.581287 (3.2155)  Time: 1.080s,  948.08/s  (1.091s,  938.88/s)  LR: 2.665e-04  Data: 0.011 (0.013)
Train: 198 [1250/1251 (100%)]  Loss:  3.040802 (3.2088)  Time: 1.079s,  949.26/s  (1.091s,  938.97/s)  LR: 2.665e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.824 (5.824)  Loss:  0.4759 (0.4759)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5772 (0.8996)  Acc@1: 86.3207 (78.8980)  Acc@5: 97.7594 (94.8960)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-191.pth.tar', 78.60800005615235)

Train: 199 [   0/1251 (  0%)]  Loss:  3.325533 (3.3255)  Time: 1.083s,  945.51/s  (1.083s,  945.51/s)  LR: 2.620e-04  Data: 0.022 (0.022)
Train: 199 [  50/1251 (  4%)]  Loss:  3.033092 (3.1793)  Time: 1.100s,  930.62/s  (1.087s,  941.89/s)  LR: 2.620e-04  Data: 0.013 (0.013)
Train: 199 [ 100/1251 (  8%)]  Loss:  3.317520 (3.2254)  Time: 1.077s,  950.46/s  (1.088s,  940.91/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 150/1251 ( 12%)]  Loss:  3.520513 (3.2992)  Time: 1.079s,  948.75/s  (1.089s,  940.16/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 200/1251 ( 16%)]  Loss:  3.202144 (3.2798)  Time: 1.094s,  936.38/s  (1.089s,  940.62/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 199 [ 250/1251 ( 20%)]  Loss:  3.346749 (3.2909)  Time: 1.079s,  949.36/s  (1.089s,  940.65/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 300/1251 ( 24%)]  Loss:  2.997339 (3.2490)  Time: 1.075s,  952.65/s  (1.088s,  940.77/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 350/1251 ( 28%)]  Loss:  3.441200 (3.2730)  Time: 1.077s,  950.37/s  (1.088s,  941.06/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 400/1251 ( 32%)]  Loss:  3.186541 (3.2634)  Time: 1.094s,  936.02/s  (1.089s,  940.65/s)  LR: 2.620e-04  Data: 0.014 (0.013)
Train: 199 [ 450/1251 ( 36%)]  Loss:  3.218257 (3.2589)  Time: 1.077s,  950.45/s  (1.088s,  940.98/s)  LR: 2.620e-04  Data: 0.016 (0.013)
Train: 199 [ 500/1251 ( 40%)]  Loss:  3.220702 (3.2554)  Time: 1.094s,  936.02/s  (1.088s,  941.55/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 550/1251 ( 44%)]  Loss:  3.327876 (3.2615)  Time: 1.097s,  933.40/s  (1.087s,  941.70/s)  LR: 2.620e-04  Data: 0.014 (0.013)
Train: 199 [ 600/1251 ( 48%)]  Loss:  3.242524 (3.2600)  Time: 1.076s,  951.77/s  (1.088s,  941.15/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 650/1251 ( 52%)]  Loss:  3.206131 (3.2562)  Time: 1.104s,  927.42/s  (1.088s,  940.81/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 700/1251 ( 56%)]  Loss:  3.048508 (3.2423)  Time: 1.171s,  874.41/s  (1.088s,  940.98/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 750/1251 ( 60%)]  Loss:  3.070187 (3.2316)  Time: 1.095s,  935.32/s  (1.088s,  940.96/s)  LR: 2.620e-04  Data: 0.011 (0.013)
Train: 199 [ 800/1251 ( 64%)]  Loss:  3.207625 (3.2301)  Time: 1.173s,  872.91/s  (1.089s,  940.62/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [ 850/1251 ( 68%)]  Loss:  3.159570 (3.2262)  Time: 1.097s,  933.60/s  (1.089s,  940.46/s)  LR: 2.620e-04  Data: 0.014 (0.013)
Train: 199 [ 900/1251 ( 72%)]  Loss:  3.366337 (3.2336)  Time: 1.077s,  950.89/s  (1.089s,  940.47/s)  LR: 2.620e-04  Data: 0.013 (0.013)
Train: 199 [ 950/1251 ( 76%)]  Loss:  3.295114 (3.2367)  Time: 1.107s,  925.06/s  (1.089s,  940.47/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [1000/1251 ( 80%)]  Loss:  3.439944 (3.2464)  Time: 1.076s,  951.94/s  (1.089s,  940.68/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [1050/1251 ( 84%)]  Loss:  2.979861 (3.2342)  Time: 1.096s,  934.14/s  (1.088s,  940.80/s)  LR: 2.620e-04  Data: 0.017 (0.013)
Train: 199 [1100/1251 ( 88%)]  Loss:  3.595350 (3.2499)  Time: 1.074s,  953.12/s  (1.089s,  940.53/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [1150/1251 ( 92%)]  Loss:  3.329306 (3.2532)  Time: 1.083s,  945.17/s  (1.089s,  940.67/s)  LR: 2.620e-04  Data: 0.013 (0.013)
Train: 199 [1200/1251 ( 96%)]  Loss:  3.459083 (3.2615)  Time: 1.095s,  935.31/s  (1.089s,  940.58/s)  LR: 2.620e-04  Data: 0.012 (0.013)
Train: 199 [1250/1251 (100%)]  Loss:  3.220561 (3.2599)  Time: 1.079s,  949.34/s  (1.089s,  940.35/s)  LR: 2.620e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.914 (5.914)  Loss:  0.4728 (0.4728)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.6024 (0.9350)  Acc@1: 87.1462 (79.0200)  Acc@5: 97.2877 (94.7480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-188.pth.tar', 78.62600008300781)

Train: 200 [   0/1251 (  0%)]  Loss:  3.236328 (3.2363)  Time: 1.084s,  944.61/s  (1.084s,  944.61/s)  LR: 2.575e-04  Data: 0.024 (0.024)
Train: 200 [  50/1251 (  4%)]  Loss:  3.363831 (3.3001)  Time: 1.078s,  949.78/s  (1.087s,  942.08/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 100/1251 (  8%)]  Loss:  3.503820 (3.3680)  Time: 1.082s,  946.34/s  (1.090s,  939.61/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 150/1251 ( 12%)]  Loss:  3.340801 (3.3612)  Time: 1.078s,  950.01/s  (1.090s,  939.34/s)  LR: 2.575e-04  Data: 0.013 (0.013)
Train: 200 [ 200/1251 ( 16%)]  Loss:  3.169602 (3.3229)  Time: 1.079s,  949.30/s  (1.091s,  938.90/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 250/1251 ( 20%)]  Loss:  3.184393 (3.2998)  Time: 1.172s,  873.93/s  (1.091s,  938.70/s)  LR: 2.575e-04  Data: 0.013 (0.013)
Train: 200 [ 300/1251 ( 24%)]  Loss:  3.141263 (3.2771)  Time: 1.077s,  950.41/s  (1.090s,  939.53/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 350/1251 ( 28%)]  Loss:  3.247783 (3.2735)  Time: 1.082s,  946.77/s  (1.090s,  939.84/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 400/1251 ( 32%)]  Loss:  3.238669 (3.2696)  Time: 1.085s,  943.76/s  (1.089s,  940.30/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 450/1251 ( 36%)]  Loss:  2.961432 (3.2388)  Time: 1.083s,  945.41/s  (1.090s,  939.77/s)  LR: 2.575e-04  Data: 0.011 (0.013)
Train: 200 [ 500/1251 ( 40%)]  Loss:  3.192435 (3.2346)  Time: 1.084s,  944.47/s  (1.089s,  940.45/s)  LR: 2.575e-04  Data: 0.013 (0.013)
Train: 200 [ 550/1251 ( 44%)]  Loss:  3.134421 (3.2262)  Time: 1.107s,  924.97/s  (1.090s,  939.86/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 600/1251 ( 48%)]  Loss:  3.294552 (3.2315)  Time: 1.095s,  935.01/s  (1.090s,  939.82/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 650/1251 ( 52%)]  Loss:  3.287481 (3.2355)  Time: 1.078s,  949.76/s  (1.089s,  939.90/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 700/1251 ( 56%)]  Loss:  3.017297 (3.2209)  Time: 1.096s,  934.69/s  (1.090s,  939.76/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 750/1251 ( 60%)]  Loss:  3.231383 (3.2216)  Time: 1.080s,  948.02/s  (1.090s,  939.74/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 800/1251 ( 64%)]  Loss:  2.845758 (3.1995)  Time: 1.082s,  945.98/s  (1.090s,  939.69/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 850/1251 ( 68%)]  Loss:  2.906679 (3.1832)  Time: 1.099s,  932.04/s  (1.090s,  939.36/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [ 900/1251 ( 72%)]  Loss:  3.259891 (3.1873)  Time: 1.095s,  934.91/s  (1.090s,  939.54/s)  LR: 2.575e-04  Data: 0.013 (0.013)
Train: 200 [ 950/1251 ( 76%)]  Loss:  3.252114 (3.1905)  Time: 1.078s,  949.79/s  (1.090s,  939.47/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1000/1251 ( 80%)]  Loss:  3.078923 (3.1852)  Time: 1.077s,  951.18/s  (1.090s,  939.56/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1050/1251 ( 84%)]  Loss:  3.328812 (3.1917)  Time: 1.095s,  935.40/s  (1.090s,  939.55/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1100/1251 ( 88%)]  Loss:  3.343481 (3.1983)  Time: 1.103s,  928.12/s  (1.090s,  939.51/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1150/1251 ( 92%)]  Loss:  3.430352 (3.2080)  Time: 1.078s,  949.88/s  (1.090s,  939.59/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1200/1251 ( 96%)]  Loss:  3.285402 (3.2111)  Time: 1.077s,  950.54/s  (1.090s,  939.50/s)  LR: 2.575e-04  Data: 0.012 (0.013)
Train: 200 [1250/1251 (100%)]  Loss:  3.036841 (3.2044)  Time: 1.079s,  948.68/s  (1.090s,  939.46/s)  LR: 2.575e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.899 (5.899)  Loss:  0.4543 (0.4543)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5796 (0.9139)  Acc@1: 86.9104 (79.0220)  Acc@5: 97.8774 (94.8340)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-190.pth.tar', 78.63199997802734)

Train: 201 [   0/1251 (  0%)]  Loss:  3.265906 (3.2659)  Time: 1.085s,  943.97/s  (1.085s,  943.97/s)  LR: 2.530e-04  Data: 0.023 (0.023)
Train: 201 [  50/1251 (  4%)]  Loss:  3.327925 (3.2969)  Time: 1.076s,  951.97/s  (1.084s,  944.73/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 100/1251 (  8%)]  Loss:  3.469695 (3.3545)  Time: 1.078s,  950.19/s  (1.087s,  941.75/s)  LR: 2.530e-04  Data: 0.013 (0.013)
Train: 201 [ 150/1251 ( 12%)]  Loss:  3.363202 (3.3567)  Time: 1.082s,  946.45/s  (1.086s,  942.70/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [ 200/1251 ( 16%)]  Loss:  3.054396 (3.2962)  Time: 1.096s,  933.89/s  (1.088s,  940.84/s)  LR: 2.530e-04  Data: 0.014 (0.013)
Train: 201 [ 250/1251 ( 20%)]  Loss:  3.077498 (3.2598)  Time: 1.100s,  931.06/s  (1.090s,  939.51/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 300/1251 ( 24%)]  Loss:  2.955542 (3.2163)  Time: 1.105s,  926.48/s  (1.090s,  939.07/s)  LR: 2.530e-04  Data: 0.016 (0.013)
Train: 201 [ 350/1251 ( 28%)]  Loss:  3.094254 (3.2011)  Time: 1.083s,  945.32/s  (1.090s,  939.17/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 400/1251 ( 32%)]  Loss:  3.256052 (3.2072)  Time: 1.105s,  927.09/s  (1.091s,  938.37/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 450/1251 ( 36%)]  Loss:  3.094304 (3.1959)  Time: 1.079s,  949.31/s  (1.091s,  938.53/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 500/1251 ( 40%)]  Loss:  3.306607 (3.2059)  Time: 1.095s,  934.89/s  (1.091s,  938.25/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 550/1251 ( 44%)]  Loss:  3.294517 (3.2133)  Time: 1.093s,  936.60/s  (1.092s,  937.77/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 600/1251 ( 48%)]  Loss:  3.379185 (3.2261)  Time: 1.093s,  937.15/s  (1.092s,  937.49/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 650/1251 ( 52%)]  Loss:  3.256960 (3.2283)  Time: 1.099s,  932.15/s  (1.093s,  937.11/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 700/1251 ( 56%)]  Loss:  3.480356 (3.2451)  Time: 1.102s,  929.39/s  (1.093s,  937.16/s)  LR: 2.530e-04  Data: 0.014 (0.013)
Train: 201 [ 750/1251 ( 60%)]  Loss:  3.108943 (3.2366)  Time: 1.082s,  945.98/s  (1.092s,  937.51/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [ 800/1251 ( 64%)]  Loss:  2.975833 (3.2212)  Time: 1.078s,  949.95/s  (1.092s,  937.56/s)  LR: 2.530e-04  Data: 0.014 (0.013)
Train: 201 [ 850/1251 ( 68%)]  Loss:  3.426594 (3.2327)  Time: 1.076s,  951.86/s  (1.092s,  938.05/s)  LR: 2.530e-04  Data: 0.013 (0.013)
Train: 201 [ 900/1251 ( 72%)]  Loss:  3.251485 (3.2336)  Time: 1.096s,  934.22/s  (1.091s,  938.41/s)  LR: 2.530e-04  Data: 0.013 (0.013)
Train: 201 [ 950/1251 ( 76%)]  Loss:  3.267955 (3.2354)  Time: 1.084s,  944.50/s  (1.091s,  938.35/s)  LR: 2.530e-04  Data: 0.021 (0.013)
Train: 201 [1000/1251 ( 80%)]  Loss:  3.399407 (3.2432)  Time: 1.093s,  936.49/s  (1.091s,  938.38/s)  LR: 2.530e-04  Data: 0.014 (0.013)
Train: 201 [1050/1251 ( 84%)]  Loss:  3.017767 (3.2329)  Time: 1.076s,  951.44/s  (1.091s,  938.50/s)  LR: 2.530e-04  Data: 0.011 (0.013)
Train: 201 [1100/1251 ( 88%)]  Loss:  3.272278 (3.2346)  Time: 1.077s,  950.99/s  (1.091s,  938.81/s)  LR: 2.530e-04  Data: 0.014 (0.013)
Train: 201 [1150/1251 ( 92%)]  Loss:  3.430469 (3.2428)  Time: 1.084s,  944.90/s  (1.091s,  938.86/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [1200/1251 ( 96%)]  Loss:  3.197336 (3.2410)  Time: 1.087s,  942.28/s  (1.090s,  939.18/s)  LR: 2.530e-04  Data: 0.012 (0.013)
Train: 201 [1250/1251 (100%)]  Loss:  3.236209 (3.2408)  Time: 1.079s,  949.40/s  (1.090s,  939.13/s)  LR: 2.530e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.792 (5.792)  Loss:  0.4557 (0.4557)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5866 (0.9079)  Acc@1: 87.0283 (79.0480)  Acc@5: 97.6415 (94.8240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-189.pth.tar', 78.66800002441406)

Train: 202 [   0/1251 (  0%)]  Loss:  3.036220 (3.0362)  Time: 1.088s,  940.82/s  (1.088s,  940.82/s)  LR: 2.486e-04  Data: 0.027 (0.027)
Train: 202 [  50/1251 (  4%)]  Loss:  3.099049 (3.0676)  Time: 1.187s,  862.90/s  (1.102s,  929.37/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 100/1251 (  8%)]  Loss:  3.242277 (3.1258)  Time: 1.094s,  935.89/s  (1.100s,  931.33/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 150/1251 ( 12%)]  Loss:  3.312074 (3.1724)  Time: 1.077s,  950.85/s  (1.094s,  935.74/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 200/1251 ( 16%)]  Loss:  3.096345 (3.1572)  Time: 1.085s,  943.57/s  (1.094s,  935.74/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 250/1251 ( 20%)]  Loss:  3.527670 (3.2189)  Time: 1.082s,  946.80/s  (1.094s,  936.03/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [ 300/1251 ( 24%)]  Loss:  3.301497 (3.2307)  Time: 1.076s,  951.35/s  (1.092s,  937.58/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 350/1251 ( 28%)]  Loss:  3.153566 (3.2211)  Time: 1.079s,  949.38/s  (1.091s,  938.57/s)  LR: 2.486e-04  Data: 0.013 (0.013)
Train: 202 [ 400/1251 ( 32%)]  Loss:  3.440235 (3.2454)  Time: 1.102s,  929.43/s  (1.091s,  938.47/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 450/1251 ( 36%)]  Loss:  3.497788 (3.2707)  Time: 1.096s,  934.56/s  (1.091s,  938.78/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 202 [ 500/1251 ( 40%)]  Loss:  3.333871 (3.2764)  Time: 1.076s,  951.25/s  (1.090s,  939.47/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 550/1251 ( 44%)]  Loss:  3.098978 (3.2616)  Time: 1.096s,  934.21/s  (1.090s,  939.76/s)  LR: 2.486e-04  Data: 0.013 (0.013)
Train: 202 [ 600/1251 ( 48%)]  Loss:  3.195064 (3.2565)  Time: 1.097s,  933.44/s  (1.090s,  939.48/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 650/1251 ( 52%)]  Loss:  3.231854 (3.2547)  Time: 1.098s,  932.73/s  (1.090s,  939.79/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 700/1251 ( 56%)]  Loss:  3.153603 (3.2480)  Time: 1.079s,  949.15/s  (1.089s,  940.04/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 750/1251 ( 60%)]  Loss:  3.102427 (3.2389)  Time: 1.081s,  947.54/s  (1.089s,  939.90/s)  LR: 2.486e-04  Data: 0.013 (0.013)
Train: 202 [ 800/1251 ( 64%)]  Loss:  3.339839 (3.2448)  Time: 1.097s,  933.30/s  (1.089s,  940.06/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 850/1251 ( 68%)]  Loss:  3.244320 (3.2448)  Time: 1.094s,  936.19/s  (1.089s,  940.22/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 900/1251 ( 72%)]  Loss:  3.484037 (3.2574)  Time: 1.079s,  949.27/s  (1.089s,  940.12/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [ 950/1251 ( 76%)]  Loss:  3.279655 (3.2585)  Time: 1.076s,  951.80/s  (1.089s,  940.35/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [1000/1251 ( 80%)]  Loss:  3.620653 (3.2758)  Time: 1.084s,  944.49/s  (1.089s,  940.64/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [1050/1251 ( 84%)]  Loss:  3.306451 (3.2772)  Time: 1.080s,  948.27/s  (1.089s,  940.41/s)  LR: 2.486e-04  Data: 0.011 (0.013)
Train: 202 [1100/1251 ( 88%)]  Loss:  3.469927 (3.2855)  Time: 1.078s,  950.20/s  (1.089s,  940.65/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [1150/1251 ( 92%)]  Loss:  3.335462 (3.2876)  Time: 1.095s,  935.04/s  (1.089s,  940.50/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [1200/1251 ( 96%)]  Loss:  3.168739 (3.2829)  Time: 1.079s,  949.12/s  (1.089s,  940.56/s)  LR: 2.486e-04  Data: 0.012 (0.013)
Train: 202 [1250/1251 (100%)]  Loss:  3.284241 (3.2829)  Time: 1.064s,  962.19/s  (1.089s,  940.24/s)  LR: 2.486e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.822 (5.822)  Loss:  0.4182 (0.4182)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5424 (0.8899)  Acc@1: 86.6745 (79.0400)  Acc@5: 97.5236 (94.8440)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-193.pth.tar', 78.75200000488282)

Train: 203 [   0/1251 (  0%)]  Loss:  3.327673 (3.3277)  Time: 1.084s,  944.52/s  (1.084s,  944.52/s)  LR: 2.442e-04  Data: 0.022 (0.022)
Train: 203 [  50/1251 (  4%)]  Loss:  3.569189 (3.4484)  Time: 1.077s,  950.40/s  (1.086s,  942.68/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 100/1251 (  8%)]  Loss:  3.204195 (3.3670)  Time: 1.087s,  942.34/s  (1.085s,  943.64/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 150/1251 ( 12%)]  Loss:  3.252044 (3.3383)  Time: 1.082s,  946.82/s  (1.086s,  942.94/s)  LR: 2.442e-04  Data: 0.014 (0.013)
Train: 203 [ 200/1251 ( 16%)]  Loss:  3.112588 (3.2931)  Time: 1.083s,  945.54/s  (1.086s,  942.82/s)  LR: 2.442e-04  Data: 0.014 (0.013)
Train: 203 [ 250/1251 ( 20%)]  Loss:  3.152473 (3.2697)  Time: 1.076s,  951.99/s  (1.088s,  941.56/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 300/1251 ( 24%)]  Loss:  3.126611 (3.2493)  Time: 1.076s,  951.25/s  (1.088s,  941.60/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 350/1251 ( 28%)]  Loss:  2.905300 (3.2063)  Time: 1.095s,  934.87/s  (1.088s,  941.34/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 400/1251 ( 32%)]  Loss:  3.219270 (3.2077)  Time: 1.077s,  950.47/s  (1.089s,  940.49/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 450/1251 ( 36%)]  Loss:  3.456950 (3.2326)  Time: 1.095s,  935.24/s  (1.090s,  939.63/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [ 500/1251 ( 40%)]  Loss:  3.093363 (3.2200)  Time: 1.077s,  950.52/s  (1.089s,  940.17/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 550/1251 ( 44%)]  Loss:  3.042584 (3.2052)  Time: 1.095s,  935.56/s  (1.090s,  939.62/s)  LR: 2.442e-04  Data: 0.014 (0.013)
Train: 203 [ 600/1251 ( 48%)]  Loss:  3.486912 (3.2269)  Time: 1.077s,  950.59/s  (1.090s,  939.63/s)  LR: 2.442e-04  Data: 0.015 (0.013)
Train: 203 [ 650/1251 ( 52%)]  Loss:  2.891191 (3.2029)  Time: 1.076s,  951.71/s  (1.090s,  939.13/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 700/1251 ( 56%)]  Loss:  2.984448 (3.1883)  Time: 1.080s,  948.39/s  (1.090s,  939.51/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 750/1251 ( 60%)]  Loss:  2.450027 (3.1422)  Time: 1.077s,  950.81/s  (1.090s,  939.49/s)  LR: 2.442e-04  Data: 0.014 (0.013)
Train: 203 [ 800/1251 ( 64%)]  Loss:  3.244534 (3.1482)  Time: 1.100s,  931.27/s  (1.090s,  939.58/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 850/1251 ( 68%)]  Loss:  3.047020 (3.1426)  Time: 1.077s,  951.04/s  (1.090s,  939.59/s)  LR: 2.442e-04  Data: 0.014 (0.013)
Train: 203 [ 900/1251 ( 72%)]  Loss:  2.887513 (3.1292)  Time: 1.081s,  947.03/s  (1.090s,  939.41/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [ 950/1251 ( 76%)]  Loss:  3.441036 (3.1447)  Time: 1.094s,  936.35/s  (1.090s,  939.42/s)  LR: 2.442e-04  Data: 0.014 (0.013)
Train: 203 [1000/1251 ( 80%)]  Loss:  2.861842 (3.1313)  Time: 1.135s,  902.32/s  (1.090s,  939.10/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [1050/1251 ( 84%)]  Loss:  3.413425 (3.1441)  Time: 1.077s,  950.67/s  (1.091s,  938.96/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [1100/1251 ( 88%)]  Loss:  3.166746 (3.1451)  Time: 1.094s,  935.60/s  (1.091s,  938.83/s)  LR: 2.442e-04  Data: 0.011 (0.013)
Train: 203 [1150/1251 ( 92%)]  Loss:  3.296046 (3.1514)  Time: 1.094s,  936.02/s  (1.090s,  939.04/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Train: 203 [1200/1251 ( 96%)]  Loss:  3.248380 (3.1553)  Time: 1.097s,  933.36/s  (1.091s,  938.78/s)  LR: 2.442e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 203 [1250/1251 (100%)]  Loss:  3.109491 (3.1535)  Time: 1.061s,  965.02/s  (1.091s,  938.85/s)  LR: 2.442e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.806 (5.806)  Loss:  0.4512 (0.4512)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.446)  Loss:  0.5572 (0.8898)  Acc@1: 87.1462 (79.1360)  Acc@5: 97.7594 (94.8860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-194.pth.tar', 78.75600005615235)

Train: 204 [   0/1251 (  0%)]  Loss:  3.175298 (3.1753)  Time: 1.084s,  944.69/s  (1.084s,  944.69/s)  LR: 2.398e-04  Data: 0.022 (0.022)
Train: 204 [  50/1251 (  4%)]  Loss:  3.273078 (3.2242)  Time: 1.105s,  927.00/s  (1.081s,  946.90/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 100/1251 (  8%)]  Loss:  3.192253 (3.2135)  Time: 1.081s,  947.38/s  (1.086s,  942.78/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 204 [ 150/1251 ( 12%)]  Loss:  3.333825 (3.2436)  Time: 1.082s,  946.35/s  (1.085s,  944.14/s)  LR: 2.398e-04  Data: 0.014 (0.013)
Train: 204 [ 200/1251 ( 16%)]  Loss:  2.880249 (3.1709)  Time: 1.079s,  948.67/s  (1.084s,  944.81/s)  LR: 2.398e-04  Data: 0.015 (0.013)
Train: 204 [ 250/1251 ( 20%)]  Loss:  3.182825 (3.1729)  Time: 1.078s,  949.93/s  (1.083s,  945.37/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 300/1251 ( 24%)]  Loss:  3.266361 (3.1863)  Time: 1.094s,  936.09/s  (1.084s,  944.75/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [ 350/1251 ( 28%)]  Loss:  3.302756 (3.2008)  Time: 1.096s,  934.26/s  (1.085s,  943.74/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 400/1251 ( 32%)]  Loss:  3.056579 (3.1848)  Time: 1.096s,  934.20/s  (1.086s,  942.52/s)  LR: 2.398e-04  Data: 0.013 (0.013)
Train: 204 [ 450/1251 ( 36%)]  Loss:  3.013201 (3.1676)  Time: 1.097s,  933.39/s  (1.088s,  941.51/s)  LR: 2.398e-04  Data: 0.013 (0.013)
Train: 204 [ 500/1251 ( 40%)]  Loss:  3.282036 (3.1780)  Time: 1.137s,  900.99/s  (1.088s,  941.11/s)  LR: 2.398e-04  Data: 0.012 (0.012)
Train: 204 [ 550/1251 ( 44%)]  Loss:  3.383996 (3.1952)  Time: 1.095s,  934.89/s  (1.088s,  940.99/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 600/1251 ( 48%)]  Loss:  3.114129 (3.1890)  Time: 1.093s,  936.77/s  (1.089s,  940.57/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 650/1251 ( 52%)]  Loss:  3.178221 (3.1882)  Time: 1.078s,  949.90/s  (1.089s,  940.28/s)  LR: 2.398e-04  Data: 0.013 (0.012)
Train: 204 [ 700/1251 ( 56%)]  Loss:  3.060639 (3.1797)  Time: 1.076s,  951.35/s  (1.089s,  940.55/s)  LR: 2.398e-04  Data: 0.015 (0.012)
Train: 204 [ 750/1251 ( 60%)]  Loss:  3.191231 (3.1804)  Time: 1.077s,  950.76/s  (1.089s,  940.63/s)  LR: 2.398e-04  Data: 0.012 (0.012)
Train: 204 [ 800/1251 ( 64%)]  Loss:  3.129867 (3.1774)  Time: 1.082s,  946.42/s  (1.088s,  940.95/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [ 850/1251 ( 68%)]  Loss:  3.184784 (3.1779)  Time: 1.077s,  950.52/s  (1.088s,  941.00/s)  LR: 2.398e-04  Data: 0.012 (0.012)
Train: 204 [ 900/1251 ( 72%)]  Loss:  2.981927 (3.1675)  Time: 1.087s,  942.40/s  (1.088s,  940.89/s)  LR: 2.398e-04  Data: 0.014 (0.012)
Train: 204 [ 950/1251 ( 76%)]  Loss:  3.153273 (3.1668)  Time: 1.078s,  949.49/s  (1.088s,  941.09/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1000/1251 ( 80%)]  Loss:  3.073365 (3.1624)  Time: 1.078s,  949.70/s  (1.088s,  941.31/s)  LR: 2.398e-04  Data: 0.015 (0.013)
Train: 204 [1050/1251 ( 84%)]  Loss:  2.980952 (3.1541)  Time: 1.078s,  949.51/s  (1.088s,  941.26/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1100/1251 ( 88%)]  Loss:  3.277071 (3.1595)  Time: 1.080s,  948.40/s  (1.088s,  941.44/s)  LR: 2.398e-04  Data: 0.012 (0.013)
Train: 204 [1150/1251 ( 92%)]  Loss:  3.227251 (3.1623)  Time: 1.085s,  943.42/s  (1.088s,  941.37/s)  LR: 2.398e-04  Data: 0.011 (0.013)
Train: 204 [1200/1251 ( 96%)]  Loss:  3.025455 (3.1568)  Time: 1.094s,  935.99/s  (1.088s,  941.05/s)  LR: 2.398e-04  Data: 0.014 (0.013)
Train: 204 [1250/1251 (100%)]  Loss:  3.385365 (3.1656)  Time: 1.061s,  965.51/s  (1.089s,  940.55/s)  LR: 2.398e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.755 (5.755)  Loss:  0.4583 (0.4583)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.6280 (0.9139)  Acc@1: 85.8491 (79.3140)  Acc@5: 97.1698 (94.7840)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-195.pth.tar', 78.83599992675781)

Train: 205 [   0/1251 (  0%)]  Loss:  3.085181 (3.0852)  Time: 1.090s,  939.50/s  (1.090s,  939.50/s)  LR: 2.354e-04  Data: 0.029 (0.029)
Train: 205 [  50/1251 (  4%)]  Loss:  3.185681 (3.1354)  Time: 1.082s,  946.60/s  (1.085s,  943.84/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 100/1251 (  8%)]  Loss:  3.058361 (3.1097)  Time: 1.098s,  932.84/s  (1.088s,  941.10/s)  LR: 2.354e-04  Data: 0.014 (0.013)
Train: 205 [ 150/1251 ( 12%)]  Loss:  3.298892 (3.1570)  Time: 1.102s,  929.48/s  (1.090s,  939.05/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 200/1251 ( 16%)]  Loss:  2.997863 (3.1252)  Time: 1.076s,  951.79/s  (1.091s,  938.81/s)  LR: 2.354e-04  Data: 0.013 (0.013)
Train: 205 [ 250/1251 ( 20%)]  Loss:  3.083629 (3.1183)  Time: 1.097s,  933.41/s  (1.092s,  937.92/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 300/1251 ( 24%)]  Loss:  3.368972 (3.1541)  Time: 1.078s,  950.29/s  (1.092s,  937.71/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 350/1251 ( 28%)]  Loss:  3.152615 (3.1539)  Time: 1.078s,  949.98/s  (1.091s,  938.67/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 400/1251 ( 32%)]  Loss:  3.115285 (3.1496)  Time: 1.104s,  927.34/s  (1.091s,  938.66/s)  LR: 2.354e-04  Data: 0.014 (0.013)
Train: 205 [ 450/1251 ( 36%)]  Loss:  2.929472 (3.1276)  Time: 1.077s,  951.15/s  (1.091s,  938.70/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 500/1251 ( 40%)]  Loss:  3.015659 (3.1174)  Time: 1.177s,  870.24/s  (1.091s,  938.98/s)  LR: 2.354e-04  Data: 0.011 (0.013)
Train: 205 [ 550/1251 ( 44%)]  Loss:  3.092880 (3.1154)  Time: 1.077s,  950.80/s  (1.090s,  939.22/s)  LR: 2.354e-04  Data: 0.013 (0.013)
Train: 205 [ 600/1251 ( 48%)]  Loss:  3.304474 (3.1299)  Time: 1.128s,  907.75/s  (1.090s,  939.32/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 650/1251 ( 52%)]  Loss:  2.998390 (3.1205)  Time: 1.106s,  925.47/s  (1.090s,  939.52/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 700/1251 ( 56%)]  Loss:  2.931726 (3.1079)  Time: 1.087s,  942.30/s  (1.090s,  939.35/s)  LR: 2.354e-04  Data: 0.012 (0.013)
Train: 205 [ 750/1251 ( 60%)]  Loss:  3.522051 (3.1338)  Time: 1.108s,  924.44/s  (1.090s,  939.14/s)  LR: 2.354e-04  Data: 0.011 (0.012)
Train: 205 [ 800/1251 ( 64%)]  Loss:  3.294446 (3.1433)  Time: 1.081s,  947.43/s  (1.090s,  939.22/s)  LR: 2.354e-04  Data: 0.013 (0.012)
Train: 205 [ 850/1251 ( 68%)]  Loss:  3.200349 (3.1464)  Time: 1.081s,  947.49/s  (1.090s,  939.58/s)  LR: 2.354e-04  Data: 0.012 (0.012)
Train: 205 [ 900/1251 ( 72%)]  Loss:  3.270425 (3.1530)  Time: 1.086s,  943.26/s  (1.090s,  939.44/s)  LR: 2.354e-04  Data: 0.011 (0.012)
Train: 205 [ 950/1251 ( 76%)]  Loss:  2.946287 (3.1426)  Time: 1.078s,  949.82/s  (1.090s,  939.51/s)  LR: 2.354e-04  Data: 0.012 (0.012)
Train: 205 [1000/1251 ( 80%)]  Loss:  2.972281 (3.1345)  Time: 1.082s,  946.50/s  (1.090s,  939.77/s)  LR: 2.354e-04  Data: 0.012 (0.012)
Train: 205 [1050/1251 ( 84%)]  Loss:  3.266928 (3.1405)  Time: 1.077s,  950.92/s  (1.089s,  939.91/s)  LR: 2.354e-04  Data: 0.012 (0.012)
Train: 205 [1100/1251 ( 88%)]  Loss:  3.015191 (3.1351)  Time: 1.082s,  946.28/s  (1.090s,  939.70/s)  LR: 2.354e-04  Data: 0.017 (0.012)
Train: 205 [1150/1251 ( 92%)]  Loss:  3.181633 (3.1370)  Time: 1.081s,  947.43/s  (1.089s,  940.00/s)  LR: 2.354e-04  Data: 0.012 (0.012)
Train: 205 [1200/1251 ( 96%)]  Loss:  3.300700 (3.1436)  Time: 1.172s,  873.76/s  (1.089s,  940.18/s)  LR: 2.354e-04  Data: 0.013 (0.012)
Train: 205 [1250/1251 (100%)]  Loss:  2.989448 (3.1376)  Time: 1.062s,  964.46/s  (1.089s,  940.20/s)  LR: 2.354e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.820 (5.820)  Loss:  0.4530 (0.4530)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5512 (0.9209)  Acc@1: 87.3821 (79.1900)  Acc@5: 98.1132 (94.8540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-198.pth.tar', 78.89799987548828)

Train: 206 [   0/1251 (  0%)]  Loss:  2.911175 (2.9112)  Time: 1.083s,  945.46/s  (1.083s,  945.46/s)  LR: 2.311e-04  Data: 0.022 (0.022)
Train: 206 [  50/1251 (  4%)]  Loss:  3.172902 (3.0420)  Time: 1.081s,  947.12/s  (1.086s,  942.85/s)  LR: 2.311e-04  Data: 0.016 (0.013)
Train: 206 [ 100/1251 (  8%)]  Loss:  2.954793 (3.0130)  Time: 1.095s,  934.81/s  (1.091s,  938.65/s)  LR: 2.311e-04  Data: 0.014 (0.013)
Train: 206 [ 150/1251 ( 12%)]  Loss:  2.804719 (2.9609)  Time: 1.095s,  935.34/s  (1.091s,  938.92/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 200/1251 ( 16%)]  Loss:  3.294433 (3.0276)  Time: 1.082s,  946.22/s  (1.088s,  941.18/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 250/1251 ( 20%)]  Loss:  3.119632 (3.0429)  Time: 1.102s,  928.80/s  (1.089s,  940.57/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 300/1251 ( 24%)]  Loss:  3.180636 (3.0626)  Time: 1.097s,  933.64/s  (1.088s,  940.96/s)  LR: 2.311e-04  Data: 0.013 (0.013)
Train: 206 [ 350/1251 ( 28%)]  Loss:  3.198967 (3.0797)  Time: 1.095s,  934.76/s  (1.089s,  940.51/s)  LR: 2.311e-04  Data: 0.015 (0.013)
Train: 206 [ 400/1251 ( 32%)]  Loss:  3.287035 (3.1027)  Time: 1.079s,  948.74/s  (1.088s,  941.00/s)  LR: 2.311e-04  Data: 0.013 (0.013)
Train: 206 [ 450/1251 ( 36%)]  Loss:  3.147893 (3.1072)  Time: 1.092s,  938.08/s  (1.088s,  941.04/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 500/1251 ( 40%)]  Loss:  3.271559 (3.1222)  Time: 1.100s,  930.76/s  (1.088s,  941.18/s)  LR: 2.311e-04  Data: 0.012 (0.012)
Train: 206 [ 550/1251 ( 44%)]  Loss:  3.048654 (3.1160)  Time: 1.098s,  932.69/s  (1.089s,  940.46/s)  LR: 2.311e-04  Data: 0.012 (0.012)
Train: 206 [ 600/1251 ( 48%)]  Loss:  3.295686 (3.1299)  Time: 1.095s,  935.23/s  (1.088s,  940.75/s)  LR: 2.311e-04  Data: 0.011 (0.012)
Train: 206 [ 650/1251 ( 52%)]  Loss:  2.900419 (3.1135)  Time: 1.079s,  949.29/s  (1.088s,  940.89/s)  LR: 2.311e-04  Data: 0.012 (0.012)
Train: 206 [ 700/1251 ( 56%)]  Loss:  3.567149 (3.1437)  Time: 1.097s,  933.68/s  (1.088s,  940.79/s)  LR: 2.311e-04  Data: 0.013 (0.012)
Train: 206 [ 750/1251 ( 60%)]  Loss:  2.792508 (3.1218)  Time: 1.083s,  945.93/s  (1.088s,  940.85/s)  LR: 2.311e-04  Data: 0.012 (0.012)
Train: 206 [ 800/1251 ( 64%)]  Loss:  3.377241 (3.1368)  Time: 1.076s,  951.94/s  (1.088s,  940.86/s)  LR: 2.311e-04  Data: 0.012 (0.012)
Train: 206 [ 850/1251 ( 68%)]  Loss:  3.323140 (3.1471)  Time: 1.092s,  938.07/s  (1.088s,  941.25/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [ 900/1251 ( 72%)]  Loss:  3.456357 (3.1634)  Time: 1.101s,  929.72/s  (1.088s,  941.01/s)  LR: 2.311e-04  Data: 0.013 (0.013)
Train: 206 [ 950/1251 ( 76%)]  Loss:  3.254439 (3.1680)  Time: 1.078s,  950.05/s  (1.088s,  940.92/s)  LR: 2.311e-04  Data: 0.015 (0.013)
Train: 206 [1000/1251 ( 80%)]  Loss:  3.132774 (3.1663)  Time: 1.082s,  946.79/s  (1.088s,  941.10/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1050/1251 ( 84%)]  Loss:  3.008182 (3.1591)  Time: 1.076s,  951.83/s  (1.088s,  941.17/s)  LR: 2.311e-04  Data: 0.011 (0.013)
Train: 206 [1100/1251 ( 88%)]  Loss:  3.402795 (3.1697)  Time: 1.106s,  926.02/s  (1.088s,  941.17/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1150/1251 ( 92%)]  Loss:  3.453964 (3.1815)  Time: 1.077s,  950.69/s  (1.088s,  940.91/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1200/1251 ( 96%)]  Loss:  3.255564 (3.1845)  Time: 1.105s,  926.41/s  (1.088s,  940.98/s)  LR: 2.311e-04  Data: 0.012 (0.013)
Train: 206 [1250/1251 (100%)]  Loss:  3.405185 (3.1930)  Time: 1.063s,  963.66/s  (1.088s,  940.89/s)  LR: 2.311e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.757 (5.757)  Loss:  0.4692 (0.4692)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5873 (0.9294)  Acc@1: 87.3821 (79.2440)  Acc@5: 97.7594 (94.7720)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-196.pth.tar', 78.98600008056641)

Train: 207 [   0/1251 (  0%)]  Loss:  3.028780 (3.0288)  Time: 1.084s,  944.87/s  (1.084s,  944.87/s)  LR: 2.268e-04  Data: 0.024 (0.024)
Train: 207 [  50/1251 (  4%)]  Loss:  3.532248 (3.2805)  Time: 1.079s,  949.35/s  (1.089s,  940.02/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Train: 207 [ 100/1251 (  8%)]  Loss:  3.334456 (3.2985)  Time: 1.107s,  925.18/s  (1.086s,  942.88/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 150/1251 ( 12%)]  Loss:  3.186112 (3.2704)  Time: 1.093s,  936.80/s  (1.087s,  941.77/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 200/1251 ( 16%)]  Loss:  3.217782 (3.2599)  Time: 1.076s,  951.69/s  (1.086s,  943.14/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 250/1251 ( 20%)]  Loss:  3.353118 (3.2754)  Time: 1.077s,  950.51/s  (1.087s,  942.18/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Train: 207 [ 300/1251 ( 24%)]  Loss:  3.184394 (3.2624)  Time: 1.109s,  923.69/s  (1.088s,  941.32/s)  LR: 2.268e-04  Data: 0.013 (0.013)
Train: 207 [ 350/1251 ( 28%)]  Loss:  3.512177 (3.2936)  Time: 1.087s,  941.96/s  (1.087s,  941.88/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 207 [ 400/1251 ( 32%)]  Loss:  3.268178 (3.2908)  Time: 1.077s,  950.64/s  (1.087s,  941.69/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 450/1251 ( 36%)]  Loss:  3.134276 (3.2752)  Time: 1.084s,  944.43/s  (1.087s,  942.47/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 500/1251 ( 40%)]  Loss:  3.290814 (3.2766)  Time: 1.085s,  944.15/s  (1.087s,  941.84/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 550/1251 ( 44%)]  Loss:  3.169333 (3.2676)  Time: 1.080s,  948.37/s  (1.087s,  941.82/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 600/1251 ( 48%)]  Loss:  3.158685 (3.2593)  Time: 1.097s,  933.42/s  (1.087s,  941.89/s)  LR: 2.268e-04  Data: 0.013 (0.013)
Train: 207 [ 650/1251 ( 52%)]  Loss:  3.296815 (3.2619)  Time: 1.078s,  950.20/s  (1.087s,  941.89/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Train: 207 [ 700/1251 ( 56%)]  Loss:  2.923272 (3.2394)  Time: 1.077s,  951.09/s  (1.087s,  942.30/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 750/1251 ( 60%)]  Loss:  3.299377 (3.2431)  Time: 1.095s,  934.76/s  (1.087s,  941.96/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 800/1251 ( 64%)]  Loss:  2.844439 (3.2197)  Time: 1.075s,  952.77/s  (1.087s,  941.92/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [ 850/1251 ( 68%)]  Loss:  3.016830 (3.2084)  Time: 1.085s,  943.44/s  (1.087s,  941.83/s)  LR: 2.268e-04  Data: 0.013 (0.013)
Train: 207 [ 900/1251 ( 72%)]  Loss:  3.483507 (3.2229)  Time: 1.073s,  954.56/s  (1.087s,  941.88/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [ 950/1251 ( 76%)]  Loss:  3.224053 (3.2229)  Time: 1.098s,  932.33/s  (1.087s,  942.16/s)  LR: 2.268e-04  Data: 0.011 (0.013)
Train: 207 [1000/1251 ( 80%)]  Loss:  3.027766 (3.2136)  Time: 1.076s,  952.11/s  (1.087s,  941.97/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [1050/1251 ( 84%)]  Loss:  3.030204 (3.2053)  Time: 1.080s,  947.87/s  (1.087s,  941.99/s)  LR: 2.268e-04  Data: 0.014 (0.013)
Train: 207 [1100/1251 ( 88%)]  Loss:  3.332695 (3.2108)  Time: 1.121s,  913.72/s  (1.087s,  941.97/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [1150/1251 ( 92%)]  Loss:  3.181368 (3.2096)  Time: 1.099s,  931.90/s  (1.087s,  941.88/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [1200/1251 ( 96%)]  Loss:  3.091837 (3.2049)  Time: 1.099s,  931.48/s  (1.087s,  942.00/s)  LR: 2.268e-04  Data: 0.012 (0.013)
Train: 207 [1250/1251 (100%)]  Loss:  2.883887 (3.1926)  Time: 1.062s,  963.80/s  (1.087s,  941.93/s)  LR: 2.268e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.789 (5.789)  Loss:  0.4461 (0.4461)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5392 (0.8952)  Acc@1: 87.7358 (79.2020)  Acc@5: 98.1132 (94.8920)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-199.pth.tar', 79.02000010498047)

Train: 208 [   0/1251 (  0%)]  Loss:  3.055910 (3.0559)  Time: 1.084s,  944.84/s  (1.084s,  944.84/s)  LR: 2.225e-04  Data: 0.023 (0.023)
Train: 208 [  50/1251 (  4%)]  Loss:  3.141638 (3.0988)  Time: 1.080s,  948.01/s  (1.093s,  937.00/s)  LR: 2.225e-04  Data: 0.015 (0.013)
Train: 208 [ 100/1251 (  8%)]  Loss:  3.130948 (3.1095)  Time: 1.085s,  943.51/s  (1.089s,  940.59/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 150/1251 ( 12%)]  Loss:  3.198369 (3.1317)  Time: 1.095s,  934.92/s  (1.089s,  940.52/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 200/1251 ( 16%)]  Loss:  3.498165 (3.2050)  Time: 1.167s,  877.19/s  (1.091s,  938.92/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [ 250/1251 ( 20%)]  Loss:  3.246523 (3.2119)  Time: 1.076s,  951.49/s  (1.092s,  938.08/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 300/1251 ( 24%)]  Loss:  2.934543 (3.1723)  Time: 1.103s,  928.33/s  (1.092s,  937.41/s)  LR: 2.225e-04  Data: 0.013 (0.013)
Train: 208 [ 350/1251 ( 28%)]  Loss:  3.004449 (3.1513)  Time: 1.079s,  949.16/s  (1.091s,  938.69/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 400/1251 ( 32%)]  Loss:  3.300412 (3.1679)  Time: 1.078s,  949.85/s  (1.091s,  938.81/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 450/1251 ( 36%)]  Loss:  3.321721 (3.1833)  Time: 1.077s,  951.17/s  (1.090s,  939.43/s)  LR: 2.225e-04  Data: 0.015 (0.013)
Train: 208 [ 500/1251 ( 40%)]  Loss:  3.287193 (3.1927)  Time: 1.075s,  952.31/s  (1.090s,  939.32/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 550/1251 ( 44%)]  Loss:  3.147022 (3.1889)  Time: 1.076s,  951.57/s  (1.089s,  940.04/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 600/1251 ( 48%)]  Loss:  3.368132 (3.2027)  Time: 1.084s,  944.24/s  (1.089s,  940.46/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 650/1251 ( 52%)]  Loss:  2.698715 (3.1667)  Time: 1.077s,  951.15/s  (1.089s,  940.57/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 700/1251 ( 56%)]  Loss:  3.056872 (3.1594)  Time: 1.078s,  950.05/s  (1.088s,  940.76/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 750/1251 ( 60%)]  Loss:  3.268064 (3.1662)  Time: 1.103s,  928.15/s  (1.088s,  940.95/s)  LR: 2.225e-04  Data: 0.015 (0.013)
Train: 208 [ 800/1251 ( 64%)]  Loss:  3.012156 (3.1571)  Time: 1.076s,  951.54/s  (1.088s,  941.09/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 850/1251 ( 68%)]  Loss:  3.276704 (3.1638)  Time: 1.095s,  934.77/s  (1.088s,  940.78/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 900/1251 ( 72%)]  Loss:  3.326227 (3.1723)  Time: 1.094s,  936.28/s  (1.089s,  940.43/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [ 950/1251 ( 76%)]  Loss:  3.055250 (3.1665)  Time: 1.094s,  935.69/s  (1.089s,  940.39/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1000/1251 ( 80%)]  Loss:  3.239911 (3.1699)  Time: 1.096s,  933.89/s  (1.089s,  940.49/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1050/1251 ( 84%)]  Loss:  3.238688 (3.1731)  Time: 1.081s,  947.05/s  (1.088s,  940.80/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Train: 208 [1100/1251 ( 88%)]  Loss:  2.830572 (3.1582)  Time: 1.116s,  917.38/s  (1.088s,  940.84/s)  LR: 2.225e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 208 [1150/1251 ( 92%)]  Loss:  2.880007 (3.1466)  Time: 1.102s,  929.47/s  (1.089s,  940.58/s)  LR: 2.225e-04  Data: 0.011 (0.013)
Train: 208 [1200/1251 ( 96%)]  Loss:  3.215950 (3.1494)  Time: 1.077s,  950.69/s  (1.089s,  940.66/s)  LR: 2.225e-04  Data: 0.013 (0.013)
Train: 208 [1250/1251 (100%)]  Loss:  3.414481 (3.1596)  Time: 1.062s,  964.12/s  (1.089s,  940.61/s)  LR: 2.225e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.858 (5.858)  Loss:  0.4680 (0.4680)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5933 (0.9061)  Acc@1: 87.0283 (79.2620)  Acc@5: 97.1698 (94.9420)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-200.pth.tar', 79.02200013183594)

Train: 209 [   0/1251 (  0%)]  Loss:  3.389188 (3.3892)  Time: 1.102s,  928.82/s  (1.102s,  928.82/s)  LR: 2.183e-04  Data: 0.022 (0.022)
Train: 209 [  50/1251 (  4%)]  Loss:  3.213611 (3.3014)  Time: 1.108s,  924.49/s  (1.094s,  935.87/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 100/1251 (  8%)]  Loss:  3.200597 (3.2678)  Time: 1.081s,  947.10/s  (1.089s,  940.34/s)  LR: 2.183e-04  Data: 0.016 (0.013)
Train: 209 [ 150/1251 ( 12%)]  Loss:  3.075750 (3.2198)  Time: 1.174s,  872.35/s  (1.089s,  940.06/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 200/1251 ( 16%)]  Loss:  3.274538 (3.2307)  Time: 1.094s,  936.38/s  (1.088s,  940.78/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 250/1251 ( 20%)]  Loss:  3.215952 (3.2283)  Time: 1.103s,  928.39/s  (1.090s,  939.74/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 300/1251 ( 24%)]  Loss:  3.026003 (3.1994)  Time: 1.089s,  940.09/s  (1.089s,  940.06/s)  LR: 2.183e-04  Data: 0.013 (0.013)
Train: 209 [ 350/1251 ( 28%)]  Loss:  3.144687 (3.1925)  Time: 1.095s,  935.35/s  (1.090s,  939.61/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [ 400/1251 ( 32%)]  Loss:  3.203899 (3.1938)  Time: 1.081s,  947.12/s  (1.090s,  939.43/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [ 450/1251 ( 36%)]  Loss:  3.429209 (3.2173)  Time: 1.095s,  935.12/s  (1.091s,  938.98/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 500/1251 ( 40%)]  Loss:  2.869746 (3.1857)  Time: 1.076s,  952.01/s  (1.090s,  939.16/s)  LR: 2.183e-04  Data: 0.013 (0.013)
Train: 209 [ 550/1251 ( 44%)]  Loss:  3.076941 (3.1767)  Time: 1.097s,  933.10/s  (1.091s,  938.72/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 600/1251 ( 48%)]  Loss:  3.337923 (3.1891)  Time: 1.081s,  947.56/s  (1.091s,  938.78/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 650/1251 ( 52%)]  Loss:  3.276483 (3.1953)  Time: 1.094s,  936.07/s  (1.090s,  939.11/s)  LR: 2.183e-04  Data: 0.014 (0.013)
Train: 209 [ 700/1251 ( 56%)]  Loss:  2.887336 (3.1748)  Time: 1.198s,  854.61/s  (1.091s,  938.64/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 750/1251 ( 60%)]  Loss:  3.251826 (3.1796)  Time: 1.107s,  924.81/s  (1.091s,  938.26/s)  LR: 2.183e-04  Data: 0.013 (0.012)
Train: 209 [ 800/1251 ( 64%)]  Loss:  3.243298 (3.1834)  Time: 1.075s,  952.15/s  (1.091s,  938.72/s)  LR: 2.183e-04  Data: 0.014 (0.012)
Train: 209 [ 850/1251 ( 68%)]  Loss:  3.309223 (3.1903)  Time: 1.094s,  936.02/s  (1.091s,  938.90/s)  LR: 2.183e-04  Data: 0.012 (0.013)
Train: 209 [ 900/1251 ( 72%)]  Loss:  3.167679 (3.1892)  Time: 1.077s,  950.69/s  (1.090s,  939.03/s)  LR: 2.183e-04  Data: 0.014 (0.013)
Train: 209 [ 950/1251 ( 76%)]  Loss:  3.167557 (3.1881)  Time: 1.106s,  926.06/s  (1.090s,  939.26/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [1000/1251 ( 80%)]  Loss:  3.411201 (3.1987)  Time: 1.097s,  933.05/s  (1.090s,  939.09/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [1050/1251 ( 84%)]  Loss:  3.477524 (3.2114)  Time: 1.098s,  932.79/s  (1.091s,  938.95/s)  LR: 2.183e-04  Data: 0.013 (0.012)
Train: 209 [1100/1251 ( 88%)]  Loss:  3.165019 (3.2094)  Time: 1.177s,  870.01/s  (1.090s,  939.12/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [1150/1251 ( 92%)]  Loss:  3.293269 (3.2129)  Time: 1.107s,  925.43/s  (1.090s,  939.44/s)  LR: 2.183e-04  Data: 0.014 (0.012)
Train: 209 [1200/1251 ( 96%)]  Loss:  3.156513 (3.2106)  Time: 1.082s,  946.68/s  (1.090s,  939.46/s)  LR: 2.183e-04  Data: 0.012 (0.012)
Train: 209 [1250/1251 (100%)]  Loss:  3.130565 (3.2075)  Time: 1.061s,  964.84/s  (1.090s,  939.37/s)  LR: 2.183e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.888 (5.888)  Loss:  0.4524 (0.4524)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5210 (0.8940)  Acc@1: 87.3821 (79.2240)  Acc@5: 98.3491 (94.8760)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-202.pth.tar', 79.04000002929688)

Train: 210 [   0/1251 (  0%)]  Loss:  3.352663 (3.3527)  Time: 1.087s,  942.01/s  (1.087s,  942.01/s)  LR: 2.140e-04  Data: 0.026 (0.026)
Train: 210 [  50/1251 (  4%)]  Loss:  3.289799 (3.3212)  Time: 1.079s,  949.10/s  (1.090s,  939.10/s)  LR: 2.140e-04  Data: 0.012 (0.014)
Train: 210 [ 100/1251 (  8%)]  Loss:  3.281111 (3.3079)  Time: 1.079s,  949.38/s  (1.089s,  940.30/s)  LR: 2.140e-04  Data: 0.016 (0.013)
Train: 210 [ 150/1251 ( 12%)]  Loss:  3.172410 (3.2740)  Time: 1.094s,  935.68/s  (1.088s,  941.18/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 200/1251 ( 16%)]  Loss:  3.191957 (3.2576)  Time: 1.170s,  875.19/s  (1.089s,  940.26/s)  LR: 2.140e-04  Data: 0.014 (0.013)
Train: 210 [ 250/1251 ( 20%)]  Loss:  3.184215 (3.2454)  Time: 1.094s,  935.62/s  (1.089s,  940.22/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 300/1251 ( 24%)]  Loss:  3.158994 (3.2330)  Time: 1.096s,  934.52/s  (1.089s,  940.36/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [ 350/1251 ( 28%)]  Loss:  3.243858 (3.2344)  Time: 1.085s,  944.05/s  (1.089s,  940.52/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [ 400/1251 ( 32%)]  Loss:  3.267771 (3.2381)  Time: 1.097s,  933.65/s  (1.089s,  939.97/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 450/1251 ( 36%)]  Loss:  3.070620 (3.2213)  Time: 1.176s,  870.80/s  (1.090s,  939.68/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 500/1251 ( 40%)]  Loss:  3.431491 (3.2404)  Time: 1.095s,  935.37/s  (1.090s,  939.52/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 550/1251 ( 44%)]  Loss:  3.302526 (3.2456)  Time: 1.076s,  951.89/s  (1.090s,  939.43/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 600/1251 ( 48%)]  Loss:  2.738985 (3.2066)  Time: 1.083s,  945.90/s  (1.089s,  939.97/s)  LR: 2.140e-04  Data: 0.013 (0.013)
Train: 210 [ 650/1251 ( 52%)]  Loss:  2.795862 (3.1773)  Time: 1.176s,  870.87/s  (1.090s,  939.71/s)  LR: 2.140e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 210 [ 700/1251 ( 56%)]  Loss:  3.198007 (3.1787)  Time: 1.077s,  950.87/s  (1.090s,  939.46/s)  LR: 2.140e-04  Data: 0.013 (0.013)
Train: 210 [ 750/1251 ( 60%)]  Loss:  3.236629 (3.1823)  Time: 1.096s,  933.98/s  (1.090s,  939.15/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 800/1251 ( 64%)]  Loss:  3.351368 (3.1923)  Time: 1.080s,  948.05/s  (1.090s,  939.27/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [ 850/1251 ( 68%)]  Loss:  3.253839 (3.1957)  Time: 1.076s,  951.75/s  (1.090s,  939.34/s)  LR: 2.140e-04  Data: 0.014 (0.013)
Train: 210 [ 900/1251 ( 72%)]  Loss:  3.151487 (3.1933)  Time: 1.075s,  952.91/s  (1.090s,  939.22/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [ 950/1251 ( 76%)]  Loss:  2.977842 (3.1826)  Time: 1.101s,  929.94/s  (1.090s,  939.45/s)  LR: 2.140e-04  Data: 0.015 (0.013)
Train: 210 [1000/1251 ( 80%)]  Loss:  3.008168 (3.1743)  Time: 1.093s,  937.29/s  (1.090s,  939.20/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [1050/1251 ( 84%)]  Loss:  3.404345 (3.1847)  Time: 1.093s,  936.71/s  (1.090s,  939.36/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [1100/1251 ( 88%)]  Loss:  2.929732 (3.1736)  Time: 1.080s,  948.32/s  (1.090s,  939.56/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1150/1251 ( 92%)]  Loss:  2.887014 (3.1617)  Time: 1.086s,  942.97/s  (1.090s,  939.66/s)  LR: 2.140e-04  Data: 0.011 (0.013)
Train: 210 [1200/1251 ( 96%)]  Loss:  3.185758 (3.1627)  Time: 1.079s,  948.76/s  (1.089s,  939.92/s)  LR: 2.140e-04  Data: 0.012 (0.013)
Train: 210 [1250/1251 (100%)]  Loss:  3.140279 (3.1618)  Time: 1.063s,  963.24/s  (1.089s,  939.93/s)  LR: 2.140e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.784 (5.784)  Loss:  0.4504 (0.4504)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5665 (0.8963)  Acc@1: 87.1462 (79.3480)  Acc@5: 97.7594 (94.9040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-197.pth.tar', 79.04400005126953)

Train: 211 [   0/1251 (  0%)]  Loss:  3.067162 (3.0672)  Time: 1.084s,  944.41/s  (1.084s,  944.41/s)  LR: 2.099e-04  Data: 0.022 (0.022)
Train: 211 [  50/1251 (  4%)]  Loss:  2.913633 (2.9904)  Time: 1.093s,  937.26/s  (1.087s,  941.84/s)  LR: 2.099e-04  Data: 0.015 (0.013)
Train: 211 [ 100/1251 (  8%)]  Loss:  3.425433 (3.1354)  Time: 1.107s,  925.19/s  (1.089s,  940.44/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 150/1251 ( 12%)]  Loss:  3.082969 (3.1223)  Time: 1.095s,  934.78/s  (1.089s,  940.12/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 200/1251 ( 16%)]  Loss:  3.179478 (3.1337)  Time: 1.104s,  927.15/s  (1.088s,  941.24/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 250/1251 ( 20%)]  Loss:  3.192030 (3.1435)  Time: 1.098s,  932.82/s  (1.090s,  939.66/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [ 300/1251 ( 24%)]  Loss:  3.056448 (3.1310)  Time: 1.095s,  934.91/s  (1.091s,  938.80/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 350/1251 ( 28%)]  Loss:  3.256423 (3.1467)  Time: 1.077s,  951.05/s  (1.089s,  940.16/s)  LR: 2.099e-04  Data: 0.014 (0.013)
Train: 211 [ 400/1251 ( 32%)]  Loss:  3.286950 (3.1623)  Time: 1.101s,  929.83/s  (1.089s,  939.92/s)  LR: 2.099e-04  Data: 0.014 (0.013)
Train: 211 [ 450/1251 ( 36%)]  Loss:  3.232650 (3.1693)  Time: 1.076s,  952.10/s  (1.089s,  939.93/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 500/1251 ( 40%)]  Loss:  3.119365 (3.1648)  Time: 1.078s,  950.07/s  (1.089s,  940.18/s)  LR: 2.099e-04  Data: 0.013 (0.013)
Train: 211 [ 550/1251 ( 44%)]  Loss:  3.381574 (3.1828)  Time: 1.078s,  949.87/s  (1.089s,  939.93/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 600/1251 ( 48%)]  Loss:  3.210347 (3.1850)  Time: 1.098s,  932.67/s  (1.090s,  939.78/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 650/1251 ( 52%)]  Loss:  3.385133 (3.1993)  Time: 1.097s,  933.64/s  (1.089s,  939.92/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 700/1251 ( 56%)]  Loss:  2.894460 (3.1789)  Time: 1.095s,  935.48/s  (1.089s,  940.31/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 750/1251 ( 60%)]  Loss:  3.283848 (3.1855)  Time: 1.108s,  924.19/s  (1.089s,  940.21/s)  LR: 2.099e-04  Data: 0.013 (0.013)
Train: 211 [ 800/1251 ( 64%)]  Loss:  3.362884 (3.1959)  Time: 1.080s,  947.85/s  (1.089s,  940.06/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 850/1251 ( 68%)]  Loss:  3.343781 (3.2041)  Time: 1.084s,  944.81/s  (1.089s,  940.10/s)  LR: 2.099e-04  Data: 0.013 (0.013)
Train: 211 [ 900/1251 ( 72%)]  Loss:  2.978358 (3.1923)  Time: 1.077s,  950.90/s  (1.089s,  940.18/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [ 950/1251 ( 76%)]  Loss:  3.434899 (3.2044)  Time: 1.078s,  950.26/s  (1.089s,  939.95/s)  LR: 2.099e-04  Data: 0.016 (0.013)
Train: 211 [1000/1251 ( 80%)]  Loss:  2.891937 (3.1895)  Time: 1.095s,  935.58/s  (1.089s,  940.18/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [1050/1251 ( 84%)]  Loss:  3.119451 (3.1863)  Time: 1.080s,  948.51/s  (1.089s,  940.06/s)  LR: 2.099e-04  Data: 0.011 (0.013)
Train: 211 [1100/1251 ( 88%)]  Loss:  2.936518 (3.1755)  Time: 1.079s,  948.99/s  (1.089s,  940.15/s)  LR: 2.099e-04  Data: 0.014 (0.013)
Train: 211 [1150/1251 ( 92%)]  Loss:  2.931581 (3.1653)  Time: 1.104s,  927.16/s  (1.089s,  940.11/s)  LR: 2.099e-04  Data: 0.013 (0.013)
Train: 211 [1200/1251 ( 96%)]  Loss:  3.319830 (3.1715)  Time: 1.075s,  952.84/s  (1.089s,  940.03/s)  LR: 2.099e-04  Data: 0.012 (0.013)
Train: 211 [1250/1251 (100%)]  Loss:  3.249213 (3.1745)  Time: 1.080s,  947.74/s  (1.089s,  940.18/s)  LR: 2.099e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.838 (5.838)  Loss:  0.4526 (0.4526)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5741 (0.8885)  Acc@1: 86.9104 (79.4100)  Acc@5: 97.2877 (95.0400)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-201.pth.tar', 79.04800005371094)

Train: 212 [   0/1251 (  0%)]  Loss:  3.183752 (3.1838)  Time: 1.085s,  944.15/s  (1.085s,  944.15/s)  LR: 2.057e-04  Data: 0.023 (0.023)
Train: 212 [  50/1251 (  4%)]  Loss:  3.219363 (3.2016)  Time: 1.095s,  935.28/s  (1.090s,  939.81/s)  LR: 2.057e-04  Data: 0.012 (0.012)
Train: 212 [ 100/1251 (  8%)]  Loss:  3.239888 (3.2143)  Time: 1.076s,  951.32/s  (1.089s,  939.98/s)  LR: 2.057e-04  Data: 0.012 (0.012)
Train: 212 [ 150/1251 ( 12%)]  Loss:  2.990128 (3.1583)  Time: 1.077s,  951.13/s  (1.089s,  939.99/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 212 [ 200/1251 ( 16%)]  Loss:  3.362410 (3.1991)  Time: 1.080s,  947.71/s  (1.089s,  940.38/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [ 250/1251 ( 20%)]  Loss:  3.186795 (3.1971)  Time: 1.078s,  949.53/s  (1.088s,  940.89/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 300/1251 ( 24%)]  Loss:  3.048836 (3.1759)  Time: 1.083s,  945.82/s  (1.087s,  941.73/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 350/1251 ( 28%)]  Loss:  2.918307 (3.1437)  Time: 1.082s,  946.75/s  (1.088s,  941.48/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 400/1251 ( 32%)]  Loss:  3.155889 (3.1450)  Time: 1.078s,  949.78/s  (1.087s,  942.37/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [ 450/1251 ( 36%)]  Loss:  3.321579 (3.1627)  Time: 1.077s,  950.43/s  (1.087s,  942.26/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [ 500/1251 ( 40%)]  Loss:  2.990065 (3.1470)  Time: 1.077s,  950.51/s  (1.087s,  942.38/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [ 550/1251 ( 44%)]  Loss:  3.092369 (3.1424)  Time: 1.093s,  936.61/s  (1.087s,  942.31/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 600/1251 ( 48%)]  Loss:  3.174138 (3.1449)  Time: 1.094s,  936.13/s  (1.087s,  942.06/s)  LR: 2.057e-04  Data: 0.011 (0.013)
Train: 212 [ 650/1251 ( 52%)]  Loss:  2.860513 (3.1246)  Time: 1.094s,  935.88/s  (1.087s,  941.97/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [ 700/1251 ( 56%)]  Loss:  3.026914 (3.1181)  Time: 1.076s,  951.81/s  (1.087s,  942.01/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [ 750/1251 ( 60%)]  Loss:  3.058660 (3.1144)  Time: 1.108s,  923.81/s  (1.087s,  942.07/s)  LR: 2.057e-04  Data: 0.014 (0.013)
Train: 212 [ 800/1251 ( 64%)]  Loss:  3.238420 (3.1216)  Time: 1.093s,  936.95/s  (1.087s,  941.80/s)  LR: 2.057e-04  Data: 0.013 (0.013)
Train: 212 [ 850/1251 ( 68%)]  Loss:  2.817272 (3.1047)  Time: 1.083s,  945.51/s  (1.087s,  941.65/s)  LR: 2.057e-04  Data: 0.015 (0.013)
Train: 212 [ 900/1251 ( 72%)]  Loss:  3.051856 (3.1020)  Time: 1.171s,  874.62/s  (1.088s,  941.56/s)  LR: 2.057e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 212 [ 950/1251 ( 76%)]  Loss:  3.079278 (3.1008)  Time: 1.098s,  932.54/s  (1.088s,  941.60/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [1000/1251 ( 80%)]  Loss:  3.188801 (3.1050)  Time: 1.099s,  932.11/s  (1.088s,  941.52/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [1050/1251 ( 84%)]  Loss:  3.150124 (3.1071)  Time: 1.084s,  944.87/s  (1.088s,  941.50/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [1100/1251 ( 88%)]  Loss:  3.127759 (3.1080)  Time: 1.079s,  948.89/s  (1.087s,  941.63/s)  LR: 2.057e-04  Data: 0.014 (0.013)
Train: 212 [1150/1251 ( 92%)]  Loss:  3.046253 (3.1054)  Time: 1.075s,  952.17/s  (1.088s,  941.37/s)  LR: 2.057e-04  Data: 0.012 (0.013)
Train: 212 [1200/1251 ( 96%)]  Loss:  2.665207 (3.0878)  Time: 1.101s,  930.27/s  (1.088s,  941.35/s)  LR: 2.057e-04  Data: 0.013 (0.012)
Train: 212 [1250/1251 (100%)]  Loss:  2.938361 (3.0820)  Time: 1.068s,  958.39/s  (1.088s,  941.41/s)  LR: 2.057e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.868 (5.868)  Loss:  0.4539 (0.4539)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5310 (0.8990)  Acc@1: 87.8538 (79.4980)  Acc@5: 98.2311 (95.0560)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-203.pth.tar', 79.13600010498047)

Train: 213 [   0/1251 (  0%)]  Loss:  3.363368 (3.3634)  Time: 1.109s,  923.56/s  (1.109s,  923.56/s)  LR: 2.016e-04  Data: 0.024 (0.024)
Train: 213 [  50/1251 (  4%)]  Loss:  3.173804 (3.2686)  Time: 1.080s,  947.92/s  (1.087s,  941.88/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 100/1251 (  8%)]  Loss:  3.226465 (3.2545)  Time: 1.077s,  950.74/s  (1.088s,  941.35/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 150/1251 ( 12%)]  Loss:  3.324147 (3.2719)  Time: 1.101s,  930.19/s  (1.089s,  940.55/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 200/1251 ( 16%)]  Loss:  3.056081 (3.2288)  Time: 1.080s,  948.01/s  (1.089s,  940.07/s)  LR: 2.016e-04  Data: 0.015 (0.013)
Train: 213 [ 250/1251 ( 20%)]  Loss:  3.121875 (3.2110)  Time: 1.082s,  945.98/s  (1.090s,  939.25/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 300/1251 ( 24%)]  Loss:  2.915354 (3.1687)  Time: 1.098s,  932.53/s  (1.089s,  940.07/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 350/1251 ( 28%)]  Loss:  3.115373 (3.1621)  Time: 1.079s,  949.09/s  (1.089s,  940.02/s)  LR: 2.016e-04  Data: 0.013 (0.013)
Train: 213 [ 400/1251 ( 32%)]  Loss:  2.796902 (3.1215)  Time: 1.079s,  949.27/s  (1.089s,  940.17/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 450/1251 ( 36%)]  Loss:  3.030965 (3.1124)  Time: 1.106s,  925.84/s  (1.089s,  940.17/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 500/1251 ( 40%)]  Loss:  3.216835 (3.1219)  Time: 1.096s,  934.24/s  (1.089s,  939.92/s)  LR: 2.016e-04  Data: 0.015 (0.013)
Train: 213 [ 550/1251 ( 44%)]  Loss:  2.892365 (3.1028)  Time: 1.097s,  933.57/s  (1.090s,  939.85/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 600/1251 ( 48%)]  Loss:  3.275458 (3.1161)  Time: 1.108s,  924.35/s  (1.090s,  939.83/s)  LR: 2.016e-04  Data: 0.013 (0.013)
Train: 213 [ 650/1251 ( 52%)]  Loss:  3.087873 (3.1141)  Time: 1.095s,  935.43/s  (1.090s,  939.82/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 700/1251 ( 56%)]  Loss:  3.071769 (3.1112)  Time: 1.095s,  935.30/s  (1.090s,  939.63/s)  LR: 2.016e-04  Data: 0.013 (0.013)
Train: 213 [ 750/1251 ( 60%)]  Loss:  2.921730 (3.0994)  Time: 1.077s,  951.10/s  (1.090s,  939.37/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 800/1251 ( 64%)]  Loss:  2.967966 (3.0917)  Time: 1.103s,  928.27/s  (1.090s,  939.67/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 850/1251 ( 68%)]  Loss:  3.352016 (3.1061)  Time: 1.079s,  948.66/s  (1.090s,  939.47/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [ 900/1251 ( 72%)]  Loss:  3.316054 (3.1172)  Time: 1.085s,  944.19/s  (1.090s,  939.43/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [ 950/1251 ( 76%)]  Loss:  3.027877 (3.1127)  Time: 1.075s,  952.33/s  (1.090s,  939.58/s)  LR: 2.016e-04  Data: 0.011 (0.013)
Train: 213 [1000/1251 ( 80%)]  Loss:  3.017227 (3.1082)  Time: 1.079s,  948.70/s  (1.090s,  939.63/s)  LR: 2.016e-04  Data: 0.014 (0.013)
Train: 213 [1050/1251 ( 84%)]  Loss:  2.999469 (3.1032)  Time: 1.206s,  848.89/s  (1.090s,  939.80/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [1100/1251 ( 88%)]  Loss:  3.059115 (3.1013)  Time: 1.095s,  935.41/s  (1.090s,  939.66/s)  LR: 2.016e-04  Data: 0.013 (0.013)
Train: 213 [1150/1251 ( 92%)]  Loss:  3.066829 (3.0999)  Time: 1.094s,  936.03/s  (1.090s,  939.47/s)  LR: 2.016e-04  Data: 0.014 (0.013)
Train: 213 [1200/1251 ( 96%)]  Loss:  3.119473 (3.1007)  Time: 1.079s,  948.77/s  (1.090s,  939.44/s)  LR: 2.016e-04  Data: 0.012 (0.013)
Train: 213 [1250/1251 (100%)]  Loss:  3.281458 (3.1076)  Time: 1.079s,  948.97/s  (1.090s,  939.34/s)  LR: 2.016e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.920 (5.920)  Loss:  0.4674 (0.4674)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5716 (0.8867)  Acc@1: 86.9104 (79.5540)  Acc@5: 97.5236 (95.0000)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-205.pth.tar', 79.18999994873047)

Train: 214 [   0/1251 (  0%)]  Loss:  2.947342 (2.9473)  Time: 1.088s,  941.21/s  (1.088s,  941.21/s)  LR: 1.975e-04  Data: 0.026 (0.026)
Train: 214 [  50/1251 (  4%)]  Loss:  3.154577 (3.0510)  Time: 1.075s,  952.73/s  (1.082s,  946.56/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 100/1251 (  8%)]  Loss:  3.246936 (3.1163)  Time: 1.077s,  950.55/s  (1.083s,  945.55/s)  LR: 1.975e-04  Data: 0.015 (0.013)
Train: 214 [ 150/1251 ( 12%)]  Loss:  3.229596 (3.1446)  Time: 1.081s,  947.54/s  (1.085s,  943.37/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 200/1251 ( 16%)]  Loss:  3.104139 (3.1365)  Time: 1.097s,  933.08/s  (1.085s,  943.48/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 250/1251 ( 20%)]  Loss:  3.190802 (3.1456)  Time: 1.078s,  949.99/s  (1.087s,  942.47/s)  LR: 1.975e-04  Data: 0.013 (0.013)
Train: 214 [ 300/1251 ( 24%)]  Loss:  2.703383 (3.0824)  Time: 1.077s,  950.44/s  (1.087s,  942.34/s)  LR: 1.975e-04  Data: 0.013 (0.013)
Train: 214 [ 350/1251 ( 28%)]  Loss:  3.119127 (3.0870)  Time: 1.081s,  947.12/s  (1.087s,  942.36/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 400/1251 ( 32%)]  Loss:  3.151460 (3.0942)  Time: 1.094s,  935.98/s  (1.088s,  941.32/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 450/1251 ( 36%)]  Loss:  3.070022 (3.0917)  Time: 1.104s,  927.33/s  (1.088s,  940.84/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 500/1251 ( 40%)]  Loss:  3.133391 (3.0955)  Time: 1.077s,  950.81/s  (1.088s,  941.44/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 550/1251 ( 44%)]  Loss:  2.909640 (3.0800)  Time: 1.110s,  922.40/s  (1.087s,  941.72/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 600/1251 ( 48%)]  Loss:  2.945172 (3.0697)  Time: 1.085s,  943.65/s  (1.088s,  941.15/s)  LR: 1.975e-04  Data: 0.014 (0.013)
Train: 214 [ 650/1251 ( 52%)]  Loss:  3.322409 (3.0877)  Time: 1.078s,  949.71/s  (1.087s,  941.65/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 700/1251 ( 56%)]  Loss:  2.863773 (3.0728)  Time: 1.077s,  951.00/s  (1.088s,  941.57/s)  LR: 1.975e-04  Data: 0.015 (0.013)
Train: 214 [ 750/1251 ( 60%)]  Loss:  3.095954 (3.0742)  Time: 1.096s,  934.73/s  (1.088s,  941.42/s)  LR: 1.975e-04  Data: 0.013 (0.013)
Train: 214 [ 800/1251 ( 64%)]  Loss:  2.904773 (3.0643)  Time: 1.075s,  952.30/s  (1.088s,  941.39/s)  LR: 1.975e-04  Data: 0.011 (0.013)
Train: 214 [ 850/1251 ( 68%)]  Loss:  3.092945 (3.0659)  Time: 1.094s,  935.81/s  (1.088s,  941.31/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 900/1251 ( 72%)]  Loss:  3.005910 (3.0627)  Time: 1.094s,  935.80/s  (1.088s,  941.12/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [ 950/1251 ( 76%)]  Loss:  3.145922 (3.0669)  Time: 1.080s,  948.03/s  (1.089s,  940.69/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [1000/1251 ( 80%)]  Loss:  3.429520 (3.0841)  Time: 1.094s,  935.68/s  (1.089s,  940.36/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [1050/1251 ( 84%)]  Loss:  2.815586 (3.0719)  Time: 1.077s,  950.79/s  (1.089s,  940.53/s)  LR: 1.975e-04  Data: 0.014 (0.013)
Train: 214 [1100/1251 ( 88%)]  Loss:  3.266810 (3.0804)  Time: 1.176s,  870.90/s  (1.089s,  940.48/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [1150/1251 ( 92%)]  Loss:  2.923354 (3.0739)  Time: 1.077s,  950.66/s  (1.089s,  940.64/s)  LR: 1.975e-04  Data: 0.015 (0.013)
Train: 214 [1200/1251 ( 96%)]  Loss:  3.039618 (3.0725)  Time: 1.077s,  950.78/s  (1.088s,  940.97/s)  LR: 1.975e-04  Data: 0.012 (0.013)
Train: 214 [1250/1251 (100%)]  Loss:  3.129644 (3.0747)  Time: 1.063s,  963.32/s  (1.088s,  941.04/s)  LR: 1.975e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.867 (5.867)  Loss:  0.4519 (0.4519)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5771 (0.9069)  Acc@1: 87.5000 (79.6540)  Acc@5: 97.7594 (95.1080)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-207.pth.tar', 79.20199997314454)

Train: 215 [   0/1251 (  0%)]  Loss:  3.139284 (3.1393)  Time: 1.085s,  943.77/s  (1.085s,  943.77/s)  LR: 1.935e-04  Data: 0.023 (0.023)
Train: 215 [  50/1251 (  4%)]  Loss:  2.940280 (3.0398)  Time: 1.103s,  928.80/s  (1.089s,  940.38/s)  LR: 1.935e-04  Data: 0.013 (0.013)
Train: 215 [ 100/1251 (  8%)]  Loss:  3.075608 (3.0517)  Time: 1.102s,  928.83/s  (1.089s,  940.22/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 150/1251 ( 12%)]  Loss:  3.167888 (3.0808)  Time: 1.082s,  946.80/s  (1.089s,  940.24/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 200/1251 ( 16%)]  Loss:  3.293142 (3.1232)  Time: 1.095s,  934.77/s  (1.091s,  938.36/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 250/1251 ( 20%)]  Loss:  3.224273 (3.1401)  Time: 1.096s,  934.40/s  (1.092s,  937.72/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 300/1251 ( 24%)]  Loss:  3.202492 (3.1490)  Time: 1.074s,  953.80/s  (1.091s,  938.77/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 350/1251 ( 28%)]  Loss:  3.042439 (3.1357)  Time: 1.095s,  934.96/s  (1.090s,  939.32/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 400/1251 ( 32%)]  Loss:  2.696606 (3.0869)  Time: 1.094s,  936.22/s  (1.091s,  938.79/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 450/1251 ( 36%)]  Loss:  2.943927 (3.0726)  Time: 1.094s,  936.02/s  (1.090s,  939.29/s)  LR: 1.935e-04  Data: 0.014 (0.013)
Train: 215 [ 500/1251 ( 40%)]  Loss:  3.130675 (3.0779)  Time: 1.083s,  945.90/s  (1.090s,  939.60/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 550/1251 ( 44%)]  Loss:  3.104295 (3.0801)  Time: 1.095s,  935.13/s  (1.090s,  939.65/s)  LR: 1.935e-04  Data: 0.014 (0.013)
Train: 215 [ 600/1251 ( 48%)]  Loss:  3.245631 (3.0928)  Time: 1.093s,  936.82/s  (1.089s,  940.02/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 650/1251 ( 52%)]  Loss:  3.198815 (3.1004)  Time: 1.098s,  932.95/s  (1.089s,  940.24/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 700/1251 ( 56%)]  Loss:  2.712142 (3.0745)  Time: 1.094s,  935.95/s  (1.089s,  940.03/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 750/1251 ( 60%)]  Loss:  3.425732 (3.0965)  Time: 1.077s,  950.97/s  (1.089s,  940.05/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 800/1251 ( 64%)]  Loss:  3.120991 (3.0979)  Time: 1.084s,  944.25/s  (1.089s,  940.49/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [ 850/1251 ( 68%)]  Loss:  2.979529 (3.0913)  Time: 1.077s,  950.47/s  (1.089s,  940.57/s)  LR: 1.935e-04  Data: 0.014 (0.013)
Train: 215 [ 900/1251 ( 72%)]  Loss:  3.178177 (3.0959)  Time: 1.094s,  936.01/s  (1.089s,  940.01/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [ 950/1251 ( 76%)]  Loss:  3.311040 (3.1066)  Time: 1.076s,  951.39/s  (1.089s,  939.89/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [1000/1251 ( 80%)]  Loss:  3.057096 (3.1043)  Time: 1.077s,  951.06/s  (1.089s,  940.04/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [1050/1251 ( 84%)]  Loss:  3.087101 (3.1035)  Time: 1.173s,  873.12/s  (1.090s,  939.84/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Train: 215 [1100/1251 ( 88%)]  Loss:  3.370069 (3.1151)  Time: 1.083s,  945.94/s  (1.089s,  940.05/s)  LR: 1.935e-04  Data: 0.011 (0.013)
Train: 215 [1150/1251 ( 92%)]  Loss:  3.092529 (3.1142)  Time: 1.079s,  949.17/s  (1.089s,  940.26/s)  LR: 1.935e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 215 [1200/1251 ( 96%)]  Loss:  2.919322 (3.1064)  Time: 1.106s,  925.70/s  (1.089s,  940.33/s)  LR: 1.935e-04  Data: 0.014 (0.013)
Train: 215 [1250/1251 (100%)]  Loss:  3.220854 (3.1108)  Time: 1.063s,  963.59/s  (1.089s,  940.06/s)  LR: 1.935e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.901 (5.901)  Loss:  0.4696 (0.4696)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5543 (0.8934)  Acc@1: 87.1462 (79.7160)  Acc@5: 97.6415 (95.1240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-209.pth.tar', 79.224000078125)

Train: 216 [   0/1251 (  0%)]  Loss:  3.071462 (3.0715)  Time: 1.085s,  943.56/s  (1.085s,  943.56/s)  LR: 1.895e-04  Data: 0.024 (0.024)
Train: 216 [  50/1251 (  4%)]  Loss:  2.937408 (3.0044)  Time: 1.073s,  954.22/s  (1.085s,  943.64/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [ 100/1251 (  8%)]  Loss:  3.211370 (3.0734)  Time: 1.077s,  950.38/s  (1.083s,  945.85/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 150/1251 ( 12%)]  Loss:  3.222233 (3.1106)  Time: 1.078s,  949.75/s  (1.085s,  943.92/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [ 200/1251 ( 16%)]  Loss:  2.753657 (3.0392)  Time: 1.077s,  950.96/s  (1.086s,  943.15/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 250/1251 ( 20%)]  Loss:  3.106903 (3.0505)  Time: 1.089s,  940.71/s  (1.087s,  941.88/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 300/1251 ( 24%)]  Loss:  3.080559 (3.0548)  Time: 1.078s,  950.04/s  (1.088s,  941.40/s)  LR: 1.895e-04  Data: 0.015 (0.013)
Train: 216 [ 350/1251 ( 28%)]  Loss:  3.335547 (3.0899)  Time: 1.080s,  947.78/s  (1.089s,  940.69/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 400/1251 ( 32%)]  Loss:  3.025947 (3.0828)  Time: 1.098s,  932.55/s  (1.089s,  940.19/s)  LR: 1.895e-04  Data: 0.012 (0.012)
Train: 216 [ 450/1251 ( 36%)]  Loss:  2.863570 (3.0609)  Time: 1.098s,  932.20/s  (1.090s,  939.88/s)  LR: 1.895e-04  Data: 0.012 (0.012)
Train: 216 [ 500/1251 ( 40%)]  Loss:  2.945531 (3.0504)  Time: 1.078s,  950.30/s  (1.090s,  939.26/s)  LR: 1.895e-04  Data: 0.014 (0.012)
Train: 216 [ 550/1251 ( 44%)]  Loss:  3.244206 (3.0665)  Time: 1.096s,  934.60/s  (1.091s,  938.80/s)  LR: 1.895e-04  Data: 0.013 (0.012)
Train: 216 [ 600/1251 ( 48%)]  Loss:  3.017491 (3.0628)  Time: 1.077s,  950.36/s  (1.091s,  938.74/s)  LR: 1.895e-04  Data: 0.013 (0.013)
Train: 216 [ 650/1251 ( 52%)]  Loss:  3.183092 (3.0714)  Time: 1.082s,  946.01/s  (1.091s,  938.60/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 700/1251 ( 56%)]  Loss:  2.875414 (3.0583)  Time: 1.097s,  933.80/s  (1.091s,  938.75/s)  LR: 1.895e-04  Data: 0.012 (0.012)
Train: 216 [ 750/1251 ( 60%)]  Loss:  2.927900 (3.0501)  Time: 1.086s,  943.00/s  (1.091s,  938.49/s)  LR: 1.895e-04  Data: 0.023 (0.013)
Train: 216 [ 800/1251 ( 64%)]  Loss:  2.819405 (3.0366)  Time: 1.082s,  946.82/s  (1.091s,  938.86/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 850/1251 ( 68%)]  Loss:  3.061954 (3.0380)  Time: 1.096s,  933.96/s  (1.091s,  938.85/s)  LR: 1.895e-04  Data: 0.011 (0.013)
Train: 216 [ 900/1251 ( 72%)]  Loss:  2.996133 (3.0358)  Time: 1.079s,  949.07/s  (1.091s,  938.84/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [ 950/1251 ( 76%)]  Loss:  3.316736 (3.0498)  Time: 1.096s,  934.70/s  (1.091s,  939.00/s)  LR: 1.895e-04  Data: 0.013 (0.013)
Train: 216 [1000/1251 ( 80%)]  Loss:  2.935369 (3.0444)  Time: 1.093s,  936.59/s  (1.090s,  939.11/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [1050/1251 ( 84%)]  Loss:  2.731416 (3.0302)  Time: 1.079s,  948.92/s  (1.091s,  938.94/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [1100/1251 ( 88%)]  Loss:  3.105179 (3.0334)  Time: 1.095s,  935.18/s  (1.091s,  938.98/s)  LR: 1.895e-04  Data: 0.014 (0.013)
Train: 216 [1150/1251 ( 92%)]  Loss:  3.125746 (3.0373)  Time: 1.081s,  947.21/s  (1.090s,  939.09/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [1200/1251 ( 96%)]  Loss:  3.139613 (3.0414)  Time: 1.078s,  949.57/s  (1.090s,  939.14/s)  LR: 1.895e-04  Data: 0.012 (0.013)
Train: 216 [1250/1251 (100%)]  Loss:  3.394160 (3.0549)  Time: 1.062s,  964.01/s  (1.090s,  939.14/s)  LR: 1.895e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.796 (5.796)  Loss:  0.4658 (0.4658)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5973 (0.9030)  Acc@1: 85.9670 (79.5320)  Acc@5: 97.9953 (95.0040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-206.pth.tar', 79.24399994873046)

Train: 217 [   0/1251 (  0%)]  Loss:  3.204399 (3.2044)  Time: 1.086s,  943.23/s  (1.086s,  943.23/s)  LR: 1.855e-04  Data: 0.023 (0.023)
Train: 217 [  50/1251 (  4%)]  Loss:  3.444517 (3.3245)  Time: 1.094s,  936.39/s  (1.080s,  948.25/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 100/1251 (  8%)]  Loss:  3.288949 (3.3126)  Time: 1.085s,  943.64/s  (1.083s,  945.61/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 150/1251 ( 12%)]  Loss:  3.204918 (3.2857)  Time: 1.077s,  951.10/s  (1.086s,  942.72/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 200/1251 ( 16%)]  Loss:  3.144859 (3.2575)  Time: 1.077s,  950.82/s  (1.086s,  942.49/s)  LR: 1.855e-04  Data: 0.015 (0.013)
Train: 217 [ 250/1251 ( 20%)]  Loss:  3.215058 (3.2505)  Time: 1.084s,  944.26/s  (1.087s,  942.04/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [ 300/1251 ( 24%)]  Loss:  3.151214 (3.2363)  Time: 1.101s,  929.69/s  (1.088s,  941.48/s)  LR: 1.855e-04  Data: 0.013 (0.013)
Train: 217 [ 350/1251 ( 28%)]  Loss:  3.199397 (3.2317)  Time: 1.082s,  946.22/s  (1.088s,  941.25/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 400/1251 ( 32%)]  Loss:  2.779326 (3.1814)  Time: 1.100s,  931.24/s  (1.088s,  941.52/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [ 450/1251 ( 36%)]  Loss:  3.139710 (3.1772)  Time: 1.081s,  946.95/s  (1.089s,  940.72/s)  LR: 1.855e-04  Data: 0.014 (0.013)
Train: 217 [ 500/1251 ( 40%)]  Loss:  3.071237 (3.1676)  Time: 1.093s,  936.78/s  (1.089s,  940.25/s)  LR: 1.855e-04  Data: 0.015 (0.013)
Train: 217 [ 550/1251 ( 44%)]  Loss:  3.296519 (3.1783)  Time: 1.095s,  935.50/s  (1.090s,  939.81/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 600/1251 ( 48%)]  Loss:  2.962247 (3.1617)  Time: 1.174s,  872.14/s  (1.090s,  939.77/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 650/1251 ( 52%)]  Loss:  3.276578 (3.1699)  Time: 1.077s,  951.19/s  (1.090s,  939.74/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 700/1251 ( 56%)]  Loss:  3.359923 (3.1826)  Time: 1.105s,  927.07/s  (1.090s,  939.28/s)  LR: 1.855e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 217 [ 750/1251 ( 60%)]  Loss:  3.329637 (3.1918)  Time: 1.095s,  935.24/s  (1.090s,  939.61/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 800/1251 ( 64%)]  Loss:  2.843298 (3.1713)  Time: 1.082s,  946.08/s  (1.090s,  939.79/s)  LR: 1.855e-04  Data: 0.011 (0.013)
Train: 217 [ 850/1251 ( 68%)]  Loss:  3.022478 (3.1630)  Time: 1.076s,  951.26/s  (1.090s,  939.80/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 900/1251 ( 72%)]  Loss:  2.844747 (3.1463)  Time: 1.078s,  949.50/s  (1.089s,  940.25/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [ 950/1251 ( 76%)]  Loss:  3.231461 (3.1505)  Time: 1.184s,  864.61/s  (1.089s,  940.11/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1000/1251 ( 80%)]  Loss:  3.134845 (3.1498)  Time: 1.077s,  951.21/s  (1.089s,  940.13/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1050/1251 ( 84%)]  Loss:  3.056140 (3.1455)  Time: 1.087s,  942.19/s  (1.089s,  940.52/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1100/1251 ( 88%)]  Loss:  3.380383 (3.1557)  Time: 1.103s,  928.10/s  (1.089s,  940.36/s)  LR: 1.855e-04  Data: 0.014 (0.013)
Train: 217 [1150/1251 ( 92%)]  Loss:  3.034403 (3.1507)  Time: 1.076s,  951.27/s  (1.089s,  940.31/s)  LR: 1.855e-04  Data: 0.012 (0.013)
Train: 217 [1200/1251 ( 96%)]  Loss:  3.385652 (3.1601)  Time: 1.105s,  926.47/s  (1.089s,  940.09/s)  LR: 1.855e-04  Data: 0.018 (0.013)
Train: 217 [1250/1251 (100%)]  Loss:  3.273704 (3.1644)  Time: 1.078s,  949.64/s  (1.089s,  940.12/s)  LR: 1.855e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.742 (5.742)  Loss:  0.4324 (0.4324)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.447)  Loss:  0.5552 (0.8783)  Acc@1: 86.4387 (79.9300)  Acc@5: 97.7594 (95.0620)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-208.pth.tar', 79.26200005371093)

Train: 218 [   0/1251 (  0%)]  Loss:  3.159146 (3.1591)  Time: 1.097s,  933.63/s  (1.097s,  933.63/s)  LR: 1.816e-04  Data: 0.028 (0.028)
Train: 218 [  50/1251 (  4%)]  Loss:  2.831248 (2.9952)  Time: 1.078s,  949.92/s  (1.099s,  931.92/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 100/1251 (  8%)]  Loss:  3.294218 (3.0949)  Time: 1.078s,  950.16/s  (1.094s,  936.18/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 150/1251 ( 12%)]  Loss:  3.043319 (3.0820)  Time: 1.094s,  935.82/s  (1.092s,  937.96/s)  LR: 1.816e-04  Data: 0.011 (0.013)
Train: 218 [ 200/1251 ( 16%)]  Loss:  3.220501 (3.1097)  Time: 1.079s,  949.25/s  (1.089s,  940.06/s)  LR: 1.816e-04  Data: 0.013 (0.013)
Train: 218 [ 250/1251 ( 20%)]  Loss:  3.065582 (3.1023)  Time: 1.077s,  950.95/s  (1.089s,  939.98/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 300/1251 ( 24%)]  Loss:  3.300586 (3.1307)  Time: 1.095s,  934.77/s  (1.090s,  939.80/s)  LR: 1.816e-04  Data: 0.015 (0.013)
Train: 218 [ 350/1251 ( 28%)]  Loss:  3.305550 (3.1525)  Time: 1.078s,  949.82/s  (1.089s,  940.54/s)  LR: 1.816e-04  Data: 0.016 (0.013)
Train: 218 [ 400/1251 ( 32%)]  Loss:  3.181714 (3.1558)  Time: 1.104s,  927.37/s  (1.088s,  941.00/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 218 [ 450/1251 ( 36%)]  Loss:  3.040485 (3.1442)  Time: 1.078s,  949.80/s  (1.088s,  941.31/s)  LR: 1.816e-04  Data: 0.013 (0.013)
Train: 218 [ 500/1251 ( 40%)]  Loss:  3.265314 (3.1552)  Time: 1.077s,  951.18/s  (1.087s,  941.64/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 550/1251 ( 44%)]  Loss:  3.032840 (3.1450)  Time: 1.077s,  950.71/s  (1.087s,  942.10/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 600/1251 ( 48%)]  Loss:  3.224697 (3.1512)  Time: 1.096s,  934.46/s  (1.087s,  942.29/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 650/1251 ( 52%)]  Loss:  3.199650 (3.1546)  Time: 1.094s,  936.05/s  (1.087s,  941.80/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 700/1251 ( 56%)]  Loss:  3.047392 (3.1475)  Time: 1.094s,  935.64/s  (1.087s,  941.88/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 750/1251 ( 60%)]  Loss:  3.212145 (3.1515)  Time: 1.078s,  949.82/s  (1.087s,  941.83/s)  LR: 1.816e-04  Data: 0.013 (0.013)
Train: 218 [ 800/1251 ( 64%)]  Loss:  3.191273 (3.1539)  Time: 1.096s,  934.60/s  (1.088s,  941.44/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 850/1251 ( 68%)]  Loss:  2.949954 (3.1425)  Time: 1.079s,  948.71/s  (1.088s,  941.24/s)  LR: 1.816e-04  Data: 0.014 (0.013)
Train: 218 [ 900/1251 ( 72%)]  Loss:  2.602707 (3.1141)  Time: 1.105s,  926.63/s  (1.088s,  941.04/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [ 950/1251 ( 76%)]  Loss:  2.835963 (3.1002)  Time: 1.079s,  949.25/s  (1.088s,  941.02/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [1000/1251 ( 80%)]  Loss:  3.393435 (3.1142)  Time: 1.076s,  951.71/s  (1.088s,  940.89/s)  LR: 1.816e-04  Data: 0.014 (0.013)
Train: 218 [1050/1251 ( 84%)]  Loss:  2.947874 (3.1066)  Time: 1.094s,  936.37/s  (1.089s,  940.71/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [1100/1251 ( 88%)]  Loss:  2.949204 (3.0998)  Time: 1.094s,  936.17/s  (1.089s,  940.36/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [1150/1251 ( 92%)]  Loss:  3.372520 (3.1111)  Time: 1.079s,  949.19/s  (1.089s,  940.21/s)  LR: 1.816e-04  Data: 0.014 (0.013)
Train: 218 [1200/1251 ( 96%)]  Loss:  3.145153 (3.1125)  Time: 1.173s,  872.95/s  (1.089s,  940.29/s)  LR: 1.816e-04  Data: 0.012 (0.013)
Train: 218 [1250/1251 (100%)]  Loss:  2.822261 (3.1013)  Time: 1.060s,  966.14/s  (1.089s,  940.15/s)  LR: 1.816e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.844 (5.844)  Loss:  0.4450 (0.4450)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5459 (0.8793)  Acc@1: 86.7925 (79.8560)  Acc@5: 97.8773 (95.1840)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-204.pth.tar', 79.31399992919921)

Train: 219 [   0/1251 (  0%)]  Loss:  2.961744 (2.9617)  Time: 1.084s,  944.62/s  (1.084s,  944.62/s)  LR: 1.777e-04  Data: 0.021 (0.021)
Train: 219 [  50/1251 (  4%)]  Loss:  2.933453 (2.9476)  Time: 1.078s,  949.92/s  (1.092s,  937.88/s)  LR: 1.777e-04  Data: 0.012 (0.013)
Train: 219 [ 100/1251 (  8%)]  Loss:  3.353807 (3.0830)  Time: 1.104s,  927.30/s  (1.088s,  941.51/s)  LR: 1.777e-04  Data: 0.012 (0.013)
Train: 219 [ 150/1251 ( 12%)]  Loss:  3.040380 (3.0723)  Time: 1.102s,  928.97/s  (1.087s,  941.73/s)  LR: 1.777e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 219 [ 200/1251 ( 16%)]  Loss:  3.247597 (3.1074)  Time: 1.079s,  948.89/s  (1.088s,  940.96/s)  LR: 1.777e-04  Data: 0.013 (0.012)
Train: 219 [ 250/1251 ( 20%)]  Loss:  2.979274 (3.0860)  Time: 1.093s,  937.08/s  (1.088s,  941.13/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 300/1251 ( 24%)]  Loss:  2.892044 (3.0583)  Time: 1.079s,  948.87/s  (1.088s,  941.13/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 350/1251 ( 28%)]  Loss:  3.236552 (3.0806)  Time: 1.080s,  947.84/s  (1.088s,  941.26/s)  LR: 1.777e-04  Data: 0.015 (0.013)
Train: 219 [ 400/1251 ( 32%)]  Loss:  3.108064 (3.0837)  Time: 1.100s,  930.72/s  (1.088s,  941.10/s)  LR: 1.777e-04  Data: 0.012 (0.013)
Train: 219 [ 450/1251 ( 36%)]  Loss:  3.296672 (3.1050)  Time: 1.095s,  935.48/s  (1.089s,  940.50/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 500/1251 ( 40%)]  Loss:  3.079631 (3.1027)  Time: 1.078s,  949.77/s  (1.089s,  940.29/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 550/1251 ( 44%)]  Loss:  3.213562 (3.1119)  Time: 1.081s,  947.60/s  (1.089s,  940.38/s)  LR: 1.777e-04  Data: 0.014 (0.012)
Train: 219 [ 600/1251 ( 48%)]  Loss:  2.954735 (3.0998)  Time: 1.078s,  950.06/s  (1.088s,  941.10/s)  LR: 1.777e-04  Data: 0.013 (0.012)
Train: 219 [ 650/1251 ( 52%)]  Loss:  3.381236 (3.1199)  Time: 1.103s,  928.32/s  (1.088s,  940.83/s)  LR: 1.777e-04  Data: 0.013 (0.012)
Train: 219 [ 700/1251 ( 56%)]  Loss:  2.999097 (3.1119)  Time: 1.077s,  951.06/s  (1.088s,  940.80/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 750/1251 ( 60%)]  Loss:  3.152084 (3.1144)  Time: 1.096s,  934.50/s  (1.088s,  941.05/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 800/1251 ( 64%)]  Loss:  3.237573 (3.1216)  Time: 1.097s,  933.68/s  (1.088s,  941.34/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 850/1251 ( 68%)]  Loss:  3.043558 (3.1173)  Time: 1.095s,  935.26/s  (1.088s,  941.11/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 900/1251 ( 72%)]  Loss:  3.306102 (3.1272)  Time: 1.080s,  948.14/s  (1.088s,  941.39/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [ 950/1251 ( 76%)]  Loss:  3.180824 (3.1299)  Time: 1.095s,  935.14/s  (1.088s,  941.58/s)  LR: 1.777e-04  Data: 0.014 (0.012)
Train: 219 [1000/1251 ( 80%)]  Loss:  2.859537 (3.1170)  Time: 1.080s,  947.91/s  (1.087s,  941.64/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [1050/1251 ( 84%)]  Loss:  3.135968 (3.1179)  Time: 1.082s,  946.47/s  (1.088s,  941.54/s)  LR: 1.777e-04  Data: 0.018 (0.012)
Train: 219 [1100/1251 ( 88%)]  Loss:  3.021856 (3.1137)  Time: 1.078s,  949.54/s  (1.088s,  941.54/s)  LR: 1.777e-04  Data: 0.017 (0.012)
Train: 219 [1150/1251 ( 92%)]  Loss:  3.200734 (3.1173)  Time: 1.083s,  945.14/s  (1.087s,  941.65/s)  LR: 1.777e-04  Data: 0.015 (0.012)
Train: 219 [1200/1251 ( 96%)]  Loss:  2.860891 (3.1071)  Time: 1.095s,  934.76/s  (1.088s,  941.29/s)  LR: 1.777e-04  Data: 0.012 (0.012)
Train: 219 [1250/1251 (100%)]  Loss:  2.692010 (3.0911)  Time: 1.080s,  948.05/s  (1.088s,  940.97/s)  LR: 1.777e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.798 (5.798)  Loss:  0.4411 (0.4411)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5809 (0.8821)  Acc@1: 86.0849 (79.7960)  Acc@5: 98.1132 (95.2160)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-210.pth.tar', 79.34799997558594)

Train: 220 [   0/1251 (  0%)]  Loss:  3.122000 (3.1220)  Time: 1.084s,  944.74/s  (1.084s,  944.74/s)  LR: 1.738e-04  Data: 0.021 (0.021)
Train: 220 [  50/1251 (  4%)]  Loss:  3.199861 (3.1609)  Time: 1.113s,  919.66/s  (1.090s,  939.50/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 100/1251 (  8%)]  Loss:  3.084934 (3.1356)  Time: 1.086s,  942.94/s  (1.091s,  938.66/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 150/1251 ( 12%)]  Loss:  3.195666 (3.1506)  Time: 1.082s,  946.44/s  (1.092s,  938.09/s)  LR: 1.738e-04  Data: 0.021 (0.013)
Train: 220 [ 200/1251 ( 16%)]  Loss:  2.939893 (3.1085)  Time: 1.089s,  940.48/s  (1.090s,  939.56/s)  LR: 1.738e-04  Data: 0.014 (0.013)
Train: 220 [ 250/1251 ( 20%)]  Loss:  3.076567 (3.1032)  Time: 1.083s,  945.12/s  (1.089s,  940.09/s)  LR: 1.738e-04  Data: 0.013 (0.013)
Train: 220 [ 300/1251 ( 24%)]  Loss:  3.222050 (3.1201)  Time: 1.077s,  951.01/s  (1.088s,  940.83/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 350/1251 ( 28%)]  Loss:  3.067907 (3.1136)  Time: 1.082s,  946.24/s  (1.088s,  941.28/s)  LR: 1.738e-04  Data: 0.015 (0.013)
Train: 220 [ 400/1251 ( 32%)]  Loss:  2.996106 (3.1006)  Time: 1.102s,  929.13/s  (1.088s,  941.56/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 450/1251 ( 36%)]  Loss:  2.974410 (3.0879)  Time: 1.094s,  935.97/s  (1.089s,  939.92/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 500/1251 ( 40%)]  Loss:  3.121072 (3.0910)  Time: 1.101s,  930.24/s  (1.090s,  939.40/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 550/1251 ( 44%)]  Loss:  3.073540 (3.0895)  Time: 1.075s,  952.81/s  (1.090s,  939.25/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 600/1251 ( 48%)]  Loss:  2.969347 (3.0803)  Time: 1.094s,  935.63/s  (1.091s,  939.01/s)  LR: 1.738e-04  Data: 0.015 (0.013)
Train: 220 [ 650/1251 ( 52%)]  Loss:  2.954890 (3.0713)  Time: 1.095s,  935.01/s  (1.091s,  939.00/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 700/1251 ( 56%)]  Loss:  2.921311 (3.0613)  Time: 1.077s,  950.95/s  (1.090s,  939.54/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 750/1251 ( 60%)]  Loss:  3.034273 (3.0596)  Time: 1.095s,  934.81/s  (1.091s,  938.95/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 800/1251 ( 64%)]  Loss:  3.221308 (3.0691)  Time: 1.076s,  951.47/s  (1.090s,  939.07/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 850/1251 ( 68%)]  Loss:  3.348644 (3.0847)  Time: 1.076s,  951.30/s  (1.090s,  939.37/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [ 900/1251 ( 72%)]  Loss:  2.767615 (3.0680)  Time: 1.097s,  933.44/s  (1.090s,  939.42/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [ 950/1251 ( 76%)]  Loss:  2.896801 (3.0594)  Time: 1.073s,  954.72/s  (1.090s,  939.29/s)  LR: 1.738e-04  Data: 0.011 (0.013)
Train: 220 [1000/1251 ( 80%)]  Loss:  3.092037 (3.0610)  Time: 1.094s,  935.93/s  (1.090s,  939.45/s)  LR: 1.738e-04  Data: 0.013 (0.013)
Train: 220 [1050/1251 ( 84%)]  Loss:  3.001463 (3.0583)  Time: 1.171s,  874.54/s  (1.090s,  939.49/s)  LR: 1.738e-04  Data: 0.013 (0.013)
Train: 220 [1100/1251 ( 88%)]  Loss:  3.143313 (3.0620)  Time: 1.084s,  944.63/s  (1.090s,  939.72/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [1150/1251 ( 92%)]  Loss:  3.432714 (3.0774)  Time: 1.078s,  950.27/s  (1.090s,  939.56/s)  LR: 1.738e-04  Data: 0.012 (0.013)
Train: 220 [1200/1251 ( 96%)]  Loss:  3.202576 (3.0824)  Time: 1.080s,  948.57/s  (1.090s,  939.74/s)  LR: 1.738e-04  Data: 0.015 (0.013)
Train: 220 [1250/1251 (100%)]  Loss:  3.088245 (3.0826)  Time: 1.078s,  950.15/s  (1.090s,  939.64/s)  LR: 1.738e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.762 (5.762)  Loss:  0.4412 (0.4412)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5334 (0.8806)  Acc@1: 85.8491 (79.7720)  Acc@5: 98.3491 (95.1360)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-211.pth.tar', 79.41000013183594)

Train: 221 [   0/1251 (  0%)]  Loss:  3.223665 (3.2237)  Time: 1.092s,  938.15/s  (1.092s,  938.15/s)  LR: 1.699e-04  Data: 0.030 (0.030)
Train: 221 [  50/1251 (  4%)]  Loss:  3.115425 (3.1695)  Time: 1.076s,  951.26/s  (1.094s,  936.00/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 100/1251 (  8%)]  Loss:  2.760359 (3.0331)  Time: 1.076s,  951.65/s  (1.091s,  938.64/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 150/1251 ( 12%)]  Loss:  3.255280 (3.0887)  Time: 1.095s,  935.48/s  (1.088s,  940.83/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 200/1251 ( 16%)]  Loss:  3.013476 (3.0736)  Time: 1.096s,  934.61/s  (1.090s,  939.26/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 250/1251 ( 20%)]  Loss:  3.355009 (3.1205)  Time: 1.094s,  935.88/s  (1.091s,  939.01/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 300/1251 ( 24%)]  Loss:  2.846342 (3.0814)  Time: 1.095s,  935.19/s  (1.091s,  938.19/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 350/1251 ( 28%)]  Loss:  3.423992 (3.1242)  Time: 1.078s,  949.87/s  (1.092s,  938.12/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 400/1251 ( 32%)]  Loss:  2.880991 (3.0972)  Time: 1.079s,  949.26/s  (1.092s,  937.87/s)  LR: 1.699e-04  Data: 0.014 (0.012)
Train: 221 [ 450/1251 ( 36%)]  Loss:  3.016051 (3.0891)  Time: 1.079s,  949.27/s  (1.091s,  938.55/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 500/1251 ( 40%)]  Loss:  3.123628 (3.0922)  Time: 1.101s,  930.18/s  (1.091s,  938.39/s)  LR: 1.699e-04  Data: 0.014 (0.012)
Train: 221 [ 550/1251 ( 44%)]  Loss:  3.041358 (3.0880)  Time: 1.097s,  933.53/s  (1.091s,  938.81/s)  LR: 1.699e-04  Data: 0.015 (0.013)
Train: 221 [ 600/1251 ( 48%)]  Loss:  3.191875 (3.0960)  Time: 1.096s,  934.13/s  (1.091s,  938.97/s)  LR: 1.699e-04  Data: 0.015 (0.013)
Train: 221 [ 650/1251 ( 52%)]  Loss:  3.196554 (3.1031)  Time: 1.080s,  947.88/s  (1.091s,  938.89/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 700/1251 ( 56%)]  Loss:  3.227810 (3.1115)  Time: 1.097s,  933.82/s  (1.090s,  939.52/s)  LR: 1.699e-04  Data: 0.015 (0.013)
Train: 221 [ 750/1251 ( 60%)]  Loss:  2.946942 (3.1012)  Time: 1.079s,  949.16/s  (1.090s,  939.81/s)  LR: 1.699e-04  Data: 0.015 (0.013)
Train: 221 [ 800/1251 ( 64%)]  Loss:  3.224741 (3.1084)  Time: 1.081s,  947.59/s  (1.089s,  939.94/s)  LR: 1.699e-04  Data: 0.014 (0.013)
Train: 221 [ 850/1251 ( 68%)]  Loss:  3.031350 (3.1042)  Time: 1.077s,  951.07/s  (1.089s,  940.32/s)  LR: 1.699e-04  Data: 0.011 (0.013)
Train: 221 [ 900/1251 ( 72%)]  Loss:  2.939609 (3.0955)  Time: 1.098s,  932.78/s  (1.089s,  940.27/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [ 950/1251 ( 76%)]  Loss:  3.187948 (3.1001)  Time: 1.093s,  936.51/s  (1.089s,  940.10/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [1000/1251 ( 80%)]  Loss:  3.326702 (3.1109)  Time: 1.093s,  936.71/s  (1.089s,  940.12/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [1050/1251 ( 84%)]  Loss:  3.403207 (3.1242)  Time: 1.095s,  935.17/s  (1.090s,  939.53/s)  LR: 1.699e-04  Data: 0.012 (0.013)
Train: 221 [1100/1251 ( 88%)]  Loss:  3.181005 (3.1267)  Time: 1.097s,  933.71/s  (1.090s,  939.44/s)  LR: 1.699e-04  Data: 0.011 (0.012)
Train: 221 [1150/1251 ( 92%)]  Loss:  3.280666 (3.1331)  Time: 1.103s,  928.01/s  (1.090s,  939.53/s)  LR: 1.699e-04  Data: 0.012 (0.012)
Train: 221 [1200/1251 ( 96%)]  Loss:  2.852842 (3.1219)  Time: 1.078s,  949.92/s  (1.090s,  939.65/s)  LR: 1.699e-04  Data: 0.014 (0.012)
Train: 221 [1250/1251 (100%)]  Loss:  2.872146 (3.1123)  Time: 1.100s,  931.11/s  (1.090s,  939.53/s)  LR: 1.699e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.840 (5.840)  Loss:  0.4663 (0.4663)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5133 (0.8851)  Acc@1: 87.5000 (79.8000)  Acc@5: 98.2311 (95.1520)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-212.pth.tar', 79.49800002441407)

Train: 222 [   0/1251 (  0%)]  Loss:  2.933718 (2.9337)  Time: 1.085s,  944.09/s  (1.085s,  944.09/s)  LR: 1.661e-04  Data: 0.023 (0.023)
Train: 222 [  50/1251 (  4%)]  Loss:  3.164274 (3.0490)  Time: 1.094s,  935.82/s  (1.084s,  944.36/s)  LR: 1.661e-04  Data: 0.013 (0.012)
Train: 222 [ 100/1251 (  8%)]  Loss:  2.875893 (2.9913)  Time: 1.081s,  947.62/s  (1.084s,  944.80/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 150/1251 ( 12%)]  Loss:  3.067149 (3.0103)  Time: 1.101s,  929.99/s  (1.086s,  942.88/s)  LR: 1.661e-04  Data: 0.011 (0.012)
Train: 222 [ 200/1251 ( 16%)]  Loss:  3.008845 (3.0100)  Time: 1.097s,  933.21/s  (1.090s,  939.78/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 250/1251 ( 20%)]  Loss:  2.919688 (2.9949)  Time: 1.076s,  952.02/s  (1.089s,  940.63/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 300/1251 ( 24%)]  Loss:  3.267099 (3.0338)  Time: 1.076s,  951.96/s  (1.088s,  941.13/s)  LR: 1.661e-04  Data: 0.013 (0.012)
Train: 222 [ 350/1251 ( 28%)]  Loss:  2.993267 (3.0287)  Time: 1.079s,  948.87/s  (1.089s,  940.11/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 400/1251 ( 32%)]  Loss:  3.114033 (3.0382)  Time: 1.094s,  935.67/s  (1.090s,  939.52/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 450/1251 ( 36%)]  Loss:  3.128769 (3.0473)  Time: 1.082s,  946.77/s  (1.090s,  939.06/s)  LR: 1.661e-04  Data: 0.015 (0.012)
Train: 222 [ 500/1251 ( 40%)]  Loss:  3.192117 (3.0604)  Time: 1.077s,  951.00/s  (1.090s,  939.58/s)  LR: 1.661e-04  Data: 0.013 (0.013)
Train: 222 [ 550/1251 ( 44%)]  Loss:  3.398406 (3.0886)  Time: 1.092s,  937.64/s  (1.090s,  939.59/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [ 600/1251 ( 48%)]  Loss:  2.991764 (3.0812)  Time: 1.078s,  949.72/s  (1.090s,  939.54/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [ 650/1251 ( 52%)]  Loss:  3.249662 (3.0932)  Time: 1.077s,  951.12/s  (1.089s,  939.95/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 700/1251 ( 56%)]  Loss:  3.183159 (3.0992)  Time: 1.078s,  949.77/s  (1.090s,  939.73/s)  LR: 1.661e-04  Data: 0.012 (0.012)
Train: 222 [ 750/1251 ( 60%)]  Loss:  2.917969 (3.0879)  Time: 1.076s,  951.97/s  (1.090s,  939.81/s)  LR: 1.661e-04  Data: 0.011 (0.013)
Train: 222 [ 800/1251 ( 64%)]  Loss:  3.178789 (3.0932)  Time: 1.081s,  946.94/s  (1.090s,  939.77/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [ 850/1251 ( 68%)]  Loss:  3.012881 (3.0887)  Time: 1.096s,  933.92/s  (1.090s,  939.88/s)  LR: 1.661e-04  Data: 0.013 (0.013)
Train: 222 [ 900/1251 ( 72%)]  Loss:  2.902373 (3.0789)  Time: 1.097s,  933.30/s  (1.090s,  939.76/s)  LR: 1.661e-04  Data: 0.013 (0.013)
Train: 222 [ 950/1251 ( 76%)]  Loss:  3.230917 (3.0865)  Time: 1.093s,  936.87/s  (1.090s,  939.60/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1000/1251 ( 80%)]  Loss:  3.280667 (3.0958)  Time: 1.109s,  923.03/s  (1.090s,  939.29/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1050/1251 ( 84%)]  Loss:  3.136811 (3.0976)  Time: 1.103s,  927.96/s  (1.090s,  939.15/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1100/1251 ( 88%)]  Loss:  3.371216 (3.1095)  Time: 1.077s,  950.46/s  (1.090s,  939.20/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1150/1251 ( 92%)]  Loss:  3.055883 (3.1073)  Time: 1.099s,  931.59/s  (1.090s,  939.45/s)  LR: 1.661e-04  Data: 0.012 (0.013)
Train: 222 [1200/1251 ( 96%)]  Loss:  3.087864 (3.1065)  Time: 1.095s,  935.04/s  (1.090s,  939.62/s)  LR: 1.661e-04  Data: 0.013 (0.013)
Train: 222 [1250/1251 (100%)]  Loss:  3.315019 (3.1145)  Time: 1.062s,  964.38/s  (1.090s,  939.58/s)  LR: 1.661e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.935 (5.935)  Loss:  0.4752 (0.4752)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5922 (0.9090)  Acc@1: 85.9670 (79.6560)  Acc@5: 97.8774 (95.0900)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 79.65599998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-216.pth.tar', 79.53199998046875)

Train: 223 [   0/1251 (  0%)]  Loss:  3.068424 (3.0684)  Time: 1.085s,  943.82/s  (1.085s,  943.82/s)  LR: 1.624e-04  Data: 0.024 (0.024)
Train: 223 [  50/1251 (  4%)]  Loss:  2.795612 (2.9320)  Time: 1.095s,  935.02/s  (1.096s,  934.37/s)  LR: 1.624e-04  Data: 0.015 (0.013)
Train: 223 [ 100/1251 (  8%)]  Loss:  2.876514 (2.9135)  Time: 1.079s,  948.73/s  (1.093s,  936.85/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 150/1251 ( 12%)]  Loss:  3.140577 (2.9703)  Time: 1.086s,  942.66/s  (1.093s,  936.73/s)  LR: 1.624e-04  Data: 0.014 (0.013)
Train: 223 [ 200/1251 ( 16%)]  Loss:  3.147013 (3.0056)  Time: 1.095s,  935.32/s  (1.093s,  936.50/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 250/1251 ( 20%)]  Loss:  3.110018 (3.0230)  Time: 1.077s,  951.07/s  (1.094s,  935.66/s)  LR: 1.624e-04  Data: 0.013 (0.013)
Train: 223 [ 300/1251 ( 24%)]  Loss:  3.201128 (3.0485)  Time: 1.085s,  943.91/s  (1.094s,  935.78/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 350/1251 ( 28%)]  Loss:  3.133645 (3.0591)  Time: 1.079s,  949.29/s  (1.093s,  937.04/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 223 [ 400/1251 ( 32%)]  Loss:  3.031560 (3.0561)  Time: 1.094s,  935.85/s  (1.093s,  936.99/s)  LR: 1.624e-04  Data: 0.012 (0.012)
Train: 223 [ 450/1251 ( 36%)]  Loss:  2.584889 (3.0089)  Time: 1.079s,  949.18/s  (1.093s,  936.81/s)  LR: 1.624e-04  Data: 0.013 (0.012)
Train: 223 [ 500/1251 ( 40%)]  Loss:  2.923133 (3.0011)  Time: 1.095s,  935.54/s  (1.093s,  937.21/s)  LR: 1.624e-04  Data: 0.012 (0.012)
Train: 223 [ 550/1251 ( 44%)]  Loss:  3.244682 (3.0214)  Time: 1.076s,  951.46/s  (1.092s,  937.73/s)  LR: 1.624e-04  Data: 0.014 (0.012)
Train: 223 [ 600/1251 ( 48%)]  Loss:  3.162174 (3.0323)  Time: 1.077s,  950.85/s  (1.092s,  937.70/s)  LR: 1.624e-04  Data: 0.011 (0.013)
Train: 223 [ 650/1251 ( 52%)]  Loss:  3.118465 (3.0384)  Time: 1.077s,  951.04/s  (1.092s,  937.53/s)  LR: 1.624e-04  Data: 0.013 (0.013)
Train: 223 [ 700/1251 ( 56%)]  Loss:  3.121315 (3.0439)  Time: 1.077s,  950.76/s  (1.092s,  937.99/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 750/1251 ( 60%)]  Loss:  3.110233 (3.0481)  Time: 1.098s,  932.85/s  (1.091s,  938.18/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 800/1251 ( 64%)]  Loss:  3.191873 (3.0565)  Time: 1.096s,  934.23/s  (1.092s,  937.74/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 850/1251 ( 68%)]  Loss:  2.898919 (3.0478)  Time: 1.076s,  952.11/s  (1.092s,  937.54/s)  LR: 1.624e-04  Data: 0.012 (0.013)
Train: 223 [ 900/1251 ( 72%)]  Loss:  3.072390 (3.0491)  Time: 1.094s,  936.30/s  (1.092s,  937.49/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 223 [ 950/1251 ( 76%)]  Loss:  3.353762 (3.0643)  Time: 1.093s,  936.58/s  (1.092s,  937.57/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 223 [1000/1251 ( 80%)]  Loss:  2.725510 (3.0482)  Time: 1.084s,  944.86/s  (1.092s,  937.66/s)  LR: 1.624e-04  Data: 0.011 (0.012)
Train: 223 [1050/1251 ( 84%)]  Loss:  3.103947 (3.0507)  Time: 1.106s,  925.53/s  (1.092s,  937.97/s)  LR: 1.624e-04  Data: 0.013 (0.012)
Train: 223 [1100/1251 ( 88%)]  Loss:  3.139642 (3.0546)  Time: 1.083s,  945.53/s  (1.092s,  938.00/s)  LR: 1.624e-04  Data: 0.012 (0.012)
Train: 223 [1150/1251 ( 92%)]  Loss:  3.076364 (3.0555)  Time: 1.095s,  934.79/s  (1.092s,  937.89/s)  LR: 1.624e-04  Data: 0.015 (0.013)
Train: 223 [1200/1251 ( 96%)]  Loss:  3.372076 (3.0682)  Time: 1.077s,  950.59/s  (1.092s,  938.07/s)  LR: 1.624e-04  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 223 [1250/1251 (100%)]  Loss:  3.135434 (3.0707)  Time: 1.061s,  964.70/s  (1.092s,  938.11/s)  LR: 1.624e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.840 (5.840)  Loss:  0.4332 (0.4332)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5640 (0.8732)  Acc@1: 86.9104 (79.9620)  Acc@5: 97.9953 (95.2460)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 79.65599998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-213.pth.tar', 79.55400013183593)

Train: 224 [   0/1251 (  0%)]  Loss:  3.010830 (3.0108)  Time: 1.089s,  940.46/s  (1.089s,  940.46/s)  LR: 1.587e-04  Data: 0.028 (0.028)
Train: 224 [  50/1251 (  4%)]  Loss:  3.272559 (3.1417)  Time: 1.112s,  920.66/s  (1.088s,  940.90/s)  LR: 1.587e-04  Data: 0.012 (0.012)
Train: 224 [ 100/1251 (  8%)]  Loss:  3.062310 (3.1152)  Time: 1.113s,  920.11/s  (1.092s,  937.80/s)  LR: 1.587e-04  Data: 0.012 (0.012)
Train: 224 [ 150/1251 ( 12%)]  Loss:  3.067448 (3.1033)  Time: 1.082s,  946.61/s  (1.091s,  938.55/s)  LR: 1.587e-04  Data: 0.016 (0.013)
Train: 224 [ 200/1251 ( 16%)]  Loss:  3.309205 (3.1445)  Time: 1.080s,  948.32/s  (1.089s,  940.19/s)  LR: 1.587e-04  Data: 0.012 (0.012)
Train: 224 [ 250/1251 ( 20%)]  Loss:  2.804074 (3.0877)  Time: 1.080s,  948.30/s  (1.089s,  940.30/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 300/1251 ( 24%)]  Loss:  3.110297 (3.0910)  Time: 1.079s,  949.37/s  (1.089s,  940.22/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 350/1251 ( 28%)]  Loss:  2.814942 (3.0565)  Time: 1.079s,  949.20/s  (1.088s,  941.04/s)  LR: 1.587e-04  Data: 0.013 (0.013)
Train: 224 [ 400/1251 ( 32%)]  Loss:  2.917266 (3.0410)  Time: 1.076s,  951.52/s  (1.088s,  941.53/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 450/1251 ( 36%)]  Loss:  3.154519 (3.0523)  Time: 1.076s,  951.48/s  (1.087s,  941.88/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [ 500/1251 ( 40%)]  Loss:  3.083230 (3.0552)  Time: 1.077s,  950.77/s  (1.087s,  942.13/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 550/1251 ( 44%)]  Loss:  3.034879 (3.0535)  Time: 1.076s,  951.37/s  (1.087s,  942.04/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 600/1251 ( 48%)]  Loss:  3.008070 (3.0500)  Time: 1.077s,  950.85/s  (1.087s,  942.03/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 650/1251 ( 52%)]  Loss:  3.244761 (3.0639)  Time: 1.076s,  951.27/s  (1.086s,  942.56/s)  LR: 1.587e-04  Data: 0.014 (0.013)
Train: 224 [ 700/1251 ( 56%)]  Loss:  3.269115 (3.0776)  Time: 1.081s,  947.16/s  (1.087s,  942.41/s)  LR: 1.587e-04  Data: 0.015 (0.013)
Train: 224 [ 750/1251 ( 60%)]  Loss:  2.748789 (3.0570)  Time: 1.102s,  928.85/s  (1.087s,  942.08/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 800/1251 ( 64%)]  Loss:  2.839865 (3.0442)  Time: 1.080s,  948.44/s  (1.087s,  942.18/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 850/1251 ( 68%)]  Loss:  3.239521 (3.0551)  Time: 1.082s,  946.83/s  (1.087s,  941.92/s)  LR: 1.587e-04  Data: 0.011 (0.013)
Train: 224 [ 900/1251 ( 72%)]  Loss:  3.057093 (3.0552)  Time: 1.077s,  950.96/s  (1.088s,  941.54/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [ 950/1251 ( 76%)]  Loss:  2.762963 (3.0406)  Time: 1.076s,  951.74/s  (1.088s,  941.28/s)  LR: 1.587e-04  Data: 0.013 (0.013)
Train: 224 [1000/1251 ( 80%)]  Loss:  2.971495 (3.0373)  Time: 1.077s,  951.19/s  (1.088s,  941.46/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [1050/1251 ( 84%)]  Loss:  3.265612 (3.0477)  Time: 1.095s,  935.36/s  (1.088s,  941.47/s)  LR: 1.587e-04  Data: 0.013 (0.013)
Train: 224 [1100/1251 ( 88%)]  Loss:  3.010612 (3.0461)  Time: 1.081s,  947.40/s  (1.088s,  941.41/s)  LR: 1.587e-04  Data: 0.013 (0.013)
Train: 224 [1150/1251 ( 92%)]  Loss:  3.058058 (3.0466)  Time: 1.103s,  928.77/s  (1.088s,  941.57/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [1200/1251 ( 96%)]  Loss:  3.080428 (3.0479)  Time: 1.102s,  929.58/s  (1.088s,  941.53/s)  LR: 1.587e-04  Data: 0.012 (0.013)
Train: 224 [1250/1251 (100%)]  Loss:  2.617779 (3.0314)  Time: 1.080s,  948.42/s  (1.088s,  941.48/s)  LR: 1.587e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.869 (5.869)  Loss:  0.4152 (0.4152)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5476 (0.8620)  Acc@1: 87.2642 (79.9380)  Acc@5: 97.4057 (95.2020)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 79.65599998046875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-214.pth.tar', 79.654)

Train: 225 [   0/1251 (  0%)]  Loss:  3.111532 (3.1115)  Time: 1.084s,  944.69/s  (1.084s,  944.69/s)  LR: 1.550e-04  Data: 0.021 (0.021)
Train: 225 [  50/1251 (  4%)]  Loss:  3.017321 (3.0644)  Time: 1.078s,  950.24/s  (1.092s,  937.84/s)  LR: 1.550e-04  Data: 0.012 (0.012)
Train: 225 [ 100/1251 (  8%)]  Loss:  2.944359 (3.0244)  Time: 1.077s,  950.95/s  (1.091s,  938.35/s)  LR: 1.550e-04  Data: 0.014 (0.012)
Train: 225 [ 150/1251 ( 12%)]  Loss:  3.202374 (3.0689)  Time: 1.078s,  949.88/s  (1.089s,  940.37/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 200/1251 ( 16%)]  Loss:  3.062885 (3.0677)  Time: 1.075s,  952.33/s  (1.087s,  941.78/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 250/1251 ( 20%)]  Loss:  3.340579 (3.1132)  Time: 1.093s,  937.27/s  (1.088s,  940.78/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 300/1251 ( 24%)]  Loss:  3.138362 (3.1168)  Time: 1.095s,  935.15/s  (1.088s,  940.98/s)  LR: 1.550e-04  Data: 0.013 (0.013)
Train: 225 [ 350/1251 ( 28%)]  Loss:  3.150558 (3.1210)  Time: 1.083s,  945.78/s  (1.089s,  940.04/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 400/1251 ( 32%)]  Loss:  3.203694 (3.1302)  Time: 1.080s,  947.72/s  (1.089s,  939.94/s)  LR: 1.550e-04  Data: 0.014 (0.013)
Train: 225 [ 450/1251 ( 36%)]  Loss:  3.190896 (3.1363)  Time: 1.078s,  949.73/s  (1.090s,  939.58/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 500/1251 ( 40%)]  Loss:  2.984106 (3.1224)  Time: 1.094s,  936.04/s  (1.089s,  940.10/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 550/1251 ( 44%)]  Loss:  2.755206 (3.0918)  Time: 1.082s,  946.31/s  (1.089s,  940.54/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 600/1251 ( 48%)]  Loss:  3.191881 (3.0995)  Time: 1.077s,  950.58/s  (1.088s,  941.21/s)  LR: 1.550e-04  Data: 0.013 (0.013)
Train: 225 [ 650/1251 ( 52%)]  Loss:  3.231958 (3.1090)  Time: 1.096s,  934.45/s  (1.088s,  941.18/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 700/1251 ( 56%)]  Loss:  3.297508 (3.1215)  Time: 1.077s,  950.50/s  (1.088s,  941.52/s)  LR: 1.550e-04  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 225 [ 750/1251 ( 60%)]  Loss:  3.071749 (3.1184)  Time: 1.098s,  932.38/s  (1.087s,  941.61/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [ 800/1251 ( 64%)]  Loss:  2.933392 (3.1076)  Time: 1.077s,  950.81/s  (1.088s,  941.49/s)  LR: 1.550e-04  Data: 0.011 (0.013)
Train: 225 [ 850/1251 ( 68%)]  Loss:  2.921030 (3.0972)  Time: 1.084s,  944.79/s  (1.087s,  941.70/s)  LR: 1.550e-04  Data: 0.014 (0.013)
Train: 225 [ 900/1251 ( 72%)]  Loss:  3.038898 (3.0941)  Time: 1.096s,  934.70/s  (1.087s,  941.96/s)  LR: 1.550e-04  Data: 0.013 (0.013)
Train: 225 [ 950/1251 ( 76%)]  Loss:  3.175687 (3.0982)  Time: 1.078s,  949.84/s  (1.087s,  942.03/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1000/1251 ( 80%)]  Loss:  3.273989 (3.1066)  Time: 1.083s,  945.79/s  (1.087s,  941.79/s)  LR: 1.550e-04  Data: 0.014 (0.013)
Train: 225 [1050/1251 ( 84%)]  Loss:  2.794945 (3.0924)  Time: 1.094s,  935.67/s  (1.087s,  941.80/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1100/1251 ( 88%)]  Loss:  3.290276 (3.1010)  Time: 1.078s,  949.61/s  (1.087s,  941.91/s)  LR: 1.550e-04  Data: 0.016 (0.013)
Train: 225 [1150/1251 ( 92%)]  Loss:  3.233641 (3.1065)  Time: 1.078s,  950.14/s  (1.087s,  941.99/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1200/1251 ( 96%)]  Loss:  2.697941 (3.0902)  Time: 1.094s,  935.67/s  (1.087s,  941.76/s)  LR: 1.550e-04  Data: 0.012 (0.013)
Train: 225 [1250/1251 (100%)]  Loss:  3.216927 (3.0951)  Time: 1.063s,  963.33/s  (1.087s,  941.73/s)  LR: 1.550e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.797 (5.797)  Loss:  0.4175 (0.4175)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5210 (0.8703)  Acc@1: 88.5613 (80.0660)  Acc@5: 97.7594 (95.1960)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-222.pth.tar', 79.65599998046875)

Train: 226 [   0/1251 (  0%)]  Loss:  2.835124 (2.8351)  Time: 1.083s,  945.38/s  (1.083s,  945.38/s)  LR: 1.513e-04  Data: 0.022 (0.022)
Train: 226 [  50/1251 (  4%)]  Loss:  3.045767 (2.9404)  Time: 1.083s,  945.83/s  (1.090s,  939.24/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Train: 226 [ 100/1251 (  8%)]  Loss:  3.083586 (2.9882)  Time: 1.094s,  935.93/s  (1.092s,  937.82/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 150/1251 ( 12%)]  Loss:  2.987624 (2.9880)  Time: 1.095s,  935.56/s  (1.092s,  937.68/s)  LR: 1.513e-04  Data: 0.018 (0.013)
Train: 226 [ 200/1251 ( 16%)]  Loss:  2.979512 (2.9863)  Time: 1.093s,  936.49/s  (1.091s,  938.53/s)  LR: 1.513e-04  Data: 0.014 (0.013)
Train: 226 [ 250/1251 ( 20%)]  Loss:  2.866664 (2.9664)  Time: 1.080s,  947.99/s  (1.091s,  938.67/s)  LR: 1.513e-04  Data: 0.014 (0.013)
Train: 226 [ 300/1251 ( 24%)]  Loss:  2.828999 (2.9468)  Time: 1.084s,  944.75/s  (1.091s,  938.23/s)  LR: 1.513e-04  Data: 0.015 (0.013)
Train: 226 [ 350/1251 ( 28%)]  Loss:  2.953211 (2.9476)  Time: 1.075s,  952.56/s  (1.091s,  938.26/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 400/1251 ( 32%)]  Loss:  2.958757 (2.9488)  Time: 1.083s,  945.87/s  (1.091s,  938.20/s)  LR: 1.513e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 226 [ 450/1251 ( 36%)]  Loss:  2.874396 (2.9414)  Time: 1.077s,  950.66/s  (1.091s,  938.82/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 500/1251 ( 40%)]  Loss:  3.205942 (2.9654)  Time: 1.093s,  936.61/s  (1.091s,  938.76/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 550/1251 ( 44%)]  Loss:  2.861077 (2.9567)  Time: 1.082s,  946.30/s  (1.090s,  939.04/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 600/1251 ( 48%)]  Loss:  3.075090 (2.9658)  Time: 1.095s,  934.76/s  (1.090s,  939.27/s)  LR: 1.513e-04  Data: 0.016 (0.013)
Train: 226 [ 650/1251 ( 52%)]  Loss:  3.124370 (2.9772)  Time: 1.085s,  944.13/s  (1.090s,  939.28/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 700/1251 ( 56%)]  Loss:  3.036926 (2.9811)  Time: 1.094s,  935.63/s  (1.091s,  939.01/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 750/1251 ( 60%)]  Loss:  2.833643 (2.9719)  Time: 1.093s,  936.48/s  (1.091s,  939.01/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 800/1251 ( 64%)]  Loss:  2.953280 (2.9708)  Time: 1.096s,  934.16/s  (1.091s,  938.82/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 850/1251 ( 68%)]  Loss:  2.927496 (2.9684)  Time: 1.095s,  935.37/s  (1.091s,  938.41/s)  LR: 1.513e-04  Data: 0.017 (0.013)
Train: 226 [ 900/1251 ( 72%)]  Loss:  3.077285 (2.9741)  Time: 1.076s,  951.30/s  (1.091s,  938.58/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [ 950/1251 ( 76%)]  Loss:  2.947852 (2.9728)  Time: 1.103s,  928.16/s  (1.091s,  938.71/s)  LR: 1.513e-04  Data: 0.013 (0.013)
Train: 226 [1000/1251 ( 80%)]  Loss:  2.727778 (2.9612)  Time: 1.097s,  933.78/s  (1.091s,  938.69/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1050/1251 ( 84%)]  Loss:  3.212356 (2.9726)  Time: 1.078s,  949.57/s  (1.091s,  938.60/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1100/1251 ( 88%)]  Loss:  3.110381 (2.9786)  Time: 1.079s,  949.33/s  (1.091s,  938.57/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1150/1251 ( 92%)]  Loss:  2.938375 (2.9769)  Time: 1.076s,  951.64/s  (1.091s,  938.77/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1200/1251 ( 96%)]  Loss:  2.910549 (2.9742)  Time: 1.086s,  943.07/s  (1.091s,  938.90/s)  LR: 1.513e-04  Data: 0.012 (0.013)
Train: 226 [1250/1251 (100%)]  Loss:  2.912443 (2.9719)  Time: 1.065s,  961.82/s  (1.091s,  938.94/s)  LR: 1.513e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.867 (5.867)  Loss:  0.4274 (0.4274)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5328 (0.8674)  Acc@1: 87.6179 (80.0160)  Acc@5: 98.2311 (95.2320)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-215.pth.tar', 79.71599997558594)

Train: 227 [   0/1251 (  0%)]  Loss:  3.008349 (3.0083)  Time: 1.085s,  944.16/s  (1.085s,  944.16/s)  LR: 1.477e-04  Data: 0.023 (0.023)
Train: 227 [  50/1251 (  4%)]  Loss:  3.182203 (3.0953)  Time: 1.096s,  934.71/s  (1.090s,  939.13/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 100/1251 (  8%)]  Loss:  3.124780 (3.1051)  Time: 1.095s,  935.19/s  (1.090s,  939.24/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 150/1251 ( 12%)]  Loss:  2.943498 (3.0647)  Time: 1.076s,  951.53/s  (1.090s,  939.37/s)  LR: 1.477e-04  Data: 0.013 (0.013)
Train: 227 [ 200/1251 ( 16%)]  Loss:  3.016514 (3.0551)  Time: 1.078s,  949.57/s  (1.089s,  940.44/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 250/1251 ( 20%)]  Loss:  3.283943 (3.0932)  Time: 1.187s,  862.90/s  (1.090s,  939.36/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 300/1251 ( 24%)]  Loss:  2.917488 (3.0681)  Time: 1.077s,  950.35/s  (1.090s,  939.04/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 350/1251 ( 28%)]  Loss:  3.200516 (3.0847)  Time: 1.079s,  949.39/s  (1.089s,  939.90/s)  LR: 1.477e-04  Data: 0.014 (0.013)
Train: 227 [ 400/1251 ( 32%)]  Loss:  3.000201 (3.0753)  Time: 1.095s,  935.43/s  (1.089s,  940.38/s)  LR: 1.477e-04  Data: 0.013 (0.013)
Train: 227 [ 450/1251 ( 36%)]  Loss:  2.870871 (3.0548)  Time: 1.078s,  950.24/s  (1.089s,  940.07/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 500/1251 ( 40%)]  Loss:  3.112709 (3.0601)  Time: 1.102s,  929.28/s  (1.089s,  940.24/s)  LR: 1.477e-04  Data: 0.013 (0.013)
Train: 227 [ 550/1251 ( 44%)]  Loss:  3.278613 (3.0783)  Time: 1.096s,  934.10/s  (1.089s,  940.38/s)  LR: 1.477e-04  Data: 0.019 (0.013)
Train: 227 [ 600/1251 ( 48%)]  Loss:  3.120901 (3.0816)  Time: 1.076s,  951.30/s  (1.088s,  940.88/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 650/1251 ( 52%)]  Loss:  3.069457 (3.0807)  Time: 1.089s,  940.62/s  (1.089s,  940.74/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 700/1251 ( 56%)]  Loss:  2.675352 (3.0537)  Time: 1.077s,  950.37/s  (1.088s,  941.02/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 750/1251 ( 60%)]  Loss:  2.913114 (3.0449)  Time: 1.078s,  950.32/s  (1.088s,  940.94/s)  LR: 1.477e-04  Data: 0.014 (0.013)
Train: 227 [ 800/1251 ( 64%)]  Loss:  3.013745 (3.0431)  Time: 1.090s,  939.81/s  (1.088s,  941.10/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [ 850/1251 ( 68%)]  Loss:  3.418202 (3.0639)  Time: 1.082s,  946.78/s  (1.088s,  940.77/s)  LR: 1.477e-04  Data: 0.014 (0.013)
Train: 227 [ 900/1251 ( 72%)]  Loss:  2.741773 (3.0470)  Time: 1.103s,  928.32/s  (1.089s,  940.70/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [ 950/1251 ( 76%)]  Loss:  3.003927 (3.0448)  Time: 1.098s,  932.70/s  (1.089s,  940.45/s)  LR: 1.477e-04  Data: 0.017 (0.013)
Train: 227 [1000/1251 ( 80%)]  Loss:  3.163614 (3.0505)  Time: 1.172s,  873.85/s  (1.089s,  940.52/s)  LR: 1.477e-04  Data: 0.012 (0.013)
Train: 227 [1050/1251 ( 84%)]  Loss:  2.985256 (3.0475)  Time: 1.100s,  931.09/s  (1.089s,  940.43/s)  LR: 1.477e-04  Data: 0.014 (0.013)
Train: 227 [1100/1251 ( 88%)]  Loss:  3.219114 (3.0550)  Time: 1.095s,  935.23/s  (1.089s,  940.47/s)  LR: 1.477e-04  Data: 0.015 (0.013)
Train: 227 [1150/1251 ( 92%)]  Loss:  3.007209 (3.0530)  Time: 1.107s,  925.23/s  (1.089s,  940.59/s)  LR: 1.477e-04  Data: 0.013 (0.013)
Train: 227 [1200/1251 ( 96%)]  Loss:  3.356419 (3.0651)  Time: 1.079s,  949.30/s  (1.089s,  940.64/s)  LR: 1.477e-04  Data: 0.011 (0.013)
Train: 227 [1250/1251 (100%)]  Loss:  3.208647 (3.0706)  Time: 1.080s,  948.30/s  (1.089s,  940.43/s)  LR: 1.477e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.773 (5.773)  Loss:  0.4252 (0.4252)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.5222 (0.8630)  Acc@1: 86.6745 (79.9960)  Acc@5: 98.2311 (95.2400)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-220.pth.tar', 79.77200005859375)

Train: 228 [   0/1251 (  0%)]  Loss:  3.169603 (3.1696)  Time: 1.094s,  936.33/s  (1.094s,  936.33/s)  LR: 1.442e-04  Data: 0.026 (0.026)
Train: 228 [  50/1251 (  4%)]  Loss:  3.110745 (3.1402)  Time: 1.080s,  947.82/s  (1.085s,  943.69/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 228 [ 100/1251 (  8%)]  Loss:  2.911425 (3.0639)  Time: 1.078s,  950.15/s  (1.084s,  944.48/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 150/1251 ( 12%)]  Loss:  2.937069 (3.0322)  Time: 1.082s,  946.22/s  (1.084s,  944.25/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [ 200/1251 ( 16%)]  Loss:  3.002369 (3.0262)  Time: 1.077s,  950.47/s  (1.085s,  943.62/s)  LR: 1.442e-04  Data: 0.011 (0.012)
Train: 228 [ 250/1251 ( 20%)]  Loss:  2.878547 (3.0016)  Time: 1.097s,  933.64/s  (1.085s,  943.51/s)  LR: 1.442e-04  Data: 0.012 (0.012)
Train: 228 [ 300/1251 ( 24%)]  Loss:  2.693612 (2.9576)  Time: 1.081s,  946.97/s  (1.084s,  944.25/s)  LR: 1.442e-04  Data: 0.015 (0.012)
Train: 228 [ 350/1251 ( 28%)]  Loss:  3.190772 (2.9868)  Time: 1.096s,  934.15/s  (1.085s,  943.83/s)  LR: 1.442e-04  Data: 0.012 (0.012)
Train: 228 [ 400/1251 ( 32%)]  Loss:  2.970298 (2.9849)  Time: 1.078s,  949.57/s  (1.086s,  943.12/s)  LR: 1.442e-04  Data: 0.013 (0.012)
Train: 228 [ 450/1251 ( 36%)]  Loss:  3.367643 (3.0232)  Time: 1.102s,  928.99/s  (1.086s,  943.02/s)  LR: 1.442e-04  Data: 0.013 (0.013)
Train: 228 [ 500/1251 ( 40%)]  Loss:  3.104457 (3.0306)  Time: 1.076s,  952.05/s  (1.087s,  942.38/s)  LR: 1.442e-04  Data: 0.013 (0.013)
Train: 228 [ 550/1251 ( 44%)]  Loss:  2.912521 (3.0208)  Time: 1.170s,  875.54/s  (1.087s,  942.45/s)  LR: 1.442e-04  Data: 0.015 (0.013)
Train: 228 [ 600/1251 ( 48%)]  Loss:  3.108263 (3.0275)  Time: 1.078s,  950.33/s  (1.086s,  942.53/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 650/1251 ( 52%)]  Loss:  3.161645 (3.0371)  Time: 1.077s,  950.68/s  (1.086s,  942.73/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [ 700/1251 ( 56%)]  Loss:  2.995898 (3.0343)  Time: 1.094s,  936.27/s  (1.087s,  941.82/s)  LR: 1.442e-04  Data: 0.016 (0.013)
Train: 228 [ 750/1251 ( 60%)]  Loss:  2.772649 (3.0180)  Time: 1.080s,  948.02/s  (1.088s,  941.55/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [ 800/1251 ( 64%)]  Loss:  3.085084 (3.0219)  Time: 1.078s,  949.92/s  (1.087s,  941.80/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [ 850/1251 ( 68%)]  Loss:  2.979011 (3.0195)  Time: 1.109s,  923.15/s  (1.088s,  941.53/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [ 900/1251 ( 72%)]  Loss:  3.083011 (3.0229)  Time: 1.081s,  947.06/s  (1.087s,  941.62/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [ 950/1251 ( 76%)]  Loss:  3.019236 (3.0227)  Time: 1.078s,  950.34/s  (1.088s,  941.52/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [1000/1251 ( 80%)]  Loss:  3.017298 (3.0224)  Time: 1.082s,  946.13/s  (1.087s,  941.82/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [1050/1251 ( 84%)]  Loss:  3.246349 (3.0326)  Time: 1.096s,  933.96/s  (1.087s,  941.86/s)  LR: 1.442e-04  Data: 0.011 (0.013)
Train: 228 [1100/1251 ( 88%)]  Loss:  3.020849 (3.0321)  Time: 1.081s,  946.86/s  (1.087s,  941.62/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [1150/1251 ( 92%)]  Loss:  3.154479 (3.0372)  Time: 1.197s,  855.59/s  (1.087s,  941.64/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [1200/1251 ( 96%)]  Loss:  2.912353 (3.0322)  Time: 1.097s,  933.64/s  (1.087s,  941.70/s)  LR: 1.442e-04  Data: 0.012 (0.013)
Train: 228 [1250/1251 (100%)]  Loss:  3.313186 (3.0430)  Time: 1.064s,  962.59/s  (1.088s,  941.56/s)  LR: 1.442e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.717 (5.717)  Loss:  0.4606 (0.4606)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.5948 (0.8908)  Acc@1: 85.8491 (79.9440)  Acc@5: 97.8774 (95.1460)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-219.pth.tar', 79.79600003173829)

Train: 229 [   0/1251 (  0%)]  Loss:  2.930577 (2.9306)  Time: 1.090s,  939.30/s  (1.090s,  939.30/s)  LR: 1.406e-04  Data: 0.029 (0.029)
Train: 229 [  50/1251 (  4%)]  Loss:  2.915950 (2.9233)  Time: 1.094s,  935.67/s  (1.096s,  934.07/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 100/1251 (  8%)]  Loss:  3.110587 (2.9857)  Time: 1.095s,  935.53/s  (1.091s,  938.47/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 150/1251 ( 12%)]  Loss:  3.264266 (3.0553)  Time: 1.078s,  949.83/s  (1.092s,  937.49/s)  LR: 1.406e-04  Data: 0.016 (0.013)
Train: 229 [ 200/1251 ( 16%)]  Loss:  3.025021 (3.0493)  Time: 1.080s,  947.82/s  (1.090s,  939.75/s)  LR: 1.406e-04  Data: 0.015 (0.013)
Train: 229 [ 250/1251 ( 20%)]  Loss:  2.924658 (3.0285)  Time: 1.095s,  935.37/s  (1.089s,  940.60/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [ 300/1251 ( 24%)]  Loss:  2.824207 (2.9993)  Time: 1.080s,  948.52/s  (1.090s,  939.30/s)  LR: 1.406e-04  Data: 0.014 (0.013)
Train: 229 [ 350/1251 ( 28%)]  Loss:  2.708830 (2.9630)  Time: 1.093s,  936.84/s  (1.090s,  939.59/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 400/1251 ( 32%)]  Loss:  3.186713 (2.9879)  Time: 1.094s,  936.21/s  (1.090s,  939.66/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [ 450/1251 ( 36%)]  Loss:  2.782536 (2.9673)  Time: 1.084s,  944.61/s  (1.089s,  940.11/s)  LR: 1.406e-04  Data: 0.014 (0.013)
Train: 229 [ 500/1251 ( 40%)]  Loss:  2.973217 (2.9679)  Time: 1.078s,  949.89/s  (1.089s,  940.43/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [ 550/1251 ( 44%)]  Loss:  3.162045 (2.9841)  Time: 1.083s,  945.79/s  (1.089s,  940.69/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 600/1251 ( 48%)]  Loss:  3.122249 (2.9947)  Time: 1.076s,  951.48/s  (1.089s,  940.43/s)  LR: 1.406e-04  Data: 0.014 (0.013)
Train: 229 [ 650/1251 ( 52%)]  Loss:  2.993679 (2.9946)  Time: 1.077s,  951.05/s  (1.089s,  940.29/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [ 700/1251 ( 56%)]  Loss:  2.971377 (2.9931)  Time: 1.077s,  950.99/s  (1.090s,  939.79/s)  LR: 1.406e-04  Data: 0.015 (0.013)
Train: 229 [ 750/1251 ( 60%)]  Loss:  3.226633 (3.0077)  Time: 1.092s,  937.79/s  (1.089s,  940.06/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [ 800/1251 ( 64%)]  Loss:  2.927380 (3.0029)  Time: 1.078s,  949.63/s  (1.090s,  939.60/s)  LR: 1.406e-04  Data: 0.013 (0.013)
Train: 229 [ 850/1251 ( 68%)]  Loss:  2.801755 (2.9918)  Time: 1.081s,  946.93/s  (1.089s,  939.89/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 229 [ 900/1251 ( 72%)]  Loss:  2.928273 (2.9884)  Time: 1.095s,  935.47/s  (1.089s,  939.99/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [ 950/1251 ( 76%)]  Loss:  2.870868 (2.9825)  Time: 1.096s,  933.89/s  (1.090s,  939.70/s)  LR: 1.406e-04  Data: 0.011 (0.013)
Train: 229 [1000/1251 ( 80%)]  Loss:  2.926496 (2.9799)  Time: 1.097s,  933.35/s  (1.090s,  939.49/s)  LR: 1.406e-04  Data: 0.013 (0.013)
Train: 229 [1050/1251 ( 84%)]  Loss:  2.826376 (2.9729)  Time: 1.077s,  950.57/s  (1.090s,  939.64/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [1100/1251 ( 88%)]  Loss:  3.306631 (2.9874)  Time: 1.097s,  933.76/s  (1.090s,  939.53/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [1150/1251 ( 92%)]  Loss:  2.841697 (2.9813)  Time: 1.095s,  934.87/s  (1.090s,  939.82/s)  LR: 1.406e-04  Data: 0.012 (0.013)
Train: 229 [1200/1251 ( 96%)]  Loss:  2.678936 (2.9692)  Time: 1.082s,  946.13/s  (1.090s,  939.86/s)  LR: 1.406e-04  Data: 0.013 (0.013)
Train: 229 [1250/1251 (100%)]  Loss:  3.182717 (2.9774)  Time: 1.068s,  958.39/s  (1.090s,  939.88/s)  LR: 1.406e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.766 (5.766)  Loss:  0.4358 (0.4358)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5499 (0.8715)  Acc@1: 87.1462 (79.9340)  Acc@5: 97.8774 (95.2280)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 79.93400010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-221.pth.tar', 79.80000012939453)

Train: 230 [   0/1251 (  0%)]  Loss:  2.906342 (2.9063)  Time: 1.082s,  946.67/s  (1.082s,  946.67/s)  LR: 1.371e-04  Data: 0.022 (0.022)
Train: 230 [  50/1251 (  4%)]  Loss:  3.302799 (3.1046)  Time: 1.075s,  952.19/s  (1.086s,  942.68/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [ 100/1251 (  8%)]  Loss:  3.154736 (3.1213)  Time: 1.081s,  947.42/s  (1.088s,  941.32/s)  LR: 1.371e-04  Data: 0.011 (0.013)
Train: 230 [ 150/1251 ( 12%)]  Loss:  2.874940 (3.0597)  Time: 1.078s,  949.88/s  (1.088s,  941.17/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [ 200/1251 ( 16%)]  Loss:  2.932522 (3.0343)  Time: 1.094s,  935.61/s  (1.089s,  940.29/s)  LR: 1.371e-04  Data: 0.013 (0.013)
Train: 230 [ 250/1251 ( 20%)]  Loss:  2.864616 (3.0060)  Time: 1.081s,  947.47/s  (1.090s,  939.10/s)  LR: 1.371e-04  Data: 0.013 (0.013)
Train: 230 [ 300/1251 ( 24%)]  Loss:  3.061828 (3.0140)  Time: 1.077s,  950.39/s  (1.089s,  939.99/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 350/1251 ( 28%)]  Loss:  2.805465 (2.9879)  Time: 1.094s,  936.02/s  (1.090s,  939.28/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 400/1251 ( 32%)]  Loss:  3.074248 (2.9975)  Time: 1.098s,  932.19/s  (1.090s,  939.61/s)  LR: 1.371e-04  Data: 0.011 (0.012)
Train: 230 [ 450/1251 ( 36%)]  Loss:  2.931036 (2.9909)  Time: 1.094s,  936.17/s  (1.090s,  939.31/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 500/1251 ( 40%)]  Loss:  2.811823 (2.9746)  Time: 1.105s,  926.38/s  (1.090s,  939.31/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 550/1251 ( 44%)]  Loss:  3.115119 (2.9863)  Time: 1.079s,  949.40/s  (1.091s,  938.97/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 600/1251 ( 48%)]  Loss:  3.017521 (2.9887)  Time: 1.076s,  951.54/s  (1.090s,  939.50/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 650/1251 ( 52%)]  Loss:  2.920378 (2.9838)  Time: 1.078s,  950.09/s  (1.089s,  940.13/s)  LR: 1.371e-04  Data: 0.015 (0.012)
Train: 230 [ 700/1251 ( 56%)]  Loss:  3.100943 (2.9916)  Time: 1.093s,  936.60/s  (1.089s,  940.20/s)  LR: 1.371e-04  Data: 0.013 (0.012)
Train: 230 [ 750/1251 ( 60%)]  Loss:  3.166714 (3.0026)  Time: 1.083s,  945.55/s  (1.089s,  940.22/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 800/1251 ( 64%)]  Loss:  3.143606 (3.0109)  Time: 1.093s,  937.19/s  (1.089s,  940.34/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [ 850/1251 ( 68%)]  Loss:  3.101998 (3.0159)  Time: 1.084s,  944.38/s  (1.089s,  940.19/s)  LR: 1.371e-04  Data: 0.013 (0.012)
Train: 230 [ 900/1251 ( 72%)]  Loss:  3.054961 (3.0180)  Time: 1.074s,  953.48/s  (1.089s,  940.03/s)  LR: 1.371e-04  Data: 0.011 (0.012)
Train: 230 [ 950/1251 ( 76%)]  Loss:  3.182447 (3.0262)  Time: 1.079s,  949.46/s  (1.089s,  940.14/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [1000/1251 ( 80%)]  Loss:  2.754174 (3.0132)  Time: 1.073s,  954.00/s  (1.089s,  940.35/s)  LR: 1.371e-04  Data: 0.012 (0.013)
Train: 230 [1050/1251 ( 84%)]  Loss:  3.111287 (3.0177)  Time: 1.078s,  949.59/s  (1.089s,  940.42/s)  LR: 1.371e-04  Data: 0.013 (0.013)
Train: 230 [1100/1251 ( 88%)]  Loss:  2.994950 (3.0167)  Time: 1.080s,  947.93/s  (1.089s,  939.98/s)  LR: 1.371e-04  Data: 0.015 (0.012)
Train: 230 [1150/1251 ( 92%)]  Loss:  3.005616 (3.0163)  Time: 1.098s,  932.99/s  (1.089s,  940.03/s)  LR: 1.371e-04  Data: 0.012 (0.012)
Train: 230 [1200/1251 ( 96%)]  Loss:  3.079774 (3.0188)  Time: 1.088s,  941.40/s  (1.089s,  940.05/s)  LR: 1.371e-04  Data: 0.018 (0.013)
Train: 230 [1250/1251 (100%)]  Loss:  3.311369 (3.0300)  Time: 1.106s,  925.54/s  (1.090s,  939.85/s)  LR: 1.371e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.865 (5.865)  Loss:  0.4281 (0.4281)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5414 (0.8707)  Acc@1: 87.2642 (80.1400)  Acc@5: 98.2311 (95.2860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 79.93400010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-218.pth.tar', 79.8560000805664)

Train: 231 [   0/1251 (  0%)]  Loss:  3.245948 (3.2459)  Time: 1.086s,  943.00/s  (1.086s,  943.00/s)  LR: 1.337e-04  Data: 0.023 (0.023)
Train: 231 [  50/1251 (  4%)]  Loss:  2.787388 (3.0167)  Time: 1.077s,  950.98/s  (1.096s,  934.15/s)  LR: 1.337e-04  Data: 0.014 (0.013)
Train: 231 [ 100/1251 (  8%)]  Loss:  2.924853 (2.9861)  Time: 1.076s,  951.57/s  (1.093s,  936.97/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 150/1251 ( 12%)]  Loss:  2.769461 (2.9319)  Time: 1.175s,  871.13/s  (1.091s,  938.22/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [ 200/1251 ( 16%)]  Loss:  3.068492 (2.9592)  Time: 1.095s,  934.98/s  (1.090s,  939.42/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 250/1251 ( 20%)]  Loss:  3.073014 (2.9782)  Time: 1.080s,  948.22/s  (1.090s,  939.19/s)  LR: 1.337e-04  Data: 0.015 (0.013)
Train: 231 [ 300/1251 ( 24%)]  Loss:  2.884974 (2.9649)  Time: 1.096s,  934.58/s  (1.091s,  938.88/s)  LR: 1.337e-04  Data: 0.014 (0.013)
Train: 231 [ 350/1251 ( 28%)]  Loss:  3.180992 (2.9919)  Time: 1.120s,  914.49/s  (1.090s,  939.14/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 400/1251 ( 32%)]  Loss:  3.127917 (3.0070)  Time: 1.096s,  934.41/s  (1.090s,  939.83/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 450/1251 ( 36%)]  Loss:  2.887440 (2.9950)  Time: 1.077s,  950.57/s  (1.090s,  939.55/s)  LR: 1.337e-04  Data: 0.016 (0.013)
Train: 231 [ 500/1251 ( 40%)]  Loss:  2.991068 (2.9947)  Time: 1.105s,  926.52/s  (1.091s,  938.92/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 550/1251 ( 44%)]  Loss:  2.951612 (2.9911)  Time: 1.079s,  949.36/s  (1.090s,  939.29/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 600/1251 ( 48%)]  Loss:  2.807515 (2.9770)  Time: 1.097s,  933.33/s  (1.090s,  939.67/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 650/1251 ( 52%)]  Loss:  3.038589 (2.9814)  Time: 1.097s,  933.20/s  (1.090s,  939.62/s)  LR: 1.337e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 231 [ 700/1251 ( 56%)]  Loss:  3.170371 (2.9940)  Time: 1.083s,  945.58/s  (1.090s,  939.88/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 750/1251 ( 60%)]  Loss:  3.238926 (3.0093)  Time: 1.102s,  929.41/s  (1.090s,  939.65/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 800/1251 ( 64%)]  Loss:  3.160336 (3.0182)  Time: 1.077s,  951.21/s  (1.089s,  939.90/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 850/1251 ( 68%)]  Loss:  2.977731 (3.0159)  Time: 1.173s,  872.85/s  (1.089s,  939.97/s)  LR: 1.337e-04  Data: 0.014 (0.013)
Train: 231 [ 900/1251 ( 72%)]  Loss:  3.088027 (3.0197)  Time: 1.095s,  935.29/s  (1.089s,  939.98/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [ 950/1251 ( 76%)]  Loss:  2.982145 (3.0178)  Time: 1.094s,  936.03/s  (1.090s,  939.56/s)  LR: 1.337e-04  Data: 0.013 (0.013)
Train: 231 [1000/1251 ( 80%)]  Loss:  2.986814 (3.0164)  Time: 1.077s,  950.91/s  (1.090s,  939.73/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [1050/1251 ( 84%)]  Loss:  3.344454 (3.0313)  Time: 1.094s,  936.09/s  (1.090s,  939.63/s)  LR: 1.337e-04  Data: 0.013 (0.013)
Train: 231 [1100/1251 ( 88%)]  Loss:  2.872193 (3.0244)  Time: 1.100s,  931.25/s  (1.090s,  939.71/s)  LR: 1.337e-04  Data: 0.012 (0.013)
Train: 231 [1150/1251 ( 92%)]  Loss:  3.090335 (3.0271)  Time: 1.077s,  950.68/s  (1.090s,  939.60/s)  LR: 1.337e-04  Data: 0.011 (0.013)
Train: 231 [1200/1251 ( 96%)]  Loss:  2.904909 (3.0222)  Time: 1.095s,  935.29/s  (1.090s,  939.68/s)  LR: 1.337e-04  Data: 0.014 (0.013)
Train: 231 [1250/1251 (100%)]  Loss:  3.050580 (3.0233)  Time: 1.080s,  948.08/s  (1.090s,  939.73/s)  LR: 1.337e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.917 (5.917)  Loss:  0.4466 (0.4466)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5442 (0.8673)  Acc@1: 87.6179 (80.2140)  Acc@5: 97.4057 (95.1500)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 79.93400010498047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-217.pth.tar', 79.92999992675782)

Train: 232 [   0/1251 (  0%)]  Loss:  3.271268 (3.2713)  Time: 1.084s,  944.54/s  (1.084s,  944.54/s)  LR: 1.303e-04  Data: 0.024 (0.024)
Train: 232 [  50/1251 (  4%)]  Loss:  3.048396 (3.1598)  Time: 1.096s,  934.73/s  (1.092s,  937.46/s)  LR: 1.303e-04  Data: 0.015 (0.013)
Train: 232 [ 100/1251 (  8%)]  Loss:  2.988865 (3.1028)  Time: 1.078s,  950.07/s  (1.090s,  939.44/s)  LR: 1.303e-04  Data: 0.013 (0.013)
Train: 232 [ 150/1251 ( 12%)]  Loss:  3.042943 (3.0879)  Time: 1.083s,  945.94/s  (1.088s,  941.34/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 200/1251 ( 16%)]  Loss:  3.227066 (3.1157)  Time: 1.093s,  937.25/s  (1.086s,  942.58/s)  LR: 1.303e-04  Data: 0.014 (0.013)
Train: 232 [ 250/1251 ( 20%)]  Loss:  2.703830 (3.0471)  Time: 1.094s,  935.84/s  (1.086s,  943.31/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 300/1251 ( 24%)]  Loss:  2.922612 (3.0293)  Time: 1.096s,  934.59/s  (1.086s,  942.98/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [ 350/1251 ( 28%)]  Loss:  2.657309 (2.9828)  Time: 1.172s,  873.60/s  (1.086s,  942.80/s)  LR: 1.303e-04  Data: 0.014 (0.013)
Train: 232 [ 400/1251 ( 32%)]  Loss:  2.997705 (2.9844)  Time: 1.096s,  933.93/s  (1.086s,  942.62/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 450/1251 ( 36%)]  Loss:  3.351089 (3.0211)  Time: 1.097s,  933.73/s  (1.086s,  943.03/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 500/1251 ( 40%)]  Loss:  2.953693 (3.0150)  Time: 1.079s,  948.93/s  (1.085s,  943.49/s)  LR: 1.303e-04  Data: 0.013 (0.013)
Train: 232 [ 550/1251 ( 44%)]  Loss:  3.069886 (3.0196)  Time: 1.084s,  944.84/s  (1.085s,  943.44/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 600/1251 ( 48%)]  Loss:  2.928538 (3.0126)  Time: 1.093s,  936.52/s  (1.086s,  942.97/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 650/1251 ( 52%)]  Loss:  2.770997 (2.9953)  Time: 1.077s,  950.44/s  (1.086s,  942.48/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 700/1251 ( 56%)]  Loss:  3.286652 (3.0147)  Time: 1.094s,  935.62/s  (1.086s,  942.67/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 750/1251 ( 60%)]  Loss:  3.047375 (3.0168)  Time: 1.080s,  948.27/s  (1.086s,  942.66/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 800/1251 ( 64%)]  Loss:  3.346863 (3.0362)  Time: 1.081s,  947.56/s  (1.087s,  942.39/s)  LR: 1.303e-04  Data: 0.015 (0.013)
Train: 232 [ 850/1251 ( 68%)]  Loss:  3.223587 (3.0466)  Time: 1.077s,  950.77/s  (1.087s,  942.46/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [ 900/1251 ( 72%)]  Loss:  2.706452 (3.0287)  Time: 1.077s,  950.99/s  (1.087s,  942.39/s)  LR: 1.303e-04  Data: 0.013 (0.013)
Train: 232 [ 950/1251 ( 76%)]  Loss:  3.090861 (3.0318)  Time: 1.104s,  927.17/s  (1.087s,  942.22/s)  LR: 1.303e-04  Data: 0.011 (0.013)
Train: 232 [1000/1251 ( 80%)]  Loss:  3.130034 (3.0365)  Time: 1.086s,  943.07/s  (1.087s,  942.10/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [1050/1251 ( 84%)]  Loss:  3.154937 (3.0419)  Time: 1.084s,  944.94/s  (1.087s,  942.26/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [1100/1251 ( 88%)]  Loss:  3.290415 (3.0527)  Time: 1.096s,  934.14/s  (1.087s,  942.22/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [1150/1251 ( 92%)]  Loss:  3.321501 (3.0639)  Time: 1.091s,  938.52/s  (1.087s,  942.08/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [1200/1251 ( 96%)]  Loss:  3.062350 (3.0638)  Time: 1.081s,  947.30/s  (1.087s,  942.11/s)  LR: 1.303e-04  Data: 0.012 (0.013)
Train: 232 [1250/1251 (100%)]  Loss:  3.010585 (3.0618)  Time: 1.091s,  938.40/s  (1.087s,  942.15/s)  LR: 1.303e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.796 (5.796)  Loss:  0.4572 (0.4572)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.5681 (0.8699)  Acc@1: 87.0283 (80.1900)  Acc@5: 97.9953 (95.3660)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-229.pth.tar', 79.93400010498047)

Train: 233 [   0/1251 (  0%)]  Loss:  2.764253 (2.7643)  Time: 1.085s,  943.91/s  (1.085s,  943.91/s)  LR: 1.269e-04  Data: 0.023 (0.023)
Train: 233 [  50/1251 (  4%)]  Loss:  2.638041 (2.7011)  Time: 1.164s,  879.65/s  (1.091s,  938.91/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 100/1251 (  8%)]  Loss:  3.086479 (2.8296)  Time: 1.076s,  951.32/s  (1.091s,  938.82/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [ 150/1251 ( 12%)]  Loss:  2.874282 (2.8408)  Time: 1.080s,  948.17/s  (1.090s,  939.34/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 200/1251 ( 16%)]  Loss:  2.888188 (2.8502)  Time: 1.077s,  950.81/s  (1.089s,  939.90/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 250/1251 ( 20%)]  Loss:  3.149735 (2.9002)  Time: 1.077s,  951.14/s  (1.089s,  939.89/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 300/1251 ( 24%)]  Loss:  2.945679 (2.9067)  Time: 1.098s,  932.96/s  (1.091s,  938.62/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [ 350/1251 ( 28%)]  Loss:  2.709737 (2.8820)  Time: 1.096s,  934.46/s  (1.091s,  938.48/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [ 400/1251 ( 32%)]  Loss:  3.054135 (2.9012)  Time: 1.077s,  950.49/s  (1.091s,  938.64/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 450/1251 ( 36%)]  Loss:  3.168807 (2.9279)  Time: 1.102s,  929.07/s  (1.091s,  938.71/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 500/1251 ( 40%)]  Loss:  2.992016 (2.9338)  Time: 1.079s,  948.81/s  (1.091s,  939.01/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 550/1251 ( 44%)]  Loss:  3.263538 (2.9612)  Time: 1.096s,  934.09/s  (1.090s,  939.23/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 600/1251 ( 48%)]  Loss:  2.952056 (2.9605)  Time: 1.081s,  947.63/s  (1.091s,  938.81/s)  LR: 1.269e-04  Data: 0.018 (0.013)
Train: 233 [ 650/1251 ( 52%)]  Loss:  3.168306 (2.9754)  Time: 1.079s,  949.44/s  (1.091s,  938.57/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 700/1251 ( 56%)]  Loss:  2.954469 (2.9740)  Time: 1.080s,  947.84/s  (1.091s,  938.88/s)  LR: 1.269e-04  Data: 0.017 (0.013)
Train: 233 [ 750/1251 ( 60%)]  Loss:  3.170409 (2.9863)  Time: 1.097s,  933.71/s  (1.091s,  938.87/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 800/1251 ( 64%)]  Loss:  2.892146 (2.9807)  Time: 1.096s,  934.70/s  (1.091s,  938.56/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 850/1251 ( 68%)]  Loss:  2.850932 (2.9735)  Time: 1.086s,  942.53/s  (1.091s,  938.66/s)  LR: 1.269e-04  Data: 0.013 (0.013)
Train: 233 [ 900/1251 ( 72%)]  Loss:  3.277520 (2.9895)  Time: 1.083s,  945.36/s  (1.091s,  939.01/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [ 950/1251 ( 76%)]  Loss:  3.110958 (2.9956)  Time: 1.167s,  877.64/s  (1.091s,  938.86/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [1000/1251 ( 80%)]  Loss:  3.305422 (3.0103)  Time: 1.088s,  941.42/s  (1.090s,  939.06/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [1050/1251 ( 84%)]  Loss:  3.113442 (3.0150)  Time: 1.078s,  950.07/s  (1.090s,  939.46/s)  LR: 1.269e-04  Data: 0.013 (0.013)
Train: 233 [1100/1251 ( 88%)]  Loss:  2.938751 (3.0117)  Time: 1.077s,  950.78/s  (1.090s,  939.55/s)  LR: 1.269e-04  Data: 0.014 (0.013)
Train: 233 [1150/1251 ( 92%)]  Loss:  2.793903 (3.0026)  Time: 1.075s,  952.25/s  (1.090s,  939.65/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [1200/1251 ( 96%)]  Loss:  3.140043 (3.0081)  Time: 1.095s,  935.31/s  (1.089s,  939.91/s)  LR: 1.269e-04  Data: 0.012 (0.013)
Train: 233 [1250/1251 (100%)]  Loss:  2.928353 (3.0051)  Time: 1.078s,  950.22/s  (1.090s,  939.48/s)  LR: 1.269e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.896 (5.896)  Loss:  0.4324 (0.4324)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.5427 (0.8589)  Acc@1: 87.7358 (80.3280)  Acc@5: 98.2311 (95.3620)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-224.pth.tar', 79.93800002685546)

Train: 234 [   0/1251 (  0%)]  Loss:  3.039638 (3.0396)  Time: 1.082s,  946.17/s  (1.082s,  946.17/s)  LR: 1.236e-04  Data: 0.022 (0.022)
Train: 234 [  50/1251 (  4%)]  Loss:  3.073780 (3.0567)  Time: 1.079s,  949.13/s  (1.086s,  943.03/s)  LR: 1.236e-04  Data: 0.012 (0.013)
Train: 234 [ 100/1251 (  8%)]  Loss:  3.025909 (3.0464)  Time: 1.096s,  934.48/s  (1.089s,  940.08/s)  LR: 1.236e-04  Data: 0.012 (0.013)
Train: 234 [ 150/1251 ( 12%)]  Loss:  2.901211 (3.0101)  Time: 1.086s,  943.24/s  (1.090s,  939.51/s)  LR: 1.236e-04  Data: 0.012 (0.013)
Train: 234 [ 200/1251 ( 16%)]  Loss:  3.278476 (3.0638)  Time: 1.076s,  951.56/s  (1.089s,  940.41/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 250/1251 ( 20%)]  Loss:  2.957904 (3.0462)  Time: 1.095s,  935.08/s  (1.089s,  940.65/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 300/1251 ( 24%)]  Loss:  2.958028 (3.0336)  Time: 1.077s,  951.18/s  (1.089s,  940.12/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 350/1251 ( 28%)]  Loss:  3.111204 (3.0433)  Time: 1.078s,  949.92/s  (1.088s,  941.11/s)  LR: 1.236e-04  Data: 0.013 (0.012)
Train: 234 [ 400/1251 ( 32%)]  Loss:  3.286406 (3.0703)  Time: 1.077s,  950.40/s  (1.088s,  941.55/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 450/1251 ( 36%)]  Loss:  3.043399 (3.0676)  Time: 1.081s,  947.54/s  (1.088s,  941.23/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 500/1251 ( 40%)]  Loss:  2.799425 (3.0432)  Time: 1.077s,  951.16/s  (1.088s,  941.32/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 550/1251 ( 44%)]  Loss:  3.407400 (3.0736)  Time: 1.081s,  947.10/s  (1.087s,  941.71/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 600/1251 ( 48%)]  Loss:  3.158526 (3.0801)  Time: 1.078s,  949.93/s  (1.088s,  941.17/s)  LR: 1.236e-04  Data: 0.011 (0.012)
Train: 234 [ 650/1251 ( 52%)]  Loss:  3.036734 (3.0770)  Time: 1.077s,  950.41/s  (1.088s,  941.16/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 700/1251 ( 56%)]  Loss:  3.068728 (3.0765)  Time: 1.096s,  934.42/s  (1.088s,  941.37/s)  LR: 1.236e-04  Data: 0.011 (0.012)
Train: 234 [ 750/1251 ( 60%)]  Loss:  3.018405 (3.0728)  Time: 1.097s,  933.68/s  (1.088s,  941.03/s)  LR: 1.236e-04  Data: 0.011 (0.012)
Train: 234 [ 800/1251 ( 64%)]  Loss:  3.143159 (3.0770)  Time: 1.082s,  946.20/s  (1.089s,  940.62/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 850/1251 ( 68%)]  Loss:  3.220340 (3.0849)  Time: 1.084s,  945.05/s  (1.089s,  940.63/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [ 900/1251 ( 72%)]  Loss:  3.097404 (3.0856)  Time: 1.077s,  950.74/s  (1.089s,  940.63/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 234 [ 950/1251 ( 76%)]  Loss:  3.160464 (3.0893)  Time: 1.163s,  880.18/s  (1.089s,  940.62/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [1000/1251 ( 80%)]  Loss:  3.178253 (3.0936)  Time: 1.094s,  936.31/s  (1.089s,  940.65/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [1050/1251 ( 84%)]  Loss:  3.219617 (3.0993)  Time: 1.082s,  945.97/s  (1.088s,  940.77/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [1100/1251 ( 88%)]  Loss:  2.733341 (3.0834)  Time: 1.100s,  930.83/s  (1.089s,  940.53/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [1150/1251 ( 92%)]  Loss:  2.996329 (3.0798)  Time: 1.079s,  949.38/s  (1.089s,  940.36/s)  LR: 1.236e-04  Data: 0.015 (0.012)
Train: 234 [1200/1251 ( 96%)]  Loss:  2.935989 (3.0740)  Time: 1.083s,  945.35/s  (1.089s,  940.43/s)  LR: 1.236e-04  Data: 0.012 (0.012)
Train: 234 [1250/1251 (100%)]  Loss:  3.225817 (3.0798)  Time: 1.063s,  962.92/s  (1.089s,  940.47/s)  LR: 1.236e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.758 (5.758)  Loss:  0.4169 (0.4169)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5264 (0.8617)  Acc@1: 87.8538 (80.3460)  Acc@5: 97.8774 (95.2720)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-228.pth.tar', 79.94400005859374)

Train: 235 [   0/1251 (  0%)]  Loss:  3.057523 (3.0575)  Time: 1.087s,  941.85/s  (1.087s,  941.85/s)  LR: 1.203e-04  Data: 0.024 (0.024)
Train: 235 [  50/1251 (  4%)]  Loss:  2.916643 (2.9871)  Time: 1.094s,  935.67/s  (1.091s,  938.23/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 100/1251 (  8%)]  Loss:  3.161429 (3.0452)  Time: 1.082s,  946.04/s  (1.086s,  942.87/s)  LR: 1.203e-04  Data: 0.015 (0.013)
Train: 235 [ 150/1251 ( 12%)]  Loss:  3.109515 (3.0613)  Time: 1.076s,  951.59/s  (1.086s,  943.21/s)  LR: 1.203e-04  Data: 0.014 (0.013)
Train: 235 [ 200/1251 ( 16%)]  Loss:  3.280422 (3.1051)  Time: 1.076s,  951.36/s  (1.086s,  942.95/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 250/1251 ( 20%)]  Loss:  2.869868 (3.0659)  Time: 1.075s,  952.13/s  (1.086s,  943.12/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 300/1251 ( 24%)]  Loss:  3.186398 (3.0831)  Time: 1.077s,  951.10/s  (1.087s,  941.98/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 350/1251 ( 28%)]  Loss:  2.892508 (3.0593)  Time: 1.080s,  948.25/s  (1.088s,  941.29/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 400/1251 ( 32%)]  Loss:  2.877795 (3.0391)  Time: 1.077s,  950.59/s  (1.088s,  941.54/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 450/1251 ( 36%)]  Loss:  2.852141 (3.0204)  Time: 1.105s,  926.74/s  (1.087s,  941.65/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 500/1251 ( 40%)]  Loss:  3.072884 (3.0252)  Time: 1.105s,  926.89/s  (1.088s,  940.89/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 550/1251 ( 44%)]  Loss:  3.375559 (3.0544)  Time: 1.106s,  925.64/s  (1.089s,  940.46/s)  LR: 1.203e-04  Data: 0.014 (0.013)
Train: 235 [ 600/1251 ( 48%)]  Loss:  3.401278 (3.0811)  Time: 1.081s,  947.54/s  (1.089s,  940.68/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 650/1251 ( 52%)]  Loss:  2.989563 (3.0745)  Time: 1.095s,  934.81/s  (1.088s,  941.05/s)  LR: 1.203e-04  Data: 0.011 (0.013)
Train: 235 [ 700/1251 ( 56%)]  Loss:  2.863695 (3.0605)  Time: 1.097s,  933.48/s  (1.089s,  940.55/s)  LR: 1.203e-04  Data: 0.012 (0.013)
Train: 235 [ 750/1251 ( 60%)]  Loss:  3.228789 (3.0710)  Time: 1.096s,  934.31/s  (1.089s,  940.49/s)  LR: 1.203e-04  Data: 0.014 (0.013)
Train: 235 [ 800/1251 ( 64%)]  Loss:  2.898682 (3.0609)  Time: 1.094s,  935.63/s  (1.089s,  939.99/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [ 850/1251 ( 68%)]  Loss:  3.118884 (3.0641)  Time: 1.100s,  930.93/s  (1.090s,  939.53/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [ 900/1251 ( 72%)]  Loss:  2.964919 (3.0589)  Time: 1.078s,  950.10/s  (1.090s,  939.22/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [ 950/1251 ( 76%)]  Loss:  2.866768 (3.0493)  Time: 1.081s,  947.39/s  (1.090s,  939.29/s)  LR: 1.203e-04  Data: 0.014 (0.012)
Train: 235 [1000/1251 ( 80%)]  Loss:  3.102607 (3.0518)  Time: 1.076s,  951.81/s  (1.090s,  939.19/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [1050/1251 ( 84%)]  Loss:  3.216309 (3.0593)  Time: 1.095s,  935.15/s  (1.090s,  939.37/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [1100/1251 ( 88%)]  Loss:  3.096303 (3.0609)  Time: 1.096s,  934.40/s  (1.090s,  939.35/s)  LR: 1.203e-04  Data: 0.014 (0.012)
Train: 235 [1150/1251 ( 92%)]  Loss:  2.843238 (3.0518)  Time: 1.096s,  934.26/s  (1.090s,  939.06/s)  LR: 1.203e-04  Data: 0.012 (0.012)
Train: 235 [1200/1251 ( 96%)]  Loss:  3.035595 (3.0512)  Time: 1.079s,  949.00/s  (1.090s,  939.18/s)  LR: 1.203e-04  Data: 0.014 (0.012)
Train: 235 [1250/1251 (100%)]  Loss:  3.071820 (3.0520)  Time: 1.078s,  950.34/s  (1.090s,  939.12/s)  LR: 1.203e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.800 (5.800)  Loss:  0.4329 (0.4329)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5456 (0.8736)  Acc@1: 87.0283 (80.3160)  Acc@5: 97.9953 (95.3480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-223.pth.tar', 79.9620000024414)

Train: 236 [   0/1251 (  0%)]  Loss:  2.812438 (2.8124)  Time: 1.084s,  944.31/s  (1.084s,  944.31/s)  LR: 1.171e-04  Data: 0.023 (0.023)
Train: 236 [  50/1251 (  4%)]  Loss:  3.015082 (2.9138)  Time: 1.094s,  935.95/s  (1.093s,  937.18/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 100/1251 (  8%)]  Loss:  2.855141 (2.8942)  Time: 1.076s,  951.30/s  (1.091s,  938.78/s)  LR: 1.171e-04  Data: 0.012 (0.012)
Train: 236 [ 150/1251 ( 12%)]  Loss:  2.916061 (2.8997)  Time: 1.102s,  929.09/s  (1.089s,  940.04/s)  LR: 1.171e-04  Data: 0.012 (0.012)
Train: 236 [ 200/1251 ( 16%)]  Loss:  3.116089 (2.9430)  Time: 1.097s,  933.04/s  (1.090s,  939.06/s)  LR: 1.171e-04  Data: 0.014 (0.013)
Train: 236 [ 250/1251 ( 20%)]  Loss:  3.139817 (2.9758)  Time: 1.105s,  927.03/s  (1.091s,  938.48/s)  LR: 1.171e-04  Data: 0.020 (0.013)
Train: 236 [ 300/1251 ( 24%)]  Loss:  3.226740 (3.0116)  Time: 1.095s,  935.19/s  (1.092s,  937.93/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 350/1251 ( 28%)]  Loss:  2.984756 (3.0083)  Time: 1.077s,  950.49/s  (1.091s,  938.87/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 400/1251 ( 32%)]  Loss:  2.818421 (2.9872)  Time: 1.095s,  934.87/s  (1.090s,  939.12/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 236 [ 450/1251 ( 36%)]  Loss:  2.786972 (2.9672)  Time: 1.103s,  927.98/s  (1.091s,  938.93/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 500/1251 ( 40%)]  Loss:  2.701315 (2.9430)  Time: 1.076s,  951.50/s  (1.090s,  939.84/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 550/1251 ( 44%)]  Loss:  2.784496 (2.9298)  Time: 1.080s,  948.05/s  (1.090s,  939.76/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 600/1251 ( 48%)]  Loss:  3.041264 (2.9384)  Time: 1.088s,  941.41/s  (1.090s,  939.35/s)  LR: 1.171e-04  Data: 0.014 (0.013)
Train: 236 [ 650/1251 ( 52%)]  Loss:  2.862216 (2.9329)  Time: 1.080s,  948.24/s  (1.090s,  939.28/s)  LR: 1.171e-04  Data: 0.014 (0.013)
Train: 236 [ 700/1251 ( 56%)]  Loss:  3.107369 (2.9445)  Time: 1.095s,  935.48/s  (1.090s,  939.82/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 750/1251 ( 60%)]  Loss:  3.226140 (2.9621)  Time: 1.094s,  935.65/s  (1.089s,  939.99/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 800/1251 ( 64%)]  Loss:  3.120468 (2.9715)  Time: 1.102s,  929.07/s  (1.090s,  939.83/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 850/1251 ( 68%)]  Loss:  2.926931 (2.9690)  Time: 1.094s,  936.21/s  (1.090s,  939.83/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [ 900/1251 ( 72%)]  Loss:  3.114781 (2.9767)  Time: 1.077s,  950.79/s  (1.090s,  939.82/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 236 [ 950/1251 ( 76%)]  Loss:  3.020998 (2.9789)  Time: 1.078s,  949.84/s  (1.089s,  940.13/s)  LR: 1.171e-04  Data: 0.013 (0.013)
Train: 236 [1000/1251 ( 80%)]  Loss:  3.025333 (2.9811)  Time: 1.085s,  943.49/s  (1.089s,  940.37/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [1050/1251 ( 84%)]  Loss:  3.173638 (2.9898)  Time: 1.107s,  924.97/s  (1.089s,  940.16/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [1100/1251 ( 88%)]  Loss:  3.107261 (2.9949)  Time: 1.097s,  933.59/s  (1.089s,  939.89/s)  LR: 1.171e-04  Data: 0.013 (0.013)
Train: 236 [1150/1251 ( 92%)]  Loss:  2.809848 (2.9872)  Time: 1.077s,  950.43/s  (1.090s,  939.78/s)  LR: 1.171e-04  Data: 0.014 (0.013)
Train: 236 [1200/1251 ( 96%)]  Loss:  3.328085 (3.0009)  Time: 1.099s,  931.69/s  (1.090s,  939.79/s)  LR: 1.171e-04  Data: 0.012 (0.013)
Train: 236 [1250/1251 (100%)]  Loss:  2.967523 (2.9996)  Time: 1.059s,  966.83/s  (1.090s,  939.66/s)  LR: 1.171e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.824 (5.824)  Loss:  0.4184 (0.4184)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5423 (0.8589)  Acc@1: 86.7925 (80.2960)  Acc@5: 97.7594 (95.3620)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-227.pth.tar', 79.99599989990234)

Train: 237 [   0/1251 (  0%)]  Loss:  3.237561 (3.2376)  Time: 1.091s,  938.33/s  (1.091s,  938.33/s)  LR: 1.139e-04  Data: 0.029 (0.029)
Train: 237 [  50/1251 (  4%)]  Loss:  2.952775 (3.0952)  Time: 1.095s,  935.29/s  (1.093s,  936.64/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 100/1251 (  8%)]  Loss:  3.118379 (3.1029)  Time: 1.077s,  950.64/s  (1.092s,  937.32/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 150/1251 ( 12%)]  Loss:  2.855364 (3.0410)  Time: 1.073s,  954.75/s  (1.090s,  939.51/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 200/1251 ( 16%)]  Loss:  2.836518 (3.0001)  Time: 1.094s,  935.98/s  (1.090s,  939.43/s)  LR: 1.139e-04  Data: 0.013 (0.013)
Train: 237 [ 250/1251 ( 20%)]  Loss:  3.044074 (3.0074)  Time: 1.095s,  935.39/s  (1.091s,  938.32/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 300/1251 ( 24%)]  Loss:  3.272480 (3.0453)  Time: 1.093s,  937.19/s  (1.091s,  938.58/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 350/1251 ( 28%)]  Loss:  2.903433 (3.0276)  Time: 1.093s,  936.45/s  (1.092s,  937.84/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 400/1251 ( 32%)]  Loss:  2.985770 (3.0229)  Time: 1.079s,  949.40/s  (1.091s,  938.35/s)  LR: 1.139e-04  Data: 0.014 (0.013)
Train: 237 [ 450/1251 ( 36%)]  Loss:  3.368054 (3.0574)  Time: 1.095s,  935.48/s  (1.091s,  938.41/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 500/1251 ( 40%)]  Loss:  2.891547 (3.0424)  Time: 1.094s,  936.35/s  (1.092s,  938.11/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 550/1251 ( 44%)]  Loss:  2.999196 (3.0388)  Time: 1.083s,  945.31/s  (1.092s,  938.10/s)  LR: 1.139e-04  Data: 0.011 (0.013)
Train: 237 [ 600/1251 ( 48%)]  Loss:  3.034092 (3.0384)  Time: 1.078s,  949.84/s  (1.091s,  938.17/s)  LR: 1.139e-04  Data: 0.016 (0.013)
Train: 237 [ 650/1251 ( 52%)]  Loss:  3.119223 (3.0442)  Time: 1.076s,  952.00/s  (1.091s,  938.51/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 700/1251 ( 56%)]  Loss:  3.129711 (3.0499)  Time: 1.095s,  935.10/s  (1.091s,  938.55/s)  LR: 1.139e-04  Data: 0.013 (0.013)
Train: 237 [ 750/1251 ( 60%)]  Loss:  2.831530 (3.0362)  Time: 1.095s,  935.40/s  (1.091s,  938.57/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 800/1251 ( 64%)]  Loss:  2.980930 (3.0330)  Time: 1.104s,  927.45/s  (1.091s,  938.86/s)  LR: 1.139e-04  Data: 0.013 (0.013)
Train: 237 [ 850/1251 ( 68%)]  Loss:  2.930134 (3.0273)  Time: 1.100s,  931.07/s  (1.090s,  939.27/s)  LR: 1.139e-04  Data: 0.014 (0.013)
Train: 237 [ 900/1251 ( 72%)]  Loss:  3.226403 (3.0377)  Time: 1.076s,  952.11/s  (1.090s,  939.32/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [ 950/1251 ( 76%)]  Loss:  2.908117 (3.0313)  Time: 1.076s,  951.79/s  (1.090s,  939.49/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [1000/1251 ( 80%)]  Loss:  2.860505 (3.0231)  Time: 1.084s,  944.26/s  (1.090s,  939.54/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [1050/1251 ( 84%)]  Loss:  2.930207 (3.0189)  Time: 1.175s,  871.31/s  (1.090s,  939.68/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [1100/1251 ( 88%)]  Loss:  2.932141 (3.0151)  Time: 1.078s,  949.70/s  (1.090s,  939.79/s)  LR: 1.139e-04  Data: 0.013 (0.013)
Train: 237 [1150/1251 ( 92%)]  Loss:  2.603424 (2.9980)  Time: 1.077s,  950.88/s  (1.090s,  939.80/s)  LR: 1.139e-04  Data: 0.012 (0.013)
Train: 237 [1200/1251 ( 96%)]  Loss:  3.371558 (3.0129)  Time: 1.088s,  941.55/s  (1.089s,  939.94/s)  LR: 1.139e-04  Data: 0.014 (0.013)
Train: 237 [1250/1251 (100%)]  Loss:  2.934484 (3.0099)  Time: 1.079s,  949.03/s  (1.090s,  939.55/s)  LR: 1.139e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.803 (5.803)  Loss:  0.4248 (0.4248)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5316 (0.8561)  Acc@1: 86.7924 (80.3920)  Acc@5: 98.3491 (95.3260)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-226.pth.tar', 80.01600005126953)

Train: 238 [   0/1251 (  0%)]  Loss:  3.173715 (3.1737)  Time: 1.083s,  945.53/s  (1.083s,  945.53/s)  LR: 1.107e-04  Data: 0.022 (0.022)
Train: 238 [  50/1251 (  4%)]  Loss:  2.970459 (3.0721)  Time: 1.079s,  949.06/s  (1.089s,  940.03/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 100/1251 (  8%)]  Loss:  3.057830 (3.0673)  Time: 1.077s,  951.04/s  (1.088s,  941.09/s)  LR: 1.107e-04  Data: 0.014 (0.013)
Train: 238 [ 150/1251 ( 12%)]  Loss:  2.808256 (3.0026)  Time: 1.094s,  935.76/s  (1.091s,  938.43/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 200/1251 ( 16%)]  Loss:  3.061659 (3.0144)  Time: 1.093s,  937.20/s  (1.093s,  936.71/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [ 250/1251 ( 20%)]  Loss:  2.754178 (2.9710)  Time: 1.097s,  933.46/s  (1.092s,  937.59/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 300/1251 ( 24%)]  Loss:  2.942847 (2.9670)  Time: 1.165s,  879.26/s  (1.093s,  936.60/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 350/1251 ( 28%)]  Loss:  3.132454 (2.9877)  Time: 1.097s,  933.73/s  (1.093s,  936.92/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 400/1251 ( 32%)]  Loss:  2.895385 (2.9774)  Time: 1.083s,  945.48/s  (1.092s,  937.60/s)  LR: 1.107e-04  Data: 0.015 (0.013)
Train: 238 [ 450/1251 ( 36%)]  Loss:  2.741190 (2.9538)  Time: 1.096s,  934.50/s  (1.092s,  938.06/s)  LR: 1.107e-04  Data: 0.013 (0.013)
Train: 238 [ 500/1251 ( 40%)]  Loss:  3.060194 (2.9635)  Time: 1.078s,  949.71/s  (1.092s,  937.52/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 550/1251 ( 44%)]  Loss:  3.198948 (2.9831)  Time: 1.087s,  942.39/s  (1.092s,  938.04/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 600/1251 ( 48%)]  Loss:  2.912171 (2.9776)  Time: 1.106s,  925.94/s  (1.092s,  938.14/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 650/1251 ( 52%)]  Loss:  3.158705 (2.9906)  Time: 1.083s,  945.91/s  (1.092s,  938.05/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 700/1251 ( 56%)]  Loss:  3.041655 (2.9940)  Time: 1.097s,  933.63/s  (1.092s,  937.83/s)  LR: 1.107e-04  Data: 0.013 (0.013)
Train: 238 [ 750/1251 ( 60%)]  Loss:  2.919636 (2.9893)  Time: 1.101s,  929.67/s  (1.092s,  937.77/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [ 800/1251 ( 64%)]  Loss:  3.064394 (2.9937)  Time: 1.116s,  917.55/s  (1.092s,  937.74/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [ 850/1251 ( 68%)]  Loss:  3.089337 (2.9991)  Time: 1.111s,  921.69/s  (1.092s,  938.05/s)  LR: 1.107e-04  Data: 0.017 (0.013)
Train: 238 [ 900/1251 ( 72%)]  Loss:  3.206785 (3.0100)  Time: 1.097s,  933.55/s  (1.091s,  938.43/s)  LR: 1.107e-04  Data: 0.013 (0.013)
Train: 238 [ 950/1251 ( 76%)]  Loss:  3.049705 (3.0120)  Time: 1.093s,  936.44/s  (1.091s,  938.51/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [1000/1251 ( 80%)]  Loss:  2.930462 (3.0081)  Time: 1.079s,  948.99/s  (1.091s,  938.16/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 238 [1050/1251 ( 84%)]  Loss:  2.966633 (3.0062)  Time: 1.077s,  950.62/s  (1.091s,  938.30/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [1100/1251 ( 88%)]  Loss:  3.172114 (3.0134)  Time: 1.081s,  947.50/s  (1.091s,  938.49/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [1150/1251 ( 92%)]  Loss:  3.072751 (3.0159)  Time: 1.100s,  930.84/s  (1.091s,  938.45/s)  LR: 1.107e-04  Data: 0.012 (0.013)
Train: 238 [1200/1251 ( 96%)]  Loss:  3.318763 (3.0280)  Time: 1.091s,  938.72/s  (1.091s,  938.75/s)  LR: 1.107e-04  Data: 0.011 (0.013)
Train: 238 [1250/1251 (100%)]  Loss:  3.254170 (3.0367)  Time: 1.062s,  964.30/s  (1.091s,  938.81/s)  LR: 1.107e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.893 (5.893)  Loss:  0.4363 (0.4363)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5293 (0.8524)  Acc@1: 87.9717 (80.5120)  Acc@5: 98.1132 (95.2720)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-225.pth.tar', 80.0660000732422)

Train: 239 [   0/1251 (  0%)]  Loss:  3.115475 (3.1155)  Time: 1.091s,  938.36/s  (1.091s,  938.36/s)  LR: 1.076e-04  Data: 0.028 (0.028)
Train: 239 [  50/1251 (  4%)]  Loss:  3.059678 (3.0876)  Time: 1.076s,  951.38/s  (1.087s,  941.83/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 100/1251 (  8%)]  Loss:  3.135033 (3.1034)  Time: 1.086s,  942.86/s  (1.089s,  940.01/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 150/1251 ( 12%)]  Loss:  2.909436 (3.0549)  Time: 1.076s,  951.67/s  (1.087s,  941.66/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 200/1251 ( 16%)]  Loss:  2.925282 (3.0290)  Time: 1.096s,  934.58/s  (1.087s,  941.84/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 250/1251 ( 20%)]  Loss:  2.910153 (3.0092)  Time: 1.105s,  926.31/s  (1.088s,  941.06/s)  LR: 1.076e-04  Data: 0.014 (0.013)
Train: 239 [ 300/1251 ( 24%)]  Loss:  2.938910 (2.9991)  Time: 1.079s,  949.41/s  (1.089s,  940.33/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 350/1251 ( 28%)]  Loss:  3.212121 (3.0258)  Time: 1.077s,  950.83/s  (1.088s,  941.15/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 400/1251 ( 32%)]  Loss:  2.757225 (2.9959)  Time: 1.076s,  951.55/s  (1.088s,  940.91/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 450/1251 ( 36%)]  Loss:  3.072416 (3.0036)  Time: 1.103s,  928.22/s  (1.089s,  940.58/s)  LR: 1.076e-04  Data: 0.013 (0.013)
Train: 239 [ 500/1251 ( 40%)]  Loss:  2.923836 (2.9963)  Time: 1.074s,  953.30/s  (1.089s,  940.37/s)  LR: 1.076e-04  Data: 0.011 (0.013)
Train: 239 [ 550/1251 ( 44%)]  Loss:  2.407017 (2.9472)  Time: 1.077s,  950.70/s  (1.089s,  940.72/s)  LR: 1.076e-04  Data: 0.013 (0.013)
Train: 239 [ 600/1251 ( 48%)]  Loss:  3.195400 (2.9663)  Time: 1.077s,  951.22/s  (1.088s,  940.93/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 650/1251 ( 52%)]  Loss:  2.822824 (2.9561)  Time: 1.077s,  950.74/s  (1.088s,  941.37/s)  LR: 1.076e-04  Data: 0.014 (0.013)
Train: 239 [ 700/1251 ( 56%)]  Loss:  2.969630 (2.9570)  Time: 1.084s,  944.74/s  (1.088s,  941.52/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 750/1251 ( 60%)]  Loss:  2.776412 (2.9457)  Time: 1.105s,  926.55/s  (1.087s,  941.84/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 800/1251 ( 64%)]  Loss:  2.840143 (2.9395)  Time: 1.075s,  952.36/s  (1.087s,  941.76/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 850/1251 ( 68%)]  Loss:  3.136851 (2.9504)  Time: 1.078s,  949.64/s  (1.087s,  941.88/s)  LR: 1.076e-04  Data: 0.016 (0.013)
Train: 239 [ 900/1251 ( 72%)]  Loss:  2.878466 (2.9466)  Time: 1.076s,  951.42/s  (1.087s,  942.00/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [ 950/1251 ( 76%)]  Loss:  3.156628 (2.9571)  Time: 1.078s,  950.29/s  (1.087s,  942.03/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1000/1251 ( 80%)]  Loss:  3.202567 (2.9688)  Time: 1.077s,  950.92/s  (1.087s,  941.93/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1050/1251 ( 84%)]  Loss:  3.068893 (2.9734)  Time: 1.103s,  928.69/s  (1.087s,  941.66/s)  LR: 1.076e-04  Data: 0.013 (0.013)
Train: 239 [1100/1251 ( 88%)]  Loss:  3.098686 (2.9788)  Time: 1.107s,  925.17/s  (1.088s,  941.59/s)  LR: 1.076e-04  Data: 0.015 (0.013)
Train: 239 [1150/1251 ( 92%)]  Loss:  3.044265 (2.9816)  Time: 1.098s,  932.30/s  (1.088s,  941.32/s)  LR: 1.076e-04  Data: 0.012 (0.013)
Train: 239 [1200/1251 ( 96%)]  Loss:  3.088359 (2.9858)  Time: 1.096s,  934.71/s  (1.088s,  941.04/s)  LR: 1.076e-04  Data: 0.014 (0.013)
Train: 239 [1250/1251 (100%)]  Loss:  2.977374 (2.9855)  Time: 1.071s,  956.52/s  (1.089s,  940.73/s)  LR: 1.076e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.836 (5.836)  Loss:  0.4143 (0.4143)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5370 (0.8556)  Acc@1: 87.5000 (80.5420)  Acc@5: 97.9953 (95.2800)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-230.pth.tar', 80.14000002685547)

Train: 240 [   0/1251 (  0%)]  Loss:  2.880187 (2.8802)  Time: 1.085s,  943.80/s  (1.085s,  943.80/s)  LR: 1.045e-04  Data: 0.023 (0.023)
Train: 240 [  50/1251 (  4%)]  Loss:  2.835542 (2.8579)  Time: 1.078s,  950.14/s  (1.083s,  945.27/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 100/1251 (  8%)]  Loss:  3.278246 (2.9980)  Time: 1.097s,  933.83/s  (1.088s,  941.00/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 150/1251 ( 12%)]  Loss:  3.135458 (3.0324)  Time: 1.094s,  935.93/s  (1.090s,  939.37/s)  LR: 1.045e-04  Data: 0.013 (0.013)
Train: 240 [ 200/1251 ( 16%)]  Loss:  3.137435 (3.0534)  Time: 1.095s,  935.42/s  (1.090s,  939.56/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 250/1251 ( 20%)]  Loss:  3.186787 (3.0756)  Time: 1.078s,  949.64/s  (1.089s,  940.13/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 300/1251 ( 24%)]  Loss:  3.251852 (3.1008)  Time: 1.092s,  937.85/s  (1.090s,  939.80/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 350/1251 ( 28%)]  Loss:  3.033557 (3.0924)  Time: 1.096s,  934.67/s  (1.089s,  940.00/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 400/1251 ( 32%)]  Loss:  2.810688 (3.0611)  Time: 1.078s,  949.98/s  (1.089s,  939.98/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 450/1251 ( 36%)]  Loss:  3.125164 (3.0675)  Time: 1.081s,  946.98/s  (1.089s,  939.91/s)  LR: 1.045e-04  Data: 0.014 (0.013)
Train: 240 [ 500/1251 ( 40%)]  Loss:  2.613778 (3.0262)  Time: 1.092s,  937.48/s  (1.091s,  938.96/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 550/1251 ( 44%)]  Loss:  2.871699 (3.0134)  Time: 1.094s,  935.77/s  (1.091s,  938.98/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [ 600/1251 ( 48%)]  Loss:  2.919460 (3.0061)  Time: 1.107s,  924.71/s  (1.091s,  938.83/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 650/1251 ( 52%)]  Loss:  3.092588 (3.0123)  Time: 1.077s,  950.68/s  (1.091s,  938.46/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 700/1251 ( 56%)]  Loss:  3.295655 (3.0312)  Time: 1.076s,  951.83/s  (1.091s,  938.66/s)  LR: 1.045e-04  Data: 0.013 (0.013)
Train: 240 [ 750/1251 ( 60%)]  Loss:  2.943378 (3.0257)  Time: 1.094s,  935.64/s  (1.091s,  938.74/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 800/1251 ( 64%)]  Loss:  2.821152 (3.0137)  Time: 1.094s,  935.95/s  (1.091s,  938.96/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 850/1251 ( 68%)]  Loss:  2.949705 (3.0101)  Time: 1.119s,  915.45/s  (1.091s,  938.44/s)  LR: 1.045e-04  Data: 0.014 (0.013)
Train: 240 [ 900/1251 ( 72%)]  Loss:  2.856629 (3.0021)  Time: 1.081s,  947.04/s  (1.091s,  938.42/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [ 950/1251 ( 76%)]  Loss:  2.830464 (2.9935)  Time: 1.105s,  927.00/s  (1.091s,  938.56/s)  LR: 1.045e-04  Data: 0.014 (0.013)
Train: 240 [1000/1251 ( 80%)]  Loss:  2.779675 (2.9833)  Time: 1.087s,  942.20/s  (1.092s,  938.14/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [1050/1251 ( 84%)]  Loss:  2.859480 (2.9777)  Time: 1.096s,  934.64/s  (1.092s,  938.14/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1100/1251 ( 88%)]  Loss:  2.821051 (2.9709)  Time: 1.089s,  940.03/s  (1.091s,  938.41/s)  LR: 1.045e-04  Data: 0.011 (0.013)
Train: 240 [1150/1251 ( 92%)]  Loss:  2.865433 (2.9665)  Time: 1.078s,  950.20/s  (1.091s,  938.80/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [1200/1251 ( 96%)]  Loss:  3.187237 (2.9753)  Time: 1.077s,  950.63/s  (1.091s,  938.91/s)  LR: 1.045e-04  Data: 0.012 (0.013)
Train: 240 [1250/1251 (100%)]  Loss:  2.971183 (2.9751)  Time: 1.064s,  962.16/s  (1.091s,  938.96/s)  LR: 1.045e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.844 (5.844)  Loss:  0.4322 (0.4322)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5243 (0.8569)  Acc@1: 87.6179 (80.4160)  Acc@5: 98.7028 (95.2780)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-232.pth.tar', 80.19000005371093)

Train: 241 [   0/1251 (  0%)]  Loss:  3.087376 (3.0874)  Time: 1.084s,  944.78/s  (1.084s,  944.78/s)  LR: 1.015e-04  Data: 0.023 (0.023)
Train: 241 [  50/1251 (  4%)]  Loss:  3.154140 (3.1208)  Time: 1.086s,  943.33/s  (1.091s,  938.96/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [ 100/1251 (  8%)]  Loss:  2.961005 (3.0675)  Time: 1.104s,  927.52/s  (1.089s,  940.73/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [ 150/1251 ( 12%)]  Loss:  2.951188 (3.0384)  Time: 1.083s,  945.31/s  (1.087s,  942.35/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [ 200/1251 ( 16%)]  Loss:  2.939721 (3.0187)  Time: 1.077s,  951.08/s  (1.087s,  942.32/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [ 250/1251 ( 20%)]  Loss:  2.662517 (2.9593)  Time: 1.079s,  949.16/s  (1.087s,  942.12/s)  LR: 1.015e-04  Data: 0.017 (0.013)
Train: 241 [ 300/1251 ( 24%)]  Loss:  3.348274 (3.0149)  Time: 1.093s,  936.69/s  (1.087s,  942.45/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [ 350/1251 ( 28%)]  Loss:  2.841751 (2.9932)  Time: 1.082s,  946.16/s  (1.087s,  941.75/s)  LR: 1.015e-04  Data: 0.012 (0.012)
Train: 241 [ 400/1251 ( 32%)]  Loss:  3.354136 (3.0333)  Time: 1.088s,  941.06/s  (1.088s,  941.23/s)  LR: 1.015e-04  Data: 0.012 (0.012)
Train: 241 [ 450/1251 ( 36%)]  Loss:  2.823168 (3.0123)  Time: 1.094s,  936.01/s  (1.089s,  940.28/s)  LR: 1.015e-04  Data: 0.011 (0.012)
Train: 241 [ 500/1251 ( 40%)]  Loss:  2.774612 (2.9907)  Time: 1.115s,  917.98/s  (1.090s,  939.58/s)  LR: 1.015e-04  Data: 0.014 (0.013)
Train: 241 [ 550/1251 ( 44%)]  Loss:  2.792596 (2.9742)  Time: 1.075s,  952.23/s  (1.090s,  939.32/s)  LR: 1.015e-04  Data: 0.013 (0.012)
Train: 241 [ 600/1251 ( 48%)]  Loss:  3.010694 (2.9770)  Time: 1.077s,  950.82/s  (1.090s,  939.61/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [ 650/1251 ( 52%)]  Loss:  3.051146 (2.9823)  Time: 1.103s,  928.77/s  (1.090s,  939.84/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [ 700/1251 ( 56%)]  Loss:  3.058356 (2.9874)  Time: 1.082s,  946.05/s  (1.090s,  939.33/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [ 750/1251 ( 60%)]  Loss:  3.099309 (2.9944)  Time: 1.075s,  952.96/s  (1.090s,  939.08/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [ 800/1251 ( 64%)]  Loss:  3.066770 (2.9986)  Time: 1.076s,  951.74/s  (1.091s,  938.80/s)  LR: 1.015e-04  Data: 0.015 (0.013)
Train: 241 [ 850/1251 ( 68%)]  Loss:  3.090963 (3.0038)  Time: 1.095s,  935.56/s  (1.091s,  938.90/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [ 900/1251 ( 72%)]  Loss:  2.788714 (2.9924)  Time: 1.092s,  937.64/s  (1.090s,  939.21/s)  LR: 1.015e-04  Data: 0.014 (0.013)
Train: 241 [ 950/1251 ( 76%)]  Loss:  2.935537 (2.9896)  Time: 1.079s,  949.23/s  (1.090s,  939.18/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1000/1251 ( 80%)]  Loss:  2.880273 (2.9844)  Time: 1.078s,  949.90/s  (1.090s,  939.11/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1050/1251 ( 84%)]  Loss:  3.018636 (2.9859)  Time: 1.077s,  951.00/s  (1.090s,  939.12/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1100/1251 ( 88%)]  Loss:  2.901267 (2.9823)  Time: 1.077s,  950.70/s  (1.090s,  939.22/s)  LR: 1.015e-04  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 241 [1150/1251 ( 92%)]  Loss:  2.954269 (2.9811)  Time: 1.077s,  950.57/s  (1.090s,  939.52/s)  LR: 1.015e-04  Data: 0.013 (0.013)
Train: 241 [1200/1251 ( 96%)]  Loss:  3.027795 (2.9830)  Time: 1.096s,  934.73/s  (1.090s,  939.39/s)  LR: 1.015e-04  Data: 0.012 (0.013)
Train: 241 [1250/1251 (100%)]  Loss:  2.946604 (2.9816)  Time: 1.063s,  963.17/s  (1.090s,  939.24/s)  LR: 1.015e-04  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.809 (5.809)  Loss:  0.4117 (0.4117)  Acc@1: 92.9688 (92.9688)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5206 (0.8438)  Acc@1: 87.0283 (80.5780)  Acc@5: 98.2311 (95.3980)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-231.pth.tar', 80.21400005126954)

Train: 242 [   0/1251 (  0%)]  Loss:  2.994958 (2.9950)  Time: 1.090s,  939.10/s  (1.090s,  939.10/s)  LR: 9.853e-05  Data: 0.026 (0.026)
Train: 242 [  50/1251 (  4%)]  Loss:  2.823091 (2.9090)  Time: 1.094s,  936.36/s  (1.087s,  941.91/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 100/1251 (  8%)]  Loss:  2.906869 (2.9083)  Time: 1.078s,  949.82/s  (1.088s,  941.06/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [ 150/1251 ( 12%)]  Loss:  3.134181 (2.9648)  Time: 1.096s,  934.00/s  (1.088s,  940.78/s)  LR: 9.853e-05  Data: 0.014 (0.013)
Train: 242 [ 200/1251 ( 16%)]  Loss:  3.084480 (2.9887)  Time: 1.079s,  949.14/s  (1.088s,  941.14/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 250/1251 ( 20%)]  Loss:  2.858831 (2.9671)  Time: 1.097s,  933.77/s  (1.088s,  941.27/s)  LR: 9.853e-05  Data: 0.013 (0.013)
Train: 242 [ 300/1251 ( 24%)]  Loss:  2.875644 (2.9540)  Time: 1.078s,  949.51/s  (1.089s,  940.19/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 350/1251 ( 28%)]  Loss:  3.093577 (2.9715)  Time: 1.097s,  933.53/s  (1.089s,  940.22/s)  LR: 9.853e-05  Data: 0.014 (0.013)
Train: 242 [ 400/1251 ( 32%)]  Loss:  2.864745 (2.9596)  Time: 1.096s,  934.27/s  (1.089s,  940.72/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 450/1251 ( 36%)]  Loss:  3.028109 (2.9664)  Time: 1.077s,  950.99/s  (1.088s,  941.21/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 500/1251 ( 40%)]  Loss:  3.057853 (2.9748)  Time: 1.083s,  945.73/s  (1.088s,  940.78/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 550/1251 ( 44%)]  Loss:  3.378589 (3.0084)  Time: 1.095s,  934.87/s  (1.088s,  940.98/s)  LR: 9.853e-05  Data: 0.013 (0.013)
Train: 242 [ 600/1251 ( 48%)]  Loss:  3.002280 (3.0079)  Time: 1.076s,  952.11/s  (1.088s,  941.08/s)  LR: 9.853e-05  Data: 0.013 (0.013)
Train: 242 [ 650/1251 ( 52%)]  Loss:  2.935163 (3.0027)  Time: 1.077s,  950.46/s  (1.088s,  941.06/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 700/1251 ( 56%)]  Loss:  2.838383 (2.9918)  Time: 1.082s,  946.25/s  (1.089s,  940.59/s)  LR: 9.853e-05  Data: 0.014 (0.013)
Train: 242 [ 750/1251 ( 60%)]  Loss:  2.887669 (2.9853)  Time: 1.076s,  951.65/s  (1.089s,  940.68/s)  LR: 9.853e-05  Data: 0.014 (0.013)
Train: 242 [ 800/1251 ( 64%)]  Loss:  3.042020 (2.9886)  Time: 1.078s,  949.91/s  (1.088s,  940.95/s)  LR: 9.853e-05  Data: 0.015 (0.013)
Train: 242 [ 850/1251 ( 68%)]  Loss:  3.161047 (2.9982)  Time: 1.095s,  935.47/s  (1.088s,  940.87/s)  LR: 9.853e-05  Data: 0.014 (0.013)
Train: 242 [ 900/1251 ( 72%)]  Loss:  2.899704 (2.9930)  Time: 1.076s,  951.97/s  (1.088s,  940.88/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [ 950/1251 ( 76%)]  Loss:  2.911367 (2.9889)  Time: 1.082s,  946.03/s  (1.088s,  941.17/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [1000/1251 ( 80%)]  Loss:  3.110892 (2.9947)  Time: 1.076s,  951.32/s  (1.088s,  941.06/s)  LR: 9.853e-05  Data: 0.015 (0.013)
Train: 242 [1050/1251 ( 84%)]  Loss:  3.122465 (3.0005)  Time: 1.077s,  950.87/s  (1.088s,  941.30/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [1100/1251 ( 88%)]  Loss:  2.815082 (2.9925)  Time: 1.107s,  925.29/s  (1.088s,  941.37/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [1150/1251 ( 92%)]  Loss:  2.816474 (2.9851)  Time: 1.095s,  935.36/s  (1.088s,  941.34/s)  LR: 9.853e-05  Data: 0.012 (0.013)
Train: 242 [1200/1251 ( 96%)]  Loss:  2.844780 (2.9795)  Time: 1.077s,  950.74/s  (1.088s,  941.29/s)  LR: 9.853e-05  Data: 0.011 (0.013)
Train: 242 [1250/1251 (100%)]  Loss:  2.987126 (2.9798)  Time: 1.061s,  964.69/s  (1.088s,  941.40/s)  LR: 9.853e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.807 (5.807)  Loss:  0.4281 (0.4281)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5405 (0.8472)  Acc@1: 87.2641 (80.5000)  Acc@5: 98.1132 (95.4820)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-236.pth.tar', 80.2960000805664)

Train: 243 [   0/1251 (  0%)]  Loss:  2.604042 (2.6040)  Time: 1.085s,  944.13/s  (1.085s,  944.13/s)  LR: 9.560e-05  Data: 0.023 (0.023)
Train: 243 [  50/1251 (  4%)]  Loss:  3.085116 (2.8446)  Time: 1.098s,  932.34/s  (1.096s,  934.14/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [ 100/1251 (  8%)]  Loss:  2.851098 (2.8468)  Time: 1.078s,  950.07/s  (1.093s,  936.81/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 150/1251 ( 12%)]  Loss:  2.956631 (2.8742)  Time: 1.078s,  949.61/s  (1.091s,  938.94/s)  LR: 9.560e-05  Data: 0.015 (0.013)
Train: 243 [ 200/1251 ( 16%)]  Loss:  2.830786 (2.8655)  Time: 1.077s,  951.09/s  (1.090s,  939.65/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 250/1251 ( 20%)]  Loss:  2.958033 (2.8810)  Time: 1.077s,  950.38/s  (1.090s,  939.30/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 243 [ 300/1251 ( 24%)]  Loss:  3.075702 (2.9088)  Time: 1.077s,  950.52/s  (1.089s,  940.12/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 350/1251 ( 28%)]  Loss:  2.998944 (2.9200)  Time: 1.106s,  926.11/s  (1.089s,  940.21/s)  LR: 9.560e-05  Data: 0.013 (0.013)
Train: 243 [ 400/1251 ( 32%)]  Loss:  2.607360 (2.8853)  Time: 1.169s,  876.30/s  (1.090s,  939.79/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 243 [ 450/1251 ( 36%)]  Loss:  3.145031 (2.9113)  Time: 1.095s,  935.34/s  (1.091s,  938.77/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 243 [ 500/1251 ( 40%)]  Loss:  2.770158 (2.8984)  Time: 1.093s,  936.51/s  (1.091s,  938.36/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 550/1251 ( 44%)]  Loss:  2.702301 (2.8821)  Time: 1.108s,  924.51/s  (1.092s,  938.05/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 243 [ 600/1251 ( 48%)]  Loss:  3.315899 (2.9155)  Time: 1.096s,  934.47/s  (1.091s,  938.32/s)  LR: 9.560e-05  Data: 0.015 (0.013)
Train: 243 [ 650/1251 ( 52%)]  Loss:  3.005150 (2.9219)  Time: 1.101s,  929.90/s  (1.091s,  938.26/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 700/1251 ( 56%)]  Loss:  2.675730 (2.9055)  Time: 1.094s,  936.30/s  (1.091s,  938.28/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 750/1251 ( 60%)]  Loss:  2.985943 (2.9105)  Time: 1.104s,  927.65/s  (1.092s,  937.99/s)  LR: 9.560e-05  Data: 0.013 (0.013)
Train: 243 [ 800/1251 ( 64%)]  Loss:  3.099895 (2.9216)  Time: 1.095s,  934.75/s  (1.092s,  937.52/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 850/1251 ( 68%)]  Loss:  2.901136 (2.9205)  Time: 1.093s,  936.48/s  (1.093s,  937.27/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [ 900/1251 ( 72%)]  Loss:  3.141895 (2.9322)  Time: 1.077s,  950.39/s  (1.092s,  937.46/s)  LR: 9.560e-05  Data: 0.013 (0.013)
Train: 243 [ 950/1251 ( 76%)]  Loss:  2.967936 (2.9339)  Time: 1.093s,  936.52/s  (1.092s,  937.61/s)  LR: 9.560e-05  Data: 0.015 (0.013)
Train: 243 [1000/1251 ( 80%)]  Loss:  3.102826 (2.9420)  Time: 1.101s,  930.29/s  (1.092s,  937.82/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Train: 243 [1050/1251 ( 84%)]  Loss:  3.102604 (2.9493)  Time: 1.075s,  952.74/s  (1.091s,  938.18/s)  LR: 9.560e-05  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 243 [1100/1251 ( 88%)]  Loss:  2.821523 (2.9437)  Time: 1.097s,  933.86/s  (1.092s,  938.07/s)  LR: 9.560e-05  Data: 0.012 (0.013)
Train: 243 [1150/1251 ( 92%)]  Loss:  3.138841 (2.9519)  Time: 1.111s,  921.97/s  (1.092s,  937.92/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 243 [1200/1251 ( 96%)]  Loss:  3.099655 (2.9578)  Time: 1.094s,  935.90/s  (1.092s,  937.67/s)  LR: 9.560e-05  Data: 0.014 (0.013)
Train: 243 [1250/1251 (100%)]  Loss:  2.780608 (2.9510)  Time: 1.062s,  964.62/s  (1.092s,  937.60/s)  LR: 9.560e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.837 (5.837)  Loss:  0.4354 (0.4354)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5357 (0.8465)  Acc@1: 86.7924 (80.4360)  Acc@5: 97.9953 (95.3440)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 80.43599995117188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-235.pth.tar', 80.31600018310547)

Train: 244 [   0/1251 (  0%)]  Loss:  3.171210 (3.1712)  Time: 1.085s,  943.71/s  (1.085s,  943.71/s)  LR: 9.270e-05  Data: 0.023 (0.023)
Train: 244 [  50/1251 (  4%)]  Loss:  2.924494 (3.0479)  Time: 1.079s,  949.23/s  (1.084s,  944.89/s)  LR: 9.270e-05  Data: 0.012 (0.013)
Train: 244 [ 100/1251 (  8%)]  Loss:  2.723464 (2.9397)  Time: 1.078s,  950.29/s  (1.086s,  942.52/s)  LR: 9.270e-05  Data: 0.013 (0.013)
Train: 244 [ 150/1251 ( 12%)]  Loss:  3.065679 (2.9712)  Time: 1.077s,  950.57/s  (1.088s,  941.04/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 200/1251 ( 16%)]  Loss:  2.994802 (2.9759)  Time: 1.078s,  950.03/s  (1.086s,  942.66/s)  LR: 9.270e-05  Data: 0.015 (0.012)
Train: 244 [ 250/1251 ( 20%)]  Loss:  3.056201 (2.9893)  Time: 1.095s,  935.02/s  (1.087s,  941.79/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 300/1251 ( 24%)]  Loss:  2.922455 (2.9798)  Time: 1.078s,  949.59/s  (1.087s,  941.80/s)  LR: 9.270e-05  Data: 0.011 (0.012)
Train: 244 [ 350/1251 ( 28%)]  Loss:  3.161913 (3.0025)  Time: 1.076s,  951.27/s  (1.087s,  942.02/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 400/1251 ( 32%)]  Loss:  2.987362 (3.0008)  Time: 1.094s,  936.37/s  (1.087s,  942.43/s)  LR: 9.270e-05  Data: 0.015 (0.012)
Train: 244 [ 450/1251 ( 36%)]  Loss:  3.063908 (3.0071)  Time: 1.077s,  950.74/s  (1.088s,  941.56/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 500/1251 ( 40%)]  Loss:  2.973886 (3.0041)  Time: 1.103s,  928.37/s  (1.088s,  941.58/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 550/1251 ( 44%)]  Loss:  2.854457 (2.9917)  Time: 1.096s,  933.97/s  (1.088s,  941.15/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 600/1251 ( 48%)]  Loss:  2.994344 (2.9919)  Time: 1.113s,  920.39/s  (1.088s,  941.04/s)  LR: 9.270e-05  Data: 0.013 (0.012)
Train: 244 [ 650/1251 ( 52%)]  Loss:  2.960929 (2.9897)  Time: 1.083s,  945.62/s  (1.088s,  941.09/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 700/1251 ( 56%)]  Loss:  3.005505 (2.9907)  Time: 1.087s,  942.47/s  (1.088s,  941.42/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 750/1251 ( 60%)]  Loss:  2.790335 (2.9782)  Time: 1.078s,  949.60/s  (1.087s,  941.86/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 800/1251 ( 64%)]  Loss:  3.141978 (2.9878)  Time: 1.084s,  944.58/s  (1.087s,  941.93/s)  LR: 9.270e-05  Data: 0.013 (0.012)
Train: 244 [ 850/1251 ( 68%)]  Loss:  3.281290 (3.0041)  Time: 1.097s,  933.12/s  (1.088s,  941.50/s)  LR: 9.270e-05  Data: 0.013 (0.012)
Train: 244 [ 900/1251 ( 72%)]  Loss:  3.127986 (3.0106)  Time: 1.095s,  935.35/s  (1.088s,  941.39/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [ 950/1251 ( 76%)]  Loss:  2.901907 (3.0052)  Time: 1.077s,  950.61/s  (1.088s,  941.41/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [1000/1251 ( 80%)]  Loss:  2.842078 (2.9974)  Time: 1.095s,  934.85/s  (1.088s,  941.21/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [1050/1251 ( 84%)]  Loss:  2.936949 (2.9947)  Time: 1.083s,  945.56/s  (1.088s,  941.15/s)  LR: 9.270e-05  Data: 0.014 (0.012)
Train: 244 [1100/1251 ( 88%)]  Loss:  3.034647 (2.9964)  Time: 1.096s,  934.42/s  (1.088s,  941.19/s)  LR: 9.270e-05  Data: 0.012 (0.012)
Train: 244 [1150/1251 ( 92%)]  Loss:  3.056968 (2.9989)  Time: 1.097s,  933.68/s  (1.088s,  941.46/s)  LR: 9.270e-05  Data: 0.011 (0.012)
Train: 244 [1200/1251 ( 96%)]  Loss:  2.829311 (2.9922)  Time: 1.080s,  948.13/s  (1.088s,  941.19/s)  LR: 9.270e-05  Data: 0.013 (0.012)
Train: 244 [1250/1251 (100%)]  Loss:  2.935265 (2.9900)  Time: 1.080s,  948.24/s  (1.088s,  940.97/s)  LR: 9.270e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.869 (5.869)  Loss:  0.4073 (0.4073)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5249 (0.8417)  Acc@1: 87.3821 (80.6440)  Acc@5: 98.1132 (95.4480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 80.43599995117188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-233.pth.tar', 80.32799997314453)

Train: 245 [   0/1251 (  0%)]  Loss:  2.693533 (2.6935)  Time: 1.093s,  936.95/s  (1.093s,  936.95/s)  LR: 8.986e-05  Data: 0.031 (0.031)
Train: 245 [  50/1251 (  4%)]  Loss:  2.952097 (2.8228)  Time: 1.095s,  935.02/s  (1.090s,  939.23/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 100/1251 (  8%)]  Loss:  2.971485 (2.8724)  Time: 1.082s,  946.25/s  (1.088s,  940.79/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [ 150/1251 ( 12%)]  Loss:  3.064247 (2.9203)  Time: 1.077s,  950.51/s  (1.090s,  939.44/s)  LR: 8.986e-05  Data: 0.017 (0.013)
Train: 245 [ 200/1251 ( 16%)]  Loss:  2.743051 (2.8849)  Time: 1.078s,  949.95/s  (1.090s,  939.14/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [ 250/1251 ( 20%)]  Loss:  2.989707 (2.9024)  Time: 1.075s,  952.30/s  (1.090s,  939.62/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 300/1251 ( 24%)]  Loss:  2.760878 (2.8821)  Time: 1.089s,  940.73/s  (1.090s,  939.73/s)  LR: 8.986e-05  Data: 0.020 (0.013)
Train: 245 [ 350/1251 ( 28%)]  Loss:  3.142100 (2.9146)  Time: 1.091s,  938.50/s  (1.089s,  940.64/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 400/1251 ( 32%)]  Loss:  3.056592 (2.9304)  Time: 1.097s,  933.72/s  (1.089s,  939.91/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 450/1251 ( 36%)]  Loss:  3.167888 (2.9542)  Time: 1.095s,  934.87/s  (1.091s,  938.73/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 500/1251 ( 40%)]  Loss:  2.703168 (2.9313)  Time: 1.099s,  931.95/s  (1.091s,  938.92/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 550/1251 ( 44%)]  Loss:  3.065408 (2.9425)  Time: 1.096s,  934.62/s  (1.091s,  938.47/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 600/1251 ( 48%)]  Loss:  2.915426 (2.9404)  Time: 1.092s,  937.59/s  (1.092s,  938.11/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [ 650/1251 ( 52%)]  Loss:  2.741931 (2.9263)  Time: 1.072s,  955.06/s  (1.092s,  937.81/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 700/1251 ( 56%)]  Loss:  2.791816 (2.9173)  Time: 1.098s,  932.69/s  (1.091s,  938.22/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 750/1251 ( 60%)]  Loss:  3.146068 (2.9316)  Time: 1.075s,  952.23/s  (1.091s,  938.66/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [ 800/1251 ( 64%)]  Loss:  2.918483 (2.9308)  Time: 1.095s,  935.38/s  (1.091s,  938.75/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 850/1251 ( 68%)]  Loss:  2.986803 (2.9339)  Time: 1.076s,  951.44/s  (1.091s,  938.60/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [ 900/1251 ( 72%)]  Loss:  3.023266 (2.9386)  Time: 1.096s,  934.64/s  (1.091s,  938.67/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [ 950/1251 ( 76%)]  Loss:  3.095622 (2.9465)  Time: 1.098s,  932.86/s  (1.091s,  938.71/s)  LR: 8.986e-05  Data: 0.011 (0.013)
Train: 245 [1000/1251 ( 80%)]  Loss:  2.890064 (2.9438)  Time: 1.074s,  953.50/s  (1.091s,  938.92/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [1050/1251 ( 84%)]  Loss:  2.675775 (2.9316)  Time: 1.115s,  918.25/s  (1.091s,  938.79/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [1100/1251 ( 88%)]  Loss:  2.854504 (2.9283)  Time: 1.095s,  934.93/s  (1.091s,  938.37/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [1150/1251 ( 92%)]  Loss:  2.848452 (2.9249)  Time: 1.100s,  930.87/s  (1.091s,  938.47/s)  LR: 8.986e-05  Data: 0.012 (0.013)
Train: 245 [1200/1251 ( 96%)]  Loss:  2.912827 (2.9244)  Time: 1.076s,  951.32/s  (1.091s,  938.37/s)  LR: 8.986e-05  Data: 0.013 (0.013)
Train: 245 [1250/1251 (100%)]  Loss:  2.941771 (2.9251)  Time: 1.081s,  947.06/s  (1.091s,  938.45/s)  LR: 8.986e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.817 (5.817)  Loss:  0.4127 (0.4127)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5365 (0.8508)  Acc@1: 86.9104 (80.6920)  Acc@5: 97.9953 (95.3740)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 80.43599995117188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-234.pth.tar', 80.34600002441407)

Train: 246 [   0/1251 (  0%)]  Loss:  3.133850 (3.1338)  Time: 1.083s,  945.38/s  (1.083s,  945.38/s)  LR: 8.706e-05  Data: 0.022 (0.022)
Train: 246 [  50/1251 (  4%)]  Loss:  3.135627 (3.1347)  Time: 1.096s,  934.29/s  (1.086s,  942.96/s)  LR: 8.706e-05  Data: 0.014 (0.013)
Train: 246 [ 100/1251 (  8%)]  Loss:  2.927619 (3.0657)  Time: 1.077s,  950.50/s  (1.090s,  939.42/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [ 150/1251 ( 12%)]  Loss:  2.853986 (3.0128)  Time: 1.085s,  943.83/s  (1.090s,  939.35/s)  LR: 8.706e-05  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 246 [ 200/1251 ( 16%)]  Loss:  2.930900 (2.9964)  Time: 1.088s,  940.92/s  (1.091s,  939.00/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 250/1251 ( 20%)]  Loss:  3.106863 (3.0148)  Time: 1.073s,  954.76/s  (1.089s,  940.47/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [ 300/1251 ( 24%)]  Loss:  2.768422 (2.9796)  Time: 1.078s,  949.68/s  (1.088s,  940.98/s)  LR: 8.706e-05  Data: 0.013 (0.013)
Train: 246 [ 350/1251 ( 28%)]  Loss:  2.900599 (2.9697)  Time: 1.076s,  951.52/s  (1.088s,  940.86/s)  LR: 8.706e-05  Data: 0.015 (0.013)
Train: 246 [ 400/1251 ( 32%)]  Loss:  2.936067 (2.9660)  Time: 1.096s,  934.67/s  (1.089s,  940.47/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [ 450/1251 ( 36%)]  Loss:  2.977111 (2.9671)  Time: 1.096s,  933.99/s  (1.090s,  939.63/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 500/1251 ( 40%)]  Loss:  2.656930 (2.9389)  Time: 1.096s,  934.21/s  (1.091s,  938.91/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 550/1251 ( 44%)]  Loss:  3.017886 (2.9455)  Time: 1.098s,  932.45/s  (1.091s,  938.50/s)  LR: 8.706e-05  Data: 0.015 (0.013)
Train: 246 [ 600/1251 ( 48%)]  Loss:  2.951722 (2.9460)  Time: 1.078s,  949.48/s  (1.091s,  938.49/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 650/1251 ( 52%)]  Loss:  2.929675 (2.9448)  Time: 1.077s,  950.91/s  (1.091s,  938.68/s)  LR: 8.706e-05  Data: 0.014 (0.013)
Train: 246 [ 700/1251 ( 56%)]  Loss:  3.060851 (2.9525)  Time: 1.096s,  934.51/s  (1.090s,  939.03/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 750/1251 ( 60%)]  Loss:  2.802568 (2.9432)  Time: 1.075s,  952.88/s  (1.091s,  938.58/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 800/1251 ( 64%)]  Loss:  2.983676 (2.9456)  Time: 1.076s,  951.76/s  (1.091s,  938.99/s)  LR: 8.706e-05  Data: 0.014 (0.013)
Train: 246 [ 850/1251 ( 68%)]  Loss:  2.806299 (2.9378)  Time: 1.080s,  948.13/s  (1.090s,  939.36/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [ 900/1251 ( 72%)]  Loss:  2.705015 (2.9256)  Time: 1.076s,  952.02/s  (1.090s,  939.41/s)  LR: 8.706e-05  Data: 0.013 (0.013)
Train: 246 [ 950/1251 ( 76%)]  Loss:  2.839398 (2.9213)  Time: 1.076s,  951.24/s  (1.090s,  939.32/s)  LR: 8.706e-05  Data: 0.011 (0.013)
Train: 246 [1000/1251 ( 80%)]  Loss:  3.002084 (2.9251)  Time: 1.078s,  949.89/s  (1.090s,  939.27/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [1050/1251 ( 84%)]  Loss:  2.732848 (2.9164)  Time: 1.095s,  935.43/s  (1.090s,  939.34/s)  LR: 8.706e-05  Data: 0.016 (0.013)
Train: 246 [1100/1251 ( 88%)]  Loss:  3.228898 (2.9300)  Time: 1.096s,  933.98/s  (1.090s,  939.24/s)  LR: 8.706e-05  Data: 0.013 (0.013)
Train: 246 [1150/1251 ( 92%)]  Loss:  3.165392 (2.9398)  Time: 1.076s,  952.09/s  (1.090s,  939.32/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [1200/1251 ( 96%)]  Loss:  2.870158 (2.9370)  Time: 1.096s,  934.17/s  (1.090s,  939.34/s)  LR: 8.706e-05  Data: 0.012 (0.013)
Train: 246 [1250/1251 (100%)]  Loss:  2.754626 (2.9300)  Time: 1.079s,  949.39/s  (1.090s,  939.33/s)  LR: 8.706e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.818 (5.818)  Loss:  0.4235 (0.4235)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5391 (0.8481)  Acc@1: 87.0283 (80.6180)  Acc@5: 97.8774 (95.4460)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 80.43599995117188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-237.pth.tar', 80.39199995117187)

Train: 247 [   0/1251 (  0%)]  Loss:  2.996135 (2.9961)  Time: 1.083s,  945.58/s  (1.083s,  945.58/s)  LR: 8.430e-05  Data: 0.023 (0.023)
Train: 247 [  50/1251 (  4%)]  Loss:  2.969218 (2.9827)  Time: 1.104s,  927.46/s  (1.092s,  937.45/s)  LR: 8.430e-05  Data: 0.013 (0.013)
Train: 247 [ 100/1251 (  8%)]  Loss:  2.848196 (2.9378)  Time: 1.097s,  933.49/s  (1.093s,  936.53/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 150/1251 ( 12%)]  Loss:  2.962809 (2.9441)  Time: 1.092s,  937.67/s  (1.091s,  938.96/s)  LR: 8.430e-05  Data: 0.018 (0.013)
Train: 247 [ 200/1251 ( 16%)]  Loss:  2.970672 (2.9494)  Time: 1.097s,  933.47/s  (1.088s,  940.97/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 250/1251 ( 20%)]  Loss:  2.875035 (2.9370)  Time: 1.081s,  947.57/s  (1.089s,  940.38/s)  LR: 8.430e-05  Data: 0.013 (0.013)
Train: 247 [ 300/1251 ( 24%)]  Loss:  2.742905 (2.9093)  Time: 1.076s,  952.05/s  (1.089s,  940.53/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 350/1251 ( 28%)]  Loss:  2.932380 (2.9122)  Time: 1.078s,  949.72/s  (1.088s,  941.00/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 400/1251 ( 32%)]  Loss:  2.898154 (2.9106)  Time: 1.094s,  935.91/s  (1.088s,  940.91/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 450/1251 ( 36%)]  Loss:  2.920857 (2.9116)  Time: 1.103s,  928.26/s  (1.088s,  941.05/s)  LR: 8.430e-05  Data: 0.014 (0.013)
Train: 247 [ 500/1251 ( 40%)]  Loss:  3.248706 (2.9423)  Time: 1.078s,  950.19/s  (1.088s,  941.39/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 550/1251 ( 44%)]  Loss:  3.045371 (2.9509)  Time: 1.077s,  950.55/s  (1.088s,  941.57/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 600/1251 ( 48%)]  Loss:  2.833450 (2.9418)  Time: 1.079s,  949.16/s  (1.087s,  941.92/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 650/1251 ( 52%)]  Loss:  2.901659 (2.9390)  Time: 1.077s,  951.10/s  (1.087s,  941.87/s)  LR: 8.430e-05  Data: 0.012 (0.012)
Train: 247 [ 700/1251 ( 56%)]  Loss:  2.961281 (2.9405)  Time: 1.078s,  949.49/s  (1.087s,  941.89/s)  LR: 8.430e-05  Data: 0.011 (0.013)
Train: 247 [ 750/1251 ( 60%)]  Loss:  3.035301 (2.9464)  Time: 1.082s,  946.63/s  (1.087s,  941.65/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 800/1251 ( 64%)]  Loss:  2.904818 (2.9439)  Time: 1.093s,  937.09/s  (1.087s,  941.67/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 850/1251 ( 68%)]  Loss:  3.124440 (2.9540)  Time: 1.104s,  927.14/s  (1.088s,  941.37/s)  LR: 8.430e-05  Data: 0.013 (0.013)
Train: 247 [ 900/1251 ( 72%)]  Loss:  3.036260 (2.9583)  Time: 1.077s,  950.72/s  (1.088s,  941.35/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [ 950/1251 ( 76%)]  Loss:  2.981818 (2.9595)  Time: 1.084s,  944.44/s  (1.088s,  941.53/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [1000/1251 ( 80%)]  Loss:  3.163692 (2.9692)  Time: 1.094s,  935.82/s  (1.088s,  941.57/s)  LR: 8.430e-05  Data: 0.013 (0.013)
Train: 247 [1050/1251 ( 84%)]  Loss:  2.977919 (2.9696)  Time: 1.092s,  937.52/s  (1.088s,  941.51/s)  LR: 8.430e-05  Data: 0.013 (0.013)
Train: 247 [1100/1251 ( 88%)]  Loss:  3.024602 (2.9720)  Time: 1.077s,  950.90/s  (1.088s,  941.35/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [1150/1251 ( 92%)]  Loss:  2.826378 (2.9659)  Time: 1.093s,  936.82/s  (1.088s,  941.42/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [1200/1251 ( 96%)]  Loss:  3.135423 (2.9727)  Time: 1.077s,  950.43/s  (1.088s,  941.48/s)  LR: 8.430e-05  Data: 0.012 (0.013)
Train: 247 [1250/1251 (100%)]  Loss:  2.954473 (2.9720)  Time: 1.070s,  957.02/s  (1.088s,  941.32/s)  LR: 8.430e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.864 (5.864)  Loss:  0.4384 (0.4384)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5508 (0.8627)  Acc@1: 87.9717 (80.7420)  Acc@5: 97.8774 (95.4240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 80.43599995117188)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-240.pth.tar', 80.41600005126953)

Train: 248 [   0/1251 (  0%)]  Loss:  3.037456 (3.0375)  Time: 1.095s,  935.28/s  (1.095s,  935.28/s)  LR: 8.159e-05  Data: 0.034 (0.034)
Train: 248 [  50/1251 (  4%)]  Loss:  2.776706 (2.9071)  Time: 1.078s,  950.17/s  (1.090s,  939.37/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 100/1251 (  8%)]  Loss:  2.921370 (2.9118)  Time: 1.180s,  868.10/s  (1.088s,  941.33/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 150/1251 ( 12%)]  Loss:  2.978069 (2.9284)  Time: 1.077s,  950.86/s  (1.085s,  943.37/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 248 [ 200/1251 ( 16%)]  Loss:  2.845081 (2.9117)  Time: 1.094s,  935.77/s  (1.089s,  940.00/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 250/1251 ( 20%)]  Loss:  3.040498 (2.9332)  Time: 1.096s,  934.19/s  (1.090s,  939.48/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 300/1251 ( 24%)]  Loss:  2.899114 (2.9283)  Time: 1.084s,  944.54/s  (1.089s,  940.57/s)  LR: 8.159e-05  Data: 0.015 (0.013)
Train: 248 [ 350/1251 ( 28%)]  Loss:  3.112377 (2.9513)  Time: 1.082s,  946.00/s  (1.089s,  940.64/s)  LR: 8.159e-05  Data: 0.013 (0.013)
Train: 248 [ 400/1251 ( 32%)]  Loss:  3.026588 (2.9597)  Time: 1.081s,  946.94/s  (1.088s,  941.08/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 450/1251 ( 36%)]  Loss:  2.735262 (2.9373)  Time: 1.096s,  934.04/s  (1.088s,  941.13/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 500/1251 ( 40%)]  Loss:  3.011834 (2.9440)  Time: 1.096s,  934.17/s  (1.089s,  940.64/s)  LR: 8.159e-05  Data: 0.013 (0.013)
Train: 248 [ 550/1251 ( 44%)]  Loss:  3.134679 (2.9599)  Time: 1.099s,  932.14/s  (1.089s,  940.12/s)  LR: 8.159e-05  Data: 0.013 (0.013)
Train: 248 [ 600/1251 ( 48%)]  Loss:  2.938319 (2.9583)  Time: 1.081s,  947.68/s  (1.089s,  940.17/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 650/1251 ( 52%)]  Loss:  2.731399 (2.9421)  Time: 1.077s,  950.43/s  (1.089s,  940.71/s)  LR: 8.159e-05  Data: 0.015 (0.013)
Train: 248 [ 700/1251 ( 56%)]  Loss:  3.099923 (2.9526)  Time: 1.094s,  936.25/s  (1.089s,  940.58/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [ 750/1251 ( 60%)]  Loss:  2.823671 (2.9445)  Time: 1.082s,  946.69/s  (1.088s,  940.76/s)  LR: 8.159e-05  Data: 0.013 (0.013)
Train: 248 [ 800/1251 ( 64%)]  Loss:  2.966599 (2.9458)  Time: 1.076s,  951.93/s  (1.089s,  940.69/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [ 850/1251 ( 68%)]  Loss:  2.922807 (2.9445)  Time: 1.092s,  937.35/s  (1.089s,  940.28/s)  LR: 8.159e-05  Data: 0.013 (0.013)
Train: 248 [ 900/1251 ( 72%)]  Loss:  3.219552 (2.9590)  Time: 1.095s,  935.20/s  (1.089s,  940.12/s)  LR: 8.159e-05  Data: 0.015 (0.013)
Train: 248 [ 950/1251 ( 76%)]  Loss:  2.820509 (2.9521)  Time: 1.076s,  951.57/s  (1.089s,  940.26/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [1000/1251 ( 80%)]  Loss:  2.759974 (2.9429)  Time: 1.097s,  933.37/s  (1.089s,  940.48/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1050/1251 ( 84%)]  Loss:  3.053008 (2.9479)  Time: 1.188s,  862.24/s  (1.089s,  940.38/s)  LR: 8.159e-05  Data: 0.011 (0.013)
Train: 248 [1100/1251 ( 88%)]  Loss:  2.921886 (2.9468)  Time: 1.094s,  935.87/s  (1.089s,  940.34/s)  LR: 8.159e-05  Data: 0.014 (0.013)
Train: 248 [1150/1251 ( 92%)]  Loss:  2.616203 (2.9330)  Time: 1.096s,  934.59/s  (1.089s,  940.26/s)  LR: 8.159e-05  Data: 0.024 (0.013)
Train: 248 [1200/1251 ( 96%)]  Loss:  3.035167 (2.9371)  Time: 1.084s,  944.62/s  (1.089s,  940.00/s)  LR: 8.159e-05  Data: 0.012 (0.013)
Train: 248 [1250/1251 (100%)]  Loss:  3.130382 (2.9446)  Time: 1.080s,  948.05/s  (1.090s,  939.82/s)  LR: 8.159e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.869 (5.869)  Loss:  0.4372 (0.4372)  Acc@1: 92.5781 (92.5781)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.230 (0.448)  Loss:  0.5594 (0.8639)  Acc@1: 87.0283 (80.6500)  Acc@5: 97.9953 (95.4180)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-243.pth.tar', 80.43599995117188)

Train: 249 [   0/1251 (  0%)]  Loss:  2.823363 (2.8234)  Time: 1.092s,  937.35/s  (1.092s,  937.35/s)  LR: 7.893e-05  Data: 0.031 (0.031)
Train: 249 [  50/1251 (  4%)]  Loss:  2.631398 (2.7274)  Time: 1.095s,  935.48/s  (1.084s,  944.93/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 100/1251 (  8%)]  Loss:  2.690214 (2.7150)  Time: 1.076s,  951.72/s  (1.084s,  945.01/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 150/1251 ( 12%)]  Loss:  2.922745 (2.7669)  Time: 1.078s,  949.98/s  (1.087s,  941.96/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 200/1251 ( 16%)]  Loss:  2.803553 (2.7743)  Time: 1.076s,  951.74/s  (1.087s,  942.27/s)  LR: 7.893e-05  Data: 0.014 (0.013)
Train: 249 [ 250/1251 ( 20%)]  Loss:  3.127280 (2.8331)  Time: 1.096s,  933.97/s  (1.087s,  941.95/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 300/1251 ( 24%)]  Loss:  3.197912 (2.8852)  Time: 1.076s,  951.31/s  (1.088s,  940.84/s)  LR: 7.893e-05  Data: 0.014 (0.013)
Train: 249 [ 350/1251 ( 28%)]  Loss:  2.928705 (2.8906)  Time: 1.077s,  950.37/s  (1.087s,  941.76/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 400/1251 ( 32%)]  Loss:  2.858743 (2.8871)  Time: 1.072s,  955.61/s  (1.087s,  942.07/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 450/1251 ( 36%)]  Loss:  2.855072 (2.8839)  Time: 1.076s,  951.24/s  (1.087s,  942.24/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 500/1251 ( 40%)]  Loss:  3.174214 (2.9103)  Time: 1.079s,  949.09/s  (1.087s,  941.72/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 550/1251 ( 44%)]  Loss:  2.754177 (2.8973)  Time: 1.090s,  939.03/s  (1.087s,  941.94/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Train: 249 [ 600/1251 ( 48%)]  Loss:  3.009806 (2.9059)  Time: 1.095s,  934.86/s  (1.087s,  941.85/s)  LR: 7.893e-05  Data: 0.013 (0.013)
Train: 249 [ 650/1251 ( 52%)]  Loss:  2.920351 (2.9070)  Time: 1.194s,  857.69/s  (1.087s,  941.73/s)  LR: 7.893e-05  Data: 0.014 (0.013)
Train: 249 [ 700/1251 ( 56%)]  Loss:  3.065995 (2.9176)  Time: 1.104s,  927.56/s  (1.087s,  941.61/s)  LR: 7.893e-05  Data: 0.013 (0.013)
Train: 249 [ 750/1251 ( 60%)]  Loss:  3.120132 (2.9302)  Time: 1.085s,  943.95/s  (1.088s,  941.56/s)  LR: 7.893e-05  Data: 0.013 (0.013)
Train: 249 [ 800/1251 ( 64%)]  Loss:  2.940553 (2.9308)  Time: 1.100s,  931.13/s  (1.088s,  941.10/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 850/1251 ( 68%)]  Loss:  3.098778 (2.9402)  Time: 1.075s,  952.18/s  (1.089s,  940.68/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [ 900/1251 ( 72%)]  Loss:  3.054575 (2.9462)  Time: 1.081s,  947.54/s  (1.089s,  940.61/s)  LR: 7.893e-05  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 249 [ 950/1251 ( 76%)]  Loss:  2.804132 (2.9391)  Time: 1.079s,  949.45/s  (1.088s,  940.85/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [1000/1251 ( 80%)]  Loss:  2.792788 (2.9321)  Time: 1.096s,  934.50/s  (1.089s,  940.73/s)  LR: 7.893e-05  Data: 0.013 (0.013)
Train: 249 [1050/1251 ( 84%)]  Loss:  3.080987 (2.9389)  Time: 1.084s,  944.97/s  (1.088s,  940.94/s)  LR: 7.893e-05  Data: 0.012 (0.013)
Train: 249 [1100/1251 ( 88%)]  Loss:  3.031915 (2.9429)  Time: 1.084s,  944.58/s  (1.088s,  941.18/s)  LR: 7.893e-05  Data: 0.015 (0.013)
Train: 249 [1150/1251 ( 92%)]  Loss:  2.949681 (2.9432)  Time: 1.077s,  950.69/s  (1.088s,  940.97/s)  LR: 7.893e-05  Data: 0.013 (0.013)
Train: 249 [1200/1251 ( 96%)]  Loss:  3.220294 (2.9543)  Time: 1.093s,  937.06/s  (1.088s,  941.02/s)  LR: 7.893e-05  Data: 0.018 (0.013)
Train: 249 [1250/1251 (100%)]  Loss:  2.955545 (2.9543)  Time: 1.061s,  965.26/s  (1.088s,  941.01/s)  LR: 7.893e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.837 (5.837)  Loss:  0.4359 (0.4359)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5647 (0.8625)  Acc@1: 87.1462 (80.5720)  Acc@5: 97.9953 (95.4220)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 80.5719998461914)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-242.pth.tar', 80.49999989746094)

Train: 250 [   0/1251 (  0%)]  Loss:  3.029799 (3.0298)  Time: 1.084s,  944.59/s  (1.084s,  944.59/s)  LR: 7.632e-05  Data: 0.022 (0.022)
Train: 250 [  50/1251 (  4%)]  Loss:  3.025316 (3.0276)  Time: 1.157s,  884.76/s  (1.090s,  939.25/s)  LR: 7.632e-05  Data: 0.014 (0.013)
Train: 250 [ 100/1251 (  8%)]  Loss:  2.624743 (2.8933)  Time: 1.078s,  950.16/s  (1.090s,  939.62/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 150/1251 ( 12%)]  Loss:  3.169865 (2.9624)  Time: 1.081s,  947.43/s  (1.088s,  941.37/s)  LR: 7.632e-05  Data: 0.015 (0.013)
Train: 250 [ 200/1251 ( 16%)]  Loss:  2.936063 (2.9572)  Time: 1.117s,  916.72/s  (1.088s,  940.79/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 250/1251 ( 20%)]  Loss:  3.131355 (2.9862)  Time: 1.075s,  952.32/s  (1.089s,  940.27/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [ 300/1251 ( 24%)]  Loss:  2.946871 (2.9806)  Time: 1.079s,  949.09/s  (1.088s,  941.24/s)  LR: 7.632e-05  Data: 0.012 (0.013)
Train: 250 [ 350/1251 ( 28%)]  Loss:  3.080629 (2.9931)  Time: 1.078s,  950.21/s  (1.088s,  941.52/s)  LR: 7.632e-05  Data: 0.016 (0.013)
Train: 250 [ 400/1251 ( 32%)]  Loss:  2.800122 (2.9716)  Time: 1.105s,  926.53/s  (1.087s,  941.69/s)  LR: 7.632e-05  Data: 0.011 (0.013)
Train: 250 [ 450/1251 ( 36%)]  Loss:  2.933647 (2.9678)  Time: 1.079s,  949.39/s  (1.088s,  941.15/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 250 [ 500/1251 ( 40%)]  Loss:  3.015828 (2.9722)  Time: 1.078s,  950.31/s  (1.088s,  941.37/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 550/1251 ( 44%)]  Loss:  2.893335 (2.9656)  Time: 1.079s,  949.42/s  (1.088s,  941.54/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 600/1251 ( 48%)]  Loss:  2.625918 (2.9395)  Time: 1.083s,  945.12/s  (1.088s,  941.29/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 650/1251 ( 52%)]  Loss:  3.150430 (2.9546)  Time: 1.093s,  936.88/s  (1.088s,  941.02/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 700/1251 ( 56%)]  Loss:  2.934773 (2.9532)  Time: 1.095s,  935.35/s  (1.088s,  940.84/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 750/1251 ( 60%)]  Loss:  3.079806 (2.9612)  Time: 1.077s,  950.48/s  (1.088s,  940.87/s)  LR: 7.632e-05  Data: 0.016 (0.012)
Train: 250 [ 800/1251 ( 64%)]  Loss:  3.011996 (2.9641)  Time: 1.080s,  947.82/s  (1.088s,  940.89/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 850/1251 ( 68%)]  Loss:  2.952162 (2.9635)  Time: 1.092s,  937.89/s  (1.088s,  940.84/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [ 900/1251 ( 72%)]  Loss:  2.940666 (2.9623)  Time: 1.107s,  924.80/s  (1.088s,  940.99/s)  LR: 7.632e-05  Data: 0.011 (0.012)
Train: 250 [ 950/1251 ( 76%)]  Loss:  3.148806 (2.9716)  Time: 1.095s,  935.44/s  (1.088s,  940.86/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [1000/1251 ( 80%)]  Loss:  3.102420 (2.9778)  Time: 1.104s,  927.23/s  (1.089s,  940.58/s)  LR: 7.632e-05  Data: 0.014 (0.012)
Train: 250 [1050/1251 ( 84%)]  Loss:  3.003539 (2.9790)  Time: 1.095s,  935.09/s  (1.089s,  940.20/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [1100/1251 ( 88%)]  Loss:  2.834255 (2.9727)  Time: 1.083s,  945.88/s  (1.089s,  940.13/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [1150/1251 ( 92%)]  Loss:  3.043796 (2.9757)  Time: 1.077s,  950.48/s  (1.090s,  939.71/s)  LR: 7.632e-05  Data: 0.014 (0.012)
Train: 250 [1200/1251 ( 96%)]  Loss:  3.018492 (2.9774)  Time: 1.098s,  932.78/s  (1.090s,  939.77/s)  LR: 7.632e-05  Data: 0.012 (0.012)
Train: 250 [1250/1251 (100%)]  Loss:  2.814648 (2.9711)  Time: 1.078s,  949.89/s  (1.090s,  939.70/s)  LR: 7.632e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.864 (5.864)  Loss:  0.4237 (0.4237)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5384 (0.8466)  Acc@1: 86.9104 (80.7960)  Acc@5: 97.9953 (95.4680)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 80.5719998461914)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-238.pth.tar', 80.51199994628907)

Train: 251 [   0/1251 (  0%)]  Loss:  2.978491 (2.9785)  Time: 1.084s,  944.81/s  (1.084s,  944.81/s)  LR: 7.375e-05  Data: 0.023 (0.023)
Train: 251 [  50/1251 (  4%)]  Loss:  2.907703 (2.9431)  Time: 1.174s,  872.13/s  (1.091s,  938.36/s)  LR: 7.375e-05  Data: 0.012 (0.013)
Train: 251 [ 100/1251 (  8%)]  Loss:  3.040470 (2.9756)  Time: 1.173s,  873.07/s  (1.091s,  938.97/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 150/1251 ( 12%)]  Loss:  2.931155 (2.9645)  Time: 1.109s,  923.63/s  (1.090s,  939.69/s)  LR: 7.375e-05  Data: 0.014 (0.012)
Train: 251 [ 200/1251 ( 16%)]  Loss:  2.523551 (2.8763)  Time: 1.118s,  916.25/s  (1.089s,  940.02/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 250/1251 ( 20%)]  Loss:  2.903161 (2.8808)  Time: 1.094s,  936.38/s  (1.088s,  940.75/s)  LR: 7.375e-05  Data: 0.011 (0.012)
Train: 251 [ 300/1251 ( 24%)]  Loss:  2.831576 (2.8737)  Time: 1.174s,  872.31/s  (1.089s,  940.61/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 350/1251 ( 28%)]  Loss:  2.973100 (2.8862)  Time: 1.081s,  947.56/s  (1.089s,  940.51/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 400/1251 ( 32%)]  Loss:  3.048834 (2.9042)  Time: 1.105s,  926.83/s  (1.089s,  940.25/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 450/1251 ( 36%)]  Loss:  3.187713 (2.9326)  Time: 1.098s,  932.31/s  (1.090s,  939.69/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 500/1251 ( 40%)]  Loss:  2.788966 (2.9195)  Time: 1.163s,  880.79/s  (1.091s,  938.98/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 550/1251 ( 44%)]  Loss:  2.822743 (2.9115)  Time: 1.079s,  948.62/s  (1.090s,  939.65/s)  LR: 7.375e-05  Data: 0.014 (0.012)
Train: 251 [ 600/1251 ( 48%)]  Loss:  2.838463 (2.9058)  Time: 1.097s,  933.77/s  (1.090s,  939.53/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 650/1251 ( 52%)]  Loss:  3.042588 (2.9156)  Time: 1.085s,  943.90/s  (1.090s,  939.46/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 700/1251 ( 56%)]  Loss:  2.824020 (2.9095)  Time: 1.076s,  951.60/s  (1.090s,  939.37/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 750/1251 ( 60%)]  Loss:  3.107220 (2.9219)  Time: 1.211s,  845.65/s  (1.091s,  938.94/s)  LR: 7.375e-05  Data: 0.014 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 251 [ 800/1251 ( 64%)]  Loss:  2.959018 (2.9240)  Time: 1.080s,  947.73/s  (1.090s,  939.04/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [ 850/1251 ( 68%)]  Loss:  3.194643 (2.9391)  Time: 1.082s,  946.79/s  (1.090s,  939.44/s)  LR: 7.375e-05  Data: 0.011 (0.012)
Train: 251 [ 900/1251 ( 72%)]  Loss:  3.068436 (2.9459)  Time: 1.077s,  950.85/s  (1.090s,  939.85/s)  LR: 7.375e-05  Data: 0.013 (0.012)
Train: 251 [ 950/1251 ( 76%)]  Loss:  2.694582 (2.9333)  Time: 1.079s,  948.98/s  (1.090s,  939.67/s)  LR: 7.375e-05  Data: 0.013 (0.012)
Train: 251 [1000/1251 ( 80%)]  Loss:  3.057789 (2.9392)  Time: 1.095s,  935.42/s  (1.090s,  939.68/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [1050/1251 ( 84%)]  Loss:  2.674850 (2.9272)  Time: 1.094s,  936.25/s  (1.090s,  939.84/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [1100/1251 ( 88%)]  Loss:  2.863266 (2.9244)  Time: 1.094s,  936.37/s  (1.090s,  939.85/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [1150/1251 ( 92%)]  Loss:  2.920119 (2.9243)  Time: 1.095s,  935.24/s  (1.090s,  939.54/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [1200/1251 ( 96%)]  Loss:  2.698277 (2.9152)  Time: 1.093s,  937.06/s  (1.090s,  939.41/s)  LR: 7.375e-05  Data: 0.012 (0.012)
Train: 251 [1250/1251 (100%)]  Loss:  2.832024 (2.9120)  Time: 1.068s,  958.84/s  (1.090s,  939.60/s)  LR: 7.375e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.846 (5.846)  Loss:  0.4402 (0.4402)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5496 (0.8423)  Acc@1: 87.2642 (80.7920)  Acc@5: 97.6415 (95.4480)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 80.5719998461914)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-239.pth.tar', 80.54200012939454)

Train: 252 [   0/1251 (  0%)]  Loss:  3.174052 (3.1741)  Time: 1.083s,  945.60/s  (1.083s,  945.60/s)  LR: 7.123e-05  Data: 0.022 (0.022)
Train: 252 [  50/1251 (  4%)]  Loss:  2.992033 (3.0830)  Time: 1.077s,  951.16/s  (1.091s,  938.17/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 100/1251 (  8%)]  Loss:  2.840641 (3.0022)  Time: 1.077s,  950.96/s  (1.087s,  942.33/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 150/1251 ( 12%)]  Loss:  3.109834 (3.0291)  Time: 1.095s,  934.82/s  (1.089s,  940.72/s)  LR: 7.123e-05  Data: 0.016 (0.012)
Train: 252 [ 200/1251 ( 16%)]  Loss:  3.093594 (3.0420)  Time: 1.083s,  945.88/s  (1.087s,  942.06/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 250/1251 ( 20%)]  Loss:  2.912383 (3.0204)  Time: 1.096s,  934.56/s  (1.088s,  941.30/s)  LR: 7.123e-05  Data: 0.013 (0.013)
Train: 252 [ 300/1251 ( 24%)]  Loss:  2.937869 (3.0086)  Time: 1.082s,  946.38/s  (1.088s,  940.82/s)  LR: 7.123e-05  Data: 0.013 (0.013)
Train: 252 [ 350/1251 ( 28%)]  Loss:  2.969032 (3.0037)  Time: 1.080s,  948.32/s  (1.089s,  940.40/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 400/1251 ( 32%)]  Loss:  3.001171 (3.0034)  Time: 1.094s,  936.35/s  (1.088s,  940.93/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 450/1251 ( 36%)]  Loss:  2.719882 (2.9750)  Time: 1.082s,  946.35/s  (1.088s,  940.84/s)  LR: 7.123e-05  Data: 0.017 (0.013)
Train: 252 [ 500/1251 ( 40%)]  Loss:  2.912510 (2.9694)  Time: 1.079s,  949.21/s  (1.089s,  940.61/s)  LR: 7.123e-05  Data: 0.011 (0.013)
Train: 252 [ 550/1251 ( 44%)]  Loss:  3.076263 (2.9783)  Time: 1.098s,  932.72/s  (1.089s,  940.47/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 600/1251 ( 48%)]  Loss:  3.060218 (2.9846)  Time: 1.079s,  949.18/s  (1.089s,  940.39/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 650/1251 ( 52%)]  Loss:  2.980933 (2.9843)  Time: 1.076s,  951.28/s  (1.089s,  940.51/s)  LR: 7.123e-05  Data: 0.014 (0.013)
Train: 252 [ 700/1251 ( 56%)]  Loss:  2.943361 (2.9816)  Time: 1.076s,  951.35/s  (1.089s,  940.40/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 750/1251 ( 60%)]  Loss:  2.937553 (2.9788)  Time: 1.094s,  935.88/s  (1.089s,  940.00/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 800/1251 ( 64%)]  Loss:  2.838209 (2.9706)  Time: 1.077s,  950.99/s  (1.089s,  940.07/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 850/1251 ( 68%)]  Loss:  2.922432 (2.9679)  Time: 1.095s,  935.09/s  (1.089s,  940.08/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [ 900/1251 ( 72%)]  Loss:  2.755960 (2.9567)  Time: 1.095s,  935.28/s  (1.089s,  939.96/s)  LR: 7.123e-05  Data: 0.014 (0.013)
Train: 252 [ 950/1251 ( 76%)]  Loss:  2.885550 (2.9532)  Time: 1.077s,  950.95/s  (1.089s,  940.11/s)  LR: 7.123e-05  Data: 0.015 (0.013)
Train: 252 [1000/1251 ( 80%)]  Loss:  2.855013 (2.9485)  Time: 1.094s,  935.74/s  (1.089s,  940.22/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1050/1251 ( 84%)]  Loss:  2.897295 (2.9462)  Time: 1.079s,  949.39/s  (1.089s,  940.11/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1100/1251 ( 88%)]  Loss:  2.967250 (2.9471)  Time: 1.079s,  949.34/s  (1.089s,  940.20/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1150/1251 ( 92%)]  Loss:  3.046393 (2.9512)  Time: 1.080s,  947.94/s  (1.089s,  940.39/s)  LR: 7.123e-05  Data: 0.015 (0.013)
Train: 252 [1200/1251 ( 96%)]  Loss:  3.058192 (2.9555)  Time: 1.096s,  934.55/s  (1.089s,  940.33/s)  LR: 7.123e-05  Data: 0.012 (0.013)
Train: 252 [1250/1251 (100%)]  Loss:  2.827006 (2.9506)  Time: 1.074s,  953.83/s  (1.089s,  939.99/s)  LR: 7.123e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.759 (5.759)  Loss:  0.4171 (0.4171)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5336 (0.8484)  Acc@1: 87.1462 (80.8420)  Acc@5: 97.9953 (95.4900)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-249.pth.tar', 80.5719998461914)

Train: 253 [   0/1251 (  0%)]  Loss:  2.827977 (2.8280)  Time: 1.087s,  942.26/s  (1.087s,  942.26/s)  LR: 6.875e-05  Data: 0.025 (0.025)
Train: 253 [  50/1251 (  4%)]  Loss:  2.838690 (2.8333)  Time: 1.099s,  931.42/s  (1.097s,  933.66/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [ 100/1251 (  8%)]  Loss:  2.911318 (2.8593)  Time: 1.094s,  936.17/s  (1.098s,  932.72/s)  LR: 6.875e-05  Data: 0.012 (0.012)
Train: 253 [ 150/1251 ( 12%)]  Loss:  2.993819 (2.8930)  Time: 1.104s,  927.23/s  (1.093s,  937.10/s)  LR: 6.875e-05  Data: 0.015 (0.012)
Train: 253 [ 200/1251 ( 16%)]  Loss:  2.786192 (2.8716)  Time: 1.079s,  949.26/s  (1.091s,  938.23/s)  LR: 6.875e-05  Data: 0.012 (0.012)
Train: 253 [ 250/1251 ( 20%)]  Loss:  2.815149 (2.8622)  Time: 1.076s,  952.00/s  (1.091s,  938.81/s)  LR: 6.875e-05  Data: 0.012 (0.012)
Train: 253 [ 300/1251 ( 24%)]  Loss:  3.067247 (2.8915)  Time: 1.086s,  942.74/s  (1.091s,  938.78/s)  LR: 6.875e-05  Data: 0.014 (0.012)
Train: 253 [ 350/1251 ( 28%)]  Loss:  2.866847 (2.8884)  Time: 1.095s,  935.11/s  (1.090s,  939.23/s)  LR: 6.875e-05  Data: 0.015 (0.012)
Train: 253 [ 400/1251 ( 32%)]  Loss:  2.934116 (2.8935)  Time: 1.096s,  934.62/s  (1.091s,  938.79/s)  LR: 6.875e-05  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 253 [ 450/1251 ( 36%)]  Loss:  3.065926 (2.9107)  Time: 1.078s,  950.04/s  (1.091s,  939.01/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 500/1251 ( 40%)]  Loss:  3.060452 (2.9243)  Time: 1.093s,  936.45/s  (1.091s,  938.78/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [ 550/1251 ( 44%)]  Loss:  2.870306 (2.9198)  Time: 1.093s,  936.83/s  (1.091s,  938.43/s)  LR: 6.875e-05  Data: 0.013 (0.013)
Train: 253 [ 600/1251 ( 48%)]  Loss:  2.958165 (2.9228)  Time: 1.081s,  946.90/s  (1.091s,  938.61/s)  LR: 6.875e-05  Data: 0.011 (0.013)
Train: 253 [ 650/1251 ( 52%)]  Loss:  2.876491 (2.9195)  Time: 1.078s,  949.81/s  (1.090s,  939.16/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [ 700/1251 ( 56%)]  Loss:  2.861488 (2.9156)  Time: 1.106s,  925.58/s  (1.091s,  938.93/s)  LR: 6.875e-05  Data: 0.012 (0.012)
Train: 253 [ 750/1251 ( 60%)]  Loss:  3.130291 (2.9290)  Time: 1.095s,  935.33/s  (1.091s,  938.72/s)  LR: 6.875e-05  Data: 0.014 (0.013)
Train: 253 [ 800/1251 ( 64%)]  Loss:  2.905522 (2.9276)  Time: 1.077s,  951.03/s  (1.090s,  939.09/s)  LR: 6.875e-05  Data: 0.014 (0.013)
Train: 253 [ 850/1251 ( 68%)]  Loss:  2.644643 (2.9119)  Time: 1.077s,  950.49/s  (1.090s,  939.04/s)  LR: 6.875e-05  Data: 0.013 (0.013)
Train: 253 [ 900/1251 ( 72%)]  Loss:  2.819783 (2.9071)  Time: 1.095s,  934.93/s  (1.091s,  938.93/s)  LR: 6.875e-05  Data: 0.013 (0.013)
Train: 253 [ 950/1251 ( 76%)]  Loss:  3.130244 (2.9182)  Time: 1.176s,  870.79/s  (1.090s,  939.03/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [1000/1251 ( 80%)]  Loss:  2.924447 (2.9185)  Time: 1.079s,  949.13/s  (1.090s,  939.26/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [1050/1251 ( 84%)]  Loss:  3.134803 (2.9284)  Time: 1.098s,  932.46/s  (1.090s,  939.58/s)  LR: 6.875e-05  Data: 0.012 (0.013)
Train: 253 [1100/1251 ( 88%)]  Loss:  2.971296 (2.9302)  Time: 1.077s,  950.67/s  (1.090s,  939.62/s)  LR: 6.875e-05  Data: 0.013 (0.013)
Train: 253 [1150/1251 ( 92%)]  Loss:  3.056782 (2.9355)  Time: 1.199s,  853.96/s  (1.090s,  939.86/s)  LR: 6.875e-05  Data: 0.012 (0.012)
Train: 253 [1200/1251 ( 96%)]  Loss:  3.026220 (2.9391)  Time: 1.100s,  930.70/s  (1.089s,  940.00/s)  LR: 6.875e-05  Data: 0.014 (0.013)
Train: 253 [1250/1251 (100%)]  Loss:  2.978376 (2.9406)  Time: 1.057s,  968.52/s  (1.090s,  939.86/s)  LR: 6.875e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.800 (5.800)  Loss:  0.4297 (0.4297)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5386 (0.8521)  Acc@1: 86.9104 (80.8160)  Acc@5: 98.2311 (95.4440)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-241.pth.tar', 80.57800005371094)

Train: 254 [   0/1251 (  0%)]  Loss:  3.108790 (3.1088)  Time: 1.084s,  944.95/s  (1.084s,  944.95/s)  LR: 6.633e-05  Data: 0.022 (0.022)
Train: 254 [  50/1251 (  4%)]  Loss:  2.934613 (3.0217)  Time: 1.076s,  951.36/s  (1.085s,  943.83/s)  LR: 6.633e-05  Data: 0.016 (0.013)
Train: 254 [ 100/1251 (  8%)]  Loss:  3.134535 (3.0593)  Time: 1.104s,  927.38/s  (1.088s,  941.36/s)  LR: 6.633e-05  Data: 0.013 (0.013)
Train: 254 [ 150/1251 ( 12%)]  Loss:  2.864923 (3.0107)  Time: 1.205s,  849.96/s  (1.091s,  938.21/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 200/1251 ( 16%)]  Loss:  3.125864 (3.0337)  Time: 1.077s,  950.42/s  (1.092s,  937.84/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 250/1251 ( 20%)]  Loss:  3.015623 (3.0307)  Time: 1.102s,  929.15/s  (1.091s,  938.86/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 300/1251 ( 24%)]  Loss:  2.936191 (3.0172)  Time: 1.095s,  935.45/s  (1.090s,  939.10/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [ 350/1251 ( 28%)]  Loss:  2.921975 (3.0053)  Time: 1.096s,  934.11/s  (1.091s,  938.66/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 400/1251 ( 32%)]  Loss:  2.812960 (2.9839)  Time: 1.093s,  936.62/s  (1.090s,  939.57/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 450/1251 ( 36%)]  Loss:  3.100655 (2.9956)  Time: 1.096s,  934.34/s  (1.090s,  939.67/s)  LR: 6.633e-05  Data: 0.013 (0.013)
Train: 254 [ 500/1251 ( 40%)]  Loss:  2.913530 (2.9882)  Time: 1.078s,  949.96/s  (1.090s,  939.57/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 550/1251 ( 44%)]  Loss:  2.855645 (2.9771)  Time: 1.076s,  951.45/s  (1.089s,  939.98/s)  LR: 6.633e-05  Data: 0.012 (0.012)
Train: 254 [ 600/1251 ( 48%)]  Loss:  3.001376 (2.9790)  Time: 1.102s,  929.52/s  (1.089s,  940.05/s)  LR: 6.633e-05  Data: 0.012 (0.012)
Train: 254 [ 650/1251 ( 52%)]  Loss:  2.742437 (2.9621)  Time: 1.082s,  946.71/s  (1.089s,  940.26/s)  LR: 6.633e-05  Data: 0.012 (0.012)
Train: 254 [ 700/1251 ( 56%)]  Loss:  3.103878 (2.9715)  Time: 1.093s,  937.10/s  (1.089s,  940.08/s)  LR: 6.633e-05  Data: 0.012 (0.012)
Train: 254 [ 750/1251 ( 60%)]  Loss:  3.087417 (2.9788)  Time: 1.106s,  926.08/s  (1.089s,  940.18/s)  LR: 6.633e-05  Data: 0.011 (0.012)
Train: 254 [ 800/1251 ( 64%)]  Loss:  3.184880 (2.9909)  Time: 1.078s,  949.52/s  (1.089s,  940.21/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 850/1251 ( 68%)]  Loss:  3.196272 (3.0023)  Time: 1.095s,  934.79/s  (1.089s,  940.14/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [ 900/1251 ( 72%)]  Loss:  2.796350 (2.9915)  Time: 1.078s,  950.00/s  (1.089s,  940.12/s)  LR: 6.633e-05  Data: 0.014 (0.013)
Train: 254 [ 950/1251 ( 76%)]  Loss:  3.023962 (2.9931)  Time: 1.096s,  934.73/s  (1.089s,  940.29/s)  LR: 6.633e-05  Data: 0.011 (0.013)
Train: 254 [1000/1251 ( 80%)]  Loss:  2.595039 (2.9741)  Time: 1.077s,  950.95/s  (1.089s,  940.38/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [1050/1251 ( 84%)]  Loss:  3.081116 (2.9790)  Time: 1.077s,  950.74/s  (1.089s,  940.20/s)  LR: 6.633e-05  Data: 0.015 (0.013)
Train: 254 [1100/1251 ( 88%)]  Loss:  2.823767 (2.9723)  Time: 1.075s,  952.30/s  (1.089s,  940.37/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [1150/1251 ( 92%)]  Loss:  2.888346 (2.9688)  Time: 1.091s,  938.83/s  (1.089s,  940.63/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Train: 254 [1200/1251 ( 96%)]  Loss:  2.862298 (2.9645)  Time: 1.091s,  938.67/s  (1.089s,  940.27/s)  LR: 6.633e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 254 [1250/1251 (100%)]  Loss:  3.048463 (2.9677)  Time: 1.079s,  948.65/s  (1.089s,  939.94/s)  LR: 6.633e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.898 (5.898)  Loss:  0.4235 (0.4235)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5310 (0.8408)  Acc@1: 86.9104 (80.9640)  Acc@5: 98.4670 (95.5100)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-246.pth.tar', 80.61800005371094)

Train: 255 [   0/1251 (  0%)]  Loss:  2.889824 (2.8898)  Time: 1.091s,  938.61/s  (1.091s,  938.61/s)  LR: 6.395e-05  Data: 0.028 (0.028)
Train: 255 [  50/1251 (  4%)]  Loss:  2.851369 (2.8706)  Time: 1.079s,  948.60/s  (1.091s,  938.26/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 100/1251 (  8%)]  Loss:  3.084973 (2.9421)  Time: 1.079s,  949.20/s  (1.089s,  940.65/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 150/1251 ( 12%)]  Loss:  2.869751 (2.9240)  Time: 1.076s,  951.39/s  (1.087s,  942.34/s)  LR: 6.395e-05  Data: 0.013 (0.013)
Train: 255 [ 200/1251 ( 16%)]  Loss:  3.057273 (2.9506)  Time: 1.094s,  936.44/s  (1.088s,  940.82/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 250/1251 ( 20%)]  Loss:  3.063679 (2.9695)  Time: 1.080s,  948.38/s  (1.089s,  940.73/s)  LR: 6.395e-05  Data: 0.014 (0.013)
Train: 255 [ 300/1251 ( 24%)]  Loss:  3.295259 (3.0160)  Time: 1.084s,  944.77/s  (1.088s,  941.00/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 350/1251 ( 28%)]  Loss:  2.897495 (3.0012)  Time: 1.096s,  934.64/s  (1.088s,  940.95/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 400/1251 ( 32%)]  Loss:  3.170643 (3.0200)  Time: 1.080s,  947.90/s  (1.088s,  940.92/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [ 450/1251 ( 36%)]  Loss:  2.741360 (2.9922)  Time: 1.083s,  945.13/s  (1.088s,  940.99/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 500/1251 ( 40%)]  Loss:  2.774624 (2.9724)  Time: 1.173s,  873.01/s  (1.088s,  941.06/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 550/1251 ( 44%)]  Loss:  2.736427 (2.9527)  Time: 1.096s,  934.26/s  (1.088s,  941.09/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 600/1251 ( 48%)]  Loss:  3.013741 (2.9574)  Time: 1.084s,  944.96/s  (1.089s,  940.35/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 650/1251 ( 52%)]  Loss:  3.017658 (2.9617)  Time: 1.095s,  934.92/s  (1.089s,  940.34/s)  LR: 6.395e-05  Data: 0.014 (0.013)
Train: 255 [ 700/1251 ( 56%)]  Loss:  2.866574 (2.9554)  Time: 1.104s,  927.69/s  (1.089s,  939.97/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 750/1251 ( 60%)]  Loss:  3.028083 (2.9599)  Time: 1.094s,  935.83/s  (1.090s,  939.48/s)  LR: 6.395e-05  Data: 0.011 (0.013)
Train: 255 [ 800/1251 ( 64%)]  Loss:  2.848196 (2.9533)  Time: 1.076s,  951.25/s  (1.090s,  939.74/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 850/1251 ( 68%)]  Loss:  2.911408 (2.9510)  Time: 1.083s,  945.35/s  (1.089s,  939.99/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 900/1251 ( 72%)]  Loss:  2.872002 (2.9469)  Time: 1.097s,  933.66/s  (1.090s,  939.73/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [ 950/1251 ( 76%)]  Loss:  3.419358 (2.9705)  Time: 1.094s,  935.92/s  (1.090s,  939.70/s)  LR: 6.395e-05  Data: 0.016 (0.013)
Train: 255 [1000/1251 ( 80%)]  Loss:  2.779368 (2.9614)  Time: 1.094s,  935.93/s  (1.090s,  939.56/s)  LR: 6.395e-05  Data: 0.014 (0.013)
Train: 255 [1050/1251 ( 84%)]  Loss:  3.013074 (2.9637)  Time: 1.170s,  874.87/s  (1.090s,  939.40/s)  LR: 6.395e-05  Data: 0.014 (0.013)
Train: 255 [1100/1251 ( 88%)]  Loss:  3.044713 (2.9673)  Time: 1.094s,  936.26/s  (1.090s,  939.49/s)  LR: 6.395e-05  Data: 0.014 (0.013)
Train: 255 [1150/1251 ( 92%)]  Loss:  2.852473 (2.9625)  Time: 1.098s,  932.90/s  (1.090s,  939.42/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [1200/1251 ( 96%)]  Loss:  2.804346 (2.9561)  Time: 1.076s,  952.01/s  (1.090s,  939.45/s)  LR: 6.395e-05  Data: 0.012 (0.013)
Train: 255 [1250/1251 (100%)]  Loss:  2.803654 (2.9503)  Time: 1.080s,  948.38/s  (1.090s,  939.33/s)  LR: 6.395e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.910 (5.910)  Loss:  0.4388 (0.4388)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5334 (0.8510)  Acc@1: 87.5000 (81.0180)  Acc@5: 98.2311 (95.5220)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-244.pth.tar', 80.644000078125)

Train: 256 [   0/1251 (  0%)]  Loss:  3.020589 (3.0206)  Time: 1.082s,  945.99/s  (1.082s,  945.99/s)  LR: 6.162e-05  Data: 0.021 (0.021)
Train: 256 [  50/1251 (  4%)]  Loss:  2.948964 (2.9848)  Time: 1.096s,  934.45/s  (1.094s,  936.00/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 100/1251 (  8%)]  Loss:  2.789421 (2.9197)  Time: 1.078s,  950.33/s  (1.096s,  934.53/s)  LR: 6.162e-05  Data: 0.011 (0.012)
Train: 256 [ 150/1251 ( 12%)]  Loss:  3.096493 (2.9639)  Time: 1.077s,  950.96/s  (1.093s,  936.70/s)  LR: 6.162e-05  Data: 0.012 (0.012)
Train: 256 [ 200/1251 ( 16%)]  Loss:  3.021519 (2.9754)  Time: 1.078s,  950.16/s  (1.091s,  938.17/s)  LR: 6.162e-05  Data: 0.011 (0.012)
Train: 256 [ 250/1251 ( 20%)]  Loss:  2.880412 (2.9596)  Time: 1.077s,  950.95/s  (1.091s,  938.36/s)  LR: 6.162e-05  Data: 0.012 (0.012)
Train: 256 [ 300/1251 ( 24%)]  Loss:  2.966206 (2.9605)  Time: 1.078s,  949.60/s  (1.090s,  939.84/s)  LR: 6.162e-05  Data: 0.012 (0.012)
Train: 256 [ 350/1251 ( 28%)]  Loss:  2.722839 (2.9308)  Time: 1.078s,  950.12/s  (1.089s,  939.91/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 400/1251 ( 32%)]  Loss:  2.917789 (2.9294)  Time: 1.093s,  936.46/s  (1.089s,  940.15/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 450/1251 ( 36%)]  Loss:  2.802093 (2.9166)  Time: 1.074s,  953.88/s  (1.089s,  940.60/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [ 500/1251 ( 40%)]  Loss:  3.159841 (2.9387)  Time: 1.075s,  952.20/s  (1.088s,  940.91/s)  LR: 6.162e-05  Data: 0.014 (0.013)
Train: 256 [ 550/1251 ( 44%)]  Loss:  3.048433 (2.9479)  Time: 1.096s,  933.98/s  (1.088s,  941.09/s)  LR: 6.162e-05  Data: 0.014 (0.013)
Train: 256 [ 600/1251 ( 48%)]  Loss:  2.893594 (2.9437)  Time: 1.095s,  935.03/s  (1.089s,  940.49/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 650/1251 ( 52%)]  Loss:  3.096665 (2.9546)  Time: 1.094s,  936.04/s  (1.089s,  939.99/s)  LR: 6.162e-05  Data: 0.013 (0.013)
Train: 256 [ 700/1251 ( 56%)]  Loss:  2.978715 (2.9562)  Time: 1.077s,  950.54/s  (1.089s,  939.92/s)  LR: 6.162e-05  Data: 0.013 (0.013)
Train: 256 [ 750/1251 ( 60%)]  Loss:  2.755789 (2.9437)  Time: 1.076s,  951.57/s  (1.089s,  939.93/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 800/1251 ( 64%)]  Loss:  2.802282 (2.9354)  Time: 1.077s,  950.82/s  (1.089s,  940.44/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 850/1251 ( 68%)]  Loss:  2.935302 (2.9354)  Time: 1.095s,  935.16/s  (1.089s,  940.56/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [ 900/1251 ( 72%)]  Loss:  2.921846 (2.9347)  Time: 1.080s,  948.54/s  (1.089s,  940.65/s)  LR: 6.162e-05  Data: 0.013 (0.013)
Train: 256 [ 950/1251 ( 76%)]  Loss:  2.824302 (2.9292)  Time: 1.076s,  952.01/s  (1.088s,  940.94/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [1000/1251 ( 80%)]  Loss:  2.903032 (2.9279)  Time: 1.195s,  856.95/s  (1.088s,  940.84/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1050/1251 ( 84%)]  Loss:  2.884765 (2.9259)  Time: 1.156s,  885.50/s  (1.088s,  940.83/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [1100/1251 ( 88%)]  Loss:  3.015708 (2.9299)  Time: 1.080s,  948.37/s  (1.088s,  940.99/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [1150/1251 ( 92%)]  Loss:  2.869936 (2.9274)  Time: 1.097s,  933.77/s  (1.088s,  941.06/s)  LR: 6.162e-05  Data: 0.011 (0.013)
Train: 256 [1200/1251 ( 96%)]  Loss:  2.992294 (2.9300)  Time: 1.079s,  949.37/s  (1.088s,  940.98/s)  LR: 6.162e-05  Data: 0.012 (0.013)
Train: 256 [1250/1251 (100%)]  Loss:  2.675816 (2.9202)  Time: 1.075s,  952.41/s  (1.089s,  940.67/s)  LR: 6.162e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.901 (5.901)  Loss:  0.4256 (0.4256)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.442)  Loss:  0.5237 (0.8396)  Acc@1: 87.2642 (80.9760)  Acc@5: 97.8774 (95.5080)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-248.pth.tar', 80.65000005371094)

Train: 257 [   0/1251 (  0%)]  Loss:  3.138554 (3.1386)  Time: 1.086s,  942.99/s  (1.086s,  942.99/s)  LR: 5.934e-05  Data: 0.024 (0.024)
Train: 257 [  50/1251 (  4%)]  Loss:  2.934115 (3.0363)  Time: 1.109s,  923.46/s  (1.088s,  940.98/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [ 100/1251 (  8%)]  Loss:  2.834827 (2.9692)  Time: 1.105s,  926.43/s  (1.093s,  937.08/s)  LR: 5.934e-05  Data: 0.014 (0.013)
Train: 257 [ 150/1251 ( 12%)]  Loss:  3.017041 (2.9811)  Time: 1.093s,  936.91/s  (1.097s,  933.85/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 257 [ 200/1251 ( 16%)]  Loss:  2.896655 (2.9642)  Time: 1.082s,  946.35/s  (1.093s,  936.50/s)  LR: 5.934e-05  Data: 0.021 (0.012)
Train: 257 [ 250/1251 ( 20%)]  Loss:  2.945151 (2.9611)  Time: 1.175s,  871.29/s  (1.093s,  937.19/s)  LR: 5.934e-05  Data: 0.013 (0.012)
Train: 257 [ 300/1251 ( 24%)]  Loss:  2.692846 (2.9227)  Time: 1.077s,  950.36/s  (1.091s,  938.19/s)  LR: 5.934e-05  Data: 0.015 (0.012)
Train: 257 [ 350/1251 ( 28%)]  Loss:  2.741924 (2.9001)  Time: 1.098s,  932.95/s  (1.090s,  939.33/s)  LR: 5.934e-05  Data: 0.012 (0.012)
Train: 257 [ 400/1251 ( 32%)]  Loss:  3.002555 (2.9115)  Time: 1.094s,  935.88/s  (1.090s,  939.20/s)  LR: 5.934e-05  Data: 0.012 (0.012)
Train: 257 [ 450/1251 ( 36%)]  Loss:  2.917496 (2.9121)  Time: 1.080s,  948.31/s  (1.090s,  939.70/s)  LR: 5.934e-05  Data: 0.012 (0.012)
Train: 257 [ 500/1251 ( 40%)]  Loss:  2.966818 (2.9171)  Time: 1.103s,  928.79/s  (1.089s,  939.90/s)  LR: 5.934e-05  Data: 0.013 (0.012)
Train: 257 [ 550/1251 ( 44%)]  Loss:  2.936587 (2.9187)  Time: 1.079s,  949.29/s  (1.089s,  940.37/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [ 600/1251 ( 48%)]  Loss:  3.128268 (2.9348)  Time: 1.084s,  944.97/s  (1.089s,  940.63/s)  LR: 5.934e-05  Data: 0.015 (0.013)
Train: 257 [ 650/1251 ( 52%)]  Loss:  2.798780 (2.9251)  Time: 1.076s,  952.02/s  (1.089s,  940.49/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [ 700/1251 ( 56%)]  Loss:  3.249702 (2.9468)  Time: 1.095s,  934.76/s  (1.089s,  940.61/s)  LR: 5.934e-05  Data: 0.013 (0.013)
Train: 257 [ 750/1251 ( 60%)]  Loss:  2.892484 (2.9434)  Time: 1.083s,  945.57/s  (1.089s,  940.68/s)  LR: 5.934e-05  Data: 0.011 (0.013)
Train: 257 [ 800/1251 ( 64%)]  Loss:  2.574759 (2.9217)  Time: 1.075s,  952.47/s  (1.088s,  940.78/s)  LR: 5.934e-05  Data: 0.013 (0.013)
Train: 257 [ 850/1251 ( 68%)]  Loss:  3.039564 (2.9282)  Time: 1.075s,  952.46/s  (1.088s,  940.79/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [ 900/1251 ( 72%)]  Loss:  2.972208 (2.9305)  Time: 1.078s,  950.26/s  (1.089s,  940.72/s)  LR: 5.934e-05  Data: 0.014 (0.013)
Train: 257 [ 950/1251 ( 76%)]  Loss:  2.561407 (2.9121)  Time: 1.077s,  950.95/s  (1.089s,  940.64/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [1000/1251 ( 80%)]  Loss:  3.246663 (2.9280)  Time: 1.075s,  952.95/s  (1.088s,  940.93/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [1050/1251 ( 84%)]  Loss:  3.044490 (2.9333)  Time: 1.082s,  946.19/s  (1.088s,  940.79/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [1100/1251 ( 88%)]  Loss:  2.999965 (2.9362)  Time: 1.099s,  931.48/s  (1.089s,  940.44/s)  LR: 5.934e-05  Data: 0.012 (0.013)
Train: 257 [1150/1251 ( 92%)]  Loss:  2.891070 (2.9343)  Time: 1.080s,  948.58/s  (1.089s,  940.20/s)  LR: 5.934e-05  Data: 0.014 (0.013)
Train: 257 [1200/1251 ( 96%)]  Loss:  2.988433 (2.9365)  Time: 1.095s,  934.77/s  (1.089s,  940.48/s)  LR: 5.934e-05  Data: 0.014 (0.013)
Train: 257 [1250/1251 (100%)]  Loss:  2.991397 (2.9386)  Time: 1.079s,  949.37/s  (1.089s,  940.25/s)  LR: 5.934e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.860 (5.860)  Loss:  0.4214 (0.4214)  Acc@1: 93.5547 (93.5547)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5225 (0.8351)  Acc@1: 87.9717 (81.0860)  Acc@5: 97.8774 (95.5040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-245.pth.tar', 80.6920000024414)

Train: 258 [   0/1251 (  0%)]  Loss:  2.834879 (2.8349)  Time: 1.089s,  940.71/s  (1.089s,  940.71/s)  LR: 5.711e-05  Data: 0.027 (0.027)
Train: 258 [  50/1251 (  4%)]  Loss:  3.057853 (2.9464)  Time: 1.088s,  940.93/s  (1.087s,  942.05/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 100/1251 (  8%)]  Loss:  3.041217 (2.9780)  Time: 1.076s,  951.99/s  (1.087s,  942.08/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 150/1251 ( 12%)]  Loss:  3.130357 (3.0161)  Time: 1.079s,  949.07/s  (1.086s,  943.30/s)  LR: 5.711e-05  Data: 0.014 (0.013)
Train: 258 [ 200/1251 ( 16%)]  Loss:  3.325877 (3.0780)  Time: 1.082s,  946.29/s  (1.087s,  942.09/s)  LR: 5.711e-05  Data: 0.014 (0.013)
Train: 258 [ 250/1251 ( 20%)]  Loss:  3.034462 (3.0708)  Time: 1.094s,  935.69/s  (1.087s,  942.00/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 300/1251 ( 24%)]  Loss:  2.834687 (3.0370)  Time: 1.077s,  950.89/s  (1.087s,  942.17/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 350/1251 ( 28%)]  Loss:  2.697687 (2.9946)  Time: 1.096s,  933.91/s  (1.087s,  941.73/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 400/1251 ( 32%)]  Loss:  2.544606 (2.9446)  Time: 1.078s,  949.80/s  (1.088s,  940.86/s)  LR: 5.711e-05  Data: 0.013 (0.013)
Train: 258 [ 450/1251 ( 36%)]  Loss:  3.038689 (2.9540)  Time: 1.079s,  948.70/s  (1.088s,  941.28/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 500/1251 ( 40%)]  Loss:  2.738548 (2.9344)  Time: 1.080s,  948.54/s  (1.088s,  941.45/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 550/1251 ( 44%)]  Loss:  2.825233 (2.9253)  Time: 1.081s,  947.42/s  (1.088s,  941.40/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 600/1251 ( 48%)]  Loss:  2.952115 (2.9274)  Time: 1.080s,  948.09/s  (1.087s,  941.83/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 650/1251 ( 52%)]  Loss:  2.851961 (2.9220)  Time: 1.077s,  950.60/s  (1.087s,  941.90/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 700/1251 ( 56%)]  Loss:  3.042308 (2.9300)  Time: 1.074s,  953.12/s  (1.087s,  942.27/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 750/1251 ( 60%)]  Loss:  3.073651 (2.9390)  Time: 1.091s,  938.47/s  (1.088s,  941.55/s)  LR: 5.711e-05  Data: 0.011 (0.013)
Train: 258 [ 800/1251 ( 64%)]  Loss:  2.836887 (2.9330)  Time: 1.080s,  948.35/s  (1.088s,  941.35/s)  LR: 5.711e-05  Data: 0.016 (0.013)
Train: 258 [ 850/1251 ( 68%)]  Loss:  3.066307 (2.9404)  Time: 1.077s,  950.67/s  (1.088s,  940.83/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 900/1251 ( 72%)]  Loss:  3.010318 (2.9441)  Time: 1.076s,  952.03/s  (1.088s,  940.76/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [ 950/1251 ( 76%)]  Loss:  2.761718 (2.9350)  Time: 1.094s,  935.77/s  (1.088s,  940.85/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1000/1251 ( 80%)]  Loss:  2.908932 (2.9337)  Time: 1.093s,  936.93/s  (1.089s,  940.52/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 258 [1050/1251 ( 84%)]  Loss:  2.726554 (2.9243)  Time: 1.076s,  951.37/s  (1.089s,  940.55/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1100/1251 ( 88%)]  Loss:  2.635685 (2.9118)  Time: 1.098s,  932.30/s  (1.089s,  940.74/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1150/1251 ( 92%)]  Loss:  2.915800 (2.9119)  Time: 1.086s,  943.26/s  (1.089s,  940.72/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1200/1251 ( 96%)]  Loss:  3.179297 (2.9226)  Time: 1.077s,  950.90/s  (1.088s,  940.80/s)  LR: 5.711e-05  Data: 0.012 (0.013)
Train: 258 [1250/1251 (100%)]  Loss:  2.859080 (2.9202)  Time: 1.067s,  959.90/s  (1.088s,  940.80/s)  LR: 5.711e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.853 (5.853)  Loss:  0.4078 (0.4078)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5353 (0.8403)  Acc@1: 87.1462 (80.9500)  Acc@5: 98.3491 (95.4220)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-258.pth.tar', 80.94999997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-247.pth.tar', 80.74200007568359)

Train: 259 [   0/1251 (  0%)]  Loss:  2.731198 (2.7312)  Time: 1.084s,  944.57/s  (1.084s,  944.57/s)  LR: 5.493e-05  Data: 0.023 (0.023)
Train: 259 [  50/1251 (  4%)]  Loss:  3.049599 (2.8904)  Time: 1.080s,  948.25/s  (1.085s,  943.49/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 100/1251 (  8%)]  Loss:  2.750334 (2.8437)  Time: 1.082s,  946.19/s  (1.085s,  943.50/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 150/1251 ( 12%)]  Loss:  2.671011 (2.8005)  Time: 1.095s,  935.58/s  (1.087s,  942.44/s)  LR: 5.493e-05  Data: 0.013 (0.013)
Train: 259 [ 200/1251 ( 16%)]  Loss:  3.078502 (2.8561)  Time: 1.075s,  952.23/s  (1.086s,  942.63/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 250/1251 ( 20%)]  Loss:  2.916933 (2.8663)  Time: 1.189s,  861.45/s  (1.089s,  940.52/s)  LR: 5.493e-05  Data: 0.013 (0.013)
Train: 259 [ 300/1251 ( 24%)]  Loss:  2.888269 (2.8694)  Time: 1.096s,  934.35/s  (1.090s,  939.67/s)  LR: 5.493e-05  Data: 0.018 (0.013)
Train: 259 [ 350/1251 ( 28%)]  Loss:  2.651162 (2.8421)  Time: 1.076s,  951.39/s  (1.089s,  940.39/s)  LR: 5.493e-05  Data: 0.014 (0.013)
Train: 259 [ 400/1251 ( 32%)]  Loss:  3.066921 (2.8671)  Time: 1.093s,  937.24/s  (1.089s,  940.36/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 450/1251 ( 36%)]  Loss:  2.715700 (2.8520)  Time: 1.091s,  938.45/s  (1.089s,  940.09/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 500/1251 ( 40%)]  Loss:  2.879958 (2.8545)  Time: 1.098s,  932.94/s  (1.089s,  940.24/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 550/1251 ( 44%)]  Loss:  2.887968 (2.8573)  Time: 1.080s,  948.57/s  (1.089s,  940.12/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 600/1251 ( 48%)]  Loss:  2.851626 (2.8569)  Time: 1.078s,  950.18/s  (1.089s,  940.12/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 650/1251 ( 52%)]  Loss:  2.786704 (2.8518)  Time: 1.102s,  928.95/s  (1.089s,  940.27/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 700/1251 ( 56%)]  Loss:  3.094811 (2.8680)  Time: 1.096s,  934.35/s  (1.089s,  940.43/s)  LR: 5.493e-05  Data: 0.013 (0.013)
Train: 259 [ 750/1251 ( 60%)]  Loss:  3.111750 (2.8833)  Time: 1.073s,  954.55/s  (1.089s,  940.25/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 800/1251 ( 64%)]  Loss:  3.005850 (2.8905)  Time: 1.073s,  954.62/s  (1.090s,  939.81/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 850/1251 ( 68%)]  Loss:  2.877137 (2.8897)  Time: 1.095s,  935.38/s  (1.090s,  939.81/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [ 900/1251 ( 72%)]  Loss:  3.060570 (2.8987)  Time: 1.096s,  934.62/s  (1.089s,  940.19/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [ 950/1251 ( 76%)]  Loss:  3.101016 (2.9089)  Time: 1.095s,  935.09/s  (1.090s,  939.71/s)  LR: 5.493e-05  Data: 0.023 (0.013)
Train: 259 [1000/1251 ( 80%)]  Loss:  2.921324 (2.9094)  Time: 1.094s,  935.79/s  (1.090s,  939.42/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1050/1251 ( 84%)]  Loss:  2.818538 (2.9053)  Time: 1.095s,  934.83/s  (1.090s,  939.32/s)  LR: 5.493e-05  Data: 0.011 (0.013)
Train: 259 [1100/1251 ( 88%)]  Loss:  3.341633 (2.9243)  Time: 1.082s,  946.24/s  (1.090s,  939.25/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [1150/1251 ( 92%)]  Loss:  3.109849 (2.9320)  Time: 1.104s,  927.85/s  (1.091s,  938.99/s)  LR: 5.493e-05  Data: 0.016 (0.013)
Train: 259 [1200/1251 ( 96%)]  Loss:  3.015364 (2.9353)  Time: 1.115s,  918.73/s  (1.091s,  938.87/s)  LR: 5.493e-05  Data: 0.012 (0.013)
Train: 259 [1250/1251 (100%)]  Loss:  2.741060 (2.9279)  Time: 1.079s,  948.68/s  (1.091s,  938.77/s)  LR: 5.493e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.823 (5.823)  Loss:  0.4346 (0.4346)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5408 (0.8452)  Acc@1: 87.7358 (81.2100)  Acc@5: 98.3491 (95.5340)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-258.pth.tar', 80.94999997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-251.pth.tar', 80.79200015625)

Train: 260 [   0/1251 (  0%)]  Loss:  2.956704 (2.9567)  Time: 1.087s,  942.11/s  (1.087s,  942.11/s)  LR: 5.279e-05  Data: 0.024 (0.024)
Train: 260 [  50/1251 (  4%)]  Loss:  2.896943 (2.9268)  Time: 1.094s,  935.78/s  (1.090s,  939.14/s)  LR: 5.279e-05  Data: 0.014 (0.013)
Train: 260 [ 100/1251 (  8%)]  Loss:  2.796407 (2.8834)  Time: 1.096s,  934.72/s  (1.094s,  936.40/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 150/1251 ( 12%)]  Loss:  2.968148 (2.9046)  Time: 1.096s,  934.00/s  (1.093s,  937.05/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 200/1251 ( 16%)]  Loss:  3.063731 (2.9364)  Time: 1.076s,  951.74/s  (1.092s,  937.60/s)  LR: 5.279e-05  Data: 0.013 (0.013)
Train: 260 [ 250/1251 ( 20%)]  Loss:  2.613105 (2.8825)  Time: 1.079s,  948.83/s  (1.091s,  938.40/s)  LR: 5.279e-05  Data: 0.012 (0.012)
Train: 260 [ 300/1251 ( 24%)]  Loss:  2.708749 (2.8577)  Time: 1.084s,  944.23/s  (1.092s,  938.13/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 350/1251 ( 28%)]  Loss:  2.454304 (2.8073)  Time: 1.077s,  950.89/s  (1.091s,  938.48/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 400/1251 ( 32%)]  Loss:  2.909235 (2.8186)  Time: 1.078s,  949.90/s  (1.090s,  939.32/s)  LR: 5.279e-05  Data: 0.013 (0.013)
Train: 260 [ 450/1251 ( 36%)]  Loss:  2.934423 (2.8302)  Time: 1.079s,  948.89/s  (1.089s,  940.01/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 500/1251 ( 40%)]  Loss:  3.006810 (2.8462)  Time: 1.096s,  934.72/s  (1.089s,  940.17/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 550/1251 ( 44%)]  Loss:  2.756690 (2.8388)  Time: 1.094s,  935.75/s  (1.088s,  940.76/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 600/1251 ( 48%)]  Loss:  2.899537 (2.8434)  Time: 1.083s,  945.68/s  (1.089s,  940.23/s)  LR: 5.279e-05  Data: 0.013 (0.013)
Train: 260 [ 650/1251 ( 52%)]  Loss:  3.042625 (2.8577)  Time: 1.078s,  950.20/s  (1.089s,  940.29/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 700/1251 ( 56%)]  Loss:  3.042659 (2.8700)  Time: 1.094s,  936.03/s  (1.090s,  939.85/s)  LR: 5.279e-05  Data: 0.017 (0.013)
Train: 260 [ 750/1251 ( 60%)]  Loss:  3.077890 (2.8830)  Time: 1.077s,  951.05/s  (1.090s,  939.64/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [ 800/1251 ( 64%)]  Loss:  2.912532 (2.8847)  Time: 1.076s,  951.62/s  (1.089s,  939.92/s)  LR: 5.279e-05  Data: 0.013 (0.013)
Train: 260 [ 850/1251 ( 68%)]  Loss:  2.660837 (2.8723)  Time: 1.083s,  945.62/s  (1.089s,  940.23/s)  LR: 5.279e-05  Data: 0.011 (0.013)
Train: 260 [ 900/1251 ( 72%)]  Loss:  2.944481 (2.8761)  Time: 1.077s,  950.83/s  (1.089s,  940.36/s)  LR: 5.279e-05  Data: 0.014 (0.013)
Train: 260 [ 950/1251 ( 76%)]  Loss:  2.967405 (2.8807)  Time: 1.100s,  931.17/s  (1.089s,  940.23/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [1000/1251 ( 80%)]  Loss:  2.706112 (2.8723)  Time: 1.080s,  948.49/s  (1.089s,  940.45/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [1050/1251 ( 84%)]  Loss:  2.858522 (2.8717)  Time: 1.078s,  950.34/s  (1.089s,  940.68/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [1100/1251 ( 88%)]  Loss:  2.657732 (2.8624)  Time: 1.080s,  947.76/s  (1.088s,  940.94/s)  LR: 5.279e-05  Data: 0.012 (0.013)
Train: 260 [1150/1251 ( 92%)]  Loss:  2.862272 (2.8624)  Time: 1.095s,  934.85/s  (1.089s,  940.56/s)  LR: 5.279e-05  Data: 0.012 (0.012)
Train: 260 [1200/1251 ( 96%)]  Loss:  3.029465 (2.8691)  Time: 1.078s,  949.48/s  (1.089s,  940.68/s)  LR: 5.279e-05  Data: 0.016 (0.013)
Train: 260 [1250/1251 (100%)]  Loss:  2.986047 (2.8736)  Time: 1.062s,  964.48/s  (1.089s,  940.74/s)  LR: 5.279e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.824 (5.824)  Loss:  0.4217 (0.4217)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5392 (0.8391)  Acc@1: 87.6179 (81.1120)  Acc@5: 98.1132 (95.5040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-258.pth.tar', 80.94999997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-250.pth.tar', 80.7960000024414)

Train: 261 [   0/1251 (  0%)]  Loss:  2.843654 (2.8437)  Time: 1.086s,  943.05/s  (1.086s,  943.05/s)  LR: 5.071e-05  Data: 0.022 (0.022)
Train: 261 [  50/1251 (  4%)]  Loss:  2.870423 (2.8570)  Time: 1.076s,  951.31/s  (1.090s,  939.46/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 100/1251 (  8%)]  Loss:  2.725578 (2.8132)  Time: 1.096s,  934.27/s  (1.090s,  939.31/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 150/1251 ( 12%)]  Loss:  2.740182 (2.7950)  Time: 1.093s,  936.81/s  (1.091s,  938.25/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 200/1251 ( 16%)]  Loss:  2.689477 (2.7739)  Time: 1.077s,  950.41/s  (1.090s,  939.03/s)  LR: 5.071e-05  Data: 0.014 (0.013)
Train: 261 [ 250/1251 ( 20%)]  Loss:  2.706465 (2.7626)  Time: 1.077s,  950.36/s  (1.090s,  939.62/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 300/1251 ( 24%)]  Loss:  2.758511 (2.7620)  Time: 1.097s,  933.16/s  (1.090s,  939.24/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 350/1251 ( 28%)]  Loss:  2.834240 (2.7711)  Time: 1.095s,  935.20/s  (1.091s,  938.75/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 400/1251 ( 32%)]  Loss:  2.791170 (2.7733)  Time: 1.076s,  952.08/s  (1.090s,  939.45/s)  LR: 5.071e-05  Data: 0.014 (0.013)
Train: 261 [ 450/1251 ( 36%)]  Loss:  2.686566 (2.7646)  Time: 1.095s,  934.88/s  (1.090s,  939.26/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 500/1251 ( 40%)]  Loss:  2.751625 (2.7634)  Time: 1.095s,  935.18/s  (1.091s,  938.62/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 550/1251 ( 44%)]  Loss:  2.957087 (2.7796)  Time: 1.100s,  931.13/s  (1.091s,  938.64/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 600/1251 ( 48%)]  Loss:  2.778109 (2.7795)  Time: 1.084s,  944.72/s  (1.091s,  938.78/s)  LR: 5.071e-05  Data: 0.015 (0.013)
Train: 261 [ 650/1251 ( 52%)]  Loss:  2.916081 (2.7892)  Time: 1.078s,  950.19/s  (1.090s,  939.30/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 700/1251 ( 56%)]  Loss:  3.168021 (2.8145)  Time: 1.076s,  951.82/s  (1.090s,  939.78/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 750/1251 ( 60%)]  Loss:  2.729683 (2.8092)  Time: 1.079s,  949.20/s  (1.089s,  940.20/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 261 [ 800/1251 ( 64%)]  Loss:  3.240663 (2.8346)  Time: 1.078s,  949.90/s  (1.089s,  940.40/s)  LR: 5.071e-05  Data: 0.016 (0.013)
Train: 261 [ 850/1251 ( 68%)]  Loss:  3.176795 (2.8536)  Time: 1.077s,  951.11/s  (1.089s,  940.46/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 900/1251 ( 72%)]  Loss:  2.875795 (2.8547)  Time: 1.076s,  951.43/s  (1.089s,  940.43/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [ 950/1251 ( 76%)]  Loss:  3.029300 (2.8635)  Time: 1.093s,  936.59/s  (1.089s,  940.62/s)  LR: 5.071e-05  Data: 0.015 (0.013)
Train: 261 [1000/1251 ( 80%)]  Loss:  3.132872 (2.8763)  Time: 1.081s,  947.69/s  (1.089s,  940.45/s)  LR: 5.071e-05  Data: 0.014 (0.013)
Train: 261 [1050/1251 ( 84%)]  Loss:  2.942035 (2.8793)  Time: 1.090s,  939.53/s  (1.089s,  940.72/s)  LR: 5.071e-05  Data: 0.015 (0.013)
Train: 261 [1100/1251 ( 88%)]  Loss:  3.192403 (2.8929)  Time: 1.124s,  910.66/s  (1.088s,  940.85/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [1150/1251 ( 92%)]  Loss:  3.115167 (2.9022)  Time: 1.077s,  950.91/s  (1.089s,  940.73/s)  LR: 5.071e-05  Data: 0.012 (0.013)
Train: 261 [1200/1251 ( 96%)]  Loss:  2.731632 (2.8953)  Time: 1.078s,  949.85/s  (1.089s,  940.66/s)  LR: 5.071e-05  Data: 0.015 (0.013)
Train: 261 [1250/1251 (100%)]  Loss:  2.839957 (2.8932)  Time: 1.079s,  949.13/s  (1.089s,  940.71/s)  LR: 5.071e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.842 (5.842)  Loss:  0.4227 (0.4227)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5245 (0.8388)  Acc@1: 87.5000 (81.1000)  Acc@5: 98.2311 (95.5280)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-258.pth.tar', 80.94999997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-253.pth.tar', 80.81600000244141)

Train: 262 [   0/1251 (  0%)]  Loss:  3.059459 (3.0595)  Time: 1.083s,  945.63/s  (1.083s,  945.63/s)  LR: 4.868e-05  Data: 0.021 (0.021)
Train: 262 [  50/1251 (  4%)]  Loss:  2.795562 (2.9275)  Time: 1.098s,  932.85/s  (1.088s,  941.21/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 100/1251 (  8%)]  Loss:  2.851492 (2.9022)  Time: 1.078s,  949.49/s  (1.091s,  938.55/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 262 [ 150/1251 ( 12%)]  Loss:  2.344129 (2.7627)  Time: 1.113s,  919.93/s  (1.089s,  940.38/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 262 [ 200/1251 ( 16%)]  Loss:  2.832316 (2.7766)  Time: 1.171s,  874.25/s  (1.090s,  939.58/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 250/1251 ( 20%)]  Loss:  3.112016 (2.8325)  Time: 1.077s,  950.52/s  (1.088s,  941.00/s)  LR: 4.868e-05  Data: 0.015 (0.012)
Train: 262 [ 300/1251 ( 24%)]  Loss:  2.724943 (2.8171)  Time: 1.105s,  926.42/s  (1.089s,  940.23/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 350/1251 ( 28%)]  Loss:  2.844833 (2.8206)  Time: 1.076s,  951.87/s  (1.089s,  940.66/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 400/1251 ( 32%)]  Loss:  2.477676 (2.7825)  Time: 1.102s,  929.45/s  (1.088s,  941.00/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 450/1251 ( 36%)]  Loss:  2.963015 (2.8005)  Time: 1.076s,  951.99/s  (1.088s,  941.28/s)  LR: 4.868e-05  Data: 0.011 (0.012)
Train: 262 [ 500/1251 ( 40%)]  Loss:  2.870775 (2.8069)  Time: 1.077s,  950.55/s  (1.088s,  941.25/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 550/1251 ( 44%)]  Loss:  3.042426 (2.8266)  Time: 1.081s,  946.90/s  (1.088s,  941.16/s)  LR: 4.868e-05  Data: 0.016 (0.012)
Train: 262 [ 600/1251 ( 48%)]  Loss:  2.918474 (2.8336)  Time: 1.076s,  951.79/s  (1.088s,  941.36/s)  LR: 4.868e-05  Data: 0.013 (0.012)
Train: 262 [ 650/1251 ( 52%)]  Loss:  2.957621 (2.8425)  Time: 1.078s,  950.35/s  (1.088s,  940.98/s)  LR: 4.868e-05  Data: 0.014 (0.012)
Train: 262 [ 700/1251 ( 56%)]  Loss:  2.975280 (2.8513)  Time: 1.084s,  944.41/s  (1.088s,  941.38/s)  LR: 4.868e-05  Data: 0.012 (0.012)
Train: 262 [ 750/1251 ( 60%)]  Loss:  3.100130 (2.8669)  Time: 1.082s,  946.35/s  (1.088s,  941.43/s)  LR: 4.868e-05  Data: 0.013 (0.012)
Train: 262 [ 800/1251 ( 64%)]  Loss:  2.869906 (2.8671)  Time: 1.086s,  943.34/s  (1.088s,  941.25/s)  LR: 4.868e-05  Data: 0.015 (0.013)
Train: 262 [ 850/1251 ( 68%)]  Loss:  3.116619 (2.8809)  Time: 1.096s,  934.21/s  (1.088s,  941.38/s)  LR: 4.868e-05  Data: 0.012 (0.013)
Train: 262 [ 900/1251 ( 72%)]  Loss:  2.920798 (2.8830)  Time: 1.171s,  874.77/s  (1.088s,  941.05/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [ 950/1251 ( 76%)]  Loss:  2.796824 (2.8787)  Time: 1.077s,  950.74/s  (1.088s,  941.05/s)  LR: 4.868e-05  Data: 0.012 (0.013)
Train: 262 [1000/1251 ( 80%)]  Loss:  3.031608 (2.8860)  Time: 1.102s,  928.80/s  (1.088s,  940.99/s)  LR: 4.868e-05  Data: 0.012 (0.013)
Train: 262 [1050/1251 ( 84%)]  Loss:  3.009866 (2.8916)  Time: 1.095s,  934.87/s  (1.088s,  941.10/s)  LR: 4.868e-05  Data: 0.013 (0.013)
Train: 262 [1100/1251 ( 88%)]  Loss:  3.067584 (2.8993)  Time: 1.096s,  934.67/s  (1.088s,  940.84/s)  LR: 4.868e-05  Data: 0.012 (0.013)
Train: 262 [1150/1251 ( 92%)]  Loss:  2.818049 (2.8959)  Time: 1.076s,  951.64/s  (1.088s,  940.87/s)  LR: 4.868e-05  Data: 0.011 (0.013)
Train: 262 [1200/1251 ( 96%)]  Loss:  2.718349 (2.8888)  Time: 1.086s,  943.24/s  (1.088s,  941.05/s)  LR: 4.868e-05  Data: 0.012 (0.013)
Train: 262 [1250/1251 (100%)]  Loss:  2.857692 (2.8876)  Time: 1.061s,  964.88/s  (1.088s,  940.94/s)  LR: 4.868e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.812 (5.812)  Loss:  0.4213 (0.4213)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5123 (0.8429)  Acc@1: 88.2075 (81.2340)  Acc@5: 98.2311 (95.5180)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-258.pth.tar', 80.94999997558594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-252.pth.tar', 80.84199997558594)

Train: 263 [   0/1251 (  0%)]  Loss:  3.016129 (3.0161)  Time: 1.098s,  932.36/s  (1.098s,  932.36/s)  LR: 4.669e-05  Data: 0.026 (0.026)
Train: 263 [  50/1251 (  4%)]  Loss:  3.158467 (3.0873)  Time: 1.078s,  949.75/s  (1.092s,  937.85/s)  LR: 4.669e-05  Data: 0.016 (0.013)
Train: 263 [ 100/1251 (  8%)]  Loss:  3.196388 (3.1237)  Time: 1.077s,  951.02/s  (1.090s,  939.53/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 150/1251 ( 12%)]  Loss:  2.942002 (3.0782)  Time: 1.095s,  935.02/s  (1.093s,  937.19/s)  LR: 4.669e-05  Data: 0.012 (0.012)
Train: 263 [ 200/1251 ( 16%)]  Loss:  2.911532 (3.0449)  Time: 1.075s,  952.17/s  (1.090s,  939.31/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 250/1251 ( 20%)]  Loss:  2.986078 (3.0351)  Time: 1.078s,  949.85/s  (1.090s,  939.68/s)  LR: 4.669e-05  Data: 0.014 (0.013)
Train: 263 [ 300/1251 ( 24%)]  Loss:  2.915956 (3.0181)  Time: 1.097s,  933.21/s  (1.089s,  940.06/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 350/1251 ( 28%)]  Loss:  2.993609 (3.0150)  Time: 1.079s,  949.26/s  (1.089s,  939.89/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 400/1251 ( 32%)]  Loss:  3.035442 (3.0173)  Time: 1.075s,  952.18/s  (1.089s,  940.18/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 450/1251 ( 36%)]  Loss:  2.933892 (3.0089)  Time: 1.079s,  949.22/s  (1.089s,  940.30/s)  LR: 4.669e-05  Data: 0.014 (0.012)
Train: 263 [ 500/1251 ( 40%)]  Loss:  2.422400 (2.9556)  Time: 1.112s,  920.88/s  (1.089s,  939.95/s)  LR: 4.669e-05  Data: 0.011 (0.012)
Train: 263 [ 550/1251 ( 44%)]  Loss:  3.025860 (2.9615)  Time: 1.091s,  938.63/s  (1.090s,  939.61/s)  LR: 4.669e-05  Data: 0.013 (0.012)
Train: 263 [ 600/1251 ( 48%)]  Loss:  2.959512 (2.9613)  Time: 1.083s,  945.47/s  (1.090s,  939.62/s)  LR: 4.669e-05  Data: 0.012 (0.012)
Train: 263 [ 650/1251 ( 52%)]  Loss:  3.068675 (2.9690)  Time: 1.095s,  935.10/s  (1.090s,  939.50/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 700/1251 ( 56%)]  Loss:  2.958739 (2.9683)  Time: 1.086s,  942.88/s  (1.090s,  939.71/s)  LR: 4.669e-05  Data: 0.014 (0.013)
Train: 263 [ 750/1251 ( 60%)]  Loss:  2.722451 (2.9529)  Time: 1.082s,  945.99/s  (1.089s,  940.04/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 800/1251 ( 64%)]  Loss:  2.581805 (2.9311)  Time: 1.084s,  944.96/s  (1.089s,  940.14/s)  LR: 4.669e-05  Data: 0.014 (0.013)
Train: 263 [ 850/1251 ( 68%)]  Loss:  2.604548 (2.9130)  Time: 1.089s,  940.35/s  (1.089s,  939.89/s)  LR: 4.669e-05  Data: 0.013 (0.013)
Train: 263 [ 900/1251 ( 72%)]  Loss:  2.643448 (2.8988)  Time: 1.096s,  933.95/s  (1.089s,  940.25/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [ 950/1251 ( 76%)]  Loss:  3.098326 (2.9088)  Time: 1.096s,  934.14/s  (1.089s,  939.99/s)  LR: 4.669e-05  Data: 0.013 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 263 [1000/1251 ( 80%)]  Loss:  3.020630 (2.9141)  Time: 1.097s,  933.05/s  (1.089s,  940.03/s)  LR: 4.669e-05  Data: 0.012 (0.013)
Train: 263 [1050/1251 ( 84%)]  Loss:  2.709452 (2.9048)  Time: 1.095s,  935.55/s  (1.089s,  939.96/s)  LR: 4.669e-05  Data: 0.013 (0.013)
Train: 263 [1100/1251 ( 88%)]  Loss:  3.021495 (2.9099)  Time: 1.096s,  933.93/s  (1.090s,  939.88/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1150/1251 ( 92%)]  Loss:  2.772224 (2.9041)  Time: 1.084s,  945.02/s  (1.089s,  940.02/s)  LR: 4.669e-05  Data: 0.011 (0.013)
Train: 263 [1200/1251 ( 96%)]  Loss:  2.829521 (2.9011)  Time: 1.086s,  942.49/s  (1.089s,  940.29/s)  LR: 4.669e-05  Data: 0.014 (0.013)
Train: 263 [1250/1251 (100%)]  Loss:  3.122145 (2.9096)  Time: 1.081s,  947.56/s  (1.089s,  940.41/s)  LR: 4.669e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.803 (5.803)  Loss:  0.4150 (0.4150)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5165 (0.8354)  Acc@1: 87.3821 (81.2440)  Acc@5: 97.8774 (95.5380)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-258.pth.tar', 80.94999997558594)

Train: 264 [   0/1251 (  0%)]  Loss:  2.798177 (2.7982)  Time: 1.085s,  943.58/s  (1.085s,  943.58/s)  LR: 4.476e-05  Data: 0.025 (0.025)
Train: 264 [  50/1251 (  4%)]  Loss:  3.211672 (3.0049)  Time: 1.093s,  937.23/s  (1.088s,  940.89/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 100/1251 (  8%)]  Loss:  2.815067 (2.9416)  Time: 1.079s,  948.86/s  (1.087s,  941.63/s)  LR: 4.476e-05  Data: 0.016 (0.013)
Train: 264 [ 150/1251 ( 12%)]  Loss:  3.009671 (2.9586)  Time: 1.113s,  920.24/s  (1.087s,  941.77/s)  LR: 4.476e-05  Data: 0.014 (0.013)
Train: 264 [ 200/1251 ( 16%)]  Loss:  2.972398 (2.9614)  Time: 1.098s,  932.87/s  (1.089s,  940.52/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 250/1251 ( 20%)]  Loss:  3.038936 (2.9743)  Time: 1.096s,  934.35/s  (1.089s,  940.55/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 300/1251 ( 24%)]  Loss:  3.290996 (3.0196)  Time: 1.076s,  951.82/s  (1.090s,  939.87/s)  LR: 4.476e-05  Data: 0.014 (0.013)
Train: 264 [ 350/1251 ( 28%)]  Loss:  2.972714 (3.0137)  Time: 1.077s,  950.91/s  (1.089s,  940.45/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 400/1251 ( 32%)]  Loss:  2.833626 (2.9937)  Time: 1.082s,  946.00/s  (1.088s,  941.04/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 450/1251 ( 36%)]  Loss:  2.869133 (2.9812)  Time: 1.170s,  874.91/s  (1.088s,  941.14/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [ 500/1251 ( 40%)]  Loss:  2.887830 (2.9727)  Time: 1.081s,  947.50/s  (1.087s,  941.64/s)  LR: 4.476e-05  Data: 0.013 (0.013)
Train: 264 [ 550/1251 ( 44%)]  Loss:  3.099291 (2.9833)  Time: 1.191s,  859.59/s  (1.088s,  941.37/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 600/1251 ( 48%)]  Loss:  3.186401 (2.9989)  Time: 1.094s,  936.03/s  (1.089s,  940.73/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 650/1251 ( 52%)]  Loss:  2.915492 (2.9930)  Time: 1.092s,  937.37/s  (1.089s,  940.23/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 700/1251 ( 56%)]  Loss:  2.932474 (2.9889)  Time: 1.083s,  945.95/s  (1.089s,  940.23/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [ 750/1251 ( 60%)]  Loss:  2.864738 (2.9812)  Time: 1.080s,  948.37/s  (1.089s,  940.13/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 800/1251 ( 64%)]  Loss:  2.827628 (2.9721)  Time: 1.075s,  952.30/s  (1.090s,  939.80/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 850/1251 ( 68%)]  Loss:  2.966843 (2.9718)  Time: 1.087s,  942.00/s  (1.089s,  940.16/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [ 900/1251 ( 72%)]  Loss:  2.653630 (2.9551)  Time: 1.077s,  950.79/s  (1.089s,  940.15/s)  LR: 4.476e-05  Data: 0.013 (0.013)
Train: 264 [ 950/1251 ( 76%)]  Loss:  2.901435 (2.9524)  Time: 1.084s,  944.64/s  (1.089s,  940.27/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [1000/1251 ( 80%)]  Loss:  3.165884 (2.9626)  Time: 1.079s,  949.38/s  (1.089s,  940.29/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [1050/1251 ( 84%)]  Loss:  2.928284 (2.9610)  Time: 1.096s,  933.89/s  (1.089s,  940.39/s)  LR: 4.476e-05  Data: 0.015 (0.013)
Train: 264 [1100/1251 ( 88%)]  Loss:  2.899327 (2.9583)  Time: 1.077s,  950.52/s  (1.089s,  940.37/s)  LR: 4.476e-05  Data: 0.014 (0.013)
Train: 264 [1150/1251 ( 92%)]  Loss:  2.892142 (2.9556)  Time: 1.078s,  950.34/s  (1.089s,  940.43/s)  LR: 4.476e-05  Data: 0.012 (0.013)
Train: 264 [1200/1251 ( 96%)]  Loss:  2.735490 (2.9468)  Time: 1.101s,  930.02/s  (1.089s,  940.41/s)  LR: 4.476e-05  Data: 0.011 (0.013)
Train: 264 [1250/1251 (100%)]  Loss:  2.841871 (2.9427)  Time: 1.079s,  949.03/s  (1.089s,  940.42/s)  LR: 4.476e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.861 (5.861)  Loss:  0.4178 (0.4178)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.229 (0.447)  Loss:  0.5292 (0.8419)  Acc@1: 86.9104 (81.2180)  Acc@5: 97.9953 (95.5240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-254.pth.tar', 80.96400000244141)

Train: 265 [   0/1251 (  0%)]  Loss:  2.939507 (2.9395)  Time: 1.083s,  945.52/s  (1.083s,  945.52/s)  LR: 4.288e-05  Data: 0.023 (0.023)
Train: 265 [  50/1251 (  4%)]  Loss:  3.102108 (3.0208)  Time: 1.082s,  946.72/s  (1.088s,  941.09/s)  LR: 4.288e-05  Data: 0.013 (0.013)
Train: 265 [ 100/1251 (  8%)]  Loss:  2.916298 (2.9860)  Time: 1.078s,  949.60/s  (1.085s,  943.62/s)  LR: 4.288e-05  Data: 0.014 (0.013)
Train: 265 [ 150/1251 ( 12%)]  Loss:  3.120002 (3.0195)  Time: 1.101s,  930.37/s  (1.088s,  941.46/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 200/1251 ( 16%)]  Loss:  2.747990 (2.9652)  Time: 1.075s,  952.59/s  (1.089s,  940.36/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 250/1251 ( 20%)]  Loss:  2.604263 (2.9050)  Time: 1.082s,  946.82/s  (1.088s,  941.38/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 300/1251 ( 24%)]  Loss:  2.764587 (2.8850)  Time: 1.079s,  949.16/s  (1.088s,  940.96/s)  LR: 4.288e-05  Data: 0.017 (0.013)
Train: 265 [ 350/1251 ( 28%)]  Loss:  2.769408 (2.8705)  Time: 1.077s,  950.97/s  (1.088s,  941.56/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 400/1251 ( 32%)]  Loss:  2.944236 (2.8787)  Time: 1.077s,  950.93/s  (1.087s,  942.27/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 450/1251 ( 36%)]  Loss:  3.284537 (2.9193)  Time: 1.076s,  951.51/s  (1.087s,  942.07/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 500/1251 ( 40%)]  Loss:  2.821782 (2.9104)  Time: 1.103s,  928.30/s  (1.087s,  942.16/s)  LR: 4.288e-05  Data: 0.015 (0.013)
Train: 265 [ 550/1251 ( 44%)]  Loss:  2.914882 (2.9108)  Time: 1.077s,  950.63/s  (1.087s,  941.70/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 600/1251 ( 48%)]  Loss:  3.118167 (2.9268)  Time: 1.096s,  934.28/s  (1.088s,  941.42/s)  LR: 4.288e-05  Data: 0.011 (0.013)
Train: 265 [ 650/1251 ( 52%)]  Loss:  2.961786 (2.9293)  Time: 1.096s,  934.01/s  (1.088s,  940.84/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 700/1251 ( 56%)]  Loss:  2.565958 (2.9050)  Time: 1.077s,  951.12/s  (1.088s,  940.85/s)  LR: 4.288e-05  Data: 0.014 (0.013)
Train: 265 [ 750/1251 ( 60%)]  Loss:  3.072521 (2.9155)  Time: 1.098s,  932.71/s  (1.089s,  940.66/s)  LR: 4.288e-05  Data: 0.013 (0.013)
Train: 265 [ 800/1251 ( 64%)]  Loss:  3.021114 (2.9217)  Time: 1.096s,  934.59/s  (1.089s,  940.34/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 850/1251 ( 68%)]  Loss:  2.969255 (2.9244)  Time: 1.083s,  945.84/s  (1.089s,  940.32/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 900/1251 ( 72%)]  Loss:  2.871838 (2.9216)  Time: 1.083s,  945.85/s  (1.089s,  940.44/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [ 950/1251 ( 76%)]  Loss:  3.046912 (2.9279)  Time: 1.085s,  943.81/s  (1.089s,  940.57/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 265 [1000/1251 ( 80%)]  Loss:  2.753100 (2.9195)  Time: 1.096s,  934.14/s  (1.089s,  940.63/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1050/1251 ( 84%)]  Loss:  2.928794 (2.9200)  Time: 1.077s,  950.96/s  (1.089s,  940.60/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1100/1251 ( 88%)]  Loss:  2.871279 (2.9178)  Time: 1.106s,  925.97/s  (1.089s,  940.44/s)  LR: 4.288e-05  Data: 0.014 (0.013)
Train: 265 [1150/1251 ( 92%)]  Loss:  2.711431 (2.9092)  Time: 1.078s,  949.66/s  (1.089s,  940.31/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1200/1251 ( 96%)]  Loss:  3.021020 (2.9137)  Time: 1.076s,  951.72/s  (1.089s,  940.58/s)  LR: 4.288e-05  Data: 0.012 (0.013)
Train: 265 [1250/1251 (100%)]  Loss:  2.865783 (2.9119)  Time: 1.063s,  963.69/s  (1.089s,  940.49/s)  LR: 4.288e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.884 (5.884)  Loss:  0.4124 (0.4124)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5206 (0.8386)  Acc@1: 87.7358 (81.1280)  Acc@5: 97.9953 (95.5040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-265.pth.tar', 81.12799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-256.pth.tar', 80.97600002685547)

Train: 266 [   0/1251 (  0%)]  Loss:  3.074887 (3.0749)  Time: 1.083s,  945.42/s  (1.083s,  945.42/s)  LR: 4.105e-05  Data: 0.022 (0.022)
Train: 266 [  50/1251 (  4%)]  Loss:  2.913238 (2.9941)  Time: 1.093s,  937.06/s  (1.094s,  936.41/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 100/1251 (  8%)]  Loss:  2.704523 (2.8975)  Time: 1.076s,  952.02/s  (1.087s,  942.07/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 150/1251 ( 12%)]  Loss:  3.010116 (2.9257)  Time: 1.082s,  946.06/s  (1.086s,  943.18/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 200/1251 ( 16%)]  Loss:  2.982401 (2.9370)  Time: 1.085s,  944.13/s  (1.086s,  942.99/s)  LR: 4.105e-05  Data: 0.011 (0.013)
Train: 266 [ 250/1251 ( 20%)]  Loss:  2.925154 (2.9351)  Time: 1.091s,  938.28/s  (1.086s,  943.28/s)  LR: 4.105e-05  Data: 0.015 (0.013)
Train: 266 [ 300/1251 ( 24%)]  Loss:  2.987216 (2.9425)  Time: 1.088s,  941.09/s  (1.086s,  942.99/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 350/1251 ( 28%)]  Loss:  2.868436 (2.9332)  Time: 1.094s,  936.24/s  (1.087s,  942.29/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 400/1251 ( 32%)]  Loss:  2.874510 (2.9267)  Time: 1.084s,  944.35/s  (1.087s,  941.92/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 450/1251 ( 36%)]  Loss:  2.751991 (2.9092)  Time: 1.095s,  934.95/s  (1.087s,  941.87/s)  LR: 4.105e-05  Data: 0.014 (0.013)
Train: 266 [ 500/1251 ( 40%)]  Loss:  2.794529 (2.8988)  Time: 1.078s,  950.26/s  (1.087s,  941.78/s)  LR: 4.105e-05  Data: 0.014 (0.013)
Train: 266 [ 550/1251 ( 44%)]  Loss:  3.018061 (2.9088)  Time: 1.094s,  936.43/s  (1.088s,  941.32/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 600/1251 ( 48%)]  Loss:  2.855620 (2.9047)  Time: 1.076s,  952.01/s  (1.088s,  941.39/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 650/1251 ( 52%)]  Loss:  2.851120 (2.9008)  Time: 1.077s,  950.84/s  (1.087s,  941.62/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 700/1251 ( 56%)]  Loss:  2.947112 (2.9039)  Time: 1.078s,  949.83/s  (1.088s,  941.06/s)  LR: 4.105e-05  Data: 0.013 (0.013)
Train: 266 [ 750/1251 ( 60%)]  Loss:  2.838017 (2.8998)  Time: 1.083s,  945.65/s  (1.088s,  941.10/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 800/1251 ( 64%)]  Loss:  2.628753 (2.8839)  Time: 1.076s,  951.34/s  (1.088s,  941.21/s)  LR: 4.105e-05  Data: 0.013 (0.013)
Train: 266 [ 850/1251 ( 68%)]  Loss:  2.944487 (2.8872)  Time: 1.111s,  921.52/s  (1.088s,  940.93/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 900/1251 ( 72%)]  Loss:  2.992725 (2.8928)  Time: 1.096s,  934.37/s  (1.088s,  940.75/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [ 950/1251 ( 76%)]  Loss:  3.024550 (2.8994)  Time: 1.096s,  934.04/s  (1.089s,  940.69/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1000/1251 ( 80%)]  Loss:  2.639886 (2.8870)  Time: 1.097s,  933.61/s  (1.089s,  940.42/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1050/1251 ( 84%)]  Loss:  3.107595 (2.8970)  Time: 1.106s,  925.74/s  (1.089s,  940.27/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1100/1251 ( 88%)]  Loss:  2.961866 (2.8999)  Time: 1.077s,  951.06/s  (1.089s,  940.44/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1150/1251 ( 92%)]  Loss:  3.176619 (2.9114)  Time: 1.078s,  950.22/s  (1.089s,  940.53/s)  LR: 4.105e-05  Data: 0.013 (0.013)
Train: 266 [1200/1251 ( 96%)]  Loss:  2.704821 (2.9031)  Time: 1.179s,  868.53/s  (1.089s,  940.58/s)  LR: 4.105e-05  Data: 0.012 (0.013)
Train: 266 [1250/1251 (100%)]  Loss:  2.876597 (2.9021)  Time: 1.079s,  948.66/s  (1.089s,  940.32/s)  LR: 4.105e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.823 (5.823)  Loss:  0.4204 (0.4204)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5525 (0.8395)  Acc@1: 87.3821 (81.2260)  Acc@5: 97.6415 (95.5300)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-265.pth.tar', 81.12799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-255.pth.tar', 81.018)

Train: 267 [   0/1251 (  0%)]  Loss:  2.998020 (2.9980)  Time: 1.085s,  943.44/s  (1.085s,  943.44/s)  LR: 3.926e-05  Data: 0.022 (0.022)
Train: 267 [  50/1251 (  4%)]  Loss:  2.847486 (2.9228)  Time: 1.100s,  931.33/s  (1.090s,  939.38/s)  LR: 3.926e-05  Data: 0.013 (0.013)
Train: 267 [ 100/1251 (  8%)]  Loss:  2.936673 (2.9274)  Time: 1.076s,  952.10/s  (1.090s,  939.55/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 150/1251 ( 12%)]  Loss:  2.725303 (2.8769)  Time: 1.077s,  951.15/s  (1.089s,  940.46/s)  LR: 3.926e-05  Data: 0.014 (0.013)
Train: 267 [ 200/1251 ( 16%)]  Loss:  3.036486 (2.9088)  Time: 1.096s,  934.07/s  (1.090s,  939.67/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 250/1251 ( 20%)]  Loss:  2.957384 (2.9169)  Time: 1.095s,  935.45/s  (1.092s,  937.86/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 300/1251 ( 24%)]  Loss:  2.911443 (2.9161)  Time: 1.077s,  951.03/s  (1.093s,  937.03/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 350/1251 ( 28%)]  Loss:  3.084439 (2.9372)  Time: 1.095s,  934.85/s  (1.092s,  937.45/s)  LR: 3.926e-05  Data: 0.013 (0.013)
Train: 267 [ 400/1251 ( 32%)]  Loss:  3.279090 (2.9751)  Time: 1.078s,  949.84/s  (1.092s,  937.91/s)  LR: 3.926e-05  Data: 0.015 (0.013)
Train: 267 [ 450/1251 ( 36%)]  Loss:  2.806401 (2.9583)  Time: 1.097s,  933.84/s  (1.092s,  937.58/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [ 500/1251 ( 40%)]  Loss:  2.950311 (2.9575)  Time: 1.095s,  935.09/s  (1.092s,  937.99/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 267 [ 550/1251 ( 44%)]  Loss:  2.777663 (2.9426)  Time: 1.077s,  950.88/s  (1.091s,  938.61/s)  LR: 3.926e-05  Data: 0.014 (0.013)
Train: 267 [ 600/1251 ( 48%)]  Loss:  2.738261 (2.9268)  Time: 1.097s,  933.41/s  (1.091s,  938.73/s)  LR: 3.926e-05  Data: 0.013 (0.013)
Train: 267 [ 650/1251 ( 52%)]  Loss:  2.945450 (2.9282)  Time: 1.094s,  935.79/s  (1.091s,  938.68/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 700/1251 ( 56%)]  Loss:  2.869804 (2.9243)  Time: 1.095s,  935.08/s  (1.091s,  938.66/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 750/1251 ( 60%)]  Loss:  3.030127 (2.9309)  Time: 1.118s,  915.84/s  (1.091s,  938.55/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 800/1251 ( 64%)]  Loss:  2.696655 (2.9171)  Time: 1.075s,  952.40/s  (1.091s,  938.55/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 850/1251 ( 68%)]  Loss:  2.965924 (2.9198)  Time: 1.095s,  935.01/s  (1.091s,  938.96/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [ 900/1251 ( 72%)]  Loss:  3.094698 (2.9290)  Time: 1.078s,  949.60/s  (1.090s,  939.26/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [ 950/1251 ( 76%)]  Loss:  2.762218 (2.9207)  Time: 1.105s,  926.61/s  (1.090s,  939.30/s)  LR: 3.926e-05  Data: 0.013 (0.013)
Train: 267 [1000/1251 ( 80%)]  Loss:  3.156556 (2.9319)  Time: 1.114s,  918.96/s  (1.090s,  939.10/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [1050/1251 ( 84%)]  Loss:  2.445642 (2.9098)  Time: 1.096s,  934.65/s  (1.090s,  939.08/s)  LR: 3.926e-05  Data: 0.012 (0.013)
Train: 267 [1100/1251 ( 88%)]  Loss:  3.159701 (2.9207)  Time: 1.093s,  936.92/s  (1.090s,  939.13/s)  LR: 3.926e-05  Data: 0.013 (0.013)
Train: 267 [1150/1251 ( 92%)]  Loss:  2.795808 (2.9155)  Time: 1.096s,  933.88/s  (1.091s,  938.74/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1200/1251 ( 96%)]  Loss:  2.892727 (2.9146)  Time: 1.075s,  952.30/s  (1.091s,  938.64/s)  LR: 3.926e-05  Data: 0.011 (0.013)
Train: 267 [1250/1251 (100%)]  Loss:  3.008869 (2.9182)  Time: 1.079s,  948.85/s  (1.091s,  938.34/s)  LR: 3.926e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.971 (5.971)  Loss:  0.4065 (0.4065)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5144 (0.8323)  Acc@1: 87.3821 (81.1920)  Acc@5: 98.1132 (95.5280)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-267.pth.tar', 81.19199994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-265.pth.tar', 81.12799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-257.pth.tar', 81.08599994628906)

Train: 268 [   0/1251 (  0%)]  Loss:  2.954275 (2.9543)  Time: 1.084s,  944.41/s  (1.084s,  944.41/s)  LR: 3.753e-05  Data: 0.022 (0.022)
Train: 268 [  50/1251 (  4%)]  Loss:  2.715310 (2.8348)  Time: 1.083s,  945.49/s  (1.094s,  936.42/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 100/1251 (  8%)]  Loss:  2.975278 (2.8816)  Time: 1.077s,  951.18/s  (1.089s,  939.94/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 150/1251 ( 12%)]  Loss:  2.744780 (2.8474)  Time: 1.093s,  937.20/s  (1.088s,  940.76/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 200/1251 ( 16%)]  Loss:  3.021537 (2.8822)  Time: 1.086s,  943.17/s  (1.089s,  940.20/s)  LR: 3.753e-05  Data: 0.011 (0.012)
Train: 268 [ 250/1251 ( 20%)]  Loss:  2.870544 (2.8803)  Time: 1.083s,  945.71/s  (1.089s,  940.53/s)  LR: 3.753e-05  Data: 0.013 (0.013)
Train: 268 [ 300/1251 ( 24%)]  Loss:  2.771271 (2.8647)  Time: 1.101s,  929.96/s  (1.089s,  940.52/s)  LR: 3.753e-05  Data: 0.012 (0.012)
Train: 268 [ 350/1251 ( 28%)]  Loss:  2.641119 (2.8368)  Time: 1.077s,  951.21/s  (1.089s,  940.53/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 400/1251 ( 32%)]  Loss:  2.472316 (2.7963)  Time: 1.097s,  933.14/s  (1.088s,  940.98/s)  LR: 3.753e-05  Data: 0.012 (0.012)
Train: 268 [ 450/1251 ( 36%)]  Loss:  2.970560 (2.8137)  Time: 1.077s,  950.98/s  (1.088s,  941.41/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 500/1251 ( 40%)]  Loss:  3.031449 (2.8335)  Time: 1.095s,  934.89/s  (1.088s,  940.83/s)  LR: 3.753e-05  Data: 0.016 (0.013)
Train: 268 [ 550/1251 ( 44%)]  Loss:  2.910942 (2.8399)  Time: 1.080s,  948.57/s  (1.089s,  940.47/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 600/1251 ( 48%)]  Loss:  2.765742 (2.8342)  Time: 1.078s,  949.78/s  (1.089s,  940.60/s)  LR: 3.753e-05  Data: 0.014 (0.013)
Train: 268 [ 650/1251 ( 52%)]  Loss:  2.742035 (2.8277)  Time: 1.077s,  950.56/s  (1.088s,  940.77/s)  LR: 3.753e-05  Data: 0.015 (0.013)
Train: 268 [ 700/1251 ( 56%)]  Loss:  2.874628 (2.8308)  Time: 1.098s,  932.58/s  (1.089s,  940.50/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 750/1251 ( 60%)]  Loss:  2.582988 (2.8153)  Time: 1.078s,  949.89/s  (1.089s,  940.54/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 800/1251 ( 64%)]  Loss:  2.828084 (2.8161)  Time: 1.081s,  946.96/s  (1.089s,  940.54/s)  LR: 3.753e-05  Data: 0.011 (0.013)
Train: 268 [ 850/1251 ( 68%)]  Loss:  2.776483 (2.8139)  Time: 1.106s,  925.63/s  (1.089s,  940.24/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 900/1251 ( 72%)]  Loss:  2.622232 (2.8038)  Time: 1.095s,  934.85/s  (1.089s,  940.05/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [ 950/1251 ( 76%)]  Loss:  3.032849 (2.8152)  Time: 1.102s,  929.53/s  (1.089s,  940.05/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1000/1251 ( 80%)]  Loss:  3.127358 (2.8301)  Time: 1.082s,  946.28/s  (1.089s,  940.06/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1050/1251 ( 84%)]  Loss:  2.778402 (2.8277)  Time: 1.078s,  950.21/s  (1.089s,  939.98/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1100/1251 ( 88%)]  Loss:  2.816175 (2.8272)  Time: 1.078s,  949.98/s  (1.089s,  940.16/s)  LR: 3.753e-05  Data: 0.012 (0.013)
Train: 268 [1150/1251 ( 92%)]  Loss:  2.830493 (2.8274)  Time: 1.078s,  949.97/s  (1.089s,  940.45/s)  LR: 3.753e-05  Data: 0.012 (0.012)
Train: 268 [1200/1251 ( 96%)]  Loss:  2.955355 (2.8325)  Time: 1.080s,  947.91/s  (1.089s,  940.33/s)  LR: 3.753e-05  Data: 0.015 (0.013)
Train: 268 [1250/1251 (100%)]  Loss:  2.926286 (2.8361)  Time: 1.061s,  964.80/s  (1.089s,  940.44/s)  LR: 3.753e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.846 (5.846)  Loss:  0.4237 (0.4237)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5339 (0.8390)  Acc@1: 87.5000 (81.2180)  Acc@5: 97.7594 (95.5780)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-267.pth.tar', 81.19199994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-265.pth.tar', 81.12799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-261.pth.tar', 81.1)

Train: 269 [   0/1251 (  0%)]  Loss:  3.022510 (3.0225)  Time: 1.085s,  943.52/s  (1.085s,  943.52/s)  LR: 3.585e-05  Data: 0.025 (0.025)
Train: 269 [  50/1251 (  4%)]  Loss:  2.748841 (2.8857)  Time: 1.083s,  945.48/s  (1.085s,  943.65/s)  LR: 3.585e-05  Data: 0.012 (0.013)
Train: 269 [ 100/1251 (  8%)]  Loss:  2.823000 (2.8648)  Time: 1.108s,  924.60/s  (1.087s,  942.02/s)  LR: 3.585e-05  Data: 0.014 (0.013)
Train: 269 [ 150/1251 ( 12%)]  Loss:  2.806959 (2.8503)  Time: 1.076s,  951.51/s  (1.091s,  938.92/s)  LR: 3.585e-05  Data: 0.012 (0.013)
Train: 269 [ 200/1251 ( 16%)]  Loss:  2.804990 (2.8413)  Time: 1.076s,  951.26/s  (1.091s,  938.73/s)  LR: 3.585e-05  Data: 0.013 (0.013)
Train: 269 [ 250/1251 ( 20%)]  Loss:  2.548165 (2.7924)  Time: 1.073s,  954.31/s  (1.090s,  939.38/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 300/1251 ( 24%)]  Loss:  2.710886 (2.7808)  Time: 1.103s,  928.49/s  (1.091s,  938.67/s)  LR: 3.585e-05  Data: 0.012 (0.013)
Train: 269 [ 350/1251 ( 28%)]  Loss:  3.014300 (2.8100)  Time: 1.075s,  952.45/s  (1.090s,  939.56/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 400/1251 ( 32%)]  Loss:  2.642029 (2.7913)  Time: 1.081s,  947.39/s  (1.089s,  940.24/s)  LR: 3.585e-05  Data: 0.013 (0.012)
Train: 269 [ 450/1251 ( 36%)]  Loss:  3.086440 (2.8208)  Time: 1.099s,  932.06/s  (1.089s,  940.23/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 500/1251 ( 40%)]  Loss:  3.142039 (2.8500)  Time: 1.093s,  936.78/s  (1.089s,  940.33/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 550/1251 ( 44%)]  Loss:  2.870876 (2.8518)  Time: 1.079s,  949.37/s  (1.089s,  940.44/s)  LR: 3.585e-05  Data: 0.011 (0.012)
Train: 269 [ 600/1251 ( 48%)]  Loss:  2.962788 (2.8603)  Time: 1.079s,  949.04/s  (1.088s,  940.91/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 650/1251 ( 52%)]  Loss:  2.763520 (2.8534)  Time: 1.079s,  949.28/s  (1.088s,  940.99/s)  LR: 3.585e-05  Data: 0.012 (0.013)
Train: 269 [ 700/1251 ( 56%)]  Loss:  3.137160 (2.8723)  Time: 1.102s,  928.98/s  (1.089s,  940.54/s)  LR: 3.585e-05  Data: 0.014 (0.012)
Train: 269 [ 750/1251 ( 60%)]  Loss:  3.011241 (2.8810)  Time: 1.077s,  951.03/s  (1.088s,  940.81/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 800/1251 ( 64%)]  Loss:  3.061573 (2.8916)  Time: 1.094s,  936.02/s  (1.088s,  940.88/s)  LR: 3.585e-05  Data: 0.016 (0.012)
Train: 269 [ 850/1251 ( 68%)]  Loss:  2.836523 (2.8885)  Time: 1.094s,  936.42/s  (1.088s,  940.93/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 900/1251 ( 72%)]  Loss:  2.799884 (2.8839)  Time: 1.095s,  935.27/s  (1.089s,  940.58/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [ 950/1251 ( 76%)]  Loss:  2.670431 (2.8732)  Time: 1.084s,  944.77/s  (1.089s,  940.55/s)  LR: 3.585e-05  Data: 0.020 (0.012)
Train: 269 [1000/1251 ( 80%)]  Loss:  3.062199 (2.8822)  Time: 1.097s,  933.47/s  (1.089s,  940.65/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [1050/1251 ( 84%)]  Loss:  2.726537 (2.8751)  Time: 1.075s,  952.18/s  (1.089s,  940.74/s)  LR: 3.585e-05  Data: 0.014 (0.012)
Train: 269 [1100/1251 ( 88%)]  Loss:  3.034617 (2.8821)  Time: 1.106s,  926.01/s  (1.088s,  940.88/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [1150/1251 ( 92%)]  Loss:  2.895612 (2.8826)  Time: 1.096s,  933.97/s  (1.089s,  940.64/s)  LR: 3.585e-05  Data: 0.012 (0.012)
Train: 269 [1200/1251 ( 96%)]  Loss:  2.895363 (2.8831)  Time: 1.078s,  949.76/s  (1.088s,  940.77/s)  LR: 3.585e-05  Data: 0.017 (0.012)
Train: 269 [1250/1251 (100%)]  Loss:  2.737933 (2.8776)  Time: 1.062s,  964.51/s  (1.088s,  940.75/s)  LR: 3.585e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.875 (5.875)  Loss:  0.4237 (0.4237)  Acc@1: 92.9688 (92.9688)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5395 (0.8371)  Acc@1: 87.2642 (81.3700)  Acc@5: 97.9953 (95.5820)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-267.pth.tar', 81.19199994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-265.pth.tar', 81.12799997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-260.pth.tar', 81.11200005126953)

Train: 270 [   0/1251 (  0%)]  Loss:  2.609703 (2.6097)  Time: 1.093s,  936.56/s  (1.093s,  936.56/s)  LR: 3.423e-05  Data: 0.026 (0.026)
Train: 270 [  50/1251 (  4%)]  Loss:  3.063871 (2.8368)  Time: 1.083s,  945.33/s  (1.091s,  938.40/s)  LR: 3.423e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 270 [ 100/1251 (  8%)]  Loss:  2.904541 (2.8594)  Time: 1.078s,  950.00/s  (1.088s,  941.02/s)  LR: 3.423e-05  Data: 0.012 (0.013)
Train: 270 [ 150/1251 ( 12%)]  Loss:  2.829553 (2.8519)  Time: 1.077s,  950.79/s  (1.086s,  943.02/s)  LR: 3.423e-05  Data: 0.012 (0.013)
Train: 270 [ 200/1251 ( 16%)]  Loss:  2.885809 (2.8587)  Time: 1.188s,  861.97/s  (1.088s,  941.40/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 250/1251 ( 20%)]  Loss:  2.910732 (2.8674)  Time: 1.096s,  934.39/s  (1.090s,  939.80/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 300/1251 ( 24%)]  Loss:  2.820562 (2.8607)  Time: 1.093s,  936.46/s  (1.089s,  939.95/s)  LR: 3.423e-05  Data: 0.011 (0.013)
Train: 270 [ 350/1251 ( 28%)]  Loss:  2.588528 (2.8267)  Time: 1.093s,  936.76/s  (1.089s,  940.17/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 400/1251 ( 32%)]  Loss:  2.765965 (2.8199)  Time: 1.075s,  952.83/s  (1.089s,  940.54/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 450/1251 ( 36%)]  Loss:  2.695177 (2.8074)  Time: 1.076s,  951.30/s  (1.089s,  940.62/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 500/1251 ( 40%)]  Loss:  2.947476 (2.8202)  Time: 1.077s,  950.82/s  (1.089s,  940.55/s)  LR: 3.423e-05  Data: 0.013 (0.012)
Train: 270 [ 550/1251 ( 44%)]  Loss:  3.071566 (2.8411)  Time: 1.077s,  951.02/s  (1.088s,  941.10/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [ 600/1251 ( 48%)]  Loss:  2.714748 (2.8314)  Time: 1.096s,  933.97/s  (1.089s,  940.38/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 650/1251 ( 52%)]  Loss:  3.063485 (2.8480)  Time: 1.095s,  935.00/s  (1.090s,  939.82/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 700/1251 ( 56%)]  Loss:  3.009262 (2.8587)  Time: 1.106s,  925.59/s  (1.089s,  939.91/s)  LR: 3.423e-05  Data: 0.013 (0.012)
Train: 270 [ 750/1251 ( 60%)]  Loss:  2.861841 (2.8589)  Time: 1.106s,  925.63/s  (1.089s,  940.06/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 800/1251 ( 64%)]  Loss:  2.980458 (2.8661)  Time: 1.078s,  950.24/s  (1.090s,  939.47/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 850/1251 ( 68%)]  Loss:  3.026146 (2.8750)  Time: 1.096s,  934.11/s  (1.090s,  939.34/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 900/1251 ( 72%)]  Loss:  2.910464 (2.8768)  Time: 1.102s,  929.19/s  (1.090s,  939.44/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [ 950/1251 ( 76%)]  Loss:  2.821045 (2.8740)  Time: 1.099s,  931.61/s  (1.090s,  939.46/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [1000/1251 ( 80%)]  Loss:  2.761498 (2.8687)  Time: 1.082s,  946.74/s  (1.090s,  939.48/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1050/1251 ( 84%)]  Loss:  2.951380 (2.8724)  Time: 1.095s,  935.51/s  (1.090s,  939.41/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [1100/1251 ( 88%)]  Loss:  2.826771 (2.8705)  Time: 1.085s,  943.65/s  (1.090s,  939.20/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [1150/1251 ( 92%)]  Loss:  2.796168 (2.8674)  Time: 1.096s,  934.59/s  (1.090s,  939.18/s)  LR: 3.423e-05  Data: 0.012 (0.012)
Train: 270 [1200/1251 ( 96%)]  Loss:  3.008097 (2.8730)  Time: 1.097s,  933.81/s  (1.090s,  939.07/s)  LR: 3.423e-05  Data: 0.011 (0.012)
Train: 270 [1250/1251 (100%)]  Loss:  2.892305 (2.8737)  Time: 1.062s,  964.37/s  (1.091s,  938.82/s)  LR: 3.423e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.892 (5.892)  Loss:  0.4087 (0.4087)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5384 (0.8301)  Acc@1: 87.3821 (81.3240)  Acc@5: 98.1132 (95.5640)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-267.pth.tar', 81.19199994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-265.pth.tar', 81.12799997314453)

Train: 271 [   0/1251 (  0%)]  Loss:  2.915850 (2.9159)  Time: 1.083s,  945.38/s  (1.083s,  945.38/s)  LR: 3.265e-05  Data: 0.022 (0.022)
Train: 271 [  50/1251 (  4%)]  Loss:  3.020145 (2.9680)  Time: 1.104s,  927.74/s  (1.090s,  939.35/s)  LR: 3.265e-05  Data: 0.017 (0.013)
Train: 271 [ 100/1251 (  8%)]  Loss:  2.746597 (2.8942)  Time: 1.106s,  925.65/s  (1.090s,  939.23/s)  LR: 3.265e-05  Data: 0.012 (0.013)
Train: 271 [ 150/1251 ( 12%)]  Loss:  2.774008 (2.8642)  Time: 1.076s,  951.72/s  (1.090s,  939.15/s)  LR: 3.265e-05  Data: 0.012 (0.013)
Train: 271 [ 200/1251 ( 16%)]  Loss:  2.990457 (2.8894)  Time: 1.096s,  934.06/s  (1.090s,  939.16/s)  LR: 3.265e-05  Data: 0.011 (0.013)
Train: 271 [ 250/1251 ( 20%)]  Loss:  2.986632 (2.9056)  Time: 1.097s,  933.73/s  (1.092s,  937.80/s)  LR: 3.265e-05  Data: 0.013 (0.013)
Train: 271 [ 300/1251 ( 24%)]  Loss:  2.643115 (2.8681)  Time: 1.078s,  950.11/s  (1.093s,  937.08/s)  LR: 3.265e-05  Data: 0.012 (0.013)
Train: 271 [ 350/1251 ( 28%)]  Loss:  2.946172 (2.8779)  Time: 1.083s,  945.54/s  (1.092s,  937.82/s)  LR: 3.265e-05  Data: 0.013 (0.013)
Train: 271 [ 400/1251 ( 32%)]  Loss:  2.876014 (2.8777)  Time: 1.076s,  951.63/s  (1.091s,  938.65/s)  LR: 3.265e-05  Data: 0.013 (0.013)
Train: 271 [ 450/1251 ( 36%)]  Loss:  3.004820 (2.8904)  Time: 1.078s,  949.83/s  (1.090s,  939.15/s)  LR: 3.265e-05  Data: 0.015 (0.013)
Train: 271 [ 500/1251 ( 40%)]  Loss:  3.044068 (2.9044)  Time: 1.109s,  923.13/s  (1.090s,  939.48/s)  LR: 3.265e-05  Data: 0.012 (0.013)
Train: 271 [ 550/1251 ( 44%)]  Loss:  2.598634 (2.8789)  Time: 1.096s,  934.59/s  (1.089s,  940.10/s)  LR: 3.265e-05  Data: 0.012 (0.012)
Train: 271 [ 600/1251 ( 48%)]  Loss:  2.624035 (2.8593)  Time: 1.082s,  946.63/s  (1.089s,  940.32/s)  LR: 3.265e-05  Data: 0.012 (0.012)
Train: 271 [ 650/1251 ( 52%)]  Loss:  2.755213 (2.8518)  Time: 1.094s,  935.90/s  (1.089s,  940.16/s)  LR: 3.265e-05  Data: 0.017 (0.012)
Train: 271 [ 700/1251 ( 56%)]  Loss:  3.118845 (2.8696)  Time: 1.095s,  934.88/s  (1.089s,  940.34/s)  LR: 3.265e-05  Data: 0.013 (0.012)
Train: 271 [ 750/1251 ( 60%)]  Loss:  3.062705 (2.8817)  Time: 1.082s,  946.21/s  (1.089s,  940.38/s)  LR: 3.265e-05  Data: 0.012 (0.012)
Train: 271 [ 800/1251 ( 64%)]  Loss:  2.781996 (2.8758)  Time: 1.094s,  936.16/s  (1.089s,  940.22/s)  LR: 3.265e-05  Data: 0.013 (0.012)
Train: 271 [ 850/1251 ( 68%)]  Loss:  2.865752 (2.8753)  Time: 1.098s,  932.96/s  (1.089s,  940.24/s)  LR: 3.265e-05  Data: 0.015 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 271 [ 900/1251 ( 72%)]  Loss:  2.708370 (2.8665)  Time: 1.076s,  951.41/s  (1.089s,  940.56/s)  LR: 3.265e-05  Data: 0.012 (0.012)
Train: 271 [ 950/1251 ( 76%)]  Loss:  3.009210 (2.8736)  Time: 1.082s,  946.42/s  (1.089s,  940.68/s)  LR: 3.265e-05  Data: 0.013 (0.012)
Train: 271 [1000/1251 ( 80%)]  Loss:  2.848368 (2.8724)  Time: 1.078s,  949.55/s  (1.088s,  940.84/s)  LR: 3.265e-05  Data: 0.011 (0.012)
Train: 271 [1050/1251 ( 84%)]  Loss:  2.877192 (2.8726)  Time: 1.078s,  949.89/s  (1.088s,  941.01/s)  LR: 3.265e-05  Data: 0.014 (0.012)
Train: 271 [1100/1251 ( 88%)]  Loss:  2.934740 (2.8753)  Time: 1.081s,  947.67/s  (1.088s,  940.92/s)  LR: 3.265e-05  Data: 0.013 (0.012)
Train: 271 [1150/1251 ( 92%)]  Loss:  2.880835 (2.8756)  Time: 1.080s,  948.46/s  (1.088s,  940.76/s)  LR: 3.265e-05  Data: 0.012 (0.012)
Train: 271 [1200/1251 ( 96%)]  Loss:  2.793276 (2.8723)  Time: 1.077s,  951.22/s  (1.089s,  940.68/s)  LR: 3.265e-05  Data: 0.012 (0.012)
Train: 271 [1250/1251 (100%)]  Loss:  2.850842 (2.8715)  Time: 1.063s,  963.05/s  (1.088s,  940.77/s)  LR: 3.265e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.779 (5.779)  Loss:  0.4122 (0.4122)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5322 (0.8300)  Acc@1: 87.3821 (81.3340)  Acc@5: 97.9953 (95.5740)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-267.pth.tar', 81.19199994873047)

Train: 272 [   0/1251 (  0%)]  Loss:  2.926045 (2.9260)  Time: 1.084s,  944.65/s  (1.084s,  944.65/s)  LR: 3.113e-05  Data: 0.022 (0.022)
Train: 272 [  50/1251 (  4%)]  Loss:  2.754327 (2.8402)  Time: 1.095s,  935.25/s  (1.083s,  945.12/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 100/1251 (  8%)]  Loss:  3.021799 (2.9007)  Time: 1.093s,  936.59/s  (1.089s,  940.42/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 150/1251 ( 12%)]  Loss:  2.661044 (2.8408)  Time: 1.078s,  949.90/s  (1.089s,  940.32/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 200/1251 ( 16%)]  Loss:  2.777555 (2.8282)  Time: 1.078s,  950.33/s  (1.088s,  941.10/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 250/1251 ( 20%)]  Loss:  2.717608 (2.8097)  Time: 1.094s,  935.94/s  (1.089s,  940.62/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 300/1251 ( 24%)]  Loss:  2.912395 (2.8244)  Time: 1.096s,  934.28/s  (1.089s,  940.60/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 350/1251 ( 28%)]  Loss:  3.051375 (2.8528)  Time: 1.098s,  932.62/s  (1.090s,  939.27/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 400/1251 ( 32%)]  Loss:  3.027680 (2.8722)  Time: 1.078s,  949.53/s  (1.091s,  938.95/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 450/1251 ( 36%)]  Loss:  3.096564 (2.8946)  Time: 1.095s,  935.19/s  (1.090s,  939.55/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 500/1251 ( 40%)]  Loss:  2.961352 (2.9007)  Time: 1.076s,  951.74/s  (1.090s,  939.42/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 550/1251 ( 44%)]  Loss:  2.848273 (2.8963)  Time: 1.076s,  951.86/s  (1.090s,  939.75/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [ 600/1251 ( 48%)]  Loss:  2.812368 (2.8899)  Time: 1.078s,  949.91/s  (1.090s,  939.77/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 650/1251 ( 52%)]  Loss:  3.203423 (2.9123)  Time: 1.087s,  941.94/s  (1.090s,  939.83/s)  LR: 3.113e-05  Data: 0.014 (0.013)
Train: 272 [ 700/1251 ( 56%)]  Loss:  2.823000 (2.9063)  Time: 1.092s,  937.33/s  (1.089s,  939.99/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 750/1251 ( 60%)]  Loss:  3.041907 (2.9148)  Time: 1.076s,  951.40/s  (1.089s,  940.10/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 800/1251 ( 64%)]  Loss:  3.022038 (2.9211)  Time: 1.077s,  950.59/s  (1.089s,  940.27/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 850/1251 ( 68%)]  Loss:  2.894694 (2.9196)  Time: 1.080s,  948.25/s  (1.089s,  940.42/s)  LR: 3.113e-05  Data: 0.014 (0.013)
Train: 272 [ 900/1251 ( 72%)]  Loss:  2.805375 (2.9136)  Time: 1.098s,  932.58/s  (1.089s,  940.49/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [ 950/1251 ( 76%)]  Loss:  3.079345 (2.9219)  Time: 1.096s,  934.61/s  (1.089s,  940.18/s)  LR: 3.113e-05  Data: 0.014 (0.013)
Train: 272 [1000/1251 ( 80%)]  Loss:  2.903133 (2.9210)  Time: 1.077s,  950.58/s  (1.089s,  940.13/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [1050/1251 ( 84%)]  Loss:  2.889781 (2.9196)  Time: 1.097s,  933.10/s  (1.089s,  940.15/s)  LR: 3.113e-05  Data: 0.011 (0.013)
Train: 272 [1100/1251 ( 88%)]  Loss:  2.754048 (2.9124)  Time: 1.085s,  943.41/s  (1.089s,  939.90/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [1150/1251 ( 92%)]  Loss:  2.828269 (2.9089)  Time: 1.077s,  950.58/s  (1.089s,  940.03/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [1200/1251 ( 96%)]  Loss:  2.607914 (2.8969)  Time: 1.095s,  935.04/s  (1.089s,  939.99/s)  LR: 3.113e-05  Data: 0.012 (0.013)
Train: 272 [1250/1251 (100%)]  Loss:  2.433913 (2.8790)  Time: 1.079s,  948.82/s  (1.090s,  939.65/s)  LR: 3.113e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.880 (5.880)  Loss:  0.4177 (0.4177)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5170 (0.8315)  Acc@1: 87.3821 (81.2520)  Acc@5: 98.1132 (95.5800)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-259.pth.tar', 81.20999997314453)

Train: 273 [   0/1251 (  0%)]  Loss:  2.783700 (2.7837)  Time: 1.084s,  944.50/s  (1.084s,  944.50/s)  LR: 2.965e-05  Data: 0.022 (0.022)
Train: 273 [  50/1251 (  4%)]  Loss:  3.039033 (2.9114)  Time: 1.076s,  951.71/s  (1.090s,  939.53/s)  LR: 2.965e-05  Data: 0.012 (0.012)
Train: 273 [ 100/1251 (  8%)]  Loss:  2.563177 (2.7953)  Time: 1.081s,  947.16/s  (1.087s,  941.98/s)  LR: 2.965e-05  Data: 0.015 (0.013)
Train: 273 [ 150/1251 ( 12%)]  Loss:  2.747992 (2.7835)  Time: 1.077s,  950.91/s  (1.089s,  940.74/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 200/1251 ( 16%)]  Loss:  2.963043 (2.8194)  Time: 1.099s,  932.12/s  (1.090s,  939.82/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 250/1251 ( 20%)]  Loss:  2.766365 (2.8106)  Time: 1.076s,  951.34/s  (1.090s,  939.17/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 300/1251 ( 24%)]  Loss:  3.026605 (2.8414)  Time: 1.078s,  950.10/s  (1.090s,  939.29/s)  LR: 2.965e-05  Data: 0.013 (0.013)
Train: 273 [ 350/1251 ( 28%)]  Loss:  2.997375 (2.8609)  Time: 1.077s,  950.70/s  (1.089s,  940.59/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 400/1251 ( 32%)]  Loss:  2.773979 (2.8513)  Time: 1.076s,  951.48/s  (1.087s,  941.67/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 450/1251 ( 36%)]  Loss:  3.047416 (2.8709)  Time: 1.094s,  935.67/s  (1.088s,  941.50/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 500/1251 ( 40%)]  Loss:  2.990365 (2.8817)  Time: 1.078s,  950.17/s  (1.088s,  941.52/s)  LR: 2.965e-05  Data: 0.014 (0.013)
Train: 273 [ 550/1251 ( 44%)]  Loss:  2.956737 (2.8880)  Time: 1.096s,  934.37/s  (1.089s,  940.71/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [ 600/1251 ( 48%)]  Loss:  2.933496 (2.8915)  Time: 1.078s,  949.56/s  (1.088s,  940.90/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 650/1251 ( 52%)]  Loss:  2.759949 (2.8821)  Time: 1.078s,  949.93/s  (1.088s,  940.77/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [ 700/1251 ( 56%)]  Loss:  2.786273 (2.8757)  Time: 1.095s,  934.88/s  (1.089s,  940.51/s)  LR: 2.965e-05  Data: 0.013 (0.013)
Train: 273 [ 750/1251 ( 60%)]  Loss:  2.861581 (2.8748)  Time: 1.077s,  950.40/s  (1.089s,  940.35/s)  LR: 2.965e-05  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 273 [ 800/1251 ( 64%)]  Loss:  2.736370 (2.8667)  Time: 1.094s,  936.34/s  (1.089s,  940.58/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 850/1251 ( 68%)]  Loss:  2.891837 (2.8681)  Time: 1.092s,  937.38/s  (1.089s,  940.49/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [ 900/1251 ( 72%)]  Loss:  2.893115 (2.8694)  Time: 1.081s,  946.84/s  (1.088s,  940.84/s)  LR: 2.965e-05  Data: 0.016 (0.013)
Train: 273 [ 950/1251 ( 76%)]  Loss:  2.781805 (2.8650)  Time: 1.096s,  934.47/s  (1.089s,  940.74/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [1000/1251 ( 80%)]  Loss:  2.990477 (2.8710)  Time: 1.078s,  950.23/s  (1.088s,  940.75/s)  LR: 2.965e-05  Data: 0.011 (0.013)
Train: 273 [1050/1251 ( 84%)]  Loss:  2.649931 (2.8609)  Time: 1.075s,  952.83/s  (1.088s,  940.94/s)  LR: 2.965e-05  Data: 0.012 (0.013)
Train: 273 [1100/1251 ( 88%)]  Loss:  3.113221 (2.8719)  Time: 1.081s,  946.83/s  (1.088s,  941.23/s)  LR: 2.965e-05  Data: 0.013 (0.013)
Train: 273 [1150/1251 ( 92%)]  Loss:  2.889914 (2.8727)  Time: 1.105s,  926.29/s  (1.088s,  941.08/s)  LR: 2.965e-05  Data: 0.012 (0.012)
Train: 273 [1200/1251 ( 96%)]  Loss:  3.023545 (2.8787)  Time: 1.096s,  934.33/s  (1.088s,  941.03/s)  LR: 2.965e-05  Data: 0.014 (0.012)
Train: 273 [1250/1251 (100%)]  Loss:  3.011336 (2.8838)  Time: 1.063s,  963.03/s  (1.088s,  940.89/s)  LR: 2.965e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.682 (5.682)  Loss:  0.4315 (0.4315)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5359 (0.8342)  Acc@1: 87.1462 (81.3060)  Acc@5: 98.1132 (95.6380)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-264.pth.tar', 81.2180000024414)

Train: 274 [   0/1251 (  0%)]  Loss:  2.796876 (2.7969)  Time: 1.100s,  930.91/s  (1.100s,  930.91/s)  LR: 2.823e-05  Data: 0.023 (0.023)
Train: 274 [  50/1251 (  4%)]  Loss:  2.904137 (2.8505)  Time: 1.097s,  933.06/s  (1.091s,  938.23/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 100/1251 (  8%)]  Loss:  2.892287 (2.8644)  Time: 1.080s,  948.12/s  (1.088s,  940.75/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 274 [ 150/1251 ( 12%)]  Loss:  2.968191 (2.8904)  Time: 1.077s,  951.02/s  (1.088s,  941.38/s)  LR: 2.823e-05  Data: 0.015 (0.013)
Train: 274 [ 200/1251 ( 16%)]  Loss:  2.884938 (2.8893)  Time: 1.099s,  931.47/s  (1.088s,  941.31/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 250/1251 ( 20%)]  Loss:  2.884089 (2.8884)  Time: 1.083s,  945.74/s  (1.087s,  942.08/s)  LR: 2.823e-05  Data: 0.011 (0.013)
Train: 274 [ 300/1251 ( 24%)]  Loss:  3.044684 (2.9107)  Time: 1.078s,  950.24/s  (1.086s,  942.71/s)  LR: 2.823e-05  Data: 0.014 (0.013)
Train: 274 [ 350/1251 ( 28%)]  Loss:  2.799895 (2.8969)  Time: 1.095s,  935.11/s  (1.087s,  942.44/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 400/1251 ( 32%)]  Loss:  3.133367 (2.9232)  Time: 1.080s,  947.72/s  (1.087s,  941.62/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 450/1251 ( 36%)]  Loss:  2.831707 (2.9140)  Time: 1.077s,  950.81/s  (1.088s,  941.58/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 500/1251 ( 40%)]  Loss:  2.840664 (2.9073)  Time: 1.078s,  949.64/s  (1.087s,  941.76/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 550/1251 ( 44%)]  Loss:  3.041681 (2.9185)  Time: 1.090s,  939.25/s  (1.088s,  941.44/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [ 600/1251 ( 48%)]  Loss:  2.821357 (2.9111)  Time: 1.080s,  948.18/s  (1.088s,  941.12/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 274 [ 650/1251 ( 52%)]  Loss:  3.118947 (2.9259)  Time: 1.095s,  934.97/s  (1.088s,  940.85/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 274 [ 700/1251 ( 56%)]  Loss:  2.941000 (2.9269)  Time: 1.077s,  950.43/s  (1.089s,  940.73/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [ 750/1251 ( 60%)]  Loss:  3.020153 (2.9327)  Time: 1.096s,  934.45/s  (1.089s,  940.54/s)  LR: 2.823e-05  Data: 0.014 (0.012)
Train: 274 [ 800/1251 ( 64%)]  Loss:  2.826360 (2.9265)  Time: 1.096s,  934.23/s  (1.089s,  940.62/s)  LR: 2.823e-05  Data: 0.014 (0.012)
Train: 274 [ 850/1251 ( 68%)]  Loss:  2.830729 (2.9212)  Time: 1.078s,  950.15/s  (1.089s,  940.56/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [ 900/1251 ( 72%)]  Loss:  2.553453 (2.9018)  Time: 1.102s,  929.11/s  (1.089s,  940.42/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [ 950/1251 ( 76%)]  Loss:  2.951054 (2.9043)  Time: 1.077s,  950.97/s  (1.089s,  940.72/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [1000/1251 ( 80%)]  Loss:  2.628487 (2.8911)  Time: 1.080s,  948.38/s  (1.088s,  940.93/s)  LR: 2.823e-05  Data: 0.013 (0.013)
Train: 274 [1050/1251 ( 84%)]  Loss:  3.069822 (2.8993)  Time: 1.093s,  937.21/s  (1.088s,  940.77/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [1100/1251 ( 88%)]  Loss:  2.819067 (2.8958)  Time: 1.075s,  952.78/s  (1.089s,  940.66/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [1150/1251 ( 92%)]  Loss:  3.150064 (2.9064)  Time: 1.094s,  935.80/s  (1.089s,  940.57/s)  LR: 2.823e-05  Data: 0.012 (0.012)
Train: 274 [1200/1251 ( 96%)]  Loss:  2.763509 (2.9007)  Time: 1.076s,  951.70/s  (1.089s,  940.58/s)  LR: 2.823e-05  Data: 0.012 (0.013)
Train: 274 [1250/1251 (100%)]  Loss:  2.760689 (2.8953)  Time: 1.078s,  950.02/s  (1.089s,  940.41/s)  LR: 2.823e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.785 (5.785)  Loss:  0.4170 (0.4170)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5313 (0.8268)  Acc@1: 87.6179 (81.5180)  Acc@5: 97.8774 (95.7040)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-268.pth.tar', 81.21800012939453)

Train: 275 [   0/1251 (  0%)]  Loss:  3.071020 (3.0710)  Time: 1.084s,  944.87/s  (1.084s,  944.87/s)  LR: 2.687e-05  Data: 0.022 (0.022)
Train: 275 [  50/1251 (  4%)]  Loss:  2.704909 (2.8880)  Time: 1.092s,  937.87/s  (1.092s,  937.78/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 100/1251 (  8%)]  Loss:  2.914872 (2.8969)  Time: 1.074s,  953.15/s  (1.088s,  941.26/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 150/1251 ( 12%)]  Loss:  3.019121 (2.9275)  Time: 1.094s,  936.00/s  (1.088s,  941.23/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 200/1251 ( 16%)]  Loss:  2.978829 (2.9378)  Time: 1.094s,  936.09/s  (1.088s,  941.37/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 275 [ 250/1251 ( 20%)]  Loss:  2.921106 (2.9350)  Time: 1.095s,  934.80/s  (1.088s,  941.17/s)  LR: 2.687e-05  Data: 0.013 (0.012)
Train: 275 [ 300/1251 ( 24%)]  Loss:  3.047996 (2.9511)  Time: 1.182s,  866.68/s  (1.090s,  939.81/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 350/1251 ( 28%)]  Loss:  2.927808 (2.9482)  Time: 1.105s,  926.38/s  (1.090s,  939.48/s)  LR: 2.687e-05  Data: 0.014 (0.012)
Train: 275 [ 400/1251 ( 32%)]  Loss:  3.136407 (2.9691)  Time: 1.082s,  946.48/s  (1.091s,  938.76/s)  LR: 2.687e-05  Data: 0.011 (0.012)
Train: 275 [ 450/1251 ( 36%)]  Loss:  2.800038 (2.9522)  Time: 1.076s,  952.09/s  (1.090s,  939.15/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 500/1251 ( 40%)]  Loss:  2.659867 (2.9256)  Time: 1.087s,  942.45/s  (1.090s,  939.52/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 550/1251 ( 44%)]  Loss:  2.495323 (2.8898)  Time: 1.082s,  946.81/s  (1.089s,  939.96/s)  LR: 2.687e-05  Data: 0.013 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 275 [ 600/1251 ( 48%)]  Loss:  2.792707 (2.8823)  Time: 1.076s,  951.92/s  (1.089s,  940.73/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 650/1251 ( 52%)]  Loss:  2.804224 (2.8767)  Time: 1.094s,  936.28/s  (1.088s,  940.78/s)  LR: 2.687e-05  Data: 0.014 (0.012)
Train: 275 [ 700/1251 ( 56%)]  Loss:  2.895041 (2.8780)  Time: 1.078s,  949.97/s  (1.088s,  940.96/s)  LR: 2.687e-05  Data: 0.012 (0.012)
Train: 275 [ 750/1251 ( 60%)]  Loss:  3.013819 (2.8864)  Time: 1.105s,  926.56/s  (1.088s,  941.33/s)  LR: 2.687e-05  Data: 0.014 (0.012)
Train: 275 [ 800/1251 ( 64%)]  Loss:  2.951722 (2.8903)  Time: 1.077s,  950.46/s  (1.088s,  941.07/s)  LR: 2.687e-05  Data: 0.014 (0.012)
Train: 275 [ 850/1251 ( 68%)]  Loss:  3.112519 (2.9026)  Time: 1.094s,  935.61/s  (1.089s,  940.61/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [ 900/1251 ( 72%)]  Loss:  2.809051 (2.8977)  Time: 1.079s,  948.66/s  (1.088s,  940.95/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [ 950/1251 ( 76%)]  Loss:  2.734304 (2.8895)  Time: 1.084s,  944.35/s  (1.088s,  941.25/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [1000/1251 ( 80%)]  Loss:  2.926034 (2.8913)  Time: 1.095s,  935.40/s  (1.088s,  940.98/s)  LR: 2.687e-05  Data: 0.011 (0.013)
Train: 275 [1050/1251 ( 84%)]  Loss:  2.869977 (2.8903)  Time: 1.076s,  951.41/s  (1.088s,  941.22/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [1100/1251 ( 88%)]  Loss:  2.910852 (2.8912)  Time: 1.078s,  950.31/s  (1.088s,  941.35/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [1150/1251 ( 92%)]  Loss:  3.089773 (2.8995)  Time: 1.077s,  950.54/s  (1.088s,  941.44/s)  LR: 2.687e-05  Data: 0.012 (0.013)
Train: 275 [1200/1251 ( 96%)]  Loss:  2.869085 (2.8983)  Time: 1.077s,  950.58/s  (1.088s,  941.42/s)  LR: 2.687e-05  Data: 0.015 (0.013)
Train: 275 [1250/1251 (100%)]  Loss:  3.118884 (2.9067)  Time: 1.078s,  949.50/s  (1.088s,  941.26/s)  LR: 2.687e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.884 (5.884)  Loss:  0.4150 (0.4150)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5394 (0.8280)  Acc@1: 87.3821 (81.2500)  Acc@5: 97.9953 (95.5860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-275.pth.tar', 81.24999994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-266.pth.tar', 81.22599994873048)

Train: 276 [   0/1251 (  0%)]  Loss:  2.670016 (2.6700)  Time: 1.083s,  945.56/s  (1.083s,  945.56/s)  LR: 2.555e-05  Data: 0.022 (0.022)
Train: 276 [  50/1251 (  4%)]  Loss:  2.809819 (2.7399)  Time: 1.079s,  948.77/s  (1.087s,  941.61/s)  LR: 2.555e-05  Data: 0.013 (0.013)
Train: 276 [ 100/1251 (  8%)]  Loss:  2.798529 (2.7595)  Time: 1.076s,  951.51/s  (1.088s,  941.49/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 150/1251 ( 12%)]  Loss:  2.836580 (2.7787)  Time: 1.076s,  951.70/s  (1.088s,  940.98/s)  LR: 2.555e-05  Data: 0.014 (0.012)
Train: 276 [ 200/1251 ( 16%)]  Loss:  2.837888 (2.7906)  Time: 1.079s,  949.33/s  (1.086s,  942.52/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [ 250/1251 ( 20%)]  Loss:  2.912060 (2.8108)  Time: 1.095s,  935.56/s  (1.088s,  941.00/s)  LR: 2.555e-05  Data: 0.011 (0.013)
Train: 276 [ 300/1251 ( 24%)]  Loss:  2.798533 (2.8091)  Time: 1.080s,  948.41/s  (1.088s,  941.44/s)  LR: 2.555e-05  Data: 0.012 (0.013)
Train: 276 [ 350/1251 ( 28%)]  Loss:  2.964140 (2.8284)  Time: 1.076s,  951.50/s  (1.087s,  942.23/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 400/1251 ( 32%)]  Loss:  2.901130 (2.8365)  Time: 1.080s,  948.47/s  (1.087s,  942.16/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 450/1251 ( 36%)]  Loss:  2.928483 (2.8457)  Time: 1.096s,  934.17/s  (1.086s,  942.56/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 500/1251 ( 40%)]  Loss:  2.684525 (2.8311)  Time: 1.074s,  953.04/s  (1.086s,  942.79/s)  LR: 2.555e-05  Data: 0.011 (0.012)
Train: 276 [ 550/1251 ( 44%)]  Loss:  2.908629 (2.8375)  Time: 1.078s,  949.50/s  (1.086s,  942.60/s)  LR: 2.555e-05  Data: 0.014 (0.012)
Train: 276 [ 600/1251 ( 48%)]  Loss:  2.935580 (2.8451)  Time: 1.077s,  950.41/s  (1.086s,  942.82/s)  LR: 2.555e-05  Data: 0.014 (0.012)
Train: 276 [ 650/1251 ( 52%)]  Loss:  2.888322 (2.8482)  Time: 1.078s,  950.27/s  (1.086s,  943.13/s)  LR: 2.555e-05  Data: 0.011 (0.012)
Train: 276 [ 700/1251 ( 56%)]  Loss:  3.121205 (2.8664)  Time: 1.105s,  926.77/s  (1.086s,  942.81/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 750/1251 ( 60%)]  Loss:  2.996224 (2.8745)  Time: 1.077s,  950.94/s  (1.087s,  942.39/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 276 [ 800/1251 ( 64%)]  Loss:  2.882765 (2.8750)  Time: 1.076s,  951.64/s  (1.087s,  942.38/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 850/1251 ( 68%)]  Loss:  2.962201 (2.8798)  Time: 1.078s,  950.07/s  (1.087s,  942.47/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [ 900/1251 ( 72%)]  Loss:  2.960409 (2.8841)  Time: 1.105s,  927.08/s  (1.087s,  941.96/s)  LR: 2.555e-05  Data: 0.014 (0.012)
Train: 276 [ 950/1251 ( 76%)]  Loss:  2.963381 (2.8880)  Time: 1.092s,  937.84/s  (1.087s,  942.05/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [1000/1251 ( 80%)]  Loss:  2.894762 (2.8883)  Time: 1.103s,  928.72/s  (1.087s,  942.12/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [1050/1251 ( 84%)]  Loss:  2.890978 (2.8885)  Time: 1.079s,  948.82/s  (1.087s,  942.07/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [1100/1251 ( 88%)]  Loss:  3.090041 (2.8972)  Time: 1.095s,  934.77/s  (1.087s,  941.91/s)  LR: 2.555e-05  Data: 0.011 (0.012)
Train: 276 [1150/1251 ( 92%)]  Loss:  3.076323 (2.9047)  Time: 1.078s,  950.02/s  (1.087s,  941.94/s)  LR: 2.555e-05  Data: 0.012 (0.012)
Train: 276 [1200/1251 ( 96%)]  Loss:  2.941297 (2.9062)  Time: 1.104s,  927.76/s  (1.087s,  941.95/s)  LR: 2.555e-05  Data: 0.011 (0.012)
Train: 276 [1250/1251 (100%)]  Loss:  3.200788 (2.9175)  Time: 1.079s,  948.87/s  (1.087s,  941.85/s)  LR: 2.555e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.911 (5.911)  Loss:  0.4227 (0.4227)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5296 (0.8335)  Acc@1: 87.6179 (81.3820)  Acc@5: 97.8774 (95.6460)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-275.pth.tar', 81.24999994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-262.pth.tar', 81.2339999194336)

Train: 277 [   0/1251 (  0%)]  Loss:  2.856668 (2.8567)  Time: 1.093s,  936.64/s  (1.093s,  936.64/s)  LR: 2.429e-05  Data: 0.028 (0.028)
Train: 277 [  50/1251 (  4%)]  Loss:  3.071141 (2.9639)  Time: 1.075s,  952.83/s  (1.086s,  942.71/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 100/1251 (  8%)]  Loss:  2.923361 (2.9504)  Time: 1.079s,  948.88/s  (1.086s,  943.01/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 150/1251 ( 12%)]  Loss:  2.858662 (2.9275)  Time: 1.077s,  950.93/s  (1.084s,  944.22/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 200/1251 ( 16%)]  Loss:  3.131452 (2.9683)  Time: 1.078s,  950.25/s  (1.086s,  942.92/s)  LR: 2.429e-05  Data: 0.016 (0.013)
Train: 277 [ 250/1251 ( 20%)]  Loss:  2.707510 (2.9248)  Time: 1.082s,  946.58/s  (1.086s,  942.54/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 300/1251 ( 24%)]  Loss:  2.854250 (2.9147)  Time: 1.076s,  951.49/s  (1.087s,  942.14/s)  LR: 2.429e-05  Data: 0.014 (0.013)
Train: 277 [ 350/1251 ( 28%)]  Loss:  2.772473 (2.8969)  Time: 1.096s,  934.07/s  (1.088s,  941.40/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 400/1251 ( 32%)]  Loss:  2.993535 (2.9077)  Time: 1.080s,  948.41/s  (1.089s,  940.44/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 450/1251 ( 36%)]  Loss:  2.837642 (2.9007)  Time: 1.078s,  949.50/s  (1.090s,  939.79/s)  LR: 2.429e-05  Data: 0.015 (0.013)
Train: 277 [ 500/1251 ( 40%)]  Loss:  3.009515 (2.9106)  Time: 1.095s,  935.28/s  (1.089s,  940.07/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 550/1251 ( 44%)]  Loss:  2.783561 (2.9000)  Time: 1.076s,  951.62/s  (1.089s,  940.12/s)  LR: 2.429e-05  Data: 0.013 (0.013)
Train: 277 [ 600/1251 ( 48%)]  Loss:  2.665914 (2.8820)  Time: 1.105s,  926.54/s  (1.090s,  939.59/s)  LR: 2.429e-05  Data: 0.013 (0.013)
Train: 277 [ 650/1251 ( 52%)]  Loss:  2.725752 (2.8708)  Time: 1.098s,  932.33/s  (1.090s,  939.54/s)  LR: 2.429e-05  Data: 0.011 (0.013)
Train: 277 [ 700/1251 ( 56%)]  Loss:  3.000540 (2.8795)  Time: 1.080s,  948.50/s  (1.090s,  939.34/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 750/1251 ( 60%)]  Loss:  2.701122 (2.8683)  Time: 1.172s,  873.49/s  (1.090s,  939.42/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 800/1251 ( 64%)]  Loss:  2.724534 (2.8599)  Time: 1.076s,  951.59/s  (1.090s,  939.07/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [ 850/1251 ( 68%)]  Loss:  2.969410 (2.8659)  Time: 1.087s,  941.90/s  (1.090s,  939.32/s)  LR: 2.429e-05  Data: 0.015 (0.013)
Train: 277 [ 900/1251 ( 72%)]  Loss:  2.990137 (2.8725)  Time: 1.094s,  935.95/s  (1.090s,  939.17/s)  LR: 2.429e-05  Data: 0.013 (0.013)
Train: 277 [ 950/1251 ( 76%)]  Loss:  3.027658 (2.8802)  Time: 1.076s,  951.29/s  (1.090s,  939.37/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [1000/1251 ( 80%)]  Loss:  2.990546 (2.8855)  Time: 1.076s,  951.34/s  (1.090s,  939.59/s)  LR: 2.429e-05  Data: 0.015 (0.013)
Train: 277 [1050/1251 ( 84%)]  Loss:  3.001218 (2.8908)  Time: 1.085s,  943.43/s  (1.090s,  939.77/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [1100/1251 ( 88%)]  Loss:  2.934685 (2.8927)  Time: 1.094s,  935.86/s  (1.090s,  939.86/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [1150/1251 ( 92%)]  Loss:  2.748350 (2.8867)  Time: 1.095s,  935.30/s  (1.090s,  939.67/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [1200/1251 ( 96%)]  Loss:  2.991330 (2.8908)  Time: 1.098s,  932.47/s  (1.090s,  939.34/s)  LR: 2.429e-05  Data: 0.012 (0.013)
Train: 277 [1250/1251 (100%)]  Loss:  2.626894 (2.8807)  Time: 1.071s,  956.32/s  (1.090s,  939.16/s)  LR: 2.429e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.834 (5.834)  Loss:  0.4142 (0.4142)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5283 (0.8275)  Acc@1: 86.6745 (81.3560)  Acc@5: 97.9953 (95.5960)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-275.pth.tar', 81.24999994873046)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-263.pth.tar', 81.244000078125)

Train: 278 [   0/1251 (  0%)]  Loss:  2.602589 (2.6026)  Time: 1.084s,  944.63/s  (1.084s,  944.63/s)  LR: 2.308e-05  Data: 0.022 (0.022)
Train: 278 [  50/1251 (  4%)]  Loss:  2.960332 (2.7815)  Time: 1.089s,  940.63/s  (1.088s,  941.22/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 100/1251 (  8%)]  Loss:  2.801251 (2.7881)  Time: 1.075s,  952.40/s  (1.091s,  938.94/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 150/1251 ( 12%)]  Loss:  2.909150 (2.8183)  Time: 1.082s,  946.18/s  (1.088s,  941.50/s)  LR: 2.308e-05  Data: 0.020 (0.013)
Train: 278 [ 200/1251 ( 16%)]  Loss:  2.846030 (2.8239)  Time: 1.082s,  946.18/s  (1.087s,  941.83/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 250/1251 ( 20%)]  Loss:  2.586916 (2.7844)  Time: 1.083s,  945.95/s  (1.088s,  940.98/s)  LR: 2.308e-05  Data: 0.013 (0.013)
Train: 278 [ 300/1251 ( 24%)]  Loss:  2.704220 (2.7729)  Time: 1.081s,  946.89/s  (1.088s,  941.38/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 350/1251 ( 28%)]  Loss:  2.701059 (2.7639)  Time: 1.101s,  930.32/s  (1.089s,  940.34/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 400/1251 ( 32%)]  Loss:  2.809203 (2.7690)  Time: 1.095s,  935.20/s  (1.090s,  939.77/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 450/1251 ( 36%)]  Loss:  2.847966 (2.7769)  Time: 1.077s,  951.22/s  (1.090s,  939.74/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 500/1251 ( 40%)]  Loss:  2.993937 (2.7966)  Time: 1.111s,  921.98/s  (1.089s,  939.93/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 550/1251 ( 44%)]  Loss:  3.033710 (2.8164)  Time: 1.107s,  924.69/s  (1.089s,  939.88/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 600/1251 ( 48%)]  Loss:  2.892853 (2.8222)  Time: 1.094s,  936.10/s  (1.090s,  939.42/s)  LR: 2.308e-05  Data: 0.013 (0.013)
Train: 278 [ 650/1251 ( 52%)]  Loss:  2.992418 (2.8344)  Time: 1.097s,  933.18/s  (1.090s,  939.10/s)  LR: 2.308e-05  Data: 0.013 (0.013)
Train: 278 [ 700/1251 ( 56%)]  Loss:  2.787011 (2.8312)  Time: 1.104s,  927.94/s  (1.090s,  939.11/s)  LR: 2.308e-05  Data: 0.013 (0.013)
Train: 278 [ 750/1251 ( 60%)]  Loss:  3.267737 (2.8585)  Time: 1.104s,  927.22/s  (1.090s,  939.08/s)  LR: 2.308e-05  Data: 0.011 (0.013)
Train: 278 [ 800/1251 ( 64%)]  Loss:  2.999221 (2.8668)  Time: 1.094s,  936.42/s  (1.091s,  938.64/s)  LR: 2.308e-05  Data: 0.014 (0.013)
Train: 278 [ 850/1251 ( 68%)]  Loss:  2.880305 (2.8676)  Time: 1.075s,  952.63/s  (1.091s,  938.63/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 900/1251 ( 72%)]  Loss:  2.846182 (2.8664)  Time: 1.098s,  932.22/s  (1.091s,  938.65/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [ 950/1251 ( 76%)]  Loss:  2.896335 (2.8679)  Time: 1.077s,  950.85/s  (1.091s,  938.63/s)  LR: 2.308e-05  Data: 0.012 (0.012)
Train: 278 [1000/1251 ( 80%)]  Loss:  3.172487 (2.8824)  Time: 1.096s,  934.28/s  (1.091s,  938.30/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [1050/1251 ( 84%)]  Loss:  2.700928 (2.8742)  Time: 1.093s,  936.99/s  (1.091s,  938.48/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [1100/1251 ( 88%)]  Loss:  3.001161 (2.8797)  Time: 1.078s,  949.60/s  (1.091s,  938.44/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [1150/1251 ( 92%)]  Loss:  2.965665 (2.8833)  Time: 1.076s,  951.95/s  (1.091s,  938.42/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [1200/1251 ( 96%)]  Loss:  3.122921 (2.8929)  Time: 1.078s,  949.50/s  (1.091s,  938.49/s)  LR: 2.308e-05  Data: 0.012 (0.013)
Train: 278 [1250/1251 (100%)]  Loss:  2.960727 (2.8955)  Time: 1.061s,  965.02/s  (1.091s,  938.65/s)  LR: 2.308e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.895 (5.895)  Loss:  0.4116 (0.4116)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5292 (0.8267)  Acc@1: 86.7925 (81.4500)  Acc@5: 97.8774 (95.5560)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-275.pth.tar', 81.24999994873046)

Train: 279 [   0/1251 (  0%)]  Loss:  2.935591 (2.9356)  Time: 1.086s,  943.31/s  (1.086s,  943.31/s)  LR: 2.192e-05  Data: 0.024 (0.024)
Train: 279 [  50/1251 (  4%)]  Loss:  2.728070 (2.8318)  Time: 1.080s,  948.39/s  (1.082s,  946.69/s)  LR: 2.192e-05  Data: 0.014 (0.013)
Train: 279 [ 100/1251 (  8%)]  Loss:  2.968717 (2.8775)  Time: 1.105s,  926.75/s  (1.083s,  945.52/s)  LR: 2.192e-05  Data: 0.015 (0.013)
Train: 279 [ 150/1251 ( 12%)]  Loss:  2.950198 (2.8956)  Time: 1.093s,  936.75/s  (1.086s,  943.16/s)  LR: 2.192e-05  Data: 0.015 (0.013)
Train: 279 [ 200/1251 ( 16%)]  Loss:  3.005767 (2.9177)  Time: 1.104s,  927.52/s  (1.086s,  943.18/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 250/1251 ( 20%)]  Loss:  2.861861 (2.9084)  Time: 1.095s,  935.11/s  (1.087s,  942.19/s)  LR: 2.192e-05  Data: 0.013 (0.013)
Train: 279 [ 300/1251 ( 24%)]  Loss:  3.058837 (2.9299)  Time: 1.092s,  937.91/s  (1.087s,  941.63/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 350/1251 ( 28%)]  Loss:  2.803148 (2.9140)  Time: 1.082s,  946.31/s  (1.087s,  941.76/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 400/1251 ( 32%)]  Loss:  2.905466 (2.9131)  Time: 1.099s,  931.72/s  (1.088s,  941.18/s)  LR: 2.192e-05  Data: 0.015 (0.013)
Train: 279 [ 450/1251 ( 36%)]  Loss:  2.839752 (2.9057)  Time: 1.086s,  943.32/s  (1.089s,  940.66/s)  LR: 2.192e-05  Data: 0.014 (0.013)
Train: 279 [ 500/1251 ( 40%)]  Loss:  2.728028 (2.8896)  Time: 1.096s,  934.64/s  (1.088s,  940.77/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 550/1251 ( 44%)]  Loss:  3.052287 (2.9031)  Time: 1.094s,  935.66/s  (1.089s,  940.36/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 600/1251 ( 48%)]  Loss:  2.969850 (2.9083)  Time: 1.082s,  946.17/s  (1.089s,  940.31/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 650/1251 ( 52%)]  Loss:  2.815891 (2.9017)  Time: 1.078s,  949.79/s  (1.089s,  940.01/s)  LR: 2.192e-05  Data: 0.013 (0.013)
Train: 279 [ 700/1251 ( 56%)]  Loss:  3.072764 (2.9131)  Time: 1.107s,  925.37/s  (1.089s,  940.00/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [ 750/1251 ( 60%)]  Loss:  2.810313 (2.9067)  Time: 1.096s,  933.94/s  (1.090s,  939.44/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [ 800/1251 ( 64%)]  Loss:  2.623444 (2.8900)  Time: 1.093s,  936.52/s  (1.090s,  939.28/s)  LR: 2.192e-05  Data: 0.013 (0.013)
Train: 279 [ 850/1251 ( 68%)]  Loss:  2.786877 (2.8843)  Time: 1.077s,  950.85/s  (1.090s,  939.53/s)  LR: 2.192e-05  Data: 0.014 (0.013)
Train: 279 [ 900/1251 ( 72%)]  Loss:  2.828947 (2.8814)  Time: 1.100s,  930.96/s  (1.090s,  939.61/s)  LR: 2.192e-05  Data: 0.014 (0.013)
Train: 279 [ 950/1251 ( 76%)]  Loss:  2.766695 (2.8756)  Time: 1.094s,  935.67/s  (1.090s,  939.49/s)  LR: 2.192e-05  Data: 0.016 (0.013)
Train: 279 [1000/1251 ( 80%)]  Loss:  2.744520 (2.8694)  Time: 1.076s,  951.87/s  (1.090s,  939.60/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1050/1251 ( 84%)]  Loss:  2.646108 (2.8592)  Time: 1.073s,  954.00/s  (1.090s,  939.88/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [1100/1251 ( 88%)]  Loss:  2.870117 (2.8597)  Time: 1.082s,  946.79/s  (1.089s,  940.00/s)  LR: 2.192e-05  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 279 [1150/1251 ( 92%)]  Loss:  2.827598 (2.8584)  Time: 1.084s,  945.07/s  (1.089s,  940.08/s)  LR: 2.192e-05  Data: 0.012 (0.013)
Train: 279 [1200/1251 ( 96%)]  Loss:  3.284961 (2.8754)  Time: 1.081s,  947.51/s  (1.089s,  940.17/s)  LR: 2.192e-05  Data: 0.011 (0.013)
Train: 279 [1250/1251 (100%)]  Loss:  2.853308 (2.8746)  Time: 1.062s,  964.44/s  (1.089s,  939.96/s)  LR: 2.192e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.915 (5.915)  Loss:  0.4156 (0.4156)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5361 (0.8313)  Acc@1: 87.2642 (81.4260)  Acc@5: 97.7594 (95.6140)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-272.pth.tar', 81.252000078125)

Train: 280 [   0/1251 (  0%)]  Loss:  2.895436 (2.8954)  Time: 1.085s,  943.99/s  (1.085s,  943.99/s)  LR: 2.082e-05  Data: 0.024 (0.024)
Train: 280 [  50/1251 (  4%)]  Loss:  2.696969 (2.7962)  Time: 1.077s,  950.48/s  (1.086s,  942.68/s)  LR: 2.082e-05  Data: 0.012 (0.012)
Train: 280 [ 100/1251 (  8%)]  Loss:  3.007845 (2.8668)  Time: 1.079s,  948.66/s  (1.088s,  940.78/s)  LR: 2.082e-05  Data: 0.013 (0.012)
Train: 280 [ 150/1251 ( 12%)]  Loss:  2.784882 (2.8463)  Time: 1.077s,  950.72/s  (1.089s,  939.96/s)  LR: 2.082e-05  Data: 0.012 (0.012)
Train: 280 [ 200/1251 ( 16%)]  Loss:  2.548647 (2.7868)  Time: 1.090s,  939.53/s  (1.090s,  939.46/s)  LR: 2.082e-05  Data: 0.011 (0.012)
Train: 280 [ 250/1251 ( 20%)]  Loss:  2.917456 (2.8085)  Time: 1.078s,  950.26/s  (1.090s,  939.82/s)  LR: 2.082e-05  Data: 0.012 (0.012)
Train: 280 [ 300/1251 ( 24%)]  Loss:  2.991444 (2.8347)  Time: 1.077s,  951.13/s  (1.090s,  939.77/s)  LR: 2.082e-05  Data: 0.012 (0.012)
Train: 280 [ 350/1251 ( 28%)]  Loss:  2.883561 (2.8408)  Time: 1.081s,  947.61/s  (1.089s,  939.89/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 400/1251 ( 32%)]  Loss:  2.797174 (2.8359)  Time: 1.095s,  935.33/s  (1.089s,  940.08/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 450/1251 ( 36%)]  Loss:  2.871393 (2.8395)  Time: 1.080s,  947.76/s  (1.089s,  939.95/s)  LR: 2.082e-05  Data: 0.014 (0.013)
Train: 280 [ 500/1251 ( 40%)]  Loss:  3.047147 (2.8584)  Time: 1.076s,  951.31/s  (1.090s,  939.85/s)  LR: 2.082e-05  Data: 0.013 (0.013)
Train: 280 [ 550/1251 ( 44%)]  Loss:  3.064597 (2.8755)  Time: 1.103s,  928.22/s  (1.090s,  939.76/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 600/1251 ( 48%)]  Loss:  2.823411 (2.8715)  Time: 1.093s,  936.74/s  (1.090s,  939.30/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 650/1251 ( 52%)]  Loss:  3.044746 (2.8839)  Time: 1.080s,  948.53/s  (1.090s,  939.35/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 700/1251 ( 56%)]  Loss:  2.769361 (2.8763)  Time: 1.104s,  927.48/s  (1.090s,  939.41/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 750/1251 ( 60%)]  Loss:  2.847707 (2.8745)  Time: 1.096s,  934.55/s  (1.090s,  939.42/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 800/1251 ( 64%)]  Loss:  2.590161 (2.8578)  Time: 1.098s,  932.49/s  (1.090s,  939.56/s)  LR: 2.082e-05  Data: 0.013 (0.013)
Train: 280 [ 850/1251 ( 68%)]  Loss:  2.492479 (2.8375)  Time: 1.078s,  949.52/s  (1.090s,  939.24/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [ 900/1251 ( 72%)]  Loss:  2.752831 (2.8330)  Time: 1.096s,  934.63/s  (1.090s,  939.21/s)  LR: 2.082e-05  Data: 0.016 (0.013)
Train: 280 [ 950/1251 ( 76%)]  Loss:  2.643489 (2.8235)  Time: 1.079s,  949.14/s  (1.091s,  938.90/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [1000/1251 ( 80%)]  Loss:  2.982537 (2.8311)  Time: 1.094s,  936.00/s  (1.091s,  938.95/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [1050/1251 ( 84%)]  Loss:  2.721654 (2.8261)  Time: 1.077s,  950.61/s  (1.090s,  939.09/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [1100/1251 ( 88%)]  Loss:  2.693179 (2.8204)  Time: 1.079s,  948.69/s  (1.091s,  938.97/s)  LR: 2.082e-05  Data: 0.012 (0.013)
Train: 280 [1150/1251 ( 92%)]  Loss:  3.208106 (2.8365)  Time: 1.073s,  953.93/s  (1.091s,  938.69/s)  LR: 2.082e-05  Data: 0.011 (0.013)
Train: 280 [1200/1251 ( 96%)]  Loss:  2.835538 (2.8365)  Time: 1.089s,  940.08/s  (1.091s,  938.96/s)  LR: 2.082e-05  Data: 0.015 (0.013)
Train: 280 [1250/1251 (100%)]  Loss:  2.793348 (2.8348)  Time: 1.089s,  940.15/s  (1.090s,  939.05/s)  LR: 2.082e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.840 (5.840)  Loss:  0.4099 (0.4099)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5289 (0.8284)  Acc@1: 86.6745 (81.3940)  Acc@5: 98.2311 (95.6840)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-273.pth.tar', 81.30599997558593)

Train: 281 [   0/1251 (  0%)]  Loss:  2.958904 (2.9589)  Time: 1.085s,  944.07/s  (1.085s,  944.07/s)  LR: 1.977e-05  Data: 0.024 (0.024)
Train: 281 [  50/1251 (  4%)]  Loss:  3.130751 (3.0448)  Time: 1.093s,  936.47/s  (1.094s,  936.43/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 100/1251 (  8%)]  Loss:  2.833454 (2.9744)  Time: 1.103s,  928.78/s  (1.094s,  936.21/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 150/1251 ( 12%)]  Loss:  3.026083 (2.9873)  Time: 1.078s,  950.15/s  (1.094s,  935.71/s)  LR: 1.977e-05  Data: 0.014 (0.013)
Train: 281 [ 200/1251 ( 16%)]  Loss:  2.675180 (2.9249)  Time: 1.077s,  950.94/s  (1.094s,  936.14/s)  LR: 1.977e-05  Data: 0.015 (0.013)
Train: 281 [ 250/1251 ( 20%)]  Loss:  2.741558 (2.8943)  Time: 1.080s,  947.99/s  (1.093s,  937.20/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 300/1251 ( 24%)]  Loss:  2.725480 (2.8702)  Time: 1.168s,  876.58/s  (1.091s,  938.17/s)  LR: 1.977e-05  Data: 0.014 (0.013)
Train: 281 [ 350/1251 ( 28%)]  Loss:  2.666361 (2.8447)  Time: 1.096s,  933.90/s  (1.093s,  937.22/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 400/1251 ( 32%)]  Loss:  2.892640 (2.8500)  Time: 1.099s,  931.41/s  (1.092s,  937.60/s)  LR: 1.977e-05  Data: 0.013 (0.013)
Train: 281 [ 450/1251 ( 36%)]  Loss:  2.922539 (2.8573)  Time: 1.094s,  935.78/s  (1.092s,  937.80/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 500/1251 ( 40%)]  Loss:  2.824260 (2.8543)  Time: 1.078s,  950.23/s  (1.091s,  938.28/s)  LR: 1.977e-05  Data: 0.015 (0.013)
Train: 281 [ 550/1251 ( 44%)]  Loss:  2.718974 (2.8430)  Time: 1.097s,  933.33/s  (1.091s,  938.59/s)  LR: 1.977e-05  Data: 0.011 (0.013)
Train: 281 [ 600/1251 ( 48%)]  Loss:  2.940331 (2.8505)  Time: 1.093s,  936.98/s  (1.091s,  939.02/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 650/1251 ( 52%)]  Loss:  2.999207 (2.8611)  Time: 1.078s,  949.89/s  (1.091s,  938.58/s)  LR: 1.977e-05  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 281 [ 700/1251 ( 56%)]  Loss:  2.633204 (2.8459)  Time: 1.077s,  950.94/s  (1.091s,  938.87/s)  LR: 1.977e-05  Data: 0.014 (0.013)
Train: 281 [ 750/1251 ( 60%)]  Loss:  2.960406 (2.8531)  Time: 1.108s,  924.16/s  (1.091s,  938.57/s)  LR: 1.977e-05  Data: 0.017 (0.013)
Train: 281 [ 800/1251 ( 64%)]  Loss:  3.025242 (2.8632)  Time: 1.092s,  937.94/s  (1.091s,  938.44/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 850/1251 ( 68%)]  Loss:  2.706147 (2.8545)  Time: 1.077s,  950.51/s  (1.091s,  938.78/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 900/1251 ( 72%)]  Loss:  2.421086 (2.8317)  Time: 1.096s,  934.58/s  (1.091s,  938.74/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [ 950/1251 ( 76%)]  Loss:  2.889186 (2.8345)  Time: 1.095s,  935.55/s  (1.091s,  938.94/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [1000/1251 ( 80%)]  Loss:  2.877330 (2.8366)  Time: 1.096s,  934.57/s  (1.091s,  938.90/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [1050/1251 ( 84%)]  Loss:  2.845611 (2.8370)  Time: 1.094s,  936.05/s  (1.091s,  938.60/s)  LR: 1.977e-05  Data: 0.012 (0.013)
Train: 281 [1100/1251 ( 88%)]  Loss:  2.896556 (2.8396)  Time: 1.079s,  948.62/s  (1.091s,  938.62/s)  LR: 1.977e-05  Data: 0.014 (0.013)
Train: 281 [1150/1251 ( 92%)]  Loss:  2.608492 (2.8300)  Time: 1.097s,  933.39/s  (1.091s,  938.82/s)  LR: 1.977e-05  Data: 0.013 (0.013)
Train: 281 [1200/1251 ( 96%)]  Loss:  2.897299 (2.8327)  Time: 1.172s,  873.52/s  (1.091s,  938.89/s)  LR: 1.977e-05  Data: 0.014 (0.013)
Train: 281 [1250/1251 (100%)]  Loss:  2.924603 (2.8362)  Time: 1.080s,  948.38/s  (1.091s,  938.77/s)  LR: 1.977e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.851 (5.851)  Loss:  0.4189 (0.4189)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5178 (0.8302)  Acc@1: 87.6179 (81.3700)  Acc@5: 98.2311 (95.6860)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-281.pth.tar', 81.369999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-270.pth.tar', 81.324000078125)

Train: 282 [   0/1251 (  0%)]  Loss:  2.774091 (2.7741)  Time: 1.083s,  945.40/s  (1.083s,  945.40/s)  LR: 1.877e-05  Data: 0.022 (0.022)
Train: 282 [  50/1251 (  4%)]  Loss:  2.723007 (2.7485)  Time: 1.094s,  935.74/s  (1.093s,  936.98/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 100/1251 (  8%)]  Loss:  2.868824 (2.7886)  Time: 1.078s,  949.81/s  (1.092s,  937.99/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 150/1251 ( 12%)]  Loss:  2.990593 (2.8391)  Time: 1.082s,  946.57/s  (1.090s,  939.57/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 200/1251 ( 16%)]  Loss:  2.963966 (2.8641)  Time: 1.081s,  947.70/s  (1.089s,  940.65/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 250/1251 ( 20%)]  Loss:  3.039968 (2.8934)  Time: 1.077s,  950.72/s  (1.088s,  940.93/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 300/1251 ( 24%)]  Loss:  2.805006 (2.8808)  Time: 1.078s,  950.18/s  (1.088s,  941.12/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 350/1251 ( 28%)]  Loss:  2.905100 (2.8838)  Time: 1.096s,  934.70/s  (1.089s,  940.19/s)  LR: 1.877e-05  Data: 0.013 (0.013)
Train: 282 [ 400/1251 ( 32%)]  Loss:  2.952124 (2.8914)  Time: 1.106s,  926.21/s  (1.090s,  939.47/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [ 450/1251 ( 36%)]  Loss:  2.866976 (2.8890)  Time: 1.076s,  951.43/s  (1.090s,  939.08/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [ 500/1251 ( 40%)]  Loss:  2.693249 (2.8712)  Time: 1.103s,  928.28/s  (1.090s,  939.58/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 550/1251 ( 44%)]  Loss:  3.033310 (2.8847)  Time: 1.077s,  950.50/s  (1.090s,  939.54/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 600/1251 ( 48%)]  Loss:  3.006702 (2.8941)  Time: 1.094s,  936.04/s  (1.090s,  939.74/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 650/1251 ( 52%)]  Loss:  3.136983 (2.9114)  Time: 1.081s,  947.02/s  (1.090s,  939.82/s)  LR: 1.877e-05  Data: 0.011 (0.013)
Train: 282 [ 700/1251 ( 56%)]  Loss:  3.134055 (2.9263)  Time: 1.103s,  928.69/s  (1.089s,  939.97/s)  LR: 1.877e-05  Data: 0.013 (0.013)
Train: 282 [ 750/1251 ( 60%)]  Loss:  2.792939 (2.9179)  Time: 1.077s,  951.09/s  (1.089s,  940.09/s)  LR: 1.877e-05  Data: 0.013 (0.013)
Train: 282 [ 800/1251 ( 64%)]  Loss:  2.530077 (2.8951)  Time: 1.103s,  928.24/s  (1.090s,  939.82/s)  LR: 1.877e-05  Data: 0.013 (0.013)
Train: 282 [ 850/1251 ( 68%)]  Loss:  2.769334 (2.8881)  Time: 1.097s,  933.61/s  (1.090s,  939.32/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 900/1251 ( 72%)]  Loss:  2.833619 (2.8853)  Time: 1.104s,  927.22/s  (1.090s,  939.07/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [ 950/1251 ( 76%)]  Loss:  3.022901 (2.8921)  Time: 1.095s,  934.83/s  (1.090s,  939.22/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1000/1251 ( 80%)]  Loss:  3.109123 (2.9025)  Time: 1.096s,  933.99/s  (1.090s,  939.18/s)  LR: 1.877e-05  Data: 0.013 (0.013)
Train: 282 [1050/1251 ( 84%)]  Loss:  2.992836 (2.9066)  Time: 1.076s,  951.92/s  (1.090s,  939.21/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1100/1251 ( 88%)]  Loss:  2.809343 (2.9024)  Time: 1.092s,  937.39/s  (1.090s,  939.13/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1150/1251 ( 92%)]  Loss:  3.031653 (2.9077)  Time: 1.096s,  934.22/s  (1.090s,  939.21/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1200/1251 ( 96%)]  Loss:  2.955141 (2.9096)  Time: 1.104s,  927.77/s  (1.090s,  939.31/s)  LR: 1.877e-05  Data: 0.012 (0.013)
Train: 282 [1250/1251 (100%)]  Loss:  2.812847 (2.9059)  Time: 1.061s,  965.54/s  (1.090s,  939.30/s)  LR: 1.877e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.802 (5.802)  Loss:  0.4125 (0.4125)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5217 (0.8262)  Acc@1: 87.5000 (81.5100)  Acc@5: 98.2311 (95.6200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-281.pth.tar', 81.369999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-271.pth.tar', 81.334000078125)

Train: 283 [   0/1251 (  0%)]  Loss:  3.003440 (3.0034)  Time: 1.083s,  945.17/s  (1.083s,  945.17/s)  LR: 1.782e-05  Data: 0.022 (0.022)
Train: 283 [  50/1251 (  4%)]  Loss:  2.870762 (2.9371)  Time: 1.095s,  935.06/s  (1.093s,  936.68/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 100/1251 (  8%)]  Loss:  3.153001 (3.0091)  Time: 1.078s,  950.30/s  (1.091s,  938.64/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 150/1251 ( 12%)]  Loss:  2.822686 (2.9625)  Time: 1.103s,  928.10/s  (1.094s,  936.16/s)  LR: 1.782e-05  Data: 0.014 (0.013)
Train: 283 [ 200/1251 ( 16%)]  Loss:  2.912020 (2.9524)  Time: 1.174s,  872.06/s  (1.093s,  936.50/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 250/1251 ( 20%)]  Loss:  2.573794 (2.8893)  Time: 1.076s,  952.11/s  (1.091s,  938.29/s)  LR: 1.782e-05  Data: 0.012 (0.012)
Train: 283 [ 300/1251 ( 24%)]  Loss:  2.664526 (2.8572)  Time: 1.075s,  952.51/s  (1.092s,  937.88/s)  LR: 1.782e-05  Data: 0.011 (0.012)
Train: 283 [ 350/1251 ( 28%)]  Loss:  2.559122 (2.8199)  Time: 1.093s,  936.53/s  (1.091s,  938.21/s)  LR: 1.782e-05  Data: 0.011 (0.012)
Train: 283 [ 400/1251 ( 32%)]  Loss:  3.076360 (2.8484)  Time: 1.099s,  931.60/s  (1.091s,  938.70/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 450/1251 ( 36%)]  Loss:  2.883416 (2.8519)  Time: 1.076s,  952.06/s  (1.090s,  939.18/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 283 [ 500/1251 ( 40%)]  Loss:  3.043182 (2.8693)  Time: 1.104s,  927.56/s  (1.091s,  938.76/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 550/1251 ( 44%)]  Loss:  2.887602 (2.8708)  Time: 1.102s,  928.90/s  (1.090s,  939.18/s)  LR: 1.782e-05  Data: 0.014 (0.013)
Train: 283 [ 600/1251 ( 48%)]  Loss:  2.895925 (2.8728)  Time: 1.093s,  936.49/s  (1.091s,  938.91/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 650/1251 ( 52%)]  Loss:  2.905910 (2.8751)  Time: 1.077s,  951.01/s  (1.090s,  939.09/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 700/1251 ( 56%)]  Loss:  3.110178 (2.8908)  Time: 1.077s,  950.59/s  (1.090s,  939.02/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 750/1251 ( 60%)]  Loss:  3.149579 (2.9070)  Time: 1.084s,  945.05/s  (1.090s,  939.49/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 800/1251 ( 64%)]  Loss:  2.676155 (2.8934)  Time: 1.093s,  936.65/s  (1.090s,  939.26/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 850/1251 ( 68%)]  Loss:  2.909029 (2.8943)  Time: 1.095s,  935.38/s  (1.090s,  939.34/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 900/1251 ( 72%)]  Loss:  3.091247 (2.9046)  Time: 1.086s,  942.75/s  (1.090s,  939.53/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [ 950/1251 ( 76%)]  Loss:  2.840877 (2.9014)  Time: 1.079s,  949.02/s  (1.090s,  939.63/s)  LR: 1.782e-05  Data: 0.017 (0.013)
Train: 283 [1000/1251 ( 80%)]  Loss:  2.883807 (2.9006)  Time: 1.081s,  947.01/s  (1.090s,  939.63/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [1050/1251 ( 84%)]  Loss:  2.810584 (2.8965)  Time: 1.091s,  938.76/s  (1.090s,  939.44/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1100/1251 ( 88%)]  Loss:  3.073256 (2.9042)  Time: 1.105s,  926.69/s  (1.090s,  939.31/s)  LR: 1.782e-05  Data: 0.012 (0.013)
Train: 283 [1150/1251 ( 92%)]  Loss:  3.002218 (2.9083)  Time: 1.077s,  950.60/s  (1.090s,  939.13/s)  LR: 1.782e-05  Data: 0.015 (0.013)
Train: 283 [1200/1251 ( 96%)]  Loss:  2.678148 (2.8991)  Time: 1.095s,  935.23/s  (1.090s,  939.20/s)  LR: 1.782e-05  Data: 0.011 (0.013)
Train: 283 [1250/1251 (100%)]  Loss:  3.092418 (2.9065)  Time: 1.061s,  964.77/s  (1.090s,  939.10/s)  LR: 1.782e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.838 (5.838)  Loss:  0.4242 (0.4242)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5199 (0.8381)  Acc@1: 87.9717 (81.4940)  Acc@5: 98.2311 (95.6160)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-281.pth.tar', 81.369999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-277.pth.tar', 81.35599989990234)

Train: 284 [   0/1251 (  0%)]  Loss:  2.822115 (2.8221)  Time: 1.083s,  945.70/s  (1.083s,  945.70/s)  LR: 1.693e-05  Data: 0.022 (0.022)
Train: 284 [  50/1251 (  4%)]  Loss:  3.076251 (2.9492)  Time: 1.096s,  934.72/s  (1.089s,  940.04/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 100/1251 (  8%)]  Loss:  3.044735 (2.9810)  Time: 1.079s,  948.70/s  (1.090s,  939.35/s)  LR: 1.693e-05  Data: 0.015 (0.013)
Train: 284 [ 150/1251 ( 12%)]  Loss:  2.774992 (2.9295)  Time: 1.078s,  949.82/s  (1.089s,  939.89/s)  LR: 1.693e-05  Data: 0.017 (0.013)
Train: 284 [ 200/1251 ( 16%)]  Loss:  2.969528 (2.9375)  Time: 1.077s,  951.14/s  (1.089s,  940.45/s)  LR: 1.693e-05  Data: 0.014 (0.013)
Train: 284 [ 250/1251 ( 20%)]  Loss:  2.969430 (2.9428)  Time: 1.083s,  945.52/s  (1.088s,  941.38/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 300/1251 ( 24%)]  Loss:  2.711115 (2.9097)  Time: 1.077s,  950.87/s  (1.088s,  941.42/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 350/1251 ( 28%)]  Loss:  2.630897 (2.8749)  Time: 1.088s,  940.82/s  (1.088s,  941.42/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 400/1251 ( 32%)]  Loss:  2.668017 (2.8519)  Time: 1.096s,  934.08/s  (1.088s,  941.25/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 450/1251 ( 36%)]  Loss:  2.954155 (2.8621)  Time: 1.094s,  936.08/s  (1.089s,  940.52/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 500/1251 ( 40%)]  Loss:  2.909775 (2.8665)  Time: 1.083s,  945.67/s  (1.089s,  939.94/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 550/1251 ( 44%)]  Loss:  2.863703 (2.8662)  Time: 1.085s,  944.14/s  (1.089s,  940.21/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 600/1251 ( 48%)]  Loss:  2.972770 (2.8744)  Time: 1.081s,  947.06/s  (1.089s,  940.66/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 650/1251 ( 52%)]  Loss:  2.791333 (2.8685)  Time: 1.079s,  949.27/s  (1.089s,  940.66/s)  LR: 1.693e-05  Data: 0.015 (0.013)
Train: 284 [ 700/1251 ( 56%)]  Loss:  2.626642 (2.8524)  Time: 1.179s,  868.81/s  (1.089s,  940.56/s)  LR: 1.693e-05  Data: 0.015 (0.013)
Train: 284 [ 750/1251 ( 60%)]  Loss:  2.631143 (2.8385)  Time: 1.095s,  934.98/s  (1.089s,  939.96/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 800/1251 ( 64%)]  Loss:  2.604328 (2.8248)  Time: 1.095s,  935.26/s  (1.090s,  939.82/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 850/1251 ( 68%)]  Loss:  2.896811 (2.8288)  Time: 1.096s,  933.94/s  (1.090s,  939.60/s)  LR: 1.693e-05  Data: 0.011 (0.013)
Train: 284 [ 900/1251 ( 72%)]  Loss:  2.833242 (2.8290)  Time: 1.098s,  932.82/s  (1.090s,  939.45/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [ 950/1251 ( 76%)]  Loss:  2.798165 (2.8275)  Time: 1.093s,  937.21/s  (1.090s,  939.43/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1000/1251 ( 80%)]  Loss:  3.105301 (2.8407)  Time: 1.076s,  951.32/s  (1.090s,  939.65/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1050/1251 ( 84%)]  Loss:  2.898227 (2.8433)  Time: 1.081s,  947.35/s  (1.090s,  939.62/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1100/1251 ( 88%)]  Loss:  2.556560 (2.8308)  Time: 1.094s,  935.75/s  (1.090s,  939.64/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1150/1251 ( 92%)]  Loss:  2.861014 (2.8321)  Time: 1.105s,  926.41/s  (1.089s,  939.88/s)  LR: 1.693e-05  Data: 0.014 (0.013)
Train: 284 [1200/1251 ( 96%)]  Loss:  2.908889 (2.8352)  Time: 1.081s,  946.84/s  (1.090s,  939.78/s)  LR: 1.693e-05  Data: 0.012 (0.013)
Train: 284 [1250/1251 (100%)]  Loss:  3.062989 (2.8439)  Time: 1.062s,  964.47/s  (1.090s,  939.68/s)  LR: 1.693e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.860 (5.860)  Loss:  0.4115 (0.4115)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.445)  Loss:  0.5163 (0.8282)  Acc@1: 87.2642 (81.5280)  Acc@5: 97.9953 (95.6560)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-281.pth.tar', 81.369999921875)

Train: 285 [   0/1251 (  0%)]  Loss:  2.848404 (2.8484)  Time: 1.092s,  937.39/s  (1.092s,  937.39/s)  LR: 1.609e-05  Data: 0.030 (0.030)
Train: 285 [  50/1251 (  4%)]  Loss:  2.792186 (2.8203)  Time: 1.077s,  950.64/s  (1.081s,  947.00/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 100/1251 (  8%)]  Loss:  2.758423 (2.7997)  Time: 1.079s,  949.01/s  (1.085s,  944.11/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 150/1251 ( 12%)]  Loss:  3.039282 (2.8596)  Time: 1.078s,  950.00/s  (1.087s,  942.15/s)  LR: 1.609e-05  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 285 [ 200/1251 ( 16%)]  Loss:  2.792374 (2.8461)  Time: 1.094s,  935.79/s  (1.087s,  941.80/s)  LR: 1.609e-05  Data: 0.013 (0.013)
Train: 285 [ 250/1251 ( 20%)]  Loss:  2.998606 (2.8715)  Time: 1.094s,  936.04/s  (1.089s,  940.70/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 300/1251 ( 24%)]  Loss:  2.916476 (2.8780)  Time: 1.095s,  935.07/s  (1.088s,  941.00/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 350/1251 ( 28%)]  Loss:  2.838932 (2.8731)  Time: 1.075s,  952.39/s  (1.088s,  941.57/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 400/1251 ( 32%)]  Loss:  2.821860 (2.8674)  Time: 1.103s,  928.03/s  (1.087s,  942.16/s)  LR: 1.609e-05  Data: 0.014 (0.013)
Train: 285 [ 450/1251 ( 36%)]  Loss:  2.794750 (2.8601)  Time: 1.095s,  935.52/s  (1.087s,  941.81/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 500/1251 ( 40%)]  Loss:  2.966290 (2.8698)  Time: 1.104s,  927.65/s  (1.088s,  941.22/s)  LR: 1.609e-05  Data: 0.011 (0.013)
Train: 285 [ 550/1251 ( 44%)]  Loss:  3.020914 (2.8824)  Time: 1.093s,  936.71/s  (1.087s,  941.61/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 600/1251 ( 48%)]  Loss:  2.767996 (2.8736)  Time: 1.078s,  950.19/s  (1.088s,  941.27/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 650/1251 ( 52%)]  Loss:  2.641672 (2.8570)  Time: 1.093s,  936.53/s  (1.088s,  941.18/s)  LR: 1.609e-05  Data: 0.012 (0.013)
Train: 285 [ 700/1251 ( 56%)]  Loss:  2.969278 (2.8645)  Time: 1.094s,  935.80/s  (1.089s,  940.68/s)  LR: 1.609e-05  Data: 0.011 (0.012)
Train: 285 [ 750/1251 ( 60%)]  Loss:  2.754104 (2.8576)  Time: 1.094s,  936.02/s  (1.089s,  940.30/s)  LR: 1.609e-05  Data: 0.012 (0.012)
Train: 285 [ 800/1251 ( 64%)]  Loss:  2.910851 (2.8607)  Time: 1.081s,  947.09/s  (1.090s,  939.67/s)  LR: 1.609e-05  Data: 0.014 (0.012)
Train: 285 [ 850/1251 ( 68%)]  Loss:  3.073585 (2.8726)  Time: 1.094s,  936.04/s  (1.090s,  939.51/s)  LR: 1.609e-05  Data: 0.014 (0.012)
Train: 285 [ 900/1251 ( 72%)]  Loss:  2.686266 (2.8627)  Time: 1.094s,  935.90/s  (1.090s,  939.43/s)  LR: 1.609e-05  Data: 0.011 (0.012)
Train: 285 [ 950/1251 ( 76%)]  Loss:  2.607299 (2.8500)  Time: 1.077s,  950.43/s  (1.090s,  939.53/s)  LR: 1.609e-05  Data: 0.013 (0.012)
Train: 285 [1000/1251 ( 80%)]  Loss:  2.527281 (2.8346)  Time: 1.077s,  951.06/s  (1.090s,  939.69/s)  LR: 1.609e-05  Data: 0.014 (0.012)
Train: 285 [1050/1251 ( 84%)]  Loss:  2.667566 (2.8270)  Time: 1.082s,  946.16/s  (1.090s,  939.77/s)  LR: 1.609e-05  Data: 0.012 (0.012)
Train: 285 [1100/1251 ( 88%)]  Loss:  2.665627 (2.8200)  Time: 1.078s,  949.84/s  (1.090s,  939.59/s)  LR: 1.609e-05  Data: 0.012 (0.012)
Train: 285 [1150/1251 ( 92%)]  Loss:  2.771376 (2.8180)  Time: 1.082s,  946.59/s  (1.090s,  939.46/s)  LR: 1.609e-05  Data: 0.012 (0.012)
Train: 285 [1200/1251 ( 96%)]  Loss:  2.706150 (2.8135)  Time: 1.173s,  873.20/s  (1.090s,  939.57/s)  LR: 1.609e-05  Data: 0.014 (0.012)
Train: 285 [1250/1251 (100%)]  Loss:  2.733572 (2.8104)  Time: 1.062s,  964.31/s  (1.090s,  939.42/s)  LR: 1.609e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.743 (5.743)  Loss:  0.4231 (0.4231)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5317 (0.8320)  Acc@1: 86.9104 (81.3700)  Acc@5: 97.8774 (95.6240)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-285.pth.tar', 81.3700000024414)

Train: 286 [   0/1251 (  0%)]  Loss:  2.798299 (2.7983)  Time: 1.087s,  942.35/s  (1.087s,  942.35/s)  LR: 1.531e-05  Data: 0.026 (0.026)
Train: 286 [  50/1251 (  4%)]  Loss:  2.703733 (2.7510)  Time: 1.103s,  928.05/s  (1.087s,  942.22/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 100/1251 (  8%)]  Loss:  2.997948 (2.8333)  Time: 1.077s,  950.82/s  (1.090s,  939.03/s)  LR: 1.531e-05  Data: 0.015 (0.013)
Train: 286 [ 150/1251 ( 12%)]  Loss:  2.901742 (2.8504)  Time: 1.080s,  947.94/s  (1.090s,  939.18/s)  LR: 1.531e-05  Data: 0.014 (0.013)
Train: 286 [ 200/1251 ( 16%)]  Loss:  2.765863 (2.8335)  Time: 1.175s,  871.81/s  (1.090s,  939.47/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 250/1251 ( 20%)]  Loss:  2.875008 (2.8404)  Time: 1.097s,  933.83/s  (1.090s,  939.08/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 300/1251 ( 24%)]  Loss:  2.777327 (2.8314)  Time: 1.103s,  927.99/s  (1.091s,  938.52/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 350/1251 ( 28%)]  Loss:  2.778174 (2.8248)  Time: 1.073s,  954.64/s  (1.090s,  939.50/s)  LR: 1.531e-05  Data: 0.013 (0.013)
Train: 286 [ 400/1251 ( 32%)]  Loss:  2.731035 (2.8143)  Time: 1.095s,  935.33/s  (1.091s,  939.01/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 450/1251 ( 36%)]  Loss:  3.103238 (2.8432)  Time: 1.099s,  931.41/s  (1.090s,  939.37/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 500/1251 ( 40%)]  Loss:  2.870267 (2.8457)  Time: 1.098s,  932.66/s  (1.089s,  940.16/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 550/1251 ( 44%)]  Loss:  2.858756 (2.8468)  Time: 1.096s,  934.28/s  (1.090s,  939.87/s)  LR: 1.531e-05  Data: 0.014 (0.013)
Train: 286 [ 600/1251 ( 48%)]  Loss:  2.748079 (2.8392)  Time: 1.103s,  928.58/s  (1.090s,  939.47/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [ 650/1251 ( 52%)]  Loss:  2.962975 (2.8480)  Time: 1.078s,  949.56/s  (1.090s,  939.42/s)  LR: 1.531e-05  Data: 0.014 (0.013)
Train: 286 [ 700/1251 ( 56%)]  Loss:  2.821388 (2.8463)  Time: 1.076s,  951.68/s  (1.090s,  939.27/s)  LR: 1.531e-05  Data: 0.014 (0.013)
Train: 286 [ 750/1251 ( 60%)]  Loss:  2.949796 (2.8527)  Time: 1.096s,  934.71/s  (1.091s,  938.96/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 800/1251 ( 64%)]  Loss:  2.944837 (2.8581)  Time: 1.092s,  937.48/s  (1.091s,  938.81/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [ 850/1251 ( 68%)]  Loss:  2.427592 (2.8342)  Time: 1.082s,  946.26/s  (1.090s,  939.35/s)  LR: 1.531e-05  Data: 0.013 (0.013)
Train: 286 [ 900/1251 ( 72%)]  Loss:  2.638622 (2.8239)  Time: 1.078s,  949.99/s  (1.090s,  939.58/s)  LR: 1.531e-05  Data: 0.013 (0.013)
Train: 286 [ 950/1251 ( 76%)]  Loss:  2.812522 (2.8234)  Time: 1.095s,  935.43/s  (1.090s,  939.45/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [1000/1251 ( 80%)]  Loss:  2.792650 (2.8219)  Time: 1.103s,  928.62/s  (1.090s,  939.38/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1050/1251 ( 84%)]  Loss:  2.982689 (2.8292)  Time: 1.078s,  950.14/s  (1.090s,  939.41/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 286 [1100/1251 ( 88%)]  Loss:  2.971073 (2.8354)  Time: 1.082s,  946.45/s  (1.090s,  939.63/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [1150/1251 ( 92%)]  Loss:  2.636738 (2.8271)  Time: 1.106s,  925.65/s  (1.090s,  939.62/s)  LR: 1.531e-05  Data: 0.011 (0.013)
Train: 286 [1200/1251 ( 96%)]  Loss:  2.975833 (2.8330)  Time: 1.094s,  936.43/s  (1.090s,  939.76/s)  LR: 1.531e-05  Data: 0.012 (0.013)
Train: 286 [1250/1251 (100%)]  Loss:  2.771038 (2.8307)  Time: 1.080s,  948.29/s  (1.090s,  939.56/s)  LR: 1.531e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.847 (5.847)  Loss:  0.4239 (0.4239)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5220 (0.8332)  Acc@1: 87.5000 (81.5200)  Acc@5: 98.1132 (95.7060)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-269.pth.tar', 81.37000002685546)

Train: 287 [   0/1251 (  0%)]  Loss:  2.933009 (2.9330)  Time: 1.086s,  943.30/s  (1.086s,  943.30/s)  LR: 1.458e-05  Data: 0.023 (0.023)
Train: 287 [  50/1251 (  4%)]  Loss:  3.052063 (2.9925)  Time: 1.077s,  951.16/s  (1.087s,  941.83/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 100/1251 (  8%)]  Loss:  2.844335 (2.9431)  Time: 1.076s,  951.63/s  (1.090s,  939.05/s)  LR: 1.458e-05  Data: 0.013 (0.013)
Train: 287 [ 150/1251 ( 12%)]  Loss:  2.844788 (2.9185)  Time: 1.096s,  934.68/s  (1.091s,  938.48/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 200/1251 ( 16%)]  Loss:  2.912491 (2.9173)  Time: 1.105s,  927.11/s  (1.093s,  937.10/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 250/1251 ( 20%)]  Loss:  2.799467 (2.8977)  Time: 1.081s,  946.85/s  (1.092s,  937.83/s)  LR: 1.458e-05  Data: 0.014 (0.013)
Train: 287 [ 300/1251 ( 24%)]  Loss:  2.800828 (2.8839)  Time: 1.077s,  950.75/s  (1.091s,  938.81/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 350/1251 ( 28%)]  Loss:  3.179448 (2.9208)  Time: 1.095s,  935.29/s  (1.090s,  939.24/s)  LR: 1.458e-05  Data: 0.014 (0.013)
Train: 287 [ 400/1251 ( 32%)]  Loss:  2.919573 (2.9207)  Time: 1.188s,  862.09/s  (1.091s,  938.35/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 450/1251 ( 36%)]  Loss:  2.962688 (2.9249)  Time: 1.094s,  936.11/s  (1.091s,  938.73/s)  LR: 1.458e-05  Data: 0.013 (0.013)
Train: 287 [ 500/1251 ( 40%)]  Loss:  2.845152 (2.9176)  Time: 1.081s,  947.48/s  (1.091s,  938.85/s)  LR: 1.458e-05  Data: 0.011 (0.013)
Train: 287 [ 550/1251 ( 44%)]  Loss:  2.639906 (2.8945)  Time: 1.093s,  936.67/s  (1.090s,  939.24/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 600/1251 ( 48%)]  Loss:  2.987650 (2.9016)  Time: 1.077s,  950.58/s  (1.090s,  939.30/s)  LR: 1.458e-05  Data: 0.014 (0.013)
Train: 287 [ 650/1251 ( 52%)]  Loss:  3.028305 (2.9107)  Time: 1.097s,  933.75/s  (1.090s,  939.52/s)  LR: 1.458e-05  Data: 0.014 (0.013)
Train: 287 [ 700/1251 ( 56%)]  Loss:  2.929021 (2.9119)  Time: 1.095s,  935.04/s  (1.089s,  939.99/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 750/1251 ( 60%)]  Loss:  2.528549 (2.8880)  Time: 1.085s,  943.73/s  (1.089s,  940.12/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 800/1251 ( 64%)]  Loss:  2.730258 (2.8787)  Time: 1.077s,  951.20/s  (1.089s,  940.15/s)  LR: 1.458e-05  Data: 0.014 (0.013)
Train: 287 [ 850/1251 ( 68%)]  Loss:  2.771150 (2.8727)  Time: 1.094s,  935.94/s  (1.089s,  940.10/s)  LR: 1.458e-05  Data: 0.013 (0.013)
Train: 287 [ 900/1251 ( 72%)]  Loss:  3.001465 (2.8795)  Time: 1.095s,  935.19/s  (1.089s,  939.96/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [ 950/1251 ( 76%)]  Loss:  2.796998 (2.8754)  Time: 1.102s,  928.91/s  (1.090s,  939.59/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [1000/1251 ( 80%)]  Loss:  2.751218 (2.8694)  Time: 1.094s,  936.29/s  (1.090s,  939.38/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [1050/1251 ( 84%)]  Loss:  2.803243 (2.8664)  Time: 1.094s,  935.96/s  (1.090s,  939.45/s)  LR: 1.458e-05  Data: 0.013 (0.013)
Train: 287 [1100/1251 ( 88%)]  Loss:  2.768016 (2.8622)  Time: 1.095s,  934.98/s  (1.090s,  939.09/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [1150/1251 ( 92%)]  Loss:  2.956271 (2.8661)  Time: 1.096s,  934.42/s  (1.091s,  938.98/s)  LR: 1.458e-05  Data: 0.020 (0.013)
Train: 287 [1200/1251 ( 96%)]  Loss:  2.719300 (2.8602)  Time: 1.076s,  951.48/s  (1.090s,  939.13/s)  LR: 1.458e-05  Data: 0.012 (0.013)
Train: 287 [1250/1251 (100%)]  Loss:  2.500276 (2.8464)  Time: 1.080s,  948.22/s  (1.091s,  938.88/s)  LR: 1.458e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.815 (5.815)  Loss:  0.4164 (0.4164)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.230 (0.443)  Loss:  0.5271 (0.8308)  Acc@1: 87.5000 (81.6440)  Acc@5: 97.9953 (95.6320)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-276.pth.tar', 81.38200005126953)

Train: 288 [   0/1251 (  0%)]  Loss:  2.513010 (2.5130)  Time: 1.083s,  945.32/s  (1.083s,  945.32/s)  LR: 1.390e-05  Data: 0.021 (0.021)
Train: 288 [  50/1251 (  4%)]  Loss:  3.058234 (2.7856)  Time: 1.095s,  934.78/s  (1.092s,  938.03/s)  LR: 1.390e-05  Data: 0.013 (0.013)
Train: 288 [ 100/1251 (  8%)]  Loss:  2.967813 (2.8464)  Time: 1.094s,  935.91/s  (1.093s,  936.46/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 150/1251 ( 12%)]  Loss:  2.901402 (2.8601)  Time: 1.076s,  951.65/s  (1.091s,  938.57/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 200/1251 ( 16%)]  Loss:  2.909090 (2.8699)  Time: 1.077s,  950.35/s  (1.092s,  937.71/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 250/1251 ( 20%)]  Loss:  2.812800 (2.8604)  Time: 1.096s,  934.29/s  (1.092s,  937.91/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 300/1251 ( 24%)]  Loss:  2.936617 (2.8713)  Time: 1.080s,  947.73/s  (1.091s,  938.34/s)  LR: 1.390e-05  Data: 0.013 (0.013)
Train: 288 [ 350/1251 ( 28%)]  Loss:  2.921515 (2.8776)  Time: 1.077s,  950.79/s  (1.090s,  939.26/s)  LR: 1.390e-05  Data: 0.013 (0.013)
Train: 288 [ 400/1251 ( 32%)]  Loss:  2.910305 (2.8812)  Time: 1.104s,  927.41/s  (1.090s,  939.79/s)  LR: 1.390e-05  Data: 0.014 (0.013)
Train: 288 [ 450/1251 ( 36%)]  Loss:  2.809801 (2.8741)  Time: 1.093s,  937.07/s  (1.090s,  939.85/s)  LR: 1.390e-05  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 288 [ 500/1251 ( 40%)]  Loss:  2.707420 (2.8589)  Time: 1.078s,  950.27/s  (1.089s,  940.21/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 550/1251 ( 44%)]  Loss:  2.753068 (2.8501)  Time: 1.098s,  932.93/s  (1.089s,  939.98/s)  LR: 1.390e-05  Data: 0.014 (0.013)
Train: 288 [ 600/1251 ( 48%)]  Loss:  2.856307 (2.8506)  Time: 1.110s,  922.79/s  (1.090s,  939.58/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 650/1251 ( 52%)]  Loss:  2.859364 (2.8512)  Time: 1.093s,  937.23/s  (1.090s,  939.82/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [ 700/1251 ( 56%)]  Loss:  3.003352 (2.8613)  Time: 1.078s,  949.61/s  (1.089s,  939.99/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 750/1251 ( 60%)]  Loss:  2.862351 (2.8614)  Time: 1.091s,  938.45/s  (1.089s,  940.19/s)  LR: 1.390e-05  Data: 0.011 (0.013)
Train: 288 [ 800/1251 ( 64%)]  Loss:  2.855832 (2.8611)  Time: 1.099s,  932.01/s  (1.089s,  939.97/s)  LR: 1.390e-05  Data: 0.018 (0.013)
Train: 288 [ 850/1251 ( 68%)]  Loss:  2.923933 (2.8646)  Time: 1.107s,  924.99/s  (1.089s,  939.92/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 900/1251 ( 72%)]  Loss:  2.797526 (2.8610)  Time: 1.077s,  950.73/s  (1.090s,  939.84/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [ 950/1251 ( 76%)]  Loss:  2.877731 (2.8619)  Time: 1.077s,  950.52/s  (1.090s,  939.80/s)  LR: 1.390e-05  Data: 0.014 (0.013)
Train: 288 [1000/1251 ( 80%)]  Loss:  3.014354 (2.8691)  Time: 1.081s,  947.20/s  (1.090s,  939.88/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1050/1251 ( 84%)]  Loss:  2.779599 (2.8651)  Time: 1.095s,  935.01/s  (1.090s,  939.67/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1100/1251 ( 88%)]  Loss:  2.942315 (2.8684)  Time: 1.077s,  950.84/s  (1.090s,  939.73/s)  LR: 1.390e-05  Data: 0.013 (0.013)
Train: 288 [1150/1251 ( 92%)]  Loss:  2.948254 (2.8717)  Time: 1.081s,  947.40/s  (1.090s,  939.84/s)  LR: 1.390e-05  Data: 0.013 (0.013)
Train: 288 [1200/1251 ( 96%)]  Loss:  2.934029 (2.8742)  Time: 1.077s,  950.93/s  (1.090s,  939.82/s)  LR: 1.390e-05  Data: 0.012 (0.013)
Train: 288 [1250/1251 (100%)]  Loss:  2.712998 (2.8680)  Time: 1.061s,  965.03/s  (1.090s,  939.66/s)  LR: 1.390e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.860 (5.860)  Loss:  0.4096 (0.4096)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5160 (0.8277)  Acc@1: 87.5000 (81.5460)  Acc@5: 98.1132 (95.6780)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-280.pth.tar', 81.39400002929688)

Train: 289 [   0/1251 (  0%)]  Loss:  2.885782 (2.8858)  Time: 1.085s,  943.61/s  (1.085s,  943.61/s)  LR: 1.328e-05  Data: 0.023 (0.023)
Train: 289 [  50/1251 (  4%)]  Loss:  2.948912 (2.9173)  Time: 1.075s,  952.49/s  (1.091s,  938.91/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 100/1251 (  8%)]  Loss:  2.813584 (2.8828)  Time: 1.079s,  948.94/s  (1.090s,  939.75/s)  LR: 1.328e-05  Data: 0.014 (0.013)
Train: 289 [ 150/1251 ( 12%)]  Loss:  3.026335 (2.9187)  Time: 1.079s,  949.41/s  (1.090s,  939.04/s)  LR: 1.328e-05  Data: 0.013 (0.013)
Train: 289 [ 200/1251 ( 16%)]  Loss:  2.946488 (2.9242)  Time: 1.106s,  925.95/s  (1.089s,  940.14/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 250/1251 ( 20%)]  Loss:  2.919215 (2.9234)  Time: 1.076s,  951.93/s  (1.090s,  939.66/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 300/1251 ( 24%)]  Loss:  2.839107 (2.9113)  Time: 1.095s,  935.24/s  (1.091s,  938.35/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 350/1251 ( 28%)]  Loss:  3.088321 (2.9335)  Time: 1.078s,  950.10/s  (1.090s,  939.62/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 400/1251 ( 32%)]  Loss:  2.932642 (2.9334)  Time: 1.104s,  927.79/s  (1.090s,  939.11/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [ 450/1251 ( 36%)]  Loss:  2.836294 (2.9237)  Time: 1.093s,  936.84/s  (1.091s,  938.40/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 500/1251 ( 40%)]  Loss:  2.681620 (2.9017)  Time: 1.075s,  952.39/s  (1.091s,  938.61/s)  LR: 1.328e-05  Data: 0.014 (0.013)
Train: 289 [ 550/1251 ( 44%)]  Loss:  2.840874 (2.8966)  Time: 1.102s,  928.85/s  (1.091s,  938.97/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 600/1251 ( 48%)]  Loss:  2.899220 (2.8968)  Time: 1.089s,  939.99/s  (1.091s,  938.85/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 650/1251 ( 52%)]  Loss:  3.212629 (2.9194)  Time: 1.078s,  949.47/s  (1.091s,  938.98/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 700/1251 ( 56%)]  Loss:  2.758934 (2.9087)  Time: 1.095s,  934.96/s  (1.090s,  939.34/s)  LR: 1.328e-05  Data: 0.013 (0.013)
Train: 289 [ 750/1251 ( 60%)]  Loss:  2.544625 (2.8859)  Time: 1.103s,  928.76/s  (1.090s,  939.06/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 800/1251 ( 64%)]  Loss:  2.749761 (2.8779)  Time: 1.081s,  947.36/s  (1.090s,  939.26/s)  LR: 1.328e-05  Data: 0.014 (0.013)
Train: 289 [ 850/1251 ( 68%)]  Loss:  3.035183 (2.8866)  Time: 1.081s,  947.19/s  (1.090s,  939.48/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [ 900/1251 ( 72%)]  Loss:  3.058112 (2.8957)  Time: 1.080s,  948.28/s  (1.090s,  939.74/s)  LR: 1.328e-05  Data: 0.016 (0.013)
Train: 289 [ 950/1251 ( 76%)]  Loss:  3.055748 (2.9037)  Time: 1.095s,  935.12/s  (1.090s,  939.69/s)  LR: 1.328e-05  Data: 0.014 (0.013)
Train: 289 [1000/1251 ( 80%)]  Loss:  2.953192 (2.9060)  Time: 1.076s,  951.41/s  (1.090s,  939.45/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1050/1251 ( 84%)]  Loss:  2.640450 (2.8940)  Time: 1.095s,  935.31/s  (1.090s,  939.63/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [1100/1251 ( 88%)]  Loss:  2.859932 (2.8925)  Time: 1.080s,  948.41/s  (1.090s,  939.36/s)  LR: 1.328e-05  Data: 0.011 (0.013)
Train: 289 [1150/1251 ( 92%)]  Loss:  2.994044 (2.8967)  Time: 1.081s,  947.30/s  (1.090s,  939.39/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [1200/1251 ( 96%)]  Loss:  3.183017 (2.9082)  Time: 1.095s,  934.88/s  (1.090s,  939.55/s)  LR: 1.328e-05  Data: 0.012 (0.013)
Train: 289 [1250/1251 (100%)]  Loss:  2.779676 (2.9032)  Time: 1.062s,  963.90/s  (1.090s,  939.74/s)  LR: 1.328e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.909 (5.909)  Loss:  0.4189 (0.4189)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.444)  Loss:  0.5264 (0.8285)  Acc@1: 86.9104 (81.5960)  Acc@5: 98.1132 (95.7220)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-279.pth.tar', 81.42600002685546)

Train: 290 [   0/1251 (  0%)]  Loss:  2.808818 (2.8088)  Time: 1.086s,  942.99/s  (1.086s,  942.99/s)  LR: 1.271e-05  Data: 0.023 (0.023)
Train: 290 [  50/1251 (  4%)]  Loss:  2.852504 (2.8307)  Time: 1.089s,  940.01/s  (1.097s,  933.65/s)  LR: 1.271e-05  Data: 0.015 (0.013)
Train: 290 [ 100/1251 (  8%)]  Loss:  2.998988 (2.8868)  Time: 1.096s,  934.32/s  (1.098s,  932.56/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 150/1251 ( 12%)]  Loss:  2.850816 (2.8778)  Time: 1.095s,  935.29/s  (1.097s,  933.04/s)  LR: 1.271e-05  Data: 0.013 (0.013)
Train: 290 [ 200/1251 ( 16%)]  Loss:  3.103119 (2.9228)  Time: 1.079s,  948.70/s  (1.097s,  933.48/s)  LR: 1.271e-05  Data: 0.013 (0.013)
Train: 290 [ 250/1251 ( 20%)]  Loss:  2.780632 (2.8991)  Time: 1.080s,  948.04/s  (1.095s,  934.85/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 300/1251 ( 24%)]  Loss:  2.896794 (2.8988)  Time: 1.093s,  936.54/s  (1.094s,  936.02/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 350/1251 ( 28%)]  Loss:  2.887833 (2.8974)  Time: 1.153s,  887.84/s  (1.093s,  936.47/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 400/1251 ( 32%)]  Loss:  3.053954 (2.9148)  Time: 1.077s,  950.42/s  (1.093s,  936.46/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 450/1251 ( 36%)]  Loss:  3.022414 (2.9256)  Time: 1.079s,  949.30/s  (1.093s,  936.68/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 500/1251 ( 40%)]  Loss:  2.947503 (2.9276)  Time: 1.077s,  950.93/s  (1.093s,  937.17/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [ 550/1251 ( 44%)]  Loss:  2.966559 (2.9308)  Time: 1.081s,  946.86/s  (1.093s,  936.68/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 600/1251 ( 48%)]  Loss:  3.049629 (2.9400)  Time: 1.207s,  848.70/s  (1.093s,  936.81/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 650/1251 ( 52%)]  Loss:  2.828709 (2.9320)  Time: 1.093s,  936.51/s  (1.093s,  937.18/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 700/1251 ( 56%)]  Loss:  2.885759 (2.9289)  Time: 1.074s,  953.13/s  (1.092s,  937.37/s)  LR: 1.271e-05  Data: 0.013 (0.013)
Train: 290 [ 750/1251 ( 60%)]  Loss:  2.848103 (2.9239)  Time: 1.094s,  935.80/s  (1.092s,  937.33/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 800/1251 ( 64%)]  Loss:  3.081937 (2.9332)  Time: 1.082s,  946.34/s  (1.092s,  937.38/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 850/1251 ( 68%)]  Loss:  2.703625 (2.9204)  Time: 1.096s,  934.47/s  (1.092s,  937.32/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 900/1251 ( 72%)]  Loss:  2.908670 (2.9198)  Time: 1.085s,  943.54/s  (1.093s,  937.28/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [ 950/1251 ( 76%)]  Loss:  2.857308 (2.9167)  Time: 1.170s,  874.91/s  (1.093s,  937.20/s)  LR: 1.271e-05  Data: 0.014 (0.013)
Train: 290 [1000/1251 ( 80%)]  Loss:  2.523091 (2.8979)  Time: 1.097s,  933.68/s  (1.092s,  937.53/s)  LR: 1.271e-05  Data: 0.011 (0.013)
Train: 290 [1050/1251 ( 84%)]  Loss:  2.839498 (2.8953)  Time: 1.175s,  871.30/s  (1.092s,  937.73/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [1100/1251 ( 88%)]  Loss:  2.764437 (2.8896)  Time: 1.095s,  934.78/s  (1.092s,  937.78/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [1150/1251 ( 92%)]  Loss:  2.650984 (2.8797)  Time: 1.077s,  950.85/s  (1.092s,  937.80/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [1200/1251 ( 96%)]  Loss:  2.776496 (2.8755)  Time: 1.077s,  950.43/s  (1.092s,  937.98/s)  LR: 1.271e-05  Data: 0.012 (0.013)
Train: 290 [1250/1251 (100%)]  Loss:  2.621496 (2.8658)  Time: 1.080s,  948.24/s  (1.092s,  937.60/s)  LR: 1.271e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.736 (5.736)  Loss:  0.4163 (0.4163)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5175 (0.8309)  Acc@1: 87.7358 (81.5020)  Acc@5: 97.9953 (95.6540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-290.pth.tar', 81.50199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-278.pth.tar', 81.45000008056641)

Train: 291 [   0/1251 (  0%)]  Loss:  2.707119 (2.7071)  Time: 1.097s,  933.09/s  (1.097s,  933.09/s)  LR: 1.220e-05  Data: 0.027 (0.027)
Train: 291 [  50/1251 (  4%)]  Loss:  2.948737 (2.8279)  Time: 1.102s,  928.91/s  (1.091s,  938.74/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 100/1251 (  8%)]  Loss:  2.769669 (2.8085)  Time: 1.078s,  949.95/s  (1.092s,  937.46/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 150/1251 ( 12%)]  Loss:  2.892578 (2.8295)  Time: 1.077s,  950.90/s  (1.093s,  937.17/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 200/1251 ( 16%)]  Loss:  2.936803 (2.8510)  Time: 1.081s,  947.65/s  (1.092s,  938.09/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 250/1251 ( 20%)]  Loss:  2.642611 (2.8163)  Time: 1.079s,  948.68/s  (1.090s,  939.82/s)  LR: 1.220e-05  Data: 0.011 (0.013)
Train: 291 [ 300/1251 ( 24%)]  Loss:  2.689944 (2.7982)  Time: 1.092s,  937.73/s  (1.089s,  940.16/s)  LR: 1.220e-05  Data: 0.015 (0.013)
Train: 291 [ 350/1251 ( 28%)]  Loss:  2.820295 (2.8010)  Time: 1.081s,  947.21/s  (1.090s,  939.43/s)  LR: 1.220e-05  Data: 0.014 (0.013)
Train: 291 [ 400/1251 ( 32%)]  Loss:  2.507500 (2.7684)  Time: 1.082s,  946.22/s  (1.090s,  939.26/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 450/1251 ( 36%)]  Loss:  2.811919 (2.7727)  Time: 1.093s,  937.06/s  (1.090s,  939.38/s)  LR: 1.220e-05  Data: 0.014 (0.013)
Train: 291 [ 500/1251 ( 40%)]  Loss:  2.867518 (2.7813)  Time: 1.098s,  932.85/s  (1.090s,  939.49/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 550/1251 ( 44%)]  Loss:  2.498267 (2.7577)  Time: 1.077s,  951.19/s  (1.090s,  939.82/s)  LR: 1.220e-05  Data: 0.013 (0.013)
Train: 291 [ 600/1251 ( 48%)]  Loss:  2.831590 (2.7634)  Time: 1.078s,  949.85/s  (1.090s,  939.79/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [ 650/1251 ( 52%)]  Loss:  3.054307 (2.7842)  Time: 1.075s,  952.25/s  (1.090s,  939.88/s)  LR: 1.220e-05  Data: 0.013 (0.013)
Train: 291 [ 700/1251 ( 56%)]  Loss:  2.793788 (2.7848)  Time: 1.095s,  934.76/s  (1.089s,  940.00/s)  LR: 1.220e-05  Data: 0.013 (0.013)
Train: 291 [ 750/1251 ( 60%)]  Loss:  3.019134 (2.7995)  Time: 1.078s,  949.67/s  (1.089s,  940.43/s)  LR: 1.220e-05  Data: 0.015 (0.013)
Train: 291 [ 800/1251 ( 64%)]  Loss:  2.998785 (2.8112)  Time: 1.078s,  950.04/s  (1.089s,  940.54/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 291 [ 850/1251 ( 68%)]  Loss:  2.550613 (2.7967)  Time: 1.080s,  948.07/s  (1.088s,  940.75/s)  LR: 1.220e-05  Data: 0.015 (0.013)
Train: 291 [ 900/1251 ( 72%)]  Loss:  3.002726 (2.8076)  Time: 1.076s,  951.38/s  (1.088s,  941.26/s)  LR: 1.220e-05  Data: 0.013 (0.013)
Train: 291 [ 950/1251 ( 76%)]  Loss:  2.924935 (2.8134)  Time: 1.078s,  949.99/s  (1.088s,  941.12/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [1000/1251 ( 80%)]  Loss:  2.930623 (2.8190)  Time: 1.076s,  951.73/s  (1.088s,  941.38/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [1050/1251 ( 84%)]  Loss:  2.926697 (2.8239)  Time: 1.075s,  952.44/s  (1.088s,  941.56/s)  LR: 1.220e-05  Data: 0.012 (0.013)
Train: 291 [1100/1251 ( 88%)]  Loss:  3.075974 (2.8349)  Time: 1.081s,  947.29/s  (1.088s,  941.58/s)  LR: 1.220e-05  Data: 0.014 (0.013)
Train: 291 [1150/1251 ( 92%)]  Loss:  3.089972 (2.8455)  Time: 1.077s,  950.83/s  (1.088s,  941.57/s)  LR: 1.220e-05  Data: 0.014 (0.013)
Train: 291 [1200/1251 ( 96%)]  Loss:  3.151937 (2.8578)  Time: 1.143s,  895.85/s  (1.088s,  941.49/s)  LR: 1.220e-05  Data: 0.015 (0.013)
Train: 291 [1250/1251 (100%)]  Loss:  2.948690 (2.8613)  Time: 1.080s,  948.31/s  (1.088s,  941.38/s)  LR: 1.220e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.833 (5.833)  Loss:  0.4094 (0.4094)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5303 (0.8252)  Acc@1: 86.9104 (81.6340)  Acc@5: 98.1132 (95.6460)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-290.pth.tar', 81.50199997314454)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-283.pth.tar', 81.49399994628907)

Train: 292 [   0/1251 (  0%)]  Loss:  2.832979 (2.8330)  Time: 1.086s,  943.10/s  (1.086s,  943.10/s)  LR: 1.174e-05  Data: 0.024 (0.024)
Train: 292 [  50/1251 (  4%)]  Loss:  2.595205 (2.7141)  Time: 1.095s,  935.58/s  (1.086s,  942.59/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [ 100/1251 (  8%)]  Loss:  2.862816 (2.7637)  Time: 1.094s,  936.08/s  (1.088s,  940.85/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [ 150/1251 ( 12%)]  Loss:  2.683309 (2.7436)  Time: 1.097s,  933.43/s  (1.087s,  941.82/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [ 200/1251 ( 16%)]  Loss:  2.845132 (2.7639)  Time: 1.084s,  944.99/s  (1.088s,  940.79/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [ 250/1251 ( 20%)]  Loss:  3.031533 (2.8085)  Time: 1.094s,  935.83/s  (1.088s,  940.81/s)  LR: 1.174e-05  Data: 0.013 (0.012)
Train: 292 [ 300/1251 ( 24%)]  Loss:  2.701989 (2.7933)  Time: 1.095s,  934.86/s  (1.089s,  940.29/s)  LR: 1.174e-05  Data: 0.012 (0.012)
Train: 292 [ 350/1251 ( 28%)]  Loss:  2.987625 (2.8176)  Time: 1.175s,  871.61/s  (1.090s,  939.73/s)  LR: 1.174e-05  Data: 0.011 (0.012)
Train: 292 [ 400/1251 ( 32%)]  Loss:  2.867432 (2.8231)  Time: 1.094s,  935.97/s  (1.089s,  939.91/s)  LR: 1.174e-05  Data: 0.012 (0.012)
Train: 292 [ 450/1251 ( 36%)]  Loss:  2.555372 (2.7963)  Time: 1.095s,  935.14/s  (1.090s,  939.79/s)  LR: 1.174e-05  Data: 0.011 (0.012)
Train: 292 [ 500/1251 ( 40%)]  Loss:  2.711324 (2.7886)  Time: 1.076s,  951.68/s  (1.089s,  940.12/s)  LR: 1.174e-05  Data: 0.012 (0.012)
Train: 292 [ 550/1251 ( 44%)]  Loss:  2.954350 (2.8024)  Time: 1.094s,  935.99/s  (1.090s,  939.76/s)  LR: 1.174e-05  Data: 0.011 (0.012)
Train: 292 [ 600/1251 ( 48%)]  Loss:  2.855502 (2.8065)  Time: 1.095s,  935.40/s  (1.090s,  939.71/s)  LR: 1.174e-05  Data: 0.012 (0.012)
Train: 292 [ 650/1251 ( 52%)]  Loss:  3.147270 (2.8308)  Time: 1.095s,  934.91/s  (1.090s,  939.83/s)  LR: 1.174e-05  Data: 0.011 (0.012)
Train: 292 [ 700/1251 ( 56%)]  Loss:  2.748540 (2.8254)  Time: 1.077s,  950.50/s  (1.090s,  939.73/s)  LR: 1.174e-05  Data: 0.012 (0.012)
Train: 292 [ 750/1251 ( 60%)]  Loss:  2.884251 (2.8290)  Time: 1.101s,  930.03/s  (1.090s,  939.56/s)  LR: 1.174e-05  Data: 0.011 (0.012)
Train: 292 [ 800/1251 ( 64%)]  Loss:  2.859554 (2.8308)  Time: 1.093s,  937.19/s  (1.090s,  939.78/s)  LR: 1.174e-05  Data: 0.015 (0.012)
Train: 292 [ 850/1251 ( 68%)]  Loss:  2.713191 (2.8243)  Time: 1.080s,  947.73/s  (1.090s,  939.88/s)  LR: 1.174e-05  Data: 0.015 (0.012)
Train: 292 [ 900/1251 ( 72%)]  Loss:  2.661364 (2.8157)  Time: 1.076s,  951.85/s  (1.090s,  939.83/s)  LR: 1.174e-05  Data: 0.013 (0.012)
Train: 292 [ 950/1251 ( 76%)]  Loss:  2.906635 (2.8203)  Time: 1.082s,  946.53/s  (1.090s,  939.81/s)  LR: 1.174e-05  Data: 0.013 (0.013)
Train: 292 [1000/1251 ( 80%)]  Loss:  2.951633 (2.8265)  Time: 1.100s,  930.81/s  (1.090s,  939.69/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [1050/1251 ( 84%)]  Loss:  2.848191 (2.8275)  Time: 1.077s,  950.68/s  (1.090s,  939.78/s)  LR: 1.174e-05  Data: 0.015 (0.013)
Train: 292 [1100/1251 ( 88%)]  Loss:  2.601148 (2.8177)  Time: 1.076s,  951.49/s  (1.089s,  940.11/s)  LR: 1.174e-05  Data: 0.013 (0.013)
Train: 292 [1150/1251 ( 92%)]  Loss:  2.834955 (2.8184)  Time: 1.100s,  930.93/s  (1.089s,  940.21/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [1200/1251 ( 96%)]  Loss:  2.585173 (2.8091)  Time: 1.099s,  932.16/s  (1.089s,  940.03/s)  LR: 1.174e-05  Data: 0.012 (0.013)
Train: 292 [1250/1251 (100%)]  Loss:  2.998322 (2.8163)  Time: 1.062s,  964.16/s  (1.090s,  939.83/s)  LR: 1.174e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.809 (5.809)  Loss:  0.4076 (0.4076)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5193 (0.8263)  Acc@1: 87.2642 (81.5180)  Acc@5: 98.2311 (95.6500)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-292.pth.tar', 81.51800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-290.pth.tar', 81.50199997314454)

Train: 293 [   0/1251 (  0%)]  Loss:  2.974380 (2.9744)  Time: 1.108s,  924.44/s  (1.108s,  924.44/s)  LR: 1.133e-05  Data: 0.028 (0.028)
Train: 293 [  50/1251 (  4%)]  Loss:  2.594439 (2.7844)  Time: 1.077s,  951.03/s  (1.087s,  941.97/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 100/1251 (  8%)]  Loss:  2.903114 (2.8240)  Time: 1.087s,  942.19/s  (1.088s,  940.87/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 150/1251 ( 12%)]  Loss:  2.800521 (2.8181)  Time: 1.092s,  938.14/s  (1.089s,  940.41/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 200/1251 ( 16%)]  Loss:  2.909102 (2.8363)  Time: 1.077s,  950.62/s  (1.088s,  941.26/s)  LR: 1.133e-05  Data: 0.014 (0.013)
Train: 293 [ 250/1251 ( 20%)]  Loss:  2.901130 (2.8471)  Time: 1.078s,  950.18/s  (1.087s,  942.08/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [ 300/1251 ( 24%)]  Loss:  2.910355 (2.8561)  Time: 1.095s,  935.01/s  (1.089s,  940.47/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 350/1251 ( 28%)]  Loss:  2.893807 (2.8609)  Time: 1.093s,  936.61/s  (1.089s,  940.18/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 400/1251 ( 32%)]  Loss:  2.895211 (2.8647)  Time: 1.094s,  936.28/s  (1.089s,  940.23/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 450/1251 ( 36%)]  Loss:  3.023069 (2.8805)  Time: 1.093s,  937.20/s  (1.089s,  940.20/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 293 [ 500/1251 ( 40%)]  Loss:  2.623271 (2.8571)  Time: 1.077s,  950.73/s  (1.089s,  940.31/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 550/1251 ( 44%)]  Loss:  3.074926 (2.8753)  Time: 1.095s,  935.04/s  (1.089s,  940.52/s)  LR: 1.133e-05  Data: 0.012 (0.012)
Train: 293 [ 600/1251 ( 48%)]  Loss:  2.810028 (2.8703)  Time: 1.125s,  910.54/s  (1.089s,  940.25/s)  LR: 1.133e-05  Data: 0.014 (0.012)
Train: 293 [ 650/1251 ( 52%)]  Loss:  2.803195 (2.8655)  Time: 1.077s,  950.73/s  (1.089s,  940.62/s)  LR: 1.133e-05  Data: 0.012 (0.012)
Train: 293 [ 700/1251 ( 56%)]  Loss:  2.605364 (2.8481)  Time: 1.077s,  950.37/s  (1.089s,  940.52/s)  LR: 1.133e-05  Data: 0.012 (0.012)
Train: 293 [ 750/1251 ( 60%)]  Loss:  3.000684 (2.8577)  Time: 1.078s,  950.06/s  (1.089s,  940.34/s)  LR: 1.133e-05  Data: 0.012 (0.012)
Train: 293 [ 800/1251 ( 64%)]  Loss:  2.666367 (2.8464)  Time: 1.078s,  949.92/s  (1.089s,  940.33/s)  LR: 1.133e-05  Data: 0.015 (0.012)
Train: 293 [ 850/1251 ( 68%)]  Loss:  2.589669 (2.8321)  Time: 1.075s,  952.32/s  (1.089s,  940.48/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 900/1251 ( 72%)]  Loss:  2.954389 (2.8386)  Time: 1.094s,  935.67/s  (1.089s,  940.30/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [ 950/1251 ( 76%)]  Loss:  2.814393 (2.8374)  Time: 1.077s,  951.07/s  (1.089s,  940.50/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [1000/1251 ( 80%)]  Loss:  2.641028 (2.8280)  Time: 1.098s,  932.35/s  (1.089s,  940.22/s)  LR: 1.133e-05  Data: 0.017 (0.013)
Train: 293 [1050/1251 ( 84%)]  Loss:  2.859973 (2.8295)  Time: 1.082s,  945.98/s  (1.089s,  940.20/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1100/1251 ( 88%)]  Loss:  2.924696 (2.8336)  Time: 1.103s,  928.39/s  (1.089s,  940.11/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1150/1251 ( 92%)]  Loss:  2.997115 (2.8404)  Time: 1.085s,  943.36/s  (1.089s,  939.97/s)  LR: 1.133e-05  Data: 0.011 (0.013)
Train: 293 [1200/1251 ( 96%)]  Loss:  2.761957 (2.8373)  Time: 1.095s,  935.33/s  (1.090s,  939.65/s)  LR: 1.133e-05  Data: 0.012 (0.013)
Train: 293 [1250/1251 (100%)]  Loss:  3.002946 (2.8437)  Time: 1.078s,  950.05/s  (1.090s,  939.50/s)  LR: 1.133e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.796 (5.796)  Loss:  0.4116 (0.4116)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.444)  Loss:  0.5223 (0.8265)  Acc@1: 87.3821 (81.6260)  Acc@5: 97.9953 (95.6800)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-292.pth.tar', 81.51800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-282.pth.tar', 81.51)

Train: 294 [   0/1251 (  0%)]  Loss:  2.660985 (2.6610)  Time: 1.085s,  943.61/s  (1.085s,  943.61/s)  LR: 1.098e-05  Data: 0.022 (0.022)
Train: 294 [  50/1251 (  4%)]  Loss:  2.887542 (2.7743)  Time: 1.080s,  948.57/s  (1.093s,  936.80/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 100/1251 (  8%)]  Loss:  2.795999 (2.7815)  Time: 1.073s,  954.54/s  (1.092s,  937.84/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 150/1251 ( 12%)]  Loss:  2.916782 (2.8153)  Time: 1.095s,  935.02/s  (1.093s,  937.00/s)  LR: 1.098e-05  Data: 0.015 (0.013)
Train: 294 [ 200/1251 ( 16%)]  Loss:  2.964361 (2.8451)  Time: 1.107s,  925.03/s  (1.094s,  936.26/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 250/1251 ( 20%)]  Loss:  2.945274 (2.8618)  Time: 1.078s,  950.02/s  (1.093s,  936.64/s)  LR: 1.098e-05  Data: 0.013 (0.013)
Train: 294 [ 300/1251 ( 24%)]  Loss:  2.887783 (2.8655)  Time: 1.083s,  945.71/s  (1.092s,  937.32/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 350/1251 ( 28%)]  Loss:  2.830992 (2.8612)  Time: 1.096s,  934.34/s  (1.091s,  938.38/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 400/1251 ( 32%)]  Loss:  2.932383 (2.8691)  Time: 1.076s,  951.73/s  (1.091s,  938.29/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 450/1251 ( 36%)]  Loss:  2.847830 (2.8670)  Time: 1.081s,  947.41/s  (1.091s,  938.99/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 500/1251 ( 40%)]  Loss:  2.521042 (2.8355)  Time: 1.080s,  948.33/s  (1.091s,  938.84/s)  LR: 1.098e-05  Data: 0.013 (0.013)
Train: 294 [ 550/1251 ( 44%)]  Loss:  2.971515 (2.8469)  Time: 1.082s,  946.41/s  (1.091s,  938.83/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 600/1251 ( 48%)]  Loss:  3.002506 (2.8588)  Time: 1.094s,  936.10/s  (1.091s,  938.97/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 650/1251 ( 52%)]  Loss:  2.953884 (2.8656)  Time: 1.078s,  949.69/s  (1.091s,  938.49/s)  LR: 1.098e-05  Data: 0.015 (0.013)
Train: 294 [ 700/1251 ( 56%)]  Loss:  2.814809 (2.8622)  Time: 1.079s,  949.41/s  (1.091s,  938.93/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 750/1251 ( 60%)]  Loss:  3.050054 (2.8740)  Time: 1.076s,  951.37/s  (1.090s,  939.18/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 800/1251 ( 64%)]  Loss:  2.958688 (2.8790)  Time: 1.094s,  935.72/s  (1.091s,  939.01/s)  LR: 1.098e-05  Data: 0.014 (0.013)
Train: 294 [ 850/1251 ( 68%)]  Loss:  2.643717 (2.8659)  Time: 1.094s,  936.08/s  (1.091s,  938.88/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Train: 294 [ 900/1251 ( 72%)]  Loss:  2.844068 (2.8647)  Time: 1.097s,  933.86/s  (1.091s,  938.88/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [ 950/1251 ( 76%)]  Loss:  2.842850 (2.8637)  Time: 1.094s,  935.87/s  (1.091s,  938.86/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1000/1251 ( 80%)]  Loss:  2.857980 (2.8634)  Time: 1.094s,  935.91/s  (1.091s,  938.81/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1050/1251 ( 84%)]  Loss:  2.778865 (2.8595)  Time: 1.078s,  950.35/s  (1.091s,  938.83/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1100/1251 ( 88%)]  Loss:  2.819999 (2.8578)  Time: 1.078s,  950.15/s  (1.091s,  938.89/s)  LR: 1.098e-05  Data: 0.013 (0.013)
Train: 294 [1150/1251 ( 92%)]  Loss:  2.984527 (2.8631)  Time: 1.095s,  935.57/s  (1.091s,  938.78/s)  LR: 1.098e-05  Data: 0.012 (0.013)
Train: 294 [1200/1251 ( 96%)]  Loss:  2.845344 (2.8624)  Time: 1.106s,  925.62/s  (1.091s,  938.88/s)  LR: 1.098e-05  Data: 0.011 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 294 [1250/1251 (100%)]  Loss:  2.788316 (2.8595)  Time: 1.081s,  947.52/s  (1.091s,  938.91/s)  LR: 1.098e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.805 (5.805)  Loss:  0.4110 (0.4110)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5252 (0.8282)  Acc@1: 87.3821 (81.5400)  Acc@5: 98.1132 (95.6080)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-294.pth.tar', 81.53999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-292.pth.tar', 81.51800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-274.pth.tar', 81.517999921875)

Train: 295 [   0/1251 (  0%)]  Loss:  2.781365 (2.7814)  Time: 1.086s,  942.92/s  (1.086s,  942.92/s)  LR: 1.068e-05  Data: 0.023 (0.023)
Train: 295 [  50/1251 (  4%)]  Loss:  2.746752 (2.7641)  Time: 1.077s,  950.38/s  (1.086s,  942.99/s)  LR: 1.068e-05  Data: 0.014 (0.013)
Train: 295 [ 100/1251 (  8%)]  Loss:  2.572611 (2.7002)  Time: 1.086s,  943.22/s  (1.087s,  941.94/s)  LR: 1.068e-05  Data: 0.011 (0.013)
Train: 295 [ 150/1251 ( 12%)]  Loss:  2.681714 (2.6956)  Time: 1.094s,  936.30/s  (1.087s,  941.68/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [ 200/1251 ( 16%)]  Loss:  2.788037 (2.7141)  Time: 1.089s,  940.38/s  (1.088s,  941.19/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [ 250/1251 ( 20%)]  Loss:  2.793620 (2.7273)  Time: 1.083s,  945.78/s  (1.088s,  941.17/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [ 300/1251 ( 24%)]  Loss:  2.867347 (2.7473)  Time: 1.076s,  951.86/s  (1.089s,  940.48/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [ 350/1251 ( 28%)]  Loss:  2.895908 (2.7659)  Time: 1.076s,  951.58/s  (1.089s,  940.47/s)  LR: 1.068e-05  Data: 0.013 (0.012)
Train: 295 [ 400/1251 ( 32%)]  Loss:  3.028624 (2.7951)  Time: 1.093s,  936.71/s  (1.089s,  940.12/s)  LR: 1.068e-05  Data: 0.014 (0.012)
Train: 295 [ 450/1251 ( 36%)]  Loss:  2.742490 (2.7898)  Time: 1.113s,  920.35/s  (1.090s,  939.29/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [ 500/1251 ( 40%)]  Loss:  3.083973 (2.8166)  Time: 1.096s,  934.32/s  (1.090s,  939.37/s)  LR: 1.068e-05  Data: 0.017 (0.012)
Train: 295 [ 550/1251 ( 44%)]  Loss:  2.763906 (2.8122)  Time: 1.086s,  943.26/s  (1.090s,  939.05/s)  LR: 1.068e-05  Data: 0.013 (0.013)
Train: 295 [ 600/1251 ( 48%)]  Loss:  2.872184 (2.8168)  Time: 1.095s,  935.19/s  (1.090s,  939.21/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [ 650/1251 ( 52%)]  Loss:  2.725246 (2.8103)  Time: 1.095s,  935.11/s  (1.090s,  939.20/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [ 700/1251 ( 56%)]  Loss:  2.975222 (2.8213)  Time: 1.079s,  949.41/s  (1.090s,  939.45/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [ 750/1251 ( 60%)]  Loss:  2.762046 (2.8176)  Time: 1.085s,  943.45/s  (1.090s,  939.08/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 295 [ 800/1251 ( 64%)]  Loss:  2.750272 (2.8136)  Time: 1.086s,  942.93/s  (1.090s,  939.28/s)  LR: 1.068e-05  Data: 0.011 (0.012)
Train: 295 [ 850/1251 ( 68%)]  Loss:  3.091461 (2.8290)  Time: 1.079s,  949.43/s  (1.090s,  939.44/s)  LR: 1.068e-05  Data: 0.017 (0.012)
Train: 295 [ 900/1251 ( 72%)]  Loss:  2.638683 (2.8190)  Time: 1.170s,  875.31/s  (1.090s,  939.65/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [ 950/1251 ( 76%)]  Loss:  2.991425 (2.8276)  Time: 1.077s,  950.84/s  (1.090s,  939.68/s)  LR: 1.068e-05  Data: 0.012 (0.012)
Train: 295 [1000/1251 ( 80%)]  Loss:  2.498719 (2.8120)  Time: 1.094s,  936.44/s  (1.090s,  939.70/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [1050/1251 ( 84%)]  Loss:  2.635228 (2.8039)  Time: 1.082s,  946.19/s  (1.090s,  939.67/s)  LR: 1.068e-05  Data: 0.013 (0.013)
Train: 295 [1100/1251 ( 88%)]  Loss:  2.928664 (2.8094)  Time: 1.097s,  933.53/s  (1.090s,  939.43/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [1150/1251 ( 92%)]  Loss:  2.976683 (2.8163)  Time: 1.078s,  949.96/s  (1.090s,  939.10/s)  LR: 1.068e-05  Data: 0.016 (0.013)
Train: 295 [1200/1251 ( 96%)]  Loss:  2.915222 (2.8203)  Time: 1.083s,  945.70/s  (1.090s,  939.07/s)  LR: 1.068e-05  Data: 0.012 (0.013)
Train: 295 [1250/1251 (100%)]  Loss:  2.458394 (2.8064)  Time: 1.063s,  963.03/s  (1.090s,  939.16/s)  LR: 1.068e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.897 (5.897)  Loss:  0.4168 (0.4168)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5179 (0.8278)  Acc@1: 87.6179 (81.5440)  Acc@5: 97.9953 (95.6580)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-295.pth.tar', 81.54400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-294.pth.tar', 81.53999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-292.pth.tar', 81.51800002685547)

Train: 296 [   0/1251 (  0%)]  Loss:  2.751687 (2.7517)  Time: 1.091s,  938.53/s  (1.091s,  938.53/s)  LR: 1.043e-05  Data: 0.027 (0.027)
Train: 296 [  50/1251 (  4%)]  Loss:  2.867890 (2.8098)  Time: 1.077s,  950.80/s  (1.089s,  939.95/s)  LR: 1.043e-05  Data: 0.013 (0.013)
Train: 296 [ 100/1251 (  8%)]  Loss:  2.989501 (2.8697)  Time: 1.077s,  950.37/s  (1.088s,  940.90/s)  LR: 1.043e-05  Data: 0.013 (0.013)
Train: 296 [ 150/1251 ( 12%)]  Loss:  2.738687 (2.8369)  Time: 1.093s,  936.47/s  (1.088s,  941.27/s)  LR: 1.043e-05  Data: 0.013 (0.013)
Train: 296 [ 200/1251 ( 16%)]  Loss:  2.940491 (2.8577)  Time: 1.093s,  936.92/s  (1.089s,  939.99/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 250/1251 ( 20%)]  Loss:  2.916054 (2.8674)  Time: 1.081s,  947.12/s  (1.089s,  940.12/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 300/1251 ( 24%)]  Loss:  2.682608 (2.8410)  Time: 1.078s,  949.74/s  (1.088s,  940.91/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 350/1251 ( 28%)]  Loss:  2.938977 (2.8532)  Time: 1.094s,  936.43/s  (1.090s,  939.74/s)  LR: 1.043e-05  Data: 0.017 (0.013)
Train: 296 [ 400/1251 ( 32%)]  Loss:  2.966322 (2.8658)  Time: 1.085s,  943.89/s  (1.090s,  939.21/s)  LR: 1.043e-05  Data: 0.011 (0.013)
Train: 296 [ 450/1251 ( 36%)]  Loss:  3.000549 (2.8793)  Time: 1.096s,  934.27/s  (1.091s,  939.02/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 500/1251 ( 40%)]  Loss:  2.930835 (2.8840)  Time: 1.077s,  951.15/s  (1.091s,  939.01/s)  LR: 1.043e-05  Data: 0.014 (0.013)
Train: 296 [ 550/1251 ( 44%)]  Loss:  2.948167 (2.8893)  Time: 1.074s,  953.19/s  (1.091s,  938.66/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 600/1251 ( 48%)]  Loss:  3.010123 (2.8986)  Time: 1.077s,  950.66/s  (1.091s,  938.91/s)  LR: 1.043e-05  Data: 0.013 (0.013)
Train: 296 [ 650/1251 ( 52%)]  Loss:  2.961786 (2.9031)  Time: 1.076s,  951.72/s  (1.090s,  939.17/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 700/1251 ( 56%)]  Loss:  2.897009 (2.9027)  Time: 1.106s,  925.77/s  (1.090s,  939.23/s)  LR: 1.043e-05  Data: 0.013 (0.013)
Train: 296 [ 750/1251 ( 60%)]  Loss:  2.697214 (2.8899)  Time: 1.099s,  932.09/s  (1.090s,  939.10/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 296 [ 800/1251 ( 64%)]  Loss:  2.476325 (2.8655)  Time: 1.077s,  950.93/s  (1.090s,  939.12/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 850/1251 ( 68%)]  Loss:  2.832653 (2.8637)  Time: 1.094s,  935.66/s  (1.091s,  939.01/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 900/1251 ( 72%)]  Loss:  2.812336 (2.8610)  Time: 1.096s,  934.61/s  (1.091s,  938.75/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [ 950/1251 ( 76%)]  Loss:  2.619255 (2.8489)  Time: 1.102s,  929.02/s  (1.091s,  938.83/s)  LR: 1.043e-05  Data: 0.015 (0.013)
Train: 296 [1000/1251 ( 80%)]  Loss:  2.617144 (2.8379)  Time: 1.074s,  953.38/s  (1.090s,  939.13/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [1050/1251 ( 84%)]  Loss:  3.040272 (2.8471)  Time: 1.077s,  950.46/s  (1.090s,  939.47/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [1100/1251 ( 88%)]  Loss:  2.959937 (2.8520)  Time: 1.077s,  951.12/s  (1.090s,  939.65/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [1150/1251 ( 92%)]  Loss:  2.949697 (2.8561)  Time: 1.077s,  951.07/s  (1.090s,  939.63/s)  LR: 1.043e-05  Data: 0.013 (0.013)
Train: 296 [1200/1251 ( 96%)]  Loss:  2.862077 (2.8563)  Time: 1.076s,  951.89/s  (1.090s,  939.66/s)  LR: 1.043e-05  Data: 0.012 (0.013)
Train: 296 [1250/1251 (100%)]  Loss:  2.813255 (2.8546)  Time: 1.062s,  964.33/s  (1.090s,  939.68/s)  LR: 1.043e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.783 (5.783)  Loss:  0.4139 (0.4139)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5327 (0.8296)  Acc@1: 87.3821 (81.6140)  Acc@5: 98.1132 (95.6200)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-296.pth.tar', 81.61400020751954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-295.pth.tar', 81.54400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-294.pth.tar', 81.53999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-286.pth.tar', 81.52)

Train: 297 [   0/1251 (  0%)]  Loss:  2.694800 (2.6948)  Time: 1.084s,  944.70/s  (1.084s,  944.70/s)  LR: 1.024e-05  Data: 0.022 (0.022)
Train: 297 [  50/1251 (  4%)]  Loss:  2.811008 (2.7529)  Time: 1.096s,  934.55/s  (1.088s,  940.92/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 100/1251 (  8%)]  Loss:  2.880971 (2.7956)  Time: 1.095s,  935.19/s  (1.092s,  937.70/s)  LR: 1.024e-05  Data: 0.015 (0.013)
Train: 297 [ 150/1251 ( 12%)]  Loss:  2.757326 (2.7860)  Time: 1.087s,  942.27/s  (1.092s,  937.73/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 200/1251 ( 16%)]  Loss:  2.832844 (2.7954)  Time: 1.077s,  950.66/s  (1.091s,  938.67/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 250/1251 ( 20%)]  Loss:  2.627132 (2.7673)  Time: 1.094s,  936.32/s  (1.090s,  939.44/s)  LR: 1.024e-05  Data: 0.016 (0.013)
Train: 297 [ 300/1251 ( 24%)]  Loss:  2.705340 (2.7585)  Time: 1.093s,  936.62/s  (1.091s,  938.57/s)  LR: 1.024e-05  Data: 0.017 (0.013)
Train: 297 [ 350/1251 ( 28%)]  Loss:  2.855906 (2.7707)  Time: 1.074s,  953.09/s  (1.091s,  938.79/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 400/1251 ( 32%)]  Loss:  2.863404 (2.7810)  Time: 1.084s,  944.38/s  (1.090s,  939.30/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 450/1251 ( 36%)]  Loss:  2.857226 (2.7886)  Time: 1.077s,  950.57/s  (1.090s,  939.07/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [ 500/1251 ( 40%)]  Loss:  2.572090 (2.7689)  Time: 1.095s,  935.56/s  (1.090s,  939.63/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [ 550/1251 ( 44%)]  Loss:  2.882252 (2.7784)  Time: 1.077s,  950.50/s  (1.089s,  939.97/s)  LR: 1.024e-05  Data: 0.011 (0.013)
Train: 297 [ 600/1251 ( 48%)]  Loss:  2.829746 (2.7823)  Time: 1.103s,  928.61/s  (1.090s,  939.23/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 650/1251 ( 52%)]  Loss:  2.490515 (2.7615)  Time: 1.081s,  947.36/s  (1.090s,  939.42/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 700/1251 ( 56%)]  Loss:  2.887963 (2.7699)  Time: 1.103s,  928.29/s  (1.090s,  939.71/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [ 750/1251 ( 60%)]  Loss:  2.643353 (2.7620)  Time: 1.078s,  950.15/s  (1.090s,  939.54/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [ 800/1251 ( 64%)]  Loss:  2.497472 (2.7464)  Time: 1.105s,  927.00/s  (1.090s,  939.76/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [ 850/1251 ( 68%)]  Loss:  2.790814 (2.7489)  Time: 1.106s,  925.85/s  (1.090s,  939.82/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [ 900/1251 ( 72%)]  Loss:  2.811286 (2.7522)  Time: 1.082s,  946.43/s  (1.090s,  939.67/s)  LR: 1.024e-05  Data: 0.015 (0.013)
Train: 297 [ 950/1251 ( 76%)]  Loss:  2.750012 (2.7521)  Time: 1.077s,  950.93/s  (1.090s,  939.79/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [1000/1251 ( 80%)]  Loss:  2.955413 (2.7618)  Time: 1.095s,  934.95/s  (1.090s,  939.79/s)  LR: 1.024e-05  Data: 0.017 (0.013)
Train: 297 [1050/1251 ( 84%)]  Loss:  2.838009 (2.7652)  Time: 1.094s,  935.68/s  (1.090s,  939.79/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [1100/1251 ( 88%)]  Loss:  2.858875 (2.7693)  Time: 1.168s,  876.45/s  (1.090s,  939.55/s)  LR: 1.024e-05  Data: 0.014 (0.013)
Train: 297 [1150/1251 ( 92%)]  Loss:  2.725713 (2.7675)  Time: 1.081s,  947.23/s  (1.090s,  939.70/s)  LR: 1.024e-05  Data: 0.012 (0.013)
Train: 297 [1200/1251 ( 96%)]  Loss:  3.027869 (2.7779)  Time: 1.076s,  951.99/s  (1.090s,  939.85/s)  LR: 1.024e-05  Data: 0.013 (0.013)
Train: 297 [1250/1251 (100%)]  Loss:  2.917247 (2.7833)  Time: 1.062s,  963.90/s  (1.089s,  939.94/s)  LR: 1.024e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.837 (5.837)  Loss:  0.4142 (0.4142)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.229 (0.446)  Loss:  0.5306 (0.8267)  Acc@1: 87.2642 (81.5900)  Acc@5: 97.8774 (95.6380)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-296.pth.tar', 81.61400020751954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-297.pth.tar', 81.59000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-295.pth.tar', 81.54400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-294.pth.tar', 81.53999994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-284.pth.tar', 81.52800002685547)

Train: 298 [   0/1251 (  0%)]  Loss:  3.101446 (3.1014)  Time: 1.085s,  943.52/s  (1.085s,  943.52/s)  LR: 1.011e-05  Data: 0.023 (0.023)
Train: 298 [  50/1251 (  4%)]  Loss:  2.668682 (2.8851)  Time: 1.081s,  947.13/s  (1.091s,  938.59/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 100/1251 (  8%)]  Loss:  2.668497 (2.8129)  Time: 1.093s,  936.84/s  (1.088s,  940.82/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 150/1251 ( 12%)]  Loss:  2.727904 (2.7916)  Time: 1.096s,  933.91/s  (1.089s,  940.64/s)  LR: 1.011e-05  Data: 0.013 (0.013)
Train: 298 [ 200/1251 ( 16%)]  Loss:  2.700044 (2.7733)  Time: 1.098s,  932.91/s  (1.087s,  941.70/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 250/1251 ( 20%)]  Loss:  2.914673 (2.7969)  Time: 1.097s,  933.61/s  (1.089s,  940.24/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 300/1251 ( 24%)]  Loss:  2.495247 (2.7538)  Time: 1.095s,  934.75/s  (1.090s,  939.83/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 350/1251 ( 28%)]  Loss:  2.805018 (2.7602)  Time: 1.077s,  950.68/s  (1.089s,  940.01/s)  LR: 1.011e-05  Data: 0.015 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 298 [ 400/1251 ( 32%)]  Loss:  3.006655 (2.7876)  Time: 1.104s,  927.61/s  (1.090s,  939.73/s)  LR: 1.011e-05  Data: 0.013 (0.013)
Train: 298 [ 450/1251 ( 36%)]  Loss:  2.722766 (2.7811)  Time: 1.072s,  954.99/s  (1.089s,  940.49/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 500/1251 ( 40%)]  Loss:  2.580064 (2.7628)  Time: 1.098s,  933.00/s  (1.088s,  940.76/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 550/1251 ( 44%)]  Loss:  2.799200 (2.7658)  Time: 1.094s,  935.80/s  (1.090s,  939.84/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 600/1251 ( 48%)]  Loss:  2.645994 (2.7566)  Time: 1.079s,  949.14/s  (1.090s,  939.60/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [ 650/1251 ( 52%)]  Loss:  3.118466 (2.7825)  Time: 1.122s,  912.32/s  (1.089s,  940.08/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 700/1251 ( 56%)]  Loss:  2.838756 (2.7862)  Time: 1.096s,  934.59/s  (1.090s,  939.71/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 750/1251 ( 60%)]  Loss:  2.863447 (2.7911)  Time: 1.173s,  873.34/s  (1.090s,  939.52/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 800/1251 ( 64%)]  Loss:  2.849366 (2.7945)  Time: 1.093s,  936.57/s  (1.089s,  939.98/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 850/1251 ( 68%)]  Loss:  2.888403 (2.7997)  Time: 1.075s,  952.49/s  (1.089s,  939.92/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 900/1251 ( 72%)]  Loss:  2.557946 (2.7870)  Time: 1.094s,  936.37/s  (1.089s,  940.16/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [ 950/1251 ( 76%)]  Loss:  2.744641 (2.7849)  Time: 1.079s,  949.33/s  (1.090s,  939.87/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [1000/1251 ( 80%)]  Loss:  2.947030 (2.7926)  Time: 1.094s,  935.72/s  (1.089s,  940.00/s)  LR: 1.011e-05  Data: 0.013 (0.013)
Train: 298 [1050/1251 ( 84%)]  Loss:  2.982637 (2.8012)  Time: 1.097s,  933.04/s  (1.089s,  940.08/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1100/1251 ( 88%)]  Loss:  2.777103 (2.8002)  Time: 1.094s,  936.22/s  (1.089s,  940.05/s)  LR: 1.011e-05  Data: 0.011 (0.013)
Train: 298 [1150/1251 ( 92%)]  Loss:  2.602386 (2.7919)  Time: 1.078s,  949.77/s  (1.089s,  940.29/s)  LR: 1.011e-05  Data: 0.012 (0.013)
Train: 298 [1200/1251 ( 96%)]  Loss:  2.804895 (2.7925)  Time: 1.082s,  946.45/s  (1.089s,  940.34/s)  LR: 1.011e-05  Data: 0.013 (0.013)
Train: 298 [1250/1251 (100%)]  Loss:  3.073855 (2.8033)  Time: 1.064s,  962.35/s  (1.089s,  940.16/s)  LR: 1.011e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.849 (5.849)  Loss:  0.4106 (0.4106)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.443)  Loss:  0.5342 (0.8279)  Acc@1: 87.0283 (81.6500)  Acc@5: 97.9953 (95.6540)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-298.pth.tar', 81.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-296.pth.tar', 81.61400020751954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-297.pth.tar', 81.59000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-295.pth.tar', 81.54400005126953)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-294.pth.tar', 81.53999994873047)

Train: 299 [   0/1251 (  0%)]  Loss:  2.845220 (2.8452)  Time: 1.085s,  943.66/s  (1.085s,  943.66/s)  LR: 1.003e-05  Data: 0.024 (0.024)
Train: 299 [  50/1251 (  4%)]  Loss:  2.865639 (2.8554)  Time: 1.097s,  933.61/s  (1.089s,  940.44/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 100/1251 (  8%)]  Loss:  3.026711 (2.9125)  Time: 1.076s,  951.65/s  (1.089s,  940.72/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 150/1251 ( 12%)]  Loss:  2.900106 (2.9094)  Time: 1.076s,  952.07/s  (1.088s,  941.08/s)  LR: 1.003e-05  Data: 0.013 (0.013)
Train: 299 [ 200/1251 ( 16%)]  Loss:  2.922335 (2.9120)  Time: 1.077s,  950.99/s  (1.090s,  939.70/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 250/1251 ( 20%)]  Loss:  2.834183 (2.8990)  Time: 1.094s,  935.82/s  (1.090s,  939.57/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 300/1251 ( 24%)]  Loss:  2.851558 (2.8923)  Time: 1.085s,  943.91/s  (1.091s,  938.50/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 350/1251 ( 28%)]  Loss:  2.864678 (2.8888)  Time: 1.096s,  934.58/s  (1.092s,  937.69/s)  LR: 1.003e-05  Data: 0.013 (0.013)
Train: 299 [ 400/1251 ( 32%)]  Loss:  2.817726 (2.8809)  Time: 1.079s,  949.38/s  (1.092s,  937.78/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 450/1251 ( 36%)]  Loss:  2.738940 (2.8667)  Time: 1.084s,  944.84/s  (1.092s,  937.78/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 500/1251 ( 40%)]  Loss:  2.993710 (2.8783)  Time: 1.077s,  950.78/s  (1.092s,  937.59/s)  LR: 1.003e-05  Data: 0.013 (0.013)
Train: 299 [ 550/1251 ( 44%)]  Loss:  2.942443 (2.8836)  Time: 1.081s,  947.24/s  (1.093s,  937.24/s)  LR: 1.003e-05  Data: 0.014 (0.013)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 299 [ 600/1251 ( 48%)]  Loss:  2.624909 (2.8637)  Time: 1.076s,  951.30/s  (1.092s,  938.15/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 650/1251 ( 52%)]  Loss:  2.896421 (2.8660)  Time: 1.075s,  952.18/s  (1.091s,  938.84/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 700/1251 ( 56%)]  Loss:  2.890396 (2.8677)  Time: 1.092s,  938.11/s  (1.091s,  939.01/s)  LR: 1.003e-05  Data: 0.014 (0.013)
Train: 299 [ 750/1251 ( 60%)]  Loss:  2.975042 (2.8744)  Time: 1.101s,  929.69/s  (1.090s,  939.25/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 800/1251 ( 64%)]  Loss:  2.669595 (2.8623)  Time: 1.078s,  950.34/s  (1.090s,  939.74/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [ 850/1251 ( 68%)]  Loss:  2.973662 (2.8685)  Time: 1.082s,  946.58/s  (1.089s,  939.99/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 900/1251 ( 72%)]  Loss:  2.534765 (2.8509)  Time: 1.083s,  945.35/s  (1.089s,  940.16/s)  LR: 1.003e-05  Data: 0.011 (0.013)
Train: 299 [ 950/1251 ( 76%)]  Loss:  2.800021 (2.8484)  Time: 1.096s,  934.24/s  (1.090s,  939.82/s)  LR: 1.003e-05  Data: 0.013 (0.013)
Train: 299 [1000/1251 ( 80%)]  Loss:  2.549957 (2.8342)  Time: 1.096s,  934.48/s  (1.090s,  939.41/s)  LR: 1.003e-05  Data: 0.012 (0.013)
Train: 299 [1050/1251 ( 84%)]  Loss:  2.981209 (2.8409)  Time: 1.075s,  952.94/s  (1.090s,  939.43/s)  LR: 1.003e-05  Data: 0.013 (0.013)
Train: 299 [1100/1251 ( 88%)]  Loss:  2.793062 (2.8388)  Time: 1.095s,  935.03/s  (1.090s,  939.66/s)  LR: 1.003e-05  Data: 0.013 (0.013)
Train: 299 [1150/1251 ( 92%)]  Loss:  3.043482 (2.8473)  Time: 1.078s,  949.52/s  (1.090s,  939.69/s)  LR: 1.003e-05  Data: 0.015 (0.013)
Train: 299 [1200/1251 ( 96%)]  Loss:  2.689759 (2.8410)  Time: 1.074s,  953.48/s  (1.090s,  939.87/s)  LR: 1.003e-05  Data: 0.010 (0.013)
Train: 299 [1250/1251 (100%)]  Loss:  2.545289 (2.8296)  Time: 1.079s,  948.84/s  (1.090s,  939.83/s)  LR: 1.003e-05  Data: 0.000 (0.013)
Test: [   0/48]  Time: 5.910 (5.910)  Loss:  0.4120 (0.4120)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.445)  Loss:  0.5281 (0.8253)  Acc@1: 87.5000 (81.6180)  Acc@5: 97.8774 (95.6560)
Current checkpoints:
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-298.pth.tar', 81.65000005371094)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-287.pth.tar', 81.644)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-291.pth.tar', 81.63400000244141)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-293.pth.tar', 81.62599994873047)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-299.pth.tar', 81.61799987060547)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-296.pth.tar', 81.61400020751954)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-289.pth.tar', 81.59600013183594)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-297.pth.tar', 81.59000002685546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-288.pth.tar', 81.546)
 ('./output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-295.pth.tar', 81.54400005126953)

Train: 300 [   0/1251 (  0%)]  Loss:  2.669170 (2.6692)  Time: 1.084s,  944.53/s  (1.084s,  944.53/s)  LR: 1.000e-05  Data: 0.023 (0.023)
Train: 300 [  50/1251 (  4%)]  Loss:  3.011433 (2.8403)  Time: 1.078s,  949.92/s  (1.085s,  943.42/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 300 [ 100/1251 (  8%)]  Loss:  2.698236 (2.7929)  Time: 1.096s,  934.60/s  (1.083s,  945.96/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 150/1251 ( 12%)]  Loss:  2.915819 (2.8237)  Time: 1.101s,  930.44/s  (1.085s,  943.65/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 200/1251 ( 16%)]  Loss:  3.129563 (2.8848)  Time: 1.078s,  949.95/s  (1.085s,  943.42/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 250/1251 ( 20%)]  Loss:  3.053784 (2.9130)  Time: 1.173s,  873.22/s  (1.087s,  942.16/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 300 [ 300/1251 ( 24%)]  Loss:  2.997222 (2.9250)  Time: 1.081s,  947.10/s  (1.088s,  941.53/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 300 [ 350/1251 ( 28%)]  Loss:  2.871122 (2.9183)  Time: 1.076s,  951.85/s  (1.088s,  941.51/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 400/1251 ( 32%)]  Loss:  2.736958 (2.8981)  Time: 1.095s,  935.32/s  (1.088s,  940.82/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 450/1251 ( 36%)]  Loss:  2.992927 (2.9076)  Time: 1.093s,  936.81/s  (1.088s,  940.91/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 300 [ 500/1251 ( 40%)]  Loss:  2.697386 (2.8885)  Time: 1.096s,  934.02/s  (1.088s,  940.78/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 550/1251 ( 44%)]  Loss:  2.821744 (2.8829)  Time: 1.080s,  947.81/s  (1.089s,  940.14/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 300 [ 600/1251 ( 48%)]  Loss:  2.791141 (2.8759)  Time: 1.080s,  948.18/s  (1.089s,  940.20/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 650/1251 ( 52%)]  Loss:  2.590768 (2.8555)  Time: 1.094s,  935.63/s  (1.090s,  939.73/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 300 [ 700/1251 ( 56%)]  Loss:  3.165360 (2.8762)  Time: 1.094s,  936.11/s  (1.090s,  939.55/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 750/1251 ( 60%)]  Loss:  2.862812 (2.8753)  Time: 1.093s,  936.90/s  (1.090s,  939.61/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 800/1251 ( 64%)]  Loss:  2.527341 (2.8549)  Time: 1.181s,  867.38/s  (1.090s,  939.49/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 300 [ 850/1251 ( 68%)]  Loss:  2.775656 (2.8505)  Time: 1.173s,  872.70/s  (1.090s,  939.30/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 300 [ 900/1251 ( 72%)]  Loss:  2.742610 (2.8448)  Time: 1.077s,  950.61/s  (1.090s,  939.16/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [ 950/1251 ( 76%)]  Loss:  2.677129 (2.8364)  Time: 1.077s,  951.02/s  (1.090s,  939.17/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [1000/1251 ( 80%)]  Loss:  2.881447 (2.8386)  Time: 1.095s,  935.54/s  (1.090s,  939.38/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [1050/1251 ( 84%)]  Loss:  2.860475 (2.8396)  Time: 1.080s,  948.48/s  (1.090s,  939.24/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 300 [1100/1251 ( 88%)]  Loss:  2.627636 (2.8303)  Time: 1.093s,  936.76/s  (1.090s,  939.06/s)  LR: 1.000e-05  Data: 0.012 (0.013)
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Added key: store_based_barrier_key:1 to store for rank: 7
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Model swin_tiny_patch4_window7_224 created, param count:27110704
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Using NVIDIA APEX AMP. Training in mixed precision.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Restoring AMP loss scaler state from checkpoint...
Loaded checkpoint 'output/train/20220309-092942-swin_tiny_patch4_window7_224-224/checkpoint-299.pth.tar' (epoch 299)
Using NVIDIA APEX DistributedDataParallel.
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Scheduled epochs: 310
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:283: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/kkahatapitiy/anaconda3/envs/kkenv3/lib/python3.7/site-packages/torchvision/transforms/functional.py:401: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Train: 300 [   0/1251 (  0%)]  Loss:  2.854385 (2.8544)  Time: 15.828s,   64.70/s  (15.828s,   64.70/s)  LR: 1.000e-05  Data: 4.224 (4.224)
Train: 300 [  50/1251 (  4%)]  Loss:  2.971383 (2.9129)  Time: 1.095s,  935.06/s  (1.390s,  736.59/s)  LR: 1.000e-05  Data: 0.015 (0.096)
Train: 300 [ 100/1251 (  8%)]  Loss:  2.978977 (2.9349)  Time: 1.096s,  934.58/s  (1.244s,  823.19/s)  LR: 1.000e-05  Data: 0.015 (0.055)
Train: 300 [ 150/1251 ( 12%)]  Loss:  3.024761 (2.9574)  Time: 1.101s,  929.65/s  (1.194s,  857.56/s)  LR: 1.000e-05  Data: 0.011 (0.041)
Train: 300 [ 200/1251 ( 16%)]  Loss:  2.718626 (2.9096)  Time: 1.124s,  910.76/s  (1.171s,  874.62/s)  LR: 1.000e-05  Data: 0.018 (0.034)
Train: 300 [ 250/1251 ( 20%)]  Loss:  2.849693 (2.8996)  Time: 1.090s,  939.59/s  (1.155s,  886.20/s)  LR: 1.000e-05  Data: 0.012 (0.030)
Train: 300 [ 300/1251 ( 24%)]  Loss:  2.707003 (2.8721)  Time: 1.108s,  924.21/s  (1.146s,  893.69/s)  LR: 1.000e-05  Data: 0.011 (0.027)
Train: 300 [ 350/1251 ( 28%)]  Loss:  2.722275 (2.8534)  Time: 1.094s,  935.73/s  (1.139s,  898.95/s)  LR: 1.000e-05  Data: 0.011 (0.025)
Train: 300 [ 400/1251 ( 32%)]  Loss:  3.049669 (2.8752)  Time: 1.090s,  939.74/s  (1.134s,  903.29/s)  LR: 1.000e-05  Data: 0.012 (0.023)
Train: 300 [ 450/1251 ( 36%)]  Loss:  2.987648 (2.8864)  Time: 1.096s,  934.42/s  (1.129s,  906.91/s)  LR: 1.000e-05  Data: 0.011 (0.022)
Train: 300 [ 500/1251 ( 40%)]  Loss:  2.852318 (2.8833)  Time: 1.093s,  937.11/s  (1.125s,  909.84/s)  LR: 1.000e-05  Data: 0.015 (0.021)
Train: 300 [ 550/1251 ( 44%)]  Loss:  2.746817 (2.8720)  Time: 1.114s,  919.61/s  (1.124s,  911.26/s)  LR: 1.000e-05  Data: 0.011 (0.020)
Train: 300 [ 600/1251 ( 48%)]  Loss:  2.830297 (2.8688)  Time: 1.090s,  939.61/s  (1.121s,  913.18/s)  LR: 1.000e-05  Data: 0.012 (0.020)
Train: 300 [ 650/1251 ( 52%)]  Loss:  2.858900 (2.8681)  Time: 1.092s,  937.66/s  (1.119s,  914.88/s)  LR: 1.000e-05  Data: 0.011 (0.019)
Train: 300 [ 700/1251 ( 56%)]  Loss:  2.921122 (2.8716)  Time: 1.120s,  914.30/s  (1.118s,  916.28/s)  LR: 1.000e-05  Data: 0.012 (0.019)
Train: 300 [ 750/1251 ( 60%)]  Loss:  2.661556 (2.8585)  Time: 1.092s,  937.57/s  (1.116s,  917.52/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 300 [ 800/1251 ( 64%)]  Loss:  2.834965 (2.8571)  Time: 1.091s,  938.60/s  (1.115s,  918.72/s)  LR: 1.000e-05  Data: 0.012 (0.018)
Train: 300 [ 850/1251 ( 68%)]  Loss:  2.892658 (2.8591)  Time: 1.099s,  932.08/s  (1.113s,  919.89/s)  LR: 1.000e-05  Data: 0.011 (0.018)
Train: 300 [ 900/1251 ( 72%)]  Loss:  2.548195 (2.8427)  Time: 1.092s,  937.41/s  (1.112s,  920.71/s)  LR: 1.000e-05  Data: 0.012 (0.017)
Train: 300 [ 950/1251 ( 76%)]  Loss:  2.962793 (2.8487)  Time: 1.089s,  940.72/s  (1.111s,  921.37/s)  LR: 1.000e-05  Data: 0.012 (0.017)
Train: 300 [1000/1251 ( 80%)]  Loss:  2.906073 (2.8514)  Time: 1.094s,  935.73/s  (1.111s,  921.81/s)  LR: 1.000e-05  Data: 0.012 (0.017)
Train: 300 [1050/1251 ( 84%)]  Loss:  3.063490 (2.8611)  Time: 1.094s,  936.04/s  (1.110s,  922.47/s)  LR: 1.000e-05  Data: 0.012 (0.017)
Train: 300 [1100/1251 ( 88%)]  Loss:  2.454885 (2.8434)  Time: 1.096s,  934.33/s  (1.109s,  922.99/s)  LR: 1.000e-05  Data: 0.012 (0.016)
Train: 300 [1150/1251 ( 92%)]  Loss:  2.937883 (2.8473)  Time: 1.118s,  916.18/s  (1.109s,  923.44/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 300 [1200/1251 ( 96%)]  Loss:  2.487315 (2.8329)  Time: 1.104s,  927.44/s  (1.108s,  923.97/s)  LR: 1.000e-05  Data: 0.011 (0.016)
Train: 300 [1250/1251 (100%)]  Loss:  2.673787 (2.8268)  Time: 1.079s,  948.79/s  (1.108s,  924.36/s)  LR: 1.000e-05  Data: 0.000 (0.016)
Test: [   0/48]  Time: 8.356 (8.356)  Loss:  0.4134 (0.4134)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 2.530 (0.546)  Loss:  0.5216 (0.8279)  Acc@1: 87.9717 (81.6100)  Acc@5: 97.9953 (95.6800)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)

Train: 301 [   0/1251 (  0%)]  Loss:  2.847402 (2.8474)  Time: 1.097s,  933.32/s  (1.097s,  933.32/s)  LR: 1.000e-05  Data: 0.029 (0.029)
Train: 301 [  50/1251 (  4%)]  Loss:  3.094467 (2.9709)  Time: 1.089s,  940.53/s  (1.097s,  933.40/s)  LR: 1.000e-05  Data: 0.013 (0.013)
Train: 301 [ 100/1251 (  8%)]  Loss:  2.963513 (2.9685)  Time: 1.089s,  940.64/s  (1.098s,  933.01/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 301 [ 150/1251 ( 12%)]  Loss:  2.782856 (2.9221)  Time: 1.106s,  925.96/s  (1.097s,  933.20/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 301 [ 200/1251 ( 16%)]  Loss:  2.599116 (2.8575)  Time: 1.109s,  923.10/s  (1.100s,  930.67/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 301 [ 250/1251 ( 20%)]  Loss:  2.725722 (2.8355)  Time: 1.090s,  939.68/s  (1.100s,  930.99/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 300/1251 ( 24%)]  Loss:  3.039283 (2.8646)  Time: 1.087s,  942.23/s  (1.098s,  932.31/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 301 [ 350/1251 ( 28%)]  Loss:  2.966983 (2.8774)  Time: 1.088s,  941.09/s  (1.098s,  932.68/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 400/1251 ( 32%)]  Loss:  2.827741 (2.8719)  Time: 1.090s,  939.16/s  (1.097s,  933.32/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 450/1251 ( 36%)]  Loss:  2.596703 (2.8444)  Time: 1.079s,  948.86/s  (1.097s,  933.33/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 301 [ 500/1251 ( 40%)]  Loss:  2.666865 (2.8282)  Time: 1.175s,  871.27/s  (1.097s,  933.48/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 550/1251 ( 44%)]  Loss:  2.713385 (2.8187)  Time: 1.106s,  925.48/s  (1.097s,  933.32/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 301 [ 600/1251 ( 48%)]  Loss:  2.890789 (2.8242)  Time: 1.085s,  943.53/s  (1.098s,  932.81/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 650/1251 ( 52%)]  Loss:  2.792673 (2.8220)  Time: 1.086s,  942.79/s  (1.097s,  933.20/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 301 [ 700/1251 ( 56%)]  Loss:  2.779856 (2.8192)  Time: 1.086s,  942.53/s  (1.097s,  933.55/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 750/1251 ( 60%)]  Loss:  2.681653 (2.8106)  Time: 1.116s,  917.75/s  (1.097s,  933.66/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 301 [ 800/1251 ( 64%)]  Loss:  2.620975 (2.7994)  Time: 1.094s,  935.60/s  (1.097s,  933.51/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 301 [ 850/1251 ( 68%)]  Loss:  2.776103 (2.7981)  Time: 1.088s,  941.34/s  (1.097s,  933.72/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [ 900/1251 ( 72%)]  Loss:  2.762407 (2.7962)  Time: 1.086s,  943.14/s  (1.096s,  933.93/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 301 [ 950/1251 ( 76%)]  Loss:  2.846972 (2.7988)  Time: 1.108s,  923.91/s  (1.097s,  933.81/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [1000/1251 ( 80%)]  Loss:  2.865114 (2.8019)  Time: 1.086s,  942.65/s  (1.097s,  933.55/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [1050/1251 ( 84%)]  Loss:  2.775566 (2.8007)  Time: 1.091s,  938.41/s  (1.097s,  933.66/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [1100/1251 ( 88%)]  Loss:  2.863702 (2.8035)  Time: 1.087s,  942.21/s  (1.096s,  933.94/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [1150/1251 ( 92%)]  Loss:  2.748992 (2.8012)  Time: 1.087s,  942.22/s  (1.096s,  933.92/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 301 [1200/1251 ( 96%)]  Loss:  2.760277 (2.7996)  Time: 1.088s,  941.00/s  (1.096s,  934.05/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 301 [1250/1251 (100%)]  Loss:  2.993551 (2.8070)  Time: 1.072s,  955.04/s  (1.096s,  934.04/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.704 (5.704)  Loss:  0.4140 (0.4140)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.230 (0.451)  Loss:  0.5225 (0.8266)  Acc@1: 87.1462 (81.5980)  Acc@5: 97.8774 (95.6440)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 302 [   0/1251 (  0%)]  Loss:  2.914726 (2.9147)  Time: 1.092s,  937.81/s  (1.092s,  937.81/s)  LR: 1.000e-05  Data: 0.029 (0.029)
Train: 302 [  50/1251 (  4%)]  Loss:  2.955604 (2.9352)  Time: 1.095s,  935.14/s  (1.090s,  939.17/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 302 [ 100/1251 (  8%)]  Loss:  2.725979 (2.8654)  Time: 1.087s,  941.63/s  (1.094s,  935.68/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 302 [ 150/1251 ( 12%)]  Loss:  2.937301 (2.8834)  Time: 1.090s,  939.45/s  (1.095s,  935.34/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 302 [ 200/1251 ( 16%)]  Loss:  2.840328 (2.8748)  Time: 1.095s,  935.38/s  (1.095s,  935.23/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 250/1251 ( 20%)]  Loss:  3.008277 (2.8970)  Time: 1.093s,  936.69/s  (1.095s,  935.21/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 302 [ 300/1251 ( 24%)]  Loss:  2.806736 (2.8841)  Time: 1.085s,  944.05/s  (1.095s,  934.90/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 350/1251 ( 28%)]  Loss:  2.948917 (2.8922)  Time: 1.095s,  935.03/s  (1.095s,  934.77/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 302 [ 400/1251 ( 32%)]  Loss:  2.777505 (2.8795)  Time: 1.093s,  937.15/s  (1.095s,  935.14/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 450/1251 ( 36%)]  Loss:  2.836941 (2.8752)  Time: 1.085s,  943.89/s  (1.095s,  934.96/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 500/1251 ( 40%)]  Loss:  2.649173 (2.8547)  Time: 1.087s,  942.40/s  (1.095s,  934.93/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 550/1251 ( 44%)]  Loss:  2.936702 (2.8615)  Time: 1.096s,  934.18/s  (1.096s,  934.53/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 600/1251 ( 48%)]  Loss:  2.953582 (2.8686)  Time: 1.087s,  941.95/s  (1.096s,  934.51/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 650/1251 ( 52%)]  Loss:  3.031163 (2.8802)  Time: 1.075s,  952.27/s  (1.096s,  934.58/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 700/1251 ( 56%)]  Loss:  2.766416 (2.8726)  Time: 1.107s,  924.81/s  (1.096s,  934.69/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 750/1251 ( 60%)]  Loss:  2.668772 (2.8599)  Time: 1.086s,  942.52/s  (1.096s,  934.40/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 800/1251 ( 64%)]  Loss:  2.838322 (2.8586)  Time: 1.090s,  939.33/s  (1.096s,  934.32/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 302 [ 850/1251 ( 68%)]  Loss:  2.948860 (2.8636)  Time: 1.091s,  938.47/s  (1.096s,  934.31/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 302 [ 900/1251 ( 72%)]  Loss:  2.987965 (2.8702)  Time: 1.085s,  944.03/s  (1.096s,  934.68/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [ 950/1251 ( 76%)]  Loss:  3.090775 (2.8812)  Time: 1.098s,  932.53/s  (1.096s,  934.36/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [1000/1251 ( 80%)]  Loss:  3.069325 (2.8902)  Time: 1.118s,  916.28/s  (1.096s,  934.43/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [1050/1251 ( 84%)]  Loss:  2.831553 (2.8875)  Time: 1.094s,  936.43/s  (1.096s,  934.34/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [1100/1251 ( 88%)]  Loss:  3.007505 (2.8927)  Time: 1.094s,  936.34/s  (1.096s,  934.30/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 302 [1150/1251 ( 92%)]  Loss:  2.757247 (2.8871)  Time: 1.099s,  931.54/s  (1.096s,  934.17/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [1200/1251 ( 96%)]  Loss:  2.627882 (2.8767)  Time: 1.086s,  942.59/s  (1.096s,  934.29/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 302 [1250/1251 (100%)]  Loss:  2.938591 (2.8791)  Time: 1.073s,  954.17/s  (1.096s,  934.34/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.784 (5.784)  Loss:  0.4115 (0.4115)  Acc@1: 93.4570 (93.4570)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.450)  Loss:  0.5329 (0.8286)  Acc@1: 86.9104 (81.6160)  Acc@5: 97.9953 (95.6900)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 303 [   0/1251 (  0%)]  Loss:  2.864972 (2.8650)  Time: 1.093s,  936.47/s  (1.093s,  936.47/s)  LR: 1.000e-05  Data: 0.031 (0.031)
Train: 303 [  50/1251 (  4%)]  Loss:  2.849964 (2.8575)  Time: 1.092s,  937.36/s  (1.095s,  935.47/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 303 [ 100/1251 (  8%)]  Loss:  2.743072 (2.8193)  Time: 1.095s,  934.98/s  (1.096s,  934.56/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 303 [ 150/1251 ( 12%)]  Loss:  3.075002 (2.8833)  Time: 1.074s,  953.04/s  (1.098s,  933.01/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [ 200/1251 ( 16%)]  Loss:  2.802762 (2.8672)  Time: 1.087s,  942.35/s  (1.097s,  933.51/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [ 250/1251 ( 20%)]  Loss:  2.948882 (2.8808)  Time: 1.095s,  934.97/s  (1.097s,  933.78/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [ 300/1251 ( 24%)]  Loss:  2.871310 (2.8794)  Time: 1.087s,  942.45/s  (1.096s,  934.51/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 303 [ 350/1251 ( 28%)]  Loss:  2.802483 (2.8698)  Time: 1.109s,  923.18/s  (1.096s,  933.93/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [ 400/1251 ( 32%)]  Loss:  3.181459 (2.9044)  Time: 1.098s,  932.50/s  (1.096s,  933.95/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [ 450/1251 ( 36%)]  Loss:  2.932686 (2.9073)  Time: 1.108s,  924.54/s  (1.096s,  934.21/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [ 500/1251 ( 40%)]  Loss:  2.415298 (2.8625)  Time: 1.093s,  936.76/s  (1.096s,  934.01/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [ 550/1251 ( 44%)]  Loss:  2.744202 (2.8527)  Time: 1.092s,  938.09/s  (1.097s,  933.68/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [ 600/1251 ( 48%)]  Loss:  2.919350 (2.8578)  Time: 1.087s,  941.65/s  (1.096s,  934.13/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [ 650/1251 ( 52%)]  Loss:  2.982020 (2.8667)  Time: 1.091s,  938.98/s  (1.096s,  934.43/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 303 [ 700/1251 ( 56%)]  Loss:  2.633037 (2.8511)  Time: 1.088s,  940.83/s  (1.096s,  934.37/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 303 [ 750/1251 ( 60%)]  Loss:  2.744378 (2.8444)  Time: 1.093s,  936.98/s  (1.096s,  934.36/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [ 800/1251 ( 64%)]  Loss:  2.809934 (2.8424)  Time: 1.106s,  926.02/s  (1.096s,  934.44/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 303 [ 850/1251 ( 68%)]  Loss:  2.894772 (2.8453)  Time: 1.113s,  919.75/s  (1.096s,  934.32/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [ 900/1251 ( 72%)]  Loss:  2.696213 (2.8375)  Time: 1.091s,  938.86/s  (1.096s,  934.10/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [ 950/1251 ( 76%)]  Loss:  2.976295 (2.8444)  Time: 1.087s,  941.92/s  (1.096s,  934.24/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [1000/1251 ( 80%)]  Loss:  2.997622 (2.8517)  Time: 1.110s,  922.26/s  (1.097s,  933.86/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [1050/1251 ( 84%)]  Loss:  2.866476 (2.8524)  Time: 1.109s,  923.48/s  (1.097s,  933.36/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [1100/1251 ( 88%)]  Loss:  2.744243 (2.8477)  Time: 1.106s,  926.25/s  (1.098s,  932.92/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [1150/1251 ( 92%)]  Loss:  2.607346 (2.8377)  Time: 1.117s,  916.87/s  (1.098s,  932.50/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 303 [1200/1251 ( 96%)]  Loss:  2.553877 (2.8263)  Time: 1.106s,  926.03/s  (1.099s,  932.00/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 303 [1250/1251 (100%)]  Loss:  2.773125 (2.8243)  Time: 1.074s,  953.58/s  (1.099s,  931.61/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.700 (5.700)  Loss:  0.4168 (0.4168)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.450)  Loss:  0.5277 (0.8303)  Acc@1: 87.7358 (81.6580)  Acc@5: 98.1132 (95.7200)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 304 [   0/1251 (  0%)]  Loss:  2.576667 (2.5767)  Time: 1.099s,  932.01/s  (1.099s,  932.01/s)  LR: 1.000e-05  Data: 0.030 (0.030)
Train: 304 [  50/1251 (  4%)]  Loss:  2.781192 (2.6789)  Time: 1.106s,  926.11/s  (1.092s,  937.63/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 304 [ 100/1251 (  8%)]  Loss:  2.631087 (2.6630)  Time: 1.110s,  922.88/s  (1.097s,  933.21/s)  LR: 1.000e-05  Data: 0.016 (0.012)
Train: 304 [ 150/1251 ( 12%)]  Loss:  2.807622 (2.6991)  Time: 1.109s,  923.14/s  (1.099s,  931.62/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 304 [ 200/1251 ( 16%)]  Loss:  2.756196 (2.7106)  Time: 1.096s,  934.12/s  (1.097s,  933.45/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [ 250/1251 ( 20%)]  Loss:  2.930829 (2.7473)  Time: 1.095s,  934.76/s  (1.097s,  933.76/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 304 [ 300/1251 ( 24%)]  Loss:  2.912463 (2.7709)  Time: 1.095s,  935.03/s  (1.096s,  934.38/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 304 [ 350/1251 ( 28%)]  Loss:  2.973722 (2.7962)  Time: 1.085s,  943.98/s  (1.096s,  934.43/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [ 400/1251 ( 32%)]  Loss:  2.956082 (2.8140)  Time: 1.085s,  944.15/s  (1.096s,  934.69/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [ 450/1251 ( 36%)]  Loss:  2.831234 (2.8157)  Time: 1.098s,  932.35/s  (1.096s,  934.70/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 304 [ 500/1251 ( 40%)]  Loss:  2.682940 (2.8036)  Time: 1.081s,  947.07/s  (1.096s,  934.71/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 304 [ 550/1251 ( 44%)]  Loss:  2.876099 (2.8097)  Time: 1.087s,  941.98/s  (1.095s,  935.11/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [ 600/1251 ( 48%)]  Loss:  3.074715 (2.8301)  Time: 1.095s,  934.89/s  (1.095s,  934.98/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [ 650/1251 ( 52%)]  Loss:  2.717049 (2.8220)  Time: 1.086s,  942.70/s  (1.095s,  934.77/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [ 700/1251 ( 56%)]  Loss:  2.726629 (2.8156)  Time: 1.097s,  933.09/s  (1.096s,  934.55/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 304 [ 750/1251 ( 60%)]  Loss:  2.882411 (2.8198)  Time: 1.086s,  943.28/s  (1.096s,  934.67/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 304 [ 800/1251 ( 64%)]  Loss:  2.849066 (2.8215)  Time: 1.092s,  938.04/s  (1.096s,  934.70/s)  LR: 1.000e-05  Data: 0.019 (0.012)
Train: 304 [ 850/1251 ( 68%)]  Loss:  2.836529 (2.8224)  Time: 1.085s,  943.68/s  (1.096s,  934.63/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 304 [ 900/1251 ( 72%)]  Loss:  2.843532 (2.8235)  Time: 1.087s,  941.65/s  (1.095s,  934.76/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 304 [ 950/1251 ( 76%)]  Loss:  2.607511 (2.8127)  Time: 1.088s,  940.88/s  (1.096s,  934.59/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 304 [1000/1251 ( 80%)]  Loss:  2.816792 (2.8129)  Time: 1.191s,  860.12/s  (1.096s,  934.61/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [1050/1251 ( 84%)]  Loss:  2.867831 (2.8154)  Time: 1.094s,  936.40/s  (1.096s,  934.63/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [1100/1251 ( 88%)]  Loss:  3.148774 (2.8299)  Time: 1.093s,  936.85/s  (1.096s,  934.57/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [1150/1251 ( 92%)]  Loss:  2.930590 (2.8341)  Time: 1.095s,  935.34/s  (1.096s,  934.47/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [1200/1251 ( 96%)]  Loss:  2.886971 (2.8362)  Time: 1.094s,  935.88/s  (1.096s,  934.30/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 304 [1250/1251 (100%)]  Loss:  2.786016 (2.8343)  Time: 1.080s,  948.10/s  (1.096s,  934.18/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.677 (5.677)  Loss:  0.4095 (0.4095)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.229 (0.450)  Loss:  0.5279 (0.8262)  Acc@1: 87.8538 (81.7120)  Acc@5: 98.1132 (95.6800)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-304.pth.tar', 81.71200002441407)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 305 [   0/1251 (  0%)]  Loss:  2.718210 (2.7182)  Time: 1.101s,  930.34/s  (1.101s,  930.34/s)  LR: 1.000e-05  Data: 0.035 (0.035)
Train: 305 [  50/1251 (  4%)]  Loss:  2.787913 (2.7531)  Time: 1.085s,  943.87/s  (1.094s,  935.63/s)  LR: 1.000e-05  Data: 0.018 (0.013)
Train: 305 [ 100/1251 (  8%)]  Loss:  2.789870 (2.7653)  Time: 1.093s,  936.51/s  (1.096s,  934.53/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 305 [ 150/1251 ( 12%)]  Loss:  2.963837 (2.8150)  Time: 1.107s,  925.31/s  (1.095s,  934.85/s)  LR: 1.000e-05  Data: 0.018 (0.013)
Train: 305 [ 200/1251 ( 16%)]  Loss:  2.764309 (2.8048)  Time: 1.087s,  942.30/s  (1.095s,  935.43/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 305 [ 250/1251 ( 20%)]  Loss:  2.898666 (2.8205)  Time: 1.086s,  943.34/s  (1.094s,  935.89/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 305 [ 300/1251 ( 24%)]  Loss:  2.773361 (2.8137)  Time: 1.091s,  938.17/s  (1.095s,  935.30/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 305 [ 350/1251 ( 28%)]  Loss:  3.077160 (2.8467)  Time: 1.101s,  929.92/s  (1.095s,  935.14/s)  LR: 1.000e-05  Data: 0.019 (0.012)
Train: 305 [ 400/1251 ( 32%)]  Loss:  3.126911 (2.8778)  Time: 1.088s,  941.14/s  (1.096s,  934.52/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 450/1251 ( 36%)]  Loss:  3.047798 (2.8948)  Time: 1.087s,  941.93/s  (1.096s,  934.53/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 500/1251 ( 40%)]  Loss:  2.665374 (2.8739)  Time: 1.101s,  930.27/s  (1.096s,  934.25/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 305 [ 550/1251 ( 44%)]  Loss:  2.908919 (2.8769)  Time: 1.087s,  941.62/s  (1.096s,  934.36/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 305 [ 600/1251 ( 48%)]  Loss:  2.689906 (2.8625)  Time: 1.096s,  934.07/s  (1.096s,  934.30/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 305 [ 650/1251 ( 52%)]  Loss:  3.014627 (2.8733)  Time: 1.105s,  926.57/s  (1.096s,  934.70/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 700/1251 ( 56%)]  Loss:  2.827971 (2.8703)  Time: 1.112s,  921.22/s  (1.096s,  934.61/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 750/1251 ( 60%)]  Loss:  2.803675 (2.8662)  Time: 1.089s,  940.36/s  (1.095s,  935.02/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 800/1251 ( 64%)]  Loss:  2.978086 (2.8727)  Time: 1.088s,  941.04/s  (1.095s,  935.16/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 850/1251 ( 68%)]  Loss:  2.866395 (2.8724)  Time: 1.085s,  943.49/s  (1.095s,  935.22/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 900/1251 ( 72%)]  Loss:  2.880566 (2.8728)  Time: 1.094s,  935.78/s  (1.095s,  935.19/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [ 950/1251 ( 76%)]  Loss:  2.794011 (2.8689)  Time: 1.085s,  943.52/s  (1.095s,  935.27/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [1000/1251 ( 80%)]  Loss:  2.832988 (2.8672)  Time: 1.115s,  918.03/s  (1.095s,  935.11/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [1050/1251 ( 84%)]  Loss:  2.863264 (2.8670)  Time: 1.164s,  879.71/s  (1.095s,  935.02/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 305 [1100/1251 ( 88%)]  Loss:  2.651816 (2.8576)  Time: 1.088s,  941.40/s  (1.095s,  935.15/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [1150/1251 ( 92%)]  Loss:  3.005773 (2.8638)  Time: 1.073s,  954.07/s  (1.095s,  935.08/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [1200/1251 ( 96%)]  Loss:  2.777656 (2.8604)  Time: 1.096s,  934.45/s  (1.095s,  935.11/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 305 [1250/1251 (100%)]  Loss:  2.927836 (2.8630)  Time: 1.077s,  950.86/s  (1.095s,  934.98/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.783 (5.783)  Loss:  0.4084 (0.4084)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.229 (0.450)  Loss:  0.5334 (0.8304)  Acc@1: 87.3821 (81.6760)  Acc@5: 97.9953 (95.6780)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-304.pth.tar', 81.71200002441407)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-305.pth.tar', 81.676000078125)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 306 [   0/1251 (  0%)]  Loss:  2.637251 (2.6373)  Time: 1.103s,  928.79/s  (1.103s,  928.79/s)  LR: 1.000e-05  Data: 0.030 (0.030)
Train: 306 [  50/1251 (  4%)]  Loss:  2.785250 (2.7113)  Time: 1.097s,  933.45/s  (1.098s,  932.19/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [ 100/1251 (  8%)]  Loss:  2.774791 (2.7324)  Time: 1.097s,  933.64/s  (1.099s,  931.48/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [ 150/1251 ( 12%)]  Loss:  2.723668 (2.7302)  Time: 1.095s,  935.37/s  (1.099s,  932.01/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 306 [ 200/1251 ( 16%)]  Loss:  3.146057 (2.8134)  Time: 1.096s,  933.92/s  (1.099s,  931.83/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 250/1251 ( 20%)]  Loss:  2.790964 (2.8097)  Time: 1.095s,  935.29/s  (1.099s,  931.54/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 300/1251 ( 24%)]  Loss:  3.046699 (2.8435)  Time: 1.096s,  933.99/s  (1.099s,  931.53/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 350/1251 ( 28%)]  Loss:  2.672303 (2.8221)  Time: 1.088s,  941.54/s  (1.098s,  932.38/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 400/1251 ( 32%)]  Loss:  2.605834 (2.7981)  Time: 1.095s,  935.25/s  (1.099s,  932.14/s)  LR: 1.000e-05  Data: 0.017 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 306 [ 450/1251 ( 36%)]  Loss:  2.963042 (2.8146)  Time: 1.087s,  941.89/s  (1.098s,  932.53/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 500/1251 ( 40%)]  Loss:  3.080436 (2.8388)  Time: 1.105s,  926.41/s  (1.097s,  933.14/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 306 [ 550/1251 ( 44%)]  Loss:  2.844211 (2.8392)  Time: 1.096s,  933.96/s  (1.097s,  933.75/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [ 600/1251 ( 48%)]  Loss:  2.907143 (2.8444)  Time: 1.090s,  939.54/s  (1.096s,  934.25/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 650/1251 ( 52%)]  Loss:  2.855322 (2.8452)  Time: 1.096s,  934.71/s  (1.096s,  933.91/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [ 700/1251 ( 56%)]  Loss:  2.766103 (2.8399)  Time: 1.114s,  918.94/s  (1.096s,  934.21/s)  LR: 1.000e-05  Data: 0.017 (0.012)
Train: 306 [ 750/1251 ( 60%)]  Loss:  3.029338 (2.8518)  Time: 1.089s,  940.04/s  (1.096s,  934.20/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [ 800/1251 ( 64%)]  Loss:  2.795466 (2.8485)  Time: 1.088s,  940.86/s  (1.096s,  934.38/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [ 850/1251 ( 68%)]  Loss:  2.787705 (2.8451)  Time: 1.087s,  942.31/s  (1.096s,  934.31/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 306 [ 900/1251 ( 72%)]  Loss:  2.899466 (2.8479)  Time: 1.116s,  917.23/s  (1.096s,  934.46/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [ 950/1251 ( 76%)]  Loss:  3.279962 (2.8696)  Time: 1.088s,  940.76/s  (1.096s,  934.41/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [1000/1251 ( 80%)]  Loss:  2.915288 (2.8717)  Time: 1.107s,  924.93/s  (1.096s,  934.56/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [1050/1251 ( 84%)]  Loss:  2.888957 (2.8725)  Time: 1.078s,  949.95/s  (1.096s,  934.58/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 306 [1100/1251 ( 88%)]  Loss:  2.809063 (2.8698)  Time: 1.120s,  914.18/s  (1.096s,  934.51/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 306 [1150/1251 ( 92%)]  Loss:  2.770093 (2.8656)  Time: 1.087s,  942.10/s  (1.096s,  934.45/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [1200/1251 ( 96%)]  Loss:  2.978190 (2.8701)  Time: 1.096s,  934.49/s  (1.096s,  934.41/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 306 [1250/1251 (100%)]  Loss:  2.835584 (2.8688)  Time: 1.081s,  947.26/s  (1.096s,  934.36/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.773 (5.773)  Loss:  0.4050 (0.4050)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.230 (0.450)  Loss:  0.5261 (0.8305)  Acc@1: 87.6179 (81.6160)  Acc@5: 97.9953 (95.6400)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-304.pth.tar', 81.71200002441407)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-305.pth.tar', 81.676000078125)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-306.pth.tar', 81.61600005126954)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 307 [   0/1251 (  0%)]  Loss:  2.953637 (2.9536)  Time: 1.112s,  920.92/s  (1.112s,  920.92/s)  LR: 1.000e-05  Data: 0.038 (0.038)
Train: 307 [  50/1251 (  4%)]  Loss:  2.634250 (2.7939)  Time: 1.088s,  941.11/s  (1.096s,  934.34/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 307 [ 100/1251 (  8%)]  Loss:  2.710136 (2.7660)  Time: 1.089s,  940.67/s  (1.100s,  931.16/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 150/1251 ( 12%)]  Loss:  2.692645 (2.7477)  Time: 1.096s,  934.21/s  (1.098s,  932.82/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 200/1251 ( 16%)]  Loss:  3.087601 (2.8157)  Time: 1.095s,  934.89/s  (1.096s,  934.28/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 250/1251 ( 20%)]  Loss:  2.806964 (2.8142)  Time: 1.077s,  951.10/s  (1.097s,  933.30/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 307 [ 300/1251 ( 24%)]  Loss:  2.752552 (2.8054)  Time: 1.128s,  907.75/s  (1.097s,  933.27/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 307 [ 350/1251 ( 28%)]  Loss:  2.794576 (2.8040)  Time: 1.089s,  939.96/s  (1.097s,  933.57/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 400/1251 ( 32%)]  Loss:  2.794963 (2.8030)  Time: 1.088s,  941.27/s  (1.097s,  933.80/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 450/1251 ( 36%)]  Loss:  2.470433 (2.7698)  Time: 1.087s,  942.12/s  (1.096s,  934.09/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 307 [ 500/1251 ( 40%)]  Loss:  3.107388 (2.8005)  Time: 1.099s,  931.35/s  (1.096s,  934.07/s)  LR: 1.000e-05  Data: 0.016 (0.012)
Train: 307 [ 550/1251 ( 44%)]  Loss:  2.769246 (2.7979)  Time: 1.095s,  934.77/s  (1.096s,  934.04/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 600/1251 ( 48%)]  Loss:  2.929990 (2.8080)  Time: 1.094s,  936.14/s  (1.097s,  933.85/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 650/1251 ( 52%)]  Loss:  2.925855 (2.8164)  Time: 1.093s,  936.54/s  (1.097s,  933.57/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 700/1251 ( 56%)]  Loss:  2.698770 (2.8086)  Time: 1.182s,  866.34/s  (1.097s,  933.73/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 307 [ 750/1251 ( 60%)]  Loss:  2.974185 (2.8189)  Time: 1.108s,  923.89/s  (1.097s,  933.86/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 307 [ 800/1251 ( 64%)]  Loss:  3.094161 (2.8351)  Time: 1.090s,  939.08/s  (1.097s,  933.66/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 307 [ 850/1251 ( 68%)]  Loss:  2.926259 (2.8402)  Time: 1.088s,  941.16/s  (1.097s,  933.58/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 900/1251 ( 72%)]  Loss:  2.888096 (2.8427)  Time: 1.086s,  942.77/s  (1.097s,  933.70/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [ 950/1251 ( 76%)]  Loss:  2.899188 (2.8455)  Time: 1.093s,  937.07/s  (1.097s,  933.84/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [1000/1251 ( 80%)]  Loss:  3.099511 (2.8576)  Time: 1.087s,  942.02/s  (1.096s,  933.99/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [1050/1251 ( 84%)]  Loss:  2.637676 (2.8476)  Time: 1.087s,  942.18/s  (1.096s,  934.15/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [1100/1251 ( 88%)]  Loss:  2.826161 (2.8467)  Time: 1.092s,  937.62/s  (1.096s,  934.27/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [1150/1251 ( 92%)]  Loss:  3.125383 (2.8583)  Time: 1.081s,  947.17/s  (1.096s,  934.28/s)  LR: 1.000e-05  Data: 0.017 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 307 [1200/1251 ( 96%)]  Loss:  3.023196 (2.8649)  Time: 1.097s,  933.62/s  (1.096s,  934.42/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 307 [1250/1251 (100%)]  Loss:  2.618288 (2.8554)  Time: 1.072s,  955.27/s  (1.096s,  934.53/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.727 (5.727)  Loss:  0.4166 (0.4166)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.450)  Loss:  0.5291 (0.8328)  Acc@1: 87.7358 (81.6920)  Acc@5: 97.9953 (95.6460)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-304.pth.tar', 81.71200002441407)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-307.pth.tar', 81.69199997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-305.pth.tar', 81.676000078125)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-306.pth.tar', 81.61600005126954)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 308 [   0/1251 (  0%)]  Loss:  2.855323 (2.8553)  Time: 1.094s,  935.69/s  (1.094s,  935.69/s)  LR: 1.000e-05  Data: 0.029 (0.029)
Train: 308 [  50/1251 (  4%)]  Loss:  2.639393 (2.7474)  Time: 1.093s,  936.80/s  (1.096s,  934.13/s)  LR: 1.000e-05  Data: 0.011 (0.013)
Train: 308 [ 100/1251 (  8%)]  Loss:  2.853824 (2.7828)  Time: 1.095s,  934.79/s  (1.097s,  933.75/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [ 150/1251 ( 12%)]  Loss:  2.892241 (2.8102)  Time: 1.214s,  843.53/s  (1.097s,  933.51/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [ 200/1251 ( 16%)]  Loss:  2.583715 (2.7649)  Time: 1.087s,  942.25/s  (1.097s,  933.58/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 250/1251 ( 20%)]  Loss:  2.801797 (2.7710)  Time: 1.111s,  921.96/s  (1.096s,  934.49/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [ 300/1251 ( 24%)]  Loss:  2.749501 (2.7680)  Time: 1.088s,  940.91/s  (1.096s,  934.47/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [ 350/1251 ( 28%)]  Loss:  3.140277 (2.8145)  Time: 1.107s,  925.19/s  (1.098s,  932.39/s)  LR: 1.000e-05  Data: 0.020 (0.012)
Train: 308 [ 400/1251 ( 32%)]  Loss:  2.598124 (2.7905)  Time: 1.107s,  924.86/s  (1.099s,  931.44/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 450/1251 ( 36%)]  Loss:  2.559089 (2.7673)  Time: 1.108s,  923.95/s  (1.100s,  930.56/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 500/1251 ( 40%)]  Loss:  2.574299 (2.7498)  Time: 1.085s,  943.78/s  (1.101s,  929.77/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 308 [ 550/1251 ( 44%)]  Loss:  3.003084 (2.7709)  Time: 1.092s,  938.08/s  (1.101s,  930.04/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 600/1251 ( 48%)]  Loss:  2.841010 (2.7763)  Time: 1.171s,  874.65/s  (1.101s,  930.44/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 650/1251 ( 52%)]  Loss:  2.852119 (2.7817)  Time: 1.096s,  934.21/s  (1.100s,  930.60/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 308 [ 700/1251 ( 56%)]  Loss:  2.696799 (2.7760)  Time: 1.097s,  933.73/s  (1.100s,  930.56/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 750/1251 ( 60%)]  Loss:  2.582413 (2.7639)  Time: 1.096s,  934.17/s  (1.100s,  930.81/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [ 800/1251 ( 64%)]  Loss:  2.889423 (2.7713)  Time: 1.097s,  933.03/s  (1.100s,  930.79/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 850/1251 ( 68%)]  Loss:  2.674211 (2.7659)  Time: 1.169s,  875.93/s  (1.100s,  930.86/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [ 900/1251 ( 72%)]  Loss:  2.554139 (2.7548)  Time: 1.110s,  922.30/s  (1.100s,  931.05/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [ 950/1251 ( 76%)]  Loss:  2.791354 (2.7566)  Time: 1.087s,  941.95/s  (1.100s,  931.26/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [1000/1251 ( 80%)]  Loss:  2.945999 (2.7656)  Time: 1.088s,  940.79/s  (1.099s,  931.74/s)  LR: 1.000e-05  Data: 0.013 (0.012)
Train: 308 [1050/1251 ( 84%)]  Loss:  2.888055 (2.7712)  Time: 1.107s,  925.44/s  (1.099s,  931.96/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 308 [1100/1251 ( 88%)]  Loss:  2.805045 (2.7727)  Time: 1.076s,  952.07/s  (1.099s,  932.03/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [1150/1251 ( 92%)]  Loss:  3.040391 (2.7838)  Time: 1.084s,  944.26/s  (1.099s,  932.17/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [1200/1251 ( 96%)]  Loss:  2.888134 (2.7880)  Time: 1.094s,  936.24/s  (1.098s,  932.34/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 308 [1250/1251 (100%)]  Loss:  2.942348 (2.7939)  Time: 1.092s,  938.05/s  (1.099s,  932.07/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.665 (5.665)  Loss:  0.4089 (0.4089)  Acc@1: 93.4570 (93.4570)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.449)  Loss:  0.5265 (0.8264)  Acc@1: 87.6179 (81.6460)  Acc@5: 98.2311 (95.6880)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-304.pth.tar', 81.71200002441407)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-307.pth.tar', 81.69199997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-305.pth.tar', 81.676000078125)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-308.pth.tar', 81.645999921875)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-306.pth.tar', 81.61600005126954)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

Train: 309 [   0/1251 (  0%)]  Loss:  2.959402 (2.9594)  Time: 1.106s,  926.01/s  (1.106s,  926.01/s)  LR: 1.000e-05  Data: 0.041 (0.041)
Train: 309 [  50/1251 (  4%)]  Loss:  2.764968 (2.8622)  Time: 1.090s,  939.42/s  (1.093s,  936.67/s)  LR: 1.000e-05  Data: 0.012 (0.013)
Train: 309 [ 100/1251 (  8%)]  Loss:  2.876716 (2.8670)  Time: 1.089s,  940.47/s  (1.093s,  937.10/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 150/1251 ( 12%)]  Loss:  2.684741 (2.8215)  Time: 1.096s,  934.73/s  (1.093s,  936.82/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 309 [ 200/1251 ( 16%)]  Loss:  2.793833 (2.8159)  Time: 1.088s,  941.61/s  (1.093s,  936.67/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 309 [ 250/1251 ( 20%)]  Loss:  2.653422 (2.7888)  Time: 1.085s,  943.64/s  (1.093s,  937.10/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 309 [ 300/1251 ( 24%)]  Loss:  3.043399 (2.8252)  Time: 1.089s,  940.16/s  (1.093s,  937.14/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 350/1251 ( 28%)]  Loss:  3.084753 (2.8577)  Time: 1.087s,  942.20/s  (1.093s,  936.67/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 400/1251 ( 32%)]  Loss:  2.797333 (2.8510)  Time: 1.085s,  944.20/s  (1.094s,  936.14/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 309 [ 450/1251 ( 36%)]  Loss:  2.935775 (2.8594)  Time: 1.086s,  942.60/s  (1.093s,  936.47/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 500/1251 ( 40%)]  Loss:  3.128789 (2.8839)  Time: 1.086s,  943.18/s  (1.093s,  936.75/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 550/1251 ( 44%)]  Loss:  3.002980 (2.8938)  Time: 1.087s,  942.46/s  (1.094s,  936.43/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 600/1251 ( 48%)]  Loss:  2.943306 (2.8976)  Time: 1.094s,  935.88/s  (1.094s,  936.40/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 309 [ 650/1251 ( 52%)]  Loss:  2.900261 (2.8978)  Time: 1.096s,  934.09/s  (1.094s,  936.10/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 309 [ 700/1251 ( 56%)]  Loss:  2.403974 (2.8649)  Time: 1.097s,  933.11/s  (1.094s,  935.87/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 309 [ 750/1251 ( 60%)]  Loss:  3.008584 (2.8739)  Time: 1.087s,  942.05/s  (1.094s,  936.11/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 800/1251 ( 64%)]  Loss:  2.863463 (2.8733)  Time: 1.085s,  943.74/s  (1.094s,  936.21/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [ 850/1251 ( 68%)]  Loss:  2.754552 (2.8667)  Time: 1.088s,  941.16/s  (1.094s,  936.35/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 309 [ 900/1251 ( 72%)]  Loss:  3.095420 (2.8787)  Time: 1.082s,  946.45/s  (1.094s,  936.38/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 309 [ 950/1251 ( 76%)]  Loss:  2.825603 (2.8761)  Time: 1.088s,  940.81/s  (1.094s,  936.38/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [1000/1251 ( 80%)]  Loss:  2.702346 (2.8678)  Time: 1.086s,  942.72/s  (1.093s,  936.47/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [1050/1251 ( 84%)]  Loss:  2.769908 (2.8633)  Time: 1.098s,  932.47/s  (1.094s,  936.19/s)  LR: 1.000e-05  Data: 0.011 (0.012)
Train: 309 [1100/1251 ( 88%)]  Loss:  3.005371 (2.8695)  Time: 1.086s,  943.34/s  (1.094s,  936.38/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 309 [1150/1251 ( 92%)]  Loss:  2.487773 (2.8536)  Time: 1.087s,  941.80/s  (1.094s,  936.31/s)  LR: 1.000e-05  Data: 0.014 (0.012)
Train: 309 [1200/1251 ( 96%)]  Loss:  2.856373 (2.8537)  Time: 1.090s,  939.59/s  (1.094s,  936.19/s)  LR: 1.000e-05  Data: 0.012 (0.012)
Train: 309 [1250/1251 (100%)]  Loss:  2.758173 (2.8500)  Time: 1.079s,  949.44/s  (1.094s,  936.01/s)  LR: 1.000e-05  Data: 0.000 (0.012)
Test: [   0/48]  Time: 5.751 (5.751)  Loss:  0.4151 (0.4151)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.229 (0.449)  Loss:  0.5297 (0.8299)  Acc@1: 87.6179 (81.6620)  Acc@5: 98.1132 (95.6960)
Current checkpoints:
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-304.pth.tar', 81.71200002441407)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-307.pth.tar', 81.69199997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-305.pth.tar', 81.676000078125)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-309.pth.tar', 81.661999921875)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-303.pth.tar', 81.65799997314453)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-308.pth.tar', 81.645999921875)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-302.pth.tar', 81.61600013183593)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-306.pth.tar', 81.61600005126954)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-300.pth.tar', 81.61000007568359)
 ('./output/train/20220312-194754-swin_tiny_patch4_window7_224-224/checkpoint-301.pth.tar', 81.59800010498047)

*** Best metric: 81.71200002441407 (epoch 304)
